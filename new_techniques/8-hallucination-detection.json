{
  "name": "Hallucination Detection",
  "description": "Hallucination detection identifies when generative models produce factually incorrect, fabricated, or ungrounded outputs. This technique combines automated consistency checking, self-consistency methods, uncertainty quantification, and human evaluation. It produces detection scores distinguishing intrinsic hallucinations (contradicting sources) from extrinsic hallucinations (unverifiable claims), enabling filtering or user warnings.",
  "assurance_goals": [
    "Reliability",
    "Transparency",
    "Safety"
  ],
  "example_use_cases": [
    {
      "description": "Monitoring a medical information system to detect when it generates unsupported clinical claims or fabricates research citations, preventing patients from receiving incorrect health advice.",
      "goal": "Safety"
    },
    {
      "description": "Evaluating an AI journalism assistant to ensure generated article drafts don't fabricate quotes, misattribute sources, or create false claims that could damage credibility and public trust.",
      "goal": "Reliability"
    },
    {
      "description": "Implementing confidence scoring and uncertainty indicators in AI assistants that transparently signal when responses may contain hallucinated information versus verified facts.",
      "goal": "Transparency"
    }
  ],
  "limitations": [
    {
      "description": "Automated detection methods may miss subtle hallucinations or flag correct but unusual information as potentially fabricated."
    },
    {
      "description": "Requires access to reliable ground truth knowledge sources for fact-checking, which may not exist for many domains or recent events."
    },
    {
      "description": "Self-consistency methods assume inconsistency indicates hallucination, but models can consistently hallucinate the same false information."
    },
    {
      "description": "Human evaluation is expensive and subjective, with annotators potentially disagreeing about what constitutes hallucination versus interpretation."
    }
  ]
}
