{
  "tag": {
    "name": "data-requirements/calibration-set",
    "slug": "data-requirements-calibration-set",
    "count": 4,
    "category": "data-requirements"
  },
  "techniques": [
    {
      "slug": "conformal-prediction",
      "name": "Conformal Prediction",
      "description": "Conformal prediction provides mathematically guaranteed uncertainty quantification by creating prediction sets that contain the true outcome with a specified probability (e.g., exactly 95% coverage). The technique works by measuring how 'strange' or 'nonconforming' new predictions are compared to calibration data - if a prediction seems unusual, it gets wider intervals. For example, in medical diagnosis, instead of saying 'likely cancer', it might say 'possible diagnoses: {cancer, benign tumour} with 95% confidence'. This distribution-free method works with any underlying model (neural networks, random forests, etc.) and requires no assumptions about data distribution, making it a robust framework for reliable uncertainty estimates in high-stakes applications.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/reliability/uncertainty-quantification",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/calibration-set",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/prediction-interval",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Creating prediction sets for drug discovery that guarantee 95% coverage, such as 'this compound will likely have activity against {target A, target B, target C}', ensuring reliable decision-making in costly experimental validation.",
          "goal": "Reliability"
        },
        {
          "description": "Providing transparent multi-class predictions in judicial risk assessment by showing all plausible risk categories with guaranteed coverage, enabling judges to see the full range of possibilities rather than just a single point estimate.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring fair uncertainty quantification across demographic groups in college admissions by verifying that prediction set sizes (number of possible outcomes) are consistent across protected groups, preventing discriminatory overconfidence for certain populations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Prediction sets can be unnecessarily wide when nonconformity scores vary greatly across the feature space, leading to conservative intervals that reduce practical utility."
        },
        {
          "description": "Requires a held-out calibration set separate from training data, reducing the amount of data available for model training, which can impact performance on small datasets."
        },
        {
          "description": "Guarantees only hold under the exchangeability assumption - if test data distribution differs significantly from calibration data, coverage guarantees may be violated."
        },
        {
          "description": "For multi-class problems, prediction sets may include many classes when the model is uncertain, making decisions difficult when sets contain opposing outcomes."
        },
        {
          "description": "Computational cost increases with the number of calibration samples, and efficient implementation requires careful design for large-scale or real-time applications."
        }
      ],
      "resources": [
        {
          "title": "A tutorial on conformal prediction",
          "url": "http://arxiv.org/pdf/0706.3188v1",
          "source_type": "documentation",
          "authors": [
            "Glenn Shafer",
            "Vladimir Vovk"
          ],
          "publication_date": "2007-06-21",
          "description": "Foundational tutorial introducing conformal prediction theory and applications by the method's creators"
        },
        {
          "title": "valeman/awesome-conformal-prediction",
          "url": "https://github.com/valeman/awesome-conformal-prediction",
          "source_type": "software_package",
          "description": "Curated collection of conformal prediction resources including videos, tutorials, books, papers, and open-source libraries"
        },
        {
          "title": "scikit-learn-contrib/MAPIE",
          "url": "https://github.com/scikit-learn-contrib/MAPIE",
          "source_type": "software_package",
          "description": "Python library for uncertainty quantification using conformal prediction across regression, classification, and time series tasks"
        },
        {
          "title": "Tutorial for classification â€” MAPIE 0.8.6 documentation",
          "url": "https://mapie.readthedocs.io/en/v0.8.6/examples_classification/4-tutorials/plot_main-tutorial-classification.html",
          "source_type": "tutorial",
          "description": "Practical tutorial demonstrating conformal prediction for classification tasks with guaranteed coverage"
        },
        {
          "title": "Conformal Prediction: a Unified Review of Theory and New Challenges",
          "url": "http://arxiv.org/pdf/2005.07972v2",
          "source_type": "documentation",
          "authors": [
            "Matteo Fontana",
            "Gianluca Zeni",
            "Simone Vantini"
          ],
          "publication_date": "2020-05-16",
          "description": "Comprehensive review of conformal prediction theory, recent advances, and emerging challenges in the field"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 2,
      "related_techniques": [
        "monte-carlo-dropout",
        "prediction-intervals",
        "quantile-regression",
        "deep-ensembles",
        "bootstrapping",
        "jackknife-resampling"
      ]
    },
    {
      "slug": "empirical-calibration",
      "name": "Empirical Calibration",
      "description": "Empirical calibration adjusts a model's predicted probabilities to match observed frequencies. For example, if events predicted with 80% confidence only occur 60% of the time, calibration would correct this overconfidence. Common techniques include Platt scaling and isotonic regression, which learn transformations that map the model's raw scores to well-calibrated probabilities, improving the reliability of confidence measures for downstream decisions.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/calibration-set",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting a credit default prediction model's probabilities to ensure that loan applicants with a predicted 30% default risk actually default 30% of the time, improving decision-making.",
          "goal": "Reliability"
        },
        {
          "description": "Calibrating a medical diagnosis model's confidence scores so that stakeholders can meaningfully interpret probability outputs, enabling doctors to make informed decisions about treatment urgency based on reliable confidence estimates.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring that a hiring algorithm's confidence scores are equally well-calibrated across different demographic groups, preventing systematically overconfident predictions for certain populations that could lead to biased decision-making.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires a separate held-out calibration dataset, which reduces the amount of data available for model training."
        },
        {
          "description": "Calibration performance can degrade over time if the underlying data distribution shifts, requiring periodic recalibration."
        },
        {
          "description": "May sacrifice some discriminative power in favour of calibration, potentially reducing the model's ability to distinguish between classes."
        },
        {
          "description": "Calibration methods assume that the calibration set is representative of future data, which may not hold in dynamic environments."
        }
      ],
      "resources": [
        {
          "title": "google/empirical_calibration",
          "url": "https://github.com/google/empirical_calibration",
          "source_type": "software_package"
        },
        {
          "title": "A Python Library For Empirical Calibration",
          "url": "http://arxiv.org/pdf/1906.11920v2",
          "source_type": "technical_paper",
          "authors": [
            "Xiaojing Wang",
            "Jingang Miao",
            "Yunting Sun"
          ],
          "publication_date": "2019-07-25"
        },
        {
          "title": "Assessing the effectiveness of empirical calibration under different bias scenarios",
          "url": "http://arxiv.org/pdf/2111.04233v2",
          "source_type": "technical_paper",
          "authors": [
            "Hon Hwang",
            "Juan C Quiroz",
            "Blanca Gallego"
          ],
          "publication_date": "2021-11-08"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "related_techniques": [
        "temperature-scaling"
      ]
    },
    {
      "slug": "temperature-scaling",
      "name": "Temperature Scaling",
      "description": "Temperature scaling adjusts a model's confidence by applying a single parameter (temperature) to its predictions. When a model is too confident in its wrong answers, temperature scaling can fix this by making the predictions more realistic. It works by dividing the model's outputs by the temperature value before converting them to probabilities. Higher temperatures make the model less confident, whilst lower temperatures increase confidence. The technique maintains the model's accuracy whilst ensuring that when it says it's 90% confident, it's actually right about 90% of the time.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-requirements/calibration-set",
        "data-requirements/validation-set",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "explanatory-scope/global",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting a deep learning image classifier's confidence scores to be realistic, ensuring that when it's 90% confident, it's right 90% of the time.",
          "goal": "Reliability"
        },
        {
          "description": "Making medical diagnosis model predictions more trustworthy by providing realistic confidence scores that doctors can interpret and use to make informed decisions about patient care.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring fair treatment across patient demographics by calibrating confidence scores equally across different groups, preventing systematic over-confidence in predictions for certain populations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Only addresses calibration at the overall dataset level, not subgroup-specific miscalibration issues."
        },
        {
          "description": "Does not improve the rank ordering or accuracy of predictions, only adjusts confidence levels."
        },
        {
          "description": "Assumes that calibration errors are consistent across different types of inputs and feature values."
        },
        {
          "description": "Requires a separate validation set for temperature parameter optimisation, which may not be available in small datasets."
        }
      ],
      "resources": [
        {
          "title": "gpleiss/temperature_scaling",
          "url": "https://github.com/gpleiss/temperature_scaling",
          "source_type": "software_package"
        },
        {
          "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
          "url": "http://arxiv.org/pdf/2502.20604v1",
          "source_type": "technical_paper",
          "authors": [
            "Hao Xuan",
            "Bokai Yang",
            "Xingyu Li"
          ],
          "publication_date": "2025-02-28"
        },
        {
          "title": "Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration",
          "url": "http://arxiv.org/pdf/2209.11604v2",
          "source_type": "technical_paper",
          "authors": [
            "Yung-Chen Tang",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
          ],
          "publication_date": "2024-07-24"
        },
        {
          "title": "On Calibration of Modern Neural Networks | arXiv",
          "url": "https://arxiv.org/abs/1706.04599",
          "source_type": "technical_paper",
          "authors": [
            "Chuan Guo",
            "Geoff Pleiss",
            "Yu Sun",
            "Kilian Q. Weinberger"
          ],
          "publication_date": "2017-06-14"
        },
        {
          "title": "On the Limitations of Temperature Scaling for Distributions with Overlaps",
          "url": "http://arxiv.org/pdf/2306.00740v3",
          "source_type": "technical_paper",
          "authors": [
            "Muthu Chidambaram",
            "Rong Ge"
          ],
          "publication_date": "2023-06-01"
        }
      ],
      "complexity_rating": 1,
      "computational_cost_rating": 1,
      "related_techniques": [
        "empirical-calibration"
      ]
    },
    {
      "slug": "calibration-with-equality-of-opportunity",
      "name": "Calibration with Equality of Opportunity",
      "description": "A post-processing fairness technique that adjusts model predictions to achieve equal true positive rates across protected groups whilst maintaining calibration within each group. The method addresses fairness by ensuring that qualified individuals from different demographic groups have equal chances of receiving positive predictions, whilst preserving the meaning of probability scores within each group. This technique attempts to balance the competing objectives of group fairness and accurate probability estimation.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/calibration-set",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/testing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting a loan approval model to ensure that qualified applicants from different ethnic backgrounds have equal approval rates, whilst maintaining that a 70% predicted repayment probability means the same thing for each ethnic group in practice.",
          "goal": "Fairness"
        },
        {
          "description": "Post-processing a university admissions algorithm to equalise acceptance rates for qualified students across gender groups, whilst ensuring the predicted success scores remain well-calibrated within each gender to support transparent decision-making.",
          "goal": "Transparency"
        },
        {
          "description": "Calibrating a medical diagnosis model to maintain equal detection rates for a disease across different age groups whilst preserving the reliability of risk scores, ensuring that a 30% risk prediction accurately reflects actual disease occurrence within each age group.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Fundamental mathematical incompatibility exists between perfect calibration and exact equality of opportunity, except in highly constrained special cases."
        },
        {
          "description": "May reduce overall model accuracy or calibration when forcing equal true positive rates across groups with genuinely different base rates."
        },
        {
          "description": "Requires access to sensitive attributes during post-processing, which may not be available or legally permissible in all contexts."
        },
        {
          "description": "The technique only addresses one aspect of fairness (true positive rates) and may allow disparities in false positive rates between groups."
        },
        {
          "description": "Post-processing approaches cannot address biases inherent in the training data or model architecture, only adjust final predictions."
        }
      ],
      "resources": [
        {
          "title": "On Fairness and Calibration",
          "url": "https://arxiv.org/abs/1709.02012",
          "source_type": "technical_paper",
          "description": "Foundational paper demonstrating the mathematical tension between calibration and equalised odds fairness constraints.",
          "authors": [
            "Geoff Pleiss",
            "Manish Raghavan",
            "Felix Wu",
            "Jon Kleinberg",
            "Kilian Q. Weinberger"
          ],
          "publication_date": "2017-09-06"
        },
        {
          "title": "equalized_odds_and_calibration",
          "url": "https://github.com/gpleiss/equalized_odds_and_calibration",
          "source_type": "software_package",
          "description": "Python implementation of post-processing methods for achieving calibration with equality of opportunity constraints."
        },
        {
          "title": "Equality of Opportunity in Supervised Learning",
          "url": "https://arxiv.org/abs/1610.02413",
          "source_type": "technical_paper",
          "description": "Original paper introducing the equality of opportunity fairness criterion and post-processing algorithms.",
          "authors": [
            "Moritz Hardt",
            "Eric Price",
            "Nathan Srebro"
          ],
          "publication_date": "2016-10-07"
        },
        {
          "title": "Fairlearn: Postprocessing Methods",
          "url": "https://fairlearn.org/v0.10/user_guide/mitigation/postprocessing.html",
          "source_type": "documentation",
          "description": "Documentation for implementing threshold optimisation and calibration methods to achieve fairness constraints."
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 2,
      "related_techniques": [
        "equalised-odds-post-processing",
        "threshold-optimiser",
        "reject-option-classification"
      ]
    }
  ],
  "count": 4
}