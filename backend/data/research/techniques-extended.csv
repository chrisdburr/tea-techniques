id,name,description,model_dependency,assurance_goals,categories,subcategories,attributes,example_use_cases,limitations,resources,complexity_rating,computational_cost_rating
1,SHAP (SHapley Additive exPlanations),"SHAP explains the impact of each feature on a model's prediction by calculating Shapley values from cooperative game theory. It fairly distributes the model’s prediction among the features to show how much each one contributed.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Feature Analysis"}],[{"category": "Feature Analysis", "subcategory": "Importance and Attribution"}],[{"type": "Scope", "value": "Global"}, {"type": "Scope", "value": "Local"}],[{"description": "Explaining why a complex model (like a neural network) made a specific prediction by showing how each feature influenced that prediction.", "goal": "Explainability"}, {"description": "Identifying which factors contributed most to a loan approval decision in a financial model using SHAP values.", "goal": "Explainability"}],[{"description": "Computationally intensive for large models or datasets (calculating many Shapley values is slow)."}, {"description": "Assumes feature independence, so results can be less reliable when features are highly correlated."}],[{"type": "Paper", "title": "A Unified Approach to Interpreting Model Predictions (NIPS 2017)", "url": "https://doi.org/10.5555/3295222.3295230"}],3,3
2,Permutation Importance,"Permutation importance measures a feature's importance by shuffling its values and observing the impact on model performance. If the model's accuracy drops significantly when a feature is permuted, that feature is considered important.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Feature Analysis"}],[{"category": "Feature Analysis", "subcategory": "Importance and Attribution"}],[{"type": "Scope", "value": "Global"}],[{"description": "Assessing which features are most important in a trained Random Forest by measuring performance drop when each feature's values are shuffled.", "goal": "Explainability"}, {"description": "Checking that a model is not relying on an irrelevant field by confirming that shuffling that field doesn't change the model's accuracy.", "goal": "Explainability"}],[{"description": "Can be misleading when features are correlated (shuffling one feature might also disturb another feature's effect)."}, {"description": "Requires multiple model evaluations (for each feature shuffled), which can be time-consuming for large models or datasets."}],[{"type": "Documentation", "title": "Permutation Feature Importance - Scikit-learn Guide", "url": "https://scikit-learn.org/stable/modules/permutation_importance.html"}],2,2
3,Mean Decrease Impurity (MDI),"Mean Decrease Impurity evaluates feature importance in tree-based models by how much each feature split reduces the impurity (mix of classes) across all trees. Features that consistently lead to purer splits (lower impurity) are scored as more important.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Feature Analysis"}],[{"category": "Feature Analysis", "subcategory": "Importance and Attribution"}],[{"type": "Scope", "value": "Global"}],[{"description": "Identifying key predictors in a Random Forest classifier by examining which features yield the largest impurity reductions.", "goal": "Explainability"}, {"description": "Choosing a subset of features for a simpler model based on their high importance scores from a Random Forest's MDI analysis.", "goal": "Explainability"}],[{"description": "Biased towards features with many possible splits or categories (can overestimate their importance)."}, {"description": "Only applicable to tree-based models; cannot be used for models like SVMs or neural networks."}],[{"type": "Documentation", "title": "Feature Importance in Random Forest (scikit-learn)", "url": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"}],2,1
4,Gini Importance,"Gini importance sums the total reduction in Gini impurity that each feature provides across all splits in a decision tree or forest. It indicates which features most contribute to making the decision nodes purer (less mixed) throughout the model.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Feature Analysis"}],[{"category": "Feature Analysis", "subcategory": "Importance and Attribution"}],[{"type": "Scope", "value": "Global"}],[{"description": "Exploring which features a Random Forest model relied on most by looking at their Gini importance scores.", "goal": "Explainability"}, {"description": "Ranking variables in a decision tree to decide which ones are worth further analysis or data collection.", "goal": "Explainability"}],[{"description": "Can inflate importance for features with many unique values or splits (favoring high-cardinality features)."}, {"description": "Specific to decision tree-based models and not directly comparable across different model types or datasets."}],[{"type": "Documentation", "title": "Feature Importance in Random Forest (scikit-learn)", "url": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"}],2,1
5,Coefficient Magnitudes (in Linear Models),"Examines the absolute values of coefficients in linear models to judge feature influence. Features with larger absolute coefficients have a stronger impact on the prediction, and the sign of a coefficient shows if it pushes the outcome up or down.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Feature Analysis"}],[{"category": "Feature Analysis", "subcategory": "Importance and Attribution"}],[{"type": "Scope", "value": "Global"}],[{"description": "Determining which factors most influence house prices by looking at the largest coefficients in a linear regression model.", "goal": "Explainability"}, {"description": "Analyzing a logistic regression for customer churn to see which variables (with large positive or negative coefficients) strongly affect churn probability.", "goal": "Explainability"}],[{"description": "Only meaningful for linear relationships; if the model or data relationships are non-linear, coefficients can be misleading."}, {"description": "Sensitive to feature scaling and correlation (a large coefficient might just mean the feature was on a small scale or that features are correlated)."}],[{"type": "Tutorial", "title": "Interpreting Regression Coefficients", "url": "https://statisticsbyjim.com/regression/interpret-coefficients-linear-regression/"}],1,1
6,Integrated Gradients,"Integrated Gradients attributes each feature's contribution by accumulating the gradients of the model's output with respect to that feature as the input moves from a baseline value to its actual value. This results in a score for each feature that reflects how much it influenced the prediction compared to a reference baseline.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Feature Attribution"}],[{"category": "Feature Attribution", "subcategory": "Gradient-based"}],[{"type": "Scope", "value": "Local"}],[{"description": "Explaining why an image was classified as a 'cat' by using a baseline (such as a blank image) and computing how each pixel contributes to the 'cat' prediction as the image transitions from the baseline to the actual image.", "goal": "Explainability"}, {"description": "Interpreting a text classifier by identifying which words in a sentence contribute most to a positive sentiment score using Integrated Gradients (with an empty or neutral text as baseline).", "goal": "Explainability"}],[{"description": "Requires a suitable baseline input; the choice of baseline can significantly affect the explanation."}, {"description": "Needs a differentiable model and multiple gradient computations, so it can be computationally intensive for large models or datasets."}],[],3,3
7,DeepLIFT,"DeepLIFT assigns credit (or blame) to each input feature by comparing a neuron's activation to a reference (baseline) and tracking the difference backward through the network. It attributes the change in the output to changes in each input feature relative to what the output would be at the reference input.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Feature Attribution"}],[{"category": "Feature Attribution", "subcategory": "Gradient-based"}],[{"type": "Scope", "value": "Local"}],[{"description": "Explaining a neural network's prediction for an image by using a baseline (like a black image) and determining how each pixel contributed to the difference between the network's output for the actual image and the baseline.", "goal": "Explainability"}, {"description": "Understanding which words in a sentence led a model to classify the sentiment as positive by comparing the model’s response to that sentence versus a neutral baseline sentence.", "goal": "Explainability"}],[{"description": "The explanation is sensitive to the choice of reference baseline; a poorly chosen reference can yield misleading attributions."}, {"description": "May require custom handling for certain activation types or network structures, which can complicate its use on some architectures."}],[],3,3
8,Layer-wise Relevance Propagation (LRP),"LRP explains a model's prediction by propagating the result backwards through the network and assigning a relevance score to each input feature. Starting from the predicted output, it distributes the 'relevance' of that output to neurons in each preceding layer, all the way back to the input features, indicating how each feature contributed.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Feature Attribution"}],[{"category": "Feature Attribution", "subcategory": "Backpropagation-based"}],[{"type": "Scope", "value": "Local"}],[{"description": "Highlighting which pixels in an image contributed most to a convolutional neural network's classification of that image by using LRP to trace the classification score back to the input pixels.", "goal": "Explainability"}, {"description": "Analyzing a deep credit scoring model by propagating a high credit score backward to see how much each input factor (income, debt, etc.) was responsible for that score.", "goal": "Explainability"}],[{"description": "Requires appropriate propagation rules for each layer type; using simplified or incorrect rules can produce flawed explanations."}, {"description": "Primarily suited for certain network architectures (like feedforward neural networks); it may not easily extend to every model architecture without adjustments."}],[],3,3
9,Variable Importance in Random Forests (MDA, MDG),"Random Forests provide feature importance measures in two ways: Mean Decrease Accuracy (MDA) and Mean Decrease Gini (MDG). MDA evaluates importance by seeing how much the model's accuracy drops when a feature is permuted or removed, while MDG sums how much splitting on a feature reduces the Gini impurity across all trees. Both metrics highlight which features the Random Forest model relies on most for its predictions.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Feature Analysis"}],[{"category": "Feature Analysis", "subcategory": "Importance and Attribution"}],[{"type": "Scope", "value": "Global"}],[{"description": "Assessing which features a Random Forest uses most by observing the drop in performance when each feature is shuffled (MDA) and by the total Gini impurity reduction each feature provides (MDG).", "goal": "Explainability"}, {"description": "Using Random Forest feature importance metrics to guide feature selection for a simpler model, focusing on those with high MDA/MDG scores.", "goal": "Explainability"}],[{"description": "MDA can misjudge importance if features are correlated (another feature can compensate when one is shuffled)."}, {"description": "MDG is biased toward features with many potential split points or categories."}, {"description": "These importance measures are specific to tree-based ensembles and are not applicable to other model types."}],[],2,1
10,Contextual Decomposition,"Contextual Decomposition interprets neural network predictions by breaking down the model’s output into parts attributed to specific input features or groups of features. It isolates the contribution of a particular input (or a combination of inputs) to the final prediction, taking into account the context provided by the other inputs.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Feature Attribution"}],[{"category": "Feature Attribution", "subcategory": "Interaction-based"}],[{"type": "Scope", "value": "Local"}],[{"description": "Determining how much a particular word in a sentence contributed to a language model's prediction by decomposing the model's output into contributions from that word and the rest of the sentence.", "goal": "Explainability"}, {"description": "In an image classification model, isolating the influence of a specific region of the image (such as a suspected object) on the final classification by decomposing the network's activations.", "goal": "Explainability"}],[{"description": "Often tailored to specific network types (e.g., RNNs for text), so applying it to arbitrary architectures can be challenging."}, {"description": "The method can be complex to implement and may not scale well to very deep or complex neural networks."}],[],3,2
11,Taylor Decomposition,"Taylor Decomposition explains a prediction by using a Taylor series expansion of the model's prediction function around a given input. It breaks the prediction into components attributed to each feature (and combinations of features) by interpreting the series terms, giving an approximate contribution of each feature to the output.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Feature Attribution"}],[{"category": "Feature Attribution", "subcategory": "Taylor Series"}],[{"type": "Scope", "value": "Local"}],[{"description": "Explaining a complex model's prediction by expanding the model's output around the input and assigning the linear terms of the expansion to individual features as their contributions.", "goal": "Explainability"}, {"description": "Comparing how two features interact in their contribution by examining higher-order terms from a Taylor decomposition for a particular prediction.", "goal": "Explainability"}],[{"description": "Provides an approximation of feature contributions; if the model is highly non-linear, the Taylor series truncation can omit important effects."}, {"description": "Requires the model's prediction function to be differentiable and well-behaved around the input, which may not hold for all models or inputs."}],[],3,2
12,Sobol Indices,"Sobol Indices are global sensitivity analysis measures that quantify how much each input variable (and combinations of variables) contribute to the output's variance. They provide values for individual features and feature interactions, indicating the proportion of output variation each is responsible for in the model.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Feature Analysis"}],[{"category": "Feature Analysis", "subcategory": "Sensitivity Analysis"}],[{"type": "Scope", "value": "Global"}],[{"description": "Determining which input parameters in a climate simulation model most influence the variability of the outcome by computing Sobol indices for each parameter and their combinations.", "goal": "Explainability"}, {"description": "Assessing a financial model's sensitivity by quantifying how uncertainty in each input (like market growth, interest rate, etc.) contributes to the uncertainty in the model's profit prediction.", "goal": "Explainability"}],[{"description": "Computationally expensive, as it typically requires many model evaluations (sampling) to estimate the indices accurately."}, {"description": "Assumes independence of input features for straightforward interpretation; strong correlations between inputs can complicate the analysis."}],[],3,3
13,Feature Interaction Detection (H-statistic),"The H-statistic measures the strength of interaction between two features in a model by comparing their combined effect on the prediction to the sum of their individual effects. A higher H-statistic for a feature pair means the model's prediction cannot be explained by additive effects of the two features alone, indicating a significant interaction.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Feature Analysis"}],[{"category": "Feature Analysis", "subcategory": "Interaction Detection"}],[{"type": "Scope", "value": "Global"}],[{"description": "Analyzing a predictive model to see if features like \"age\" and \"income\" interact (i.e., if the effect of income on the outcome changes depending on age) by computing the H-statistic for that pair.", "goal": "Explainability"}, {"description": "In a medical risk model, checking whether two risk factors have a synergistic effect on predictions by evaluating their H-statistic, rather than just their separate contributions.", "goal": "Explainability"}],[{"description": "Requires computing partial dependence or similar measures for individual and pair effects, which can demand many model evaluations and lots of data."}, {"description": "Only captures pairwise interactions directly; interactions among three or more features will not be identified by the H-statistic."}, {"description": "Can be sensitive to noise or limited data in regions of the feature space, potentially giving unreliable estimates if data is sparse for certain feature combinations."}],[],2,3
14,"LIME (Local Interpretable Model-Agnostic Explanations)","LIME explains an individual prediction by training a simple surrogate model around that specific data point. It perturbs the input data point to create synthetic data, gets the complex model's predictions for these new points, and then fits an interpretable model (like a small linear model) on this local dataset. The surrogate's parameters (or rules) then highlight which features of the original data point influenced the prediction the most in that locality.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Model Approximation"}],[{"category": "Model Approximation", "subcategory": "Local Surrogates"}],[{"type": "Scope", "value": "Local"}],[{"description": "Interpreting why a classifier labeled a particular document as \"spam\" by using LIME to identify which words in the document have the most influence on the spam prediction.", "goal": "Explainability"}, {"description": "Using LIME on an image classification to highlight which regions of a specific image the model is focusing on (by seeing how turning off or altering those regions affects the local surrogate model).", "goal": "Explainability"}],[{"description": "Provides a local explanation that is specific to one instance and may not generalize to other instances or the overall model behavior."}, {"description": "Results can vary between runs due to its random sampling approach; the explanation might not be stable."}, {"description": "If the model's decision boundary is very complex near the instance, the simple surrogate might not capture it well, potentially leading to an imprecise explanation."}],[{"type": "Paper", "title": "Why Should I Trust You? Explaining the Predictions of Any Classifier (KDD 2016)", "url": "https://doi.org/10.1145/2939672.2939778"}],2,2
15,Ridge Regression Surrogates,"This technique approximates a complex model by training a ridge regression (a linear model with L2 regularization) on the original model's predictions. The ridge regression serves as a global surrogate that balances fidelity and interpretability, capturing the main linear relationships that the complex model learned while ignoring noise due to regularization.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Model Approximation"}],[{"category": "Model Approximation", "subcategory": "Global Surrogates"}],[{"type": "Scope", "value": "Global"}],[{"description": "Summarizing the behavior of a black-box credit scoring model by training a ridge regression on its outputs, so one can see roughly how features like income or debt are weighted in the simplified model.", "goal": "Explainability"}, {"description": "Using a ridge surrogate to explain a complicated ensemble model for predicting customer churn, providing a single equation that approximates the ensemble's predictions.", "goal": "Explainability"}],[{"description": "As a linear approximation, it may fail to capture non-linear dependencies or interactions present in the original model."}, {"description": "The surrogate might not fit perfectly; if the complex model's behavior isn't mostly linear, the ridge regression could have a low fidelity, making its explanations unreliable."}, {"description": "Requires careful tuning of the regularization strength: too little and it overfits noise, too much and it oversimplifies the model's behavior."}],[],2,1
16,Partial Dependence Plots (PDP),"Partial Dependence Plots illustrate how the predicted outcome changes as one (or two) features vary, averaging out the effects of all other features. They show the marginal effect of selected features on the model prediction by plotting the average prediction as a function of those features.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Visualization Techniques"}],[{"category": "Visualization Techniques", "subcategory": "Global Explanation"}],[{"type": "Scope", "value": "Global"}],[{"description": "Visualizing how a housing price prediction model responds to changes in 'number of rooms' by plotting the average predicted price for different room counts (holding other factors averaged out).", "goal": "Explainability"}, {"description": "Analyzing a loan approval model by using a PDP to see on average how increasing an applicant's income affects the approval probability.", "goal": "Explainability"}],[{"description": "Assumes the feature of interest is independent of others; if features are correlated, the plot may show unrealistic scenarios and misleading effects."}, {"description": "Averaging can mask heterogeneity in feature effects (different subsets of data might have different relationships that get smoothed out)."}, {"description": "Interpretation is limited to one or two features at a time; complex interactions involving many features are not captured."}],[],2,1
17,Accumulated Local Effects (ALE) Plots,"ALE plots show how features influence predictions, like PDPs, but they do so by calculating local changes in the prediction as the feature moves through its range. They accumulate these local effects and account for the presence of other features, making ALE plots more robust than PDPs when features are correlated.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Visualization Techniques"}],[{"category": "Visualization Techniques", "subcategory": "Global Explanation"}],[{"type": "Scope", "value": "Global"}],[{"description": "Interpreting a complex model by plotting an ALE curve for 'age' to understand its effect on predictions of a health risk model, taking into account that 'age' correlates with other health indicators.", "goal": "Explainability"}, {"description": "Using ALE plots to explain a sales forecast model, for instance, how changes in 'marketing spend' locally affect predicted sales while properly considering its correlation with 'season'.", "goal": "Explainability"}],[{"description": "Can only feasibly depict one or two features at a time, similar to PDPs, so higher-dimensional interactions are not directly shown."}, {"description": "If data is sparse in certain ranges of a feature, the local effect estimates (and thus the ALE curve) in those ranges may be less reliable or noisy."}, {"description": "Results in relative effect curves (often centered to zero mean), which might be slightly less intuitive than absolute prediction levels shown in PDPs."}],[],2,2
18,Individual Conditional Expectation (ICE) Plots,"ICE plots display the predicted output for individual instances as a function of a feature, with all other features held fixed for each instance. Each line on an ICE plot represents one instance's prediction trajectory as the feature of interest changes, revealing whether different instances are affected differently by that feature.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Visualization Techniques"}],[{"category": "Visualization Techniques", "subcategory": "Global Explanation"}],[{"type": "Scope", "value": "Global"}],[{"description": "Examining an ICE plot for a salary prediction model to see how increasing years of experience affects predicted salary for several specific individuals (each individual is a separate line).", "goal": "Explainability"}, {"description": "Using ICE plots in a customer churn model to observe how varying account tenure influences the churn probability for various individual customers, highlighting any differences among them.", "goal": "Explainability"}],[{"description": "With many instances, the plot can become cluttered; it may require focusing on a subset or aggregating patterns to interpret effectively."}, {"description": "Still examines one feature at a time; interactions with other features need separate analysis or conditioning by plotting different groups of instances."}, {"description": "If some instances behave very differently from others, the plot may be hard to read without highlighting those differences explicitly (e.g., using color coding or subplots)."}],[],2,2
19,Saliency Maps,"Saliency maps highlight parts of an input (such as pixels in an image) that strongly influence the model's prediction. Typically computed via the gradient of the output with respect to the input, they produce a heatmap where brighter regions indicate greater influence on the model's decision.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Visualization Techniques"}],[{"category": "Visualization Techniques", "subcategory": "Local Explanation"}],[{"type": "Scope", "value": "Local"}],[{"description": "Understanding which regions of an image a convolutional neural network focuses on for classification by generating a saliency map that highlights important pixels for the predicted class.", "goal": "Explainability"}, {"description": "In an image-based medical diagnosis model, using saliency maps to see which parts of an X-ray image contribute most to the model's detection of a condition, ensuring it's looking at relevant anatomical areas.", "goal": "Explainability"}],[{"description": "Can be noisy or diffuse, highlighting broad or scattered areas that are hard to interpret."}, {"description": "Shows where changes in input would most affect the output, but not whether those changes increase or decrease the prediction."}, {"description": "If the model has saturated neurons (flat regions), the gradient may be near zero, resulting in saliency maps that miss important regions despite their importance to the prediction."}],[],2,2
20,"Grad-CAM (Gradient-weighted Class Activation Mapping)","Grad-CAM produces a coarse localization map highlighting image regions important for a classification. It works by using the gradients of a target class flowing into the last convolutional layer of a CNN to weight the feature maps, and then projects these weights back to the image space as a heatmap of influential regions.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Visualization Techniques"}],[{"category": "Visualization Techniques", "subcategory": "Local Explanation"}],[{"type": "Scope", "value": "Local"}],[{"description": "Validating that a deep learning model for identifying dogs in photos is focusing on the dog itself by applying Grad-CAM and seeing the heatmap light up on the dog, not the background.", "goal": "Explainability"}, {"description": "Using Grad-CAM on a medical image classifier (e.g., pneumonia detection from chest X-rays) to visualize that the model concentrates on the lung areas with opacities, which are relevant for the condition.", "goal": "Explainability"}],[{"description": "Provides a low-resolution heatmap due to using the last convolutional layer's features, which may not pinpoint fine details."}, {"description": "Primarily highlights features that positively contribute to the target class; it might not show features that suppress the target or indicate other classes."}, {"description": "Designed for CNN-based vision models; requires adaptation or different techniques for non-vision models or architectures without convolutional layers."}],[],2,1
21,Occlusion Sensitivity,"Occlusion sensitivity tests which parts of the input are important by occluding (masking or removing) them and seeing how the model's prediction changes. For example, portions of an image can be covered up in a sliding window fashion; if the model's confidence drops significantly when a certain region is occluded, that region was important for the prediction.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Feature Attribution"}],[{"category": "Feature Attribution", "subcategory": "Occlusion"}],[{"type": "Scope", "value": "Local"}],[{"description": "Analyzing a trained image classifier by covering different patches of an image (e.g., with a gray box) to identify which region (when hidden) causes the biggest drop in prediction confidence.", "goal": "Explainability"}, {"description": "For a text classifier, removing or blanking out one word at a time in a sentence to see how much the prediction changes, indicating that word's influence.", "goal": "Explainability"}],[{"description": "Brute-force approach that can be very slow for high-dimensional inputs (e.g., large images) due to testing many occlusions."}, {"description": "The size and shape of the occlusion region affect results; too large and it may cover multiple important parts, too small and the impact might be hard to detect."}, {"description": "Occluding features can create unnatural inputs (such as an image with a blank patch or a sentence with a missing word), which may affect the model differently than normal data would."}],[],2,2
22,Attention Mechanisms in Neural Networks,"Attention mechanisms allow models, especially in sequence tasks like NLP, to weight different parts of the input when making predictions. Visualizing or examining the learned attention weights can provide insight into which parts of the input the model found most relevant for a given output. By looking at these attention heatmaps, we can interpret which parts of the input were most influential for the model's decision.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Visualization Techniques"}],[{"category": "Visualization Techniques", "subcategory": "Attention"}],[{"type": "Scope", "value": "Local"}],[{"description": "Visualizing the attention weights in a translation model to see which words in the source sentence correspond to a given word in the translated sentence.", "goal": "Explainability"}, {"description": "Inspecting a transformer-based language model's attention to understand what context words it focused on when answering a question based on a passage.", "goal": "Explainability"}],[{"description": "High attention weight doesn't always equal high importance; attention indicates correlation, not necessarily causation, in the model's decision."}, {"description": "Modern models have multiple layers and heads of attention, making comprehensive interpretation challenging when there are many attention weights."}, {"description": "Only applicable to models with an attention mechanism; other models require different interpretability techniques."}],[],2,2
23,Factor Analysis,"Factor analysis is a technique to discover latent variables (factors) that collectively explain the patterns observed in many features. It models each observed feature as a linear combination of a few underlying factors, thereby reducing dimensionality and potentially revealing the hidden structure or themes in the data.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Dimensionality Reduction"}],[{"category": "Dimensionality Reduction", "subcategory": "Linear Decomposition"}],[{"type": "Scope", "value": "Global"}],[{"description": "Reducing a set of 20 correlated survey questions into a few underlying factors (like \"customer satisfaction\" and \"product usability\") that summarize the primary dimensions of variation in responses.", "goal": "Explainability"}, {"description": "Analyzing economic indicators by extracting latent factors (such as \"market trend\" or \"inflation pressure\") that drive common movement in several financial time series.", "goal": "Explainability"}],[{"description": "The identified factors can be difficult to interpret, as they are linear combinations of original features without clear labels."}, {"description": "Assumes linear relationships and certain statistical distributions; if these assumptions don't hold, the factors found may be misleading."}, {"description": "Choosing the number of factors is subjective and too few or too many factors can either oversimplify or overfit the data."}],[],2,2
24,Principal Component Analysis (PCA),"PCA is a dimensionality reduction method that transforms the original features into a new set of orthogonal components (principal components) ordered by how much variance they explain in the data. By keeping only the top components, PCA provides a simpler representation of the data that captures the most important variance, which can aid visualization or reduce complexity.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Dimensionality Reduction"}],[{"category": "Dimensionality Reduction", "subcategory": "Linear Decomposition"}],[{"type": "Scope", "value": "Global"}],[{"description": "Visualizing a high-dimensional dataset of gene expression by projecting it onto the first two principal components to see natural groupings of samples.", "goal": "Explainability"}, {"description": "Preprocessing a large set of sensor features by using PCA to compress them into a few key features before feeding them into a prediction model, reducing noise and redundancy.", "goal": "Explainability"}],[{"description": "The principal components are often not directly interpretable, as each component mixes many original features."}, {"description": "Maximizing variance doesn't always align with what is most relevant for a specific prediction task (PCA is unsupervised)."}, {"description": "Sensitive to feature scaling and outliers; these can distort which components appear most important."}],[],2,1
25,UMAP,"UMAP is a non-linear dimensionality reduction technique, similar to t-SNE, aimed at visualizing high-dimensional data in 2D or 3D. It tends to preserve both local structure and some global structure of the data better than t-SNE, often producing more interpretable overall layouts of clusters.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Dimensionality Reduction"}],[{"category": "Dimensionality Reduction", "subcategory": "Non-linear Projection"}],[{"type": "Scope", "value": "Global"}],[{"description": "Visualizing a high-dimensional dataset of customer behaviors by reducing it to two dimensions with UMAP, to identify distinct clusters of customer types.", "goal": "Explainability"}, {"description": "Plotting feature embeddings of images using UMAP to see if images naturally group by content (e.g., all animal images together, separate from scenery images).", "goal": "Explainability"}],[{"description": "Primarily a visualization tool; like t-SNE, the resulting embedding coordinates are abstract and not useful for tasks beyond exploration."}, {"description": "Requires tuning (e.g., number of neighbors, distance metrics) and results can vary based on these settings."}, {"description": "Can sometimes create apparent clusters or gaps that are artifacts of the projection rather than true distinctions in the data."}],[],2,2
26,Prototype and Criticism Models,"This approach identifies representative examples (prototypes) of the dataset and outlier or hard-to-represent examples (criticisms). Prototypes capture the typical patterns the model or data exhibits, while criticisms highlight where those patterns fail to cover, giving insight into the diversity and exceptions in the data or model behavior.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Example-Based Methods"}],[{"category": "Example-Based Methods", "subcategory": "Prototypes & Criticisms"}],[{"type": "Scope", "value": "Global"}],[{"description": "Summarizing an image dataset by selecting a few prototypical images for each category that the model has learned, and a few unusual images the model finds hard to fit into those categories.", "goal": "Explainability"}, {"description": "Understanding a loan dataset by finding prototypes (common applicant profiles that are well-represented in the data) and criticisms (applicant profiles that are rare or not well explained by the common patterns).", "goal": "Explainability"}],[{"description": "Algorithmic selection of prototypes and criticisms can be complex and may yield different results depending on the method or distance metric used."}, {"description": "Prototypes might oversimplify the data, and a large number of criticisms might be needed to capture all outlier cases, which can be overwhelming."}, {"description": "Primarily aids in understanding and diagnosing model/data, but doesn't directly improve model fairness or accuracy on its own."}],[],2,2
27,Influence Functions,"Influence functions estimate how a model's predictions would change if a particular training point were removed. By using a second-order Taylor approximation, they identify which training examples have the most influence (positive or negative) on a given prediction. This helps identify training data points that are most responsible for a particular prediction or for a model's behavior, which can be useful for debugging or understanding model decisions.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Example-Based Methods"}],[{"category": "Example-Based Methods", "subcategory": "Data Influence"}],[{"type": "Scope", "value": "Local"}],[{"description": "Investigating a surprising prediction by identifying which specific training examples (e.g., mislabeled or atypical points) most impacted that prediction's outcome.", "goal": "Explainability"}, {"description": "Debugging model behavior by finding training data that strongly sways the model – for example, detecting if a small set of outlier training points are causing overfitting and correcting or removing them.", "goal": "Explainability"}],[{"description": "The approximation might be inaccurate for very complex models (like deep neural networks), especially if removing a point causes non-linear changes in model parameters."}, {"description": "Computationally intensive for large models, since it involves gradients and Hessian (second-order) calculations to estimate influences."}, {"description": "Assumes a relatively stable, differentiable training process (works best for convex problems); results may be unreliable if the model is highly non-convex or over-parameterized."}],[],3,3
28,Contrastive Explanation Method (CEM),"CEM explains model decisions by producing contrastive examples: it finds the minimal changes to an input that would switch the model's prediction. It outputs pertinent negatives (what could be removed from the input to change the prediction) and pertinent positives (what minimal additional features would be needed to reach the same decision) as a way to highlight what is essential in the input for that prediction.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Example-Based Methods"}],[{"category": "Example-Based Methods", "subcategory": "Counterfactual Explanations"}],[{"type": "Scope", "value": "Local"}],[{"description": "Explaining an image classification by identifying a small patch or feature in the image that, if obscured, would change the model's prediction (pertinent negative), indicating that patch was crucial for the original classification.", "goal": "Explainability"}, {"description": "For a loan approval model, using CEM to find what slight increase in an attribute (like income by a certain amount) would have led to approval instead of denial, giving a counterfactual explanation to the applicant.", "goal": "Explainability"}],[{"description": "Finding these minimal changes involves solving an optimization problem for each instance, which can be slow and may converge to odd solutions."}, {"description": "The changes identified may be hard to interpret or implement (e.g., fractional changes to pixels or features that don't correspond to real-world actions)."}, {"description": "Less straightforward to apply to discrete features like categorical variables or text without a way to vary them gradually."}],[],3,3
29,"Bayesian Networks (e.g., bnlearn)","Bayesian networks are probabilistic graphical models that use graphs to show the relationships between variables (nodes) and how they probabilistically influence each other. They allow reasoning under uncertainty by encoding joint probability distributions; one can observe evidence and propagate probabilities through the network to see how it affects other variables. Using a Bayesian network (learned from data or constructed by experts) can help trace how evidence flows to influence predictions and to perform inference like answering 'what if' questions.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Causal & Probabilistic Models"}],[{"category": "Causal & Probabilistic Models", "subcategory": "Bayesian Networks"}],[{"type": "Scope", "value": "Global"}],[{"description": "Using a Bayesian network to represent and explain causes of equipment failure, where sensors readings and environmental factors are connected; given a failure, the network can show which factors likely contributed to it.", "goal": "Explainability"}, {"description": "Building a Bayesian network from patient data to understand how symptoms and test results probabilistically relate to various diagnoses, allowing doctors to see an interpretable model of disease likelihoods.", "goal": "Explainability"}],[{"description": "Learning the network structure from data is computationally challenging and can be unreliable without sufficient data or expert knowledge."}, {"description": "Inference (calculating probabilities) can become slow in very large networks with many interdependent variables, though efficient algorithms exist for certain cases."}, {"description": "The accuracy and usefulness of the explanations depend on the correctness of the encoded relationships; a poorly specified network can lead to wrong conclusions about cause and effect."}],[],3,3
30,ANCHOR,"ANCHOR generates precision if-then rules as explanations for individual predictions. It finds a minimal set of conditions (on input features) that 'anchor' the prediction, meaning that if those conditions are met, the model will almost always give the same prediction. These anchor rules are designed to be easily understood and highly predictive for that specific instance.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Rule Extraction"}],[{"category": "Rule Extraction", "subcategory": "Local Rules"}],[{"type": "Scope", "value": "Local"}],[{"description": "Explaining a text classification by providing an anchor rule like: \"IF the review contains 'excellent' AND no mention of 'but', THEN the model predicts positive\", which helped drive the positive prediction for that review.", "goal": "Explainability"}, {"description": "Justifying a loan approval decision with an anchor such as: \"IF credit score > 700 AND debt-to-income < 30%, THEN approve\" for a particular applicant, indicating those conditions secured the approval.", "goal": "Explainability"}],[{"description": "Anchors give sufficient conditions for a prediction but may not highlight all important factors, especially those outside the anchor conditions."}, {"description": "For some instances, it can be hard to find a concise anchor; the explanation might end up very specific or not very interpretable if the model logic is complex."}, {"description": "Anchors are local to one prediction and might not apply or generalize to other cases, so they don't provide a global model understanding."}],[],2,2
31,RuleFit,"RuleFit is a method that creates an interpretable model by combining linear terms with decision rules. It first extracts potential rules from ensemble trees, then builds a sparse linear model where those rules (binary conditions) and original features are used as predictors, with regularization to keep the model simple. The final model is a linear combination of a small set of rules and original features, balancing interpretability with predictive power.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Rule Extraction"}],[{"category": "Rule Extraction", "subcategory": "Rule Ensemble"}],[{"type": "Scope", "value": "Global"}],[{"description": "Building a credit risk model that results in human-readable rules like \"IF income < 30k AND credit history = 'short' THEN high risk\" alongside linear effects for features like age or debt, providing a mix of rules and linear weights as the model.", "goal": "Explainability"}, {"description": "Explaining customer churn by using RuleFit, where the model output might include rules capturing specific customer segments (e.g., \"IF usage drops in last 3 months THEN likely churn\") and linear terms for general trends (e.g., gradual effect of contract length).", "goal": "Explainability"}],[{"description": "Can produce a large number of candidate rules; if not properly regularized, the model might still become complex and less interpretable."}, {"description": "The resulting rules depend on the initial tree generation process; important patterns might be missed if the trees don't capture them."}, {"description": "Requires careful tuning of the regularization to strike a good balance between accuracy and simplicity (too much and model underfits, too little and it overfits with too many rules)."}],[],3,2
32,Monte Carlo Dropout,"Monte Carlo Dropout uses dropout at prediction time to estimate model uncertainty. By running multiple forward passes with random dropout activated and observing the variation in outputs, it provides a distribution of predictions. A consistent prediction across runs indicates high confidence, while widely varying predictions indicate uncertainty.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Uncertainty and Reliability"}],[{"category": "Uncertainty and Reliability", "subcategory": "Bayesian Approximation"}],[{"type": "Scope", "value": "Global"}],[{"description": "Estimating uncertainty in a medical image classifier by performing 50 predictions on the same X-ray with dropout on; if the classification (e.g., detecting a tumor) is the same each time with minimal variation, the model is confident, otherwise it's flagged as unsure.", "goal": "Explainability"}, {"description": "In an autonomous driving system, using Monte Carlo Dropout for the object detection neural network to know when the system is unsure about an object's identity (e.g., pedestrian vs. cyclist) due to high prediction variability, prompting a fallback or caution.", "goal": "Explainability"}],[{"description": "Requires multiple passes through the model for each input to gauge uncertainty, which increases computation time proportionally to the number of passes."}, {"description": "The uncertainty is an approximation based on dropout; it may not capture all types of uncertainty (e.g., uncertainty due to model misspecification)."}, {"description": "Needs the model to have been trained with dropout; applying dropout to a model that wasn't trained with it might not yield meaningful uncertainty estimates."}],[],3,2
33,"ODIN (Out-of-DIstribution detector for Neural networks)","ODIN is a technique to detect when an input to a neural network is something very different from what it was trained on (out-of-distribution). It works by scaling the model's outputs (using a temperature parameter) and adding a small perturbation to inputs to better separate the confidence scores for normal vs out-of-distribution examples. By examining the adjusted confidence score, ODIN can flag inputs that are likely far from the training distribution.",Model-Specific,Explainability,[{"goal": "Explainability", "category": "Uncertainty and Reliability"}],[{"category": "Uncertainty and Reliability", "subcategory": "Out-of-Distribution Detection"}],[{"type": "Scope", "value": "Global"}],[{"description": "Enhancing a classifier with ODIN to detect unknown classes: for example, a handwriting digit model using ODIN to flag an input image that isn't a digit (like a letter or symbol) because after applying ODIN's method, the model's confidence in any digit class is very low.", "goal": "Explainability"}, {"description": "Using ODIN for an NLP classifier to detect gibberish or off-topic queries by observing that after applying ODIN's scaling and perturbation, these inputs yield very low maximum softmax scores compared to normal queries.", "goal": "Explainability"}],[{"description": "Requires tuning (temperature and perturbation magnitude) based on some knowledge of out-of-distribution vs in-distribution data, which might not always be available."}, {"description": "Adds computational overhead by requiring an additional forward pass with perturbed input for each sample to make a decision."}, {"description": "May not catch all OOD inputs, especially if an OOD sample is very similar to in-distribution ones; conversely, may sometimes flag unusual in-distribution samples as OOD."}],[],2,2
34,Permutation Tests,"Permutation tests assess the significance of an observed result (like model accuracy or feature importance) by comparing it to what would happen purely by chance. This is done by randomly shuffling labels or data many times and calculating the result each time, building a distribution of outcomes under the null hypothesis (no real relationship). If the actual result is far out in the tail of this distribution, it is deemed statistically significant.",Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Uncertainty and Reliability"}],[{"category": "Uncertainty and Reliability", "subcategory": "Statistical Testing"}],[{"type": "Scope", "value": "Global"}],[{"description": "Validating that a classifier's accuracy is not just luck: shuffle the labels of the test set many times and re-evaluate accuracy to see if the real accuracy is higher than what random labelings achieve.", "goal": "Explainability"}, {"description": "Testing whether a particular feature has a genuine effect on predictions by computing a metric (like difference in error with and without that feature) on original data and comparing it to the distribution of that metric when labels are randomly permuted.", "goal": "Explainability"}],[{"description": "Computationally intensive if it involves retraining or repeatedly evaluating a model for many random permutations, especially for complex models or large datasets."}, {"description": "The quality of the test depends on having a suitable test statistic and enough permutation samples to approximate the null distribution."}, {"description": "When permuting data, certain data dependencies or structures (like time series ordering) are destroyed, so the test must be designed carefully to avoid invalid permutations in those contexts."}],[],2,2
35,"Fairness Metrics (e.g., Equalized Odds, Demographic Parity)","Fairness metrics quantify how differently a model treats different groups of people. For example, they check things like whether a model gives positive outcomes (like loan approvals) at similar rates for different groups (Demographic Parity) or whether the true positive rates are equal across groups (Equalized Odds). Metrics like these are used to evaluate models for disparate impact or bias without changing the model itself.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Group Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "Assessing a hiring algorithm by measuring the percentage of male vs female candidates selected (Demographic Parity) and checking if both rates are roughly equal.", "goal": "Fairness"}, {"description": "Evaluating a recidivism prediction model by comparing true positive rates and false positive rates for different racial groups (Equalized Odds) to ensure the model's accuracy is similar across groups.", "goal": "Fairness"}],[{"description": "These metrics only diagnose bias; they don't tell you how to fix it and sometimes achieving one fairness metric can worsen another."}, {"description": "They require protected attribute labels for evaluation, which might not always be available or fully reliable."}, {"description": "Focusing on a single metric like Demographic Parity might not capture the full picture of fairness (e.g., it ignores true negative/false positive rates)."}],[{"type": "Documentation", "title": "Fairness Metrics in Machine Learning", "url": "https://fairnessmetrics.org"}],1,1
36,Model Pruning,"Model pruning reduces the size of a neural network by eliminating neurons or connections that have little impact on predictions. By cutting out these less important weights, the model becomes simpler, often faster, and possibly more interpretable, while maintaining nearly the same performance. Pruning can be seen as a form of simplification that can slightly improve fairness if it removes overfit to noise (though it's mainly for efficiency).",Model-Specific,Fairness,[{"goal": "Fairness", "category": "Model Simplification"}],[{"category": "Model Simplification", "subcategory": "Compression"}],[{"type": "Fairness Approach", "value": "Model Simplification"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "Pruning a complex image recognition network by removing 30% of the smallest-magnitude weights, yielding a smaller model that is easier to deploy on mobile devices without significant loss in accuracy.", "goal": "Fairness"}, {"description": "Simplifying a deep learning model for credit scoring via pruning, which might incidentally remove some overfit connections that were picking up on noise or bias, resulting in a slightly fairer and more general model.", "goal": "Fairness"}],[{"description": "Pruning is generally aimed at efficiency, not a direct fairness solution; it does not specifically target bias issues, and a pruned model can still be biased."}, {"description": "Aggressive pruning can hurt accuracy or remove nuances necessary for fairness (e.g., if underrepresented group patterns are subtle, pruning might discard them)."}],[{"type": "Paper", "title": "Learning both Weights and Connections for Efficient Neural Networks (NIPS 2015)", "url": "https://doi.org/10.1145/2783258.2783274"}],2,1
37,Knowledge Distillation,"Knowledge distillation trains a simpler 'student' model to replicate the behavior of a complex 'teacher' model. The teacher's predictions (including softened probabilities for classes) serve as training data for the student. The resulting student model is smaller, more interpretable, or faster while preserving most of the teacher's accuracy. This can indirectly aid fairness if the distilled model generalizes better and smooths out some overfitting or biases present in the teacher.",Model-Specific,Fairness,[{"goal": "Fairness", "category": "Model Simplification"}],[{"category": "Model Simplification", "subcategory": "Distillation"}],[{"type": "Fairness Approach", "value": "Model Simplification"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "Compressing a large ensemble of decision trees into a single smaller tree by training the smaller tree on the big model's predictions, aiming to keep accuracy similar but achieve faster predictions and easier interpretation.", "goal": "Fairness"}, {"description": "Distilling a complex NLP model into a simpler one for deployment on edge devices – the distilled model is more efficient and might generalize slightly better, potentially reducing overfitting to biases learned by the complex model.", "goal": "Fairness"}],[{"description": "The student model may not capture all nuances of the teacher, especially regarding fairness – if the teacher had biases, the student could learn them too from the teacher's outputs."}, {"description": "Distillation typically focuses on efficiency and may not directly address fairness, unless combined with fairness constraints during the distillation process."}],[],2,1
38,Attention Visualisation in Transformers,"This technique displays the attention scores from transformer models (like BERT or GPT) to see which words or tokens the model was focusing on when making a prediction. By looking at these attention heatmaps, we can interpret which parts of the input were most influential for the model's decision. It's a way to peek into the workings of models that use multi-head self-attention, to ensure they're attending to relevant content rather than, say, protected attributes.",Model-Specific,Fairness,[{"goal": "Fairness", "category": "Transparency"}],[{"category": "Transparency", "subcategory": "Attention Weights"}],[{"type": "Fairness Approach", "value": "Transparency"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "For a transformer-based resume screening model, visualizing attention to ensure the model focuses on skill-related sections of resumes rather than personal information sections. This helps confirm it isn't implicitly attending to protected information like names (which could reveal gender/ethnicity).", "goal": "Fairness"}, {"description": "Analyzing a translation model's attention to check if certain gendered pronouns are disproportionately influenced by gender-specific terms in the context, as a way to detect potential gender bias in translations.", "goal": "Fairness"}],[{"description": "Attention visualizations are interpretative aid; high attention on a word doesn't guarantee it's the cause of the decision (and vice versa)."}, {"description": "Transformers have many layers/heads, so focusing on one attention map can give an incomplete picture; it's possible bias is encoded in patterns not obvious from raw attention weights."}],[],2,1
39,Neuron Activation Analysis,"Neuron activation analysis in LLMs looks at how individual neurons (or groups of neurons) behave when processing lots of different inputs. By examining which neurons activate for certain concepts or contexts, we can guess what each neuron might be 'specialized' for. This provides insight into the model's internal representations and whether certain neurons correspond to biased or undesirable concepts (which could then be monitored or modified).",Model-Specific,Fairness,[{"goal": "Fairness", "category": "Transparency"}],[{"category": "Transparency", "subcategory": "Neuron Interpretability"}],[{"type": "Fairness Approach", "value": "Transparency"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "Studying a specific neuron in a language model that activates strongly whenever the text mentions age, to understand if that neuron is carrying age-related information that might be leading to biased outputs. This analysis might inform interventions like zeroing that neuron for certain tasks.", "goal": "Fairness"}, {"description": "Analyzing groups of neurons in a large text model to find if some cluster corresponds to race-related content; if those neurons overly influence negative outputs, one could consider this a source of bias to address.", "goal": "Fairness"}],[{"description": "Determining a neuron's 'meaning' is often subjective and not clear-cut; many neurons respond to multiple factors, making interpretation tricky."}, {"description": "Even if a neuron correlates with a concept, modifying or constraining that neuron might not cleanly remove a bias — the network could have redundant representations of that concept elsewhere."}],[],3,2
40,Prompt Sensitivity Analysis,"This technique tests how sensitive a language model (LLM) is to changes in its prompt or input phrasing. By altering the wording, order, or content of the prompt and observing changes in the output, we can identify what kinds of prompts strongly influence the model’s responses. This helps in understanding and mitigating biased or unsafe behaviors: if certain trigger phrases lead to unwanted outputs, we learn the model's weaknesses and can try to address them.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Testing & Evaluation"}],[{"category": "Testing & Evaluation", "subcategory": "Robustness Testing"}],[{"type": "Fairness Approach", "value": "Evaluation"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "Taking a query to a chatbot (\"Tell me about culture\") and varying it (\"Tell me about African culture\" vs \"Tell me about European culture\") to see if the answers show any bias or imbalance, indicating whether the prompt content steers the model unfairly.", "goal": "Fairness"}, {"description": "Testing a question-answering model with different formality or accent in the prompt (e.g., standard English vs. a dialect) to check if the model's ability to answer changes, which would suggest fairness issues regarding dialectal speakers.", "goal": "Fairness"}],[{"description": "This kind of analysis can surface issues but doesn’t on its own fix them; it’s a diagnostic tool that still needs model adjustments or prompt engineering to solve any observed problems."}, {"description": "It may not be systematic – one could test dozens of variations but still miss some prompt that triggers bias, given the infinite ways to phrase inputs."}],[],2,1
41,Causal Mediation Analysis in Language Models,"Causal mediation analysis in language models involves treating parts of the model (or input) as variables in a causal framework. By systematically intervening or blocking information at certain points in the model and seeing how it changes the output, researchers can figure out which components or features are causally influencing certain aspects of the model's prediction. This is a research-level approach to open the 'black box' and understand cause-effect relations inside the model, which can highlight biased pathways.",Model-Specific,Fairness,[{"goal": "Fairness", "category": "Causal Fairness"}],[{"category": "Causal Fairness", "subcategory": "Mediation"}],[{"type": "Fairness Approach", "value": "Causal"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "Intervening in a text generation model by clamping certain neuron activations (like those related to sensitive topics) and observing how the output changes, to deduce if those neurons mediate the effect of the prompt's gender on the tone of the response.", "goal": "Fairness"}, {"description": "Using mediation analysis on a sentiment classifier to see if the effect of a sensitive attribute word (like an identity term) on the output sentiment is direct or goes through other context words, helping isolate whether bias is due to that word alone or its correlation with other words.", "goal": "Fairness"}],[{"description": "Applying causal concepts to deep neural networks is very complex; defining meaningful 'treatments' and 'mediators' in a large network involves abstractions that may or may not capture reality."}, {"description": "This approach is currently mostly academic; it’s not a turnkey solution for industry practitioners due to its complexity."}],[],3,2
42,Feature Attribution with Integrated Gradients in NLP,"This uses the Integrated Gradients method to explain predictions made by language models. By applying IG to text inputs, it assigns an importance score to each word (or token), indicating how much that word contributed to the model's output. These scores can then be visualized (e.g., by highlighting words in a sentence with intensity proportional to their influence) to help understand the model's reasoning for a particular prediction (like which words made it classify a review as positive).",Model-Specific,Fairness,[{"goal": "Fairness", "category": "Transparency"}],[{"category": "Transparency", "subcategory": "Feature Attribution"}],[{"type": "Fairness Approach", "value": "Transparency"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "Explaining a toxic content detection model's decision by highlighting which words in a comment had the strongest positive or negative impact on the 'toxicity' score, using Integrated Gradients to compute each word's contribution.", "goal": "Fairness"}, {"description": "Interpreting why a translation model made a certain phrasing choice by seeing which source language words had the highest integrated gradient values for the output choice, potentially revealing bias if irrelevant words (like speaker gender) unduly influenced the phrasing.", "goal": "Fairness"}],[{"description": "While IG can highlight influential words, it might not fully capture interactions between words (two words together might have a big effect even if individually they don’t, and IG on individual words might miss that)."}, {"description": "Long text sequences can make attributions spread out and harder to interpret (many tokens each with moderate contribution, rather than a few with large contributions)."}],[],3,2
43,Concept Activation Vectors (CAVs),"CAVs are a way to probe a complex model by defining a vector that represents a concept (like 'striped' or 'female') in the model's internal feature space. By examining how aligning an input with this concept vector changes the model's output, we can gauge how sensitive the model is to that concept. In fairness, CAVs can be used to test if a model's predictions are unduly influenced by a concept like gender or race: essentially, checking how much moving an input along the 'gender' direction in feature space affects the prediction.",Model-Specific,Fairness,[{"goal": "Fairness", "category": "Transparency"}],[{"category": "Transparency", "subcategory": "Concept Probing"}],[{"type": "Fairness Approach", "value": "Causal"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "Analyzing an image classification network by constructing a 'gender' concept vector from internal activations (using sets of male vs female reference images) and then testing if shifting an image's activation in the direction of this vector significantly changes the classification score (which would indicate gender bias in the model's internal representation).", "goal": "Fairness"}, {"description": "Using CAVs on a sentiment model to see if moving in the direction of a 'race' concept (learned from word embeddings associated with race) changes the sentiment output for a review. If yes, that suggests the model might be inadvertently using racial cues in sentiment predictions.", "goal": "Fairness"}],[{"description": "Defining a concept vector requires representative examples of that concept, which can be subjective or hard to get (what exactly represents 'race' in a model's latent space?)."}, {"description": "Even if a model is sensitive to a concept as per CAV, mitigating that influence is another challenge; CAVs tell us about a problem but don't solve it on their own."}],[],3,2
44,In-Context Learning Analysis,"In-context learning analysis looks at how large language models use the examples given in their prompt to make predictions or continue text. It tries to understand to what extent the model is truly learning from the prompt on the fly (few-shot learning) versus just pattern matching. By testing different prompt strategies and seeing how the model's outputs change, one can infer how the model treats the context and whether it's fair or biased in that adaptation (e.g., does it overly rely on certain context examples and ignore others?).",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Testing & Evaluation"}],[{"category": "Testing & Evaluation", "subcategory": "Prompt Behavior"}],[{"type": "Fairness Approach", "value": "Evaluation"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "Providing a large language model with a few examples of polite responses and a few rude responses in the prompt (demonstrations) and seeing if it adopts the polite style for a new query. Analyzing if the model unduly biases towards certain example traits (like maybe the gender of names in examples) would highlight fairness considerations in how it generalizes from context.", "goal": "Fairness"}, {"description": "Testing GPT-style model prompts that include instructions like \"Assume the user is a child\" versus \"Assume the user is an adult\" to see how differently it responds. In-context learning analysis here would reveal if the model changes tone appropriately without injecting any bias (e.g., always giving simpler, more condescending answers for certain assumed profiles).", "goal": "Fairness"}],[{"description": "This analysis can be quite heuristic; it's not a formal guarantee of fairness, just an exploration. Large language model behavior is complex and can defy simple reasoning."}, {"description": "It might be hard to distinguish whether a model is truly 'learning' a bias from context or if it's reflecting biases from its pre-training; interpreting the results requires care."}],[],2,1
45,Reweighing,"Reweighing adjusts the weights of training examples for different protected groups so that each group is represented equally in each outcome. By up-weighting underrepresented groups and down-weighting overrepresented ones in the training data, the model learns from a more balanced perspective, which can reduce bias in predictions.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Pre-Processing Techniques"}],[{"category": "Pre-Processing Techniques", "subcategory": "Data Transformation"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Preprocessing and Feature Engineering"}],[{"description": "Preprocessing a hiring dataset by increasing the weights of qualified minority applicants and decreasing weights of majority applicants in the training phase, so the learned model treats both groups more fairly.", "goal": "Fairness"}, {"description": "Adjusting weights in a loan approval training set where one demographic group had far fewer approvals historically, to ensure the model doesn't just learn from the majority group's outcomes.", "goal": "Fairness"}],[{"description": "Does not change any feature values or labels themselves, so any bias inherent in the data features or labels remains."}, {"description": "If a group has very few samples, reweighing them heavily can lead to high variance or overfitting, as the model might rely too much on a small number of weighted instances."}, {"description": "Assumes the existing labels are correct and fair; if historical decisions were biased, reweighing might still propagate that bias, just with adjusted frequencies."}],[{"type": "Paper", "title": "Preprocessing for Classification with Fairness Constraints", "url": "https://doi.org/10.1109/ICDM.2011.50"}],2,2
46,"Disparate Impact Remover","Disparate Impact Remover edits feature values in the dataset to reduce any correlation between those features and a protected attribute. By adjusting distributions of features for different groups, it 'scrubs' the dataset of biases that could lead to disparate impact, while trying to maintain the overall data utility.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Pre-Processing Techniques"}],[{"category": "Pre-Processing Techniques", "subcategory": "Data Transformation"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Preprocessing and Feature Engineering"}],[{"description": "Modifying a financial dataset so that credit score distributions for different ethnic groups become aligned, preventing the model from learning spurious associations between ethnicity and creditworthiness.", "goal": "Fairness"}, {"description": "Adjusting image features used in a face recognition system so that they carry minimal information about the subject's race, aiming to reduce bias in face matching.", "goal": "Fairness"}],[{"description": "Altering feature values can distort true relationships in the data, potentially reducing model accuracy if not carefully done."}, {"description": "May not eliminate all indirect bias: some subtle correlations with protected attributes might remain or new ones might be introduced."}, {"description": "Needs domain knowledge to avoid over-correcting; aggressive adjustments could remove legitimate signal or fairness could be improved at the cost of large changes to data realism."}],[],2,2
47,"Learning Fair Representations","This approach trains a model to create a new representation of the data (like a set of transformed features) that still contains the useful information for prediction but hides information about protected attributes (e.g., race or gender). The idea is that if the sensitive info is hidden, any model using these representations will make fairer predictions. Often achieved via adversarial objectives that penalize encoding of protected attribute information.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Pre-Processing Techniques"}],[{"category": "Pre-Processing Techniques", "subcategory": "Data Transformation"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Preprocessing and Feature Engineering"}],[{"description": "Training an autoencoder on loan application data with a fairness constraint, so that the encoded features can predict loan default but make it difficult to determine the applicant's race, then using those encoded features for a fairer classifier.", "goal": "Fairness"}, {"description": "Creating a fair representation of employee performance data that captures skills and experience but filters out gender information, allowing a promotion algorithm to base decisions on merit without gender bias.", "goal": "Fairness"}],[{"description": "May reduce model accuracy because the representation might lose some useful information along with the bias-related information."}, {"description": "Often requires complex training (sometimes adversarial networks or multi-objective optimization) to ensure the representation is both predictive and fair, which can be hard to fine-tune."}, {"description": "No guarantee that all bias is removed—some protected information can leak into the representation, especially if the model isn't perfectly optimized."}],[],3,2
48,Fairness GAN,"Fairness GAN employs a Generative Adversarial Network to produce a debiased data representation. A generator network transforms data into a new form intended to be independent of protected attributes, while a discriminator network tries to guess the protected attribute from the transformed data. The generator learns to fool the discriminator, thereby creating representations that hide group membership and help ensure downstream models treat groups more equally.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Pre-Processing Techniques"}],[{"category": "Pre-Processing Techniques", "subcategory": "Data Transformation"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Preprocessing and Feature Engineering"}],[{"description": "Using a GAN to preprocess and remove gender information from resume data so that a hiring model cannot distinguish between male and female applicants, focusing only on qualifications.", "goal": "Fairness"}, {"description": "Generating fair embeddings of user profiles with a Fairness GAN such that an ad targeting model sees profiles that are similar across races, preventing the model from making race-based distinctions.", "goal": "Fairness"}],[{"description": "Training adversarial networks is complex and can be unstable; achieving a good balance where bias is removed but useful information is retained is challenging."}, {"description": "Requires a large amount of data for the generator and discriminator to train effectively; otherwise, the fairness or data utility might suffer."}, {"description": "Typically focuses on specified protected attributes; it might not address biases related to attributes not explicitly targeted in the adversarial training."}],[],3,2
49,Optimised Pre-Processing,"Optimized Pre-Processing methods alter the dataset (features and/or labels) through a formal optimization to improve fairness metrics. They seek an adjusted dataset that is as close as possible to the original data distribution but satisfies fairness criteria (like equalized outcomes across groups). This might involve changing some labels or shifting feature values in a minimal way that achieves a fairer data representation for model training.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Pre-Processing Techniques"}],[{"category": "Pre-Processing Techniques", "subcategory": "Data Transformation"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Preprocessing and Feature Engineering"}],[{"description": "Adjusting a criminal justice dataset by minimally modifying risk scores and reassigning a few outcomes so that the processed dataset yields equal false positive rates between racial groups when used for training.", "goal": "Fairness"}, {"description": "Optimizing a credit dataset by tweaking some applicant feature values and loan decisions to remove systemic bias (e.g., ensuring applicants from different demographics with similar profiles have similar chances of approval) before modeling.", "goal": "Fairness"}],[{"description": "Solving the optimization problem can be complex and computationally intensive, especially for large datasets and complicated fairness constraints."}, {"description": "Changes to data may reduce its fidelity - the modified data points might not correspond to real-world scenarios, which could affect the model's applicability."}, {"description": "A too-stringent fairness constraint might force large changes that significantly degrade model accuracy, so there's a trade-off between fairness and how much you can safely alter the data."}],[],3,2
50,Relabelling,"Relabeling involves changing the class labels of certain training instances to correct historical bias or achieve fairness targets. By flipping some labels (e.g., marking some previously 'rejected' cases as 'accepted'), especially for underrepresented or disadvantaged groups, the model is trained on a counterfactually fairer dataset. This helps the model not learn biases present in the original labels (which might reflect past discrimination).",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Pre-Processing Techniques"}],[{"category": "Pre-Processing Techniques", "subcategory": "Label Modification"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Preprocessing and Feature Engineering"}],[{"description": "Updating the training labels in a past hiring dataset by marking some qualified female candidates (who were previously rejected) as 'hired' in order to teach the model that such profiles should be considered successful.", "goal": "Fairness"}, {"description": "In a loan approval dataset with evidence of bias, relabeling a portion of declined applications from a minority group to 'approved' when their profiles are similar to approved applicants from the majority group, to reduce learned bias.", "goal": "Fairness"}],[{"description": "Determining which labels to flip and how many relies on human judgment or bias metrics, which can be subjective or contentious."}, {"description": "Changing labels means the model is trained on hypothetical outcomes, which could reduce accuracy if those adjustments are not well-grounded."}, {"description": "Only addresses biases present in past labeling decisions; if features are biased or groups were treated differently in ways not captured just by labels, relabeling alone is insufficient."}],[],2,2
51,"Preferential Sampling","Preferential Sampling involves over-sampling or under-sampling data from certain groups to balance their representation in the training set. By ensuring that protected groups are neither under- nor over-represented, the model is trained on a more balanced dataset, which can prevent it from simply learning the majority group's patterns.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Pre-Processing Techniques"}],[{"category": "Pre-Processing Techniques", "subcategory": "Data Sampling"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Preprocessing and Feature Engineering"}],[{"description": "Doubling the instances of a minority class or protected group in the training data for a classification task so that the model sees roughly equal examples from both groups during training.", "goal": "Fairness"}, {"description": "Reducing the number of examples from an over-represented group in a dataset (or weighting them less in training) to prevent the model from being biased towards that group's characteristics.", "goal": "Fairness"}],[{"description": "Over-sampling can lead to overfitting on duplicated data points if not combined with techniques like slight data augmentation."}, {"description": "Under-sampling discards potentially useful data, which may decrease overall model performance or leave the model with too little information."}, {"description": "Only balances group representation; it doesn't change feature values or labels, so any biases in those aren't directly fixed by sampling alone."}],[],2,2
52,"Fairness Through Unawareness","\"Fairness through unawareness\" means designing the model such that it does not use protected attributes (like race, gender) as inputs. The idea is that if the model doesn't know a person's protected attribute, it cannot explicitly discriminate based on it. Essentially, you train the model without those attributes or any obvious proxies for them.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Feature Exclusion"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "Building a hiring model that excludes features like gender, ethnicity, and name (which could imply gender/ethnicity) from the input data, so the model makes decisions without those factors.", "goal": "Fairness"}, {"description": "Designing a loan approval system that does not use ZIP code or other location information to avoid redlining effects (as location might proxy for race), thereby attempting fairness by not providing such information to the model.", "goal": "Fairness"}],[{"description": "Does not prevent indirect discrimination – the model can still pick up on patterns in other features that correlate with the protected attribute (proxies)."}, {"description": "Ignores the protected attribute entirely, which means it also cannot actively correct for any bias; it simply hopes the model won't learn it inadvertently."}, {"description": "In some cases, using the protected attribute in a controlled way might actually improve fairness (e.g., to enforce fairness constraints), so unawareness can be a missed opportunity to actively manage bias."}],[],1,1
53,"Adversarial Debiasing","Adversarial debiasing trains a primary model to make accurate predictions while simultaneously training an adversary model to detect protected attributes from the primary model's outputs or hidden representations. The primary model is penalized whenever the adversary succeeds, pushing it to learn representations that do not carry information about the protected attribute, thereby reducing bias.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Adversarial"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "Training a classifier on financial data to predict loan default while an adversary tries to predict the applicant's race from the classifier's intermediate layers; the classifier learns to minimize default prediction error and also fool the adversary, resulting in race-neutral predictions.", "goal": "Fairness"}, {"description": "Applying adversarial debiasing in an image classification task, where the primary model predicts an attribute like beauty score and an adversary tries to identify the ethnicity of the person from the model's features, forcing the model to not encode ethnicity in order to maintain fairness.", "goal": "Fairness"}],[{"description": "Training involves a delicate balance and can be unstable (the predictor and adversary objectives can compete in ways that make convergence difficult)."}, {"description": "There may be a trade-off where the main task performance drops if the model has to hide too much information to fool the adversary."}, {"description": "Only addresses biases related to the specific protected attributes the adversary is trained on; it won't catch biases in absence of an adversary targeting those attributes."}],[],3,2
54,"Adversarial Debiasing for Text","This technique applies adversarial debiasing in NLP or other text-based models. For example, a text classifier or language model is trained with an adversary that tries to detect sensitive attributes (like gender or ethnicity inferred from the text) from the model's latent representations or outputs, forcing the model to learn representations that do not carry that bias. It's essentially the NLP-specific case of adversarial debiasing.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Adversarial"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "Debiasing a sentiment analysis model by using an adversary that attempts to identify the writer's gender from the model's internal state; the sentiment model learns to focus on content not gender-correlated style, yielding more gender-fair sentiment predictions.", "goal": "Fairness"}, {"description": "Training a resume screening NLP model where an adversary tries to guess the applicant's gender or race from the text representations (perhaps via name or language style), thereby pushing the system to ignore those cues and only consider qualifications-related text content.", "goal": "Fairness"}],[{"description": "Requires labeled or inferable protected attributes for text data to train the adversary (e.g., having gender labels or a way to guess them, which might be noisy)."}, {"description": "May cause the model to ignore certain linguistic features that are correlated with the protected attribute, even if they carry some legitimate information, potentially reducing nuance or accuracy."}, {"description": "The adversarial training can be slow and unstable, especially with large language models and complex text features, needing careful tuning to achieve the desired debiasing effect without degenerated language understanding."}],[],3,2
55,"Fair Adversarial Networks","Fair adversarial networks refer to deep learning models that incorporate adversarial training to enforce fairness. They extend the idea of adversarial debiasing specifically in neural network architectures, where a neural network (or GAN setup) is trained to make predictions while an adversary network tries to detect protected attributes from its intermediate outputs. The result is a model that, through adversarial pressure, learns internal representations that are debiased with respect to the specified protected attributes.",Model-Specific,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Adversarial"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "Training a deep neural network for loan approval with an embedded adversary that attempts to predict the applicant's gender from the network's hidden layers, ensuring the network suppresses gender information while still accurately predicting loan outcomes.", "goal": "Fairness"}, {"description": "Using a GAN-based model where the generator produces decisions or representations and a discriminator checks if those decisions are fair (cannot tell which group the input is from), thereby directly incorporating fairness into the model's learning process.", "goal": "Fairness"}],[{"description": "Inherits the complexities of adversarial training: finding the right balance between the main task and fairness adversary is difficult and training may be unstable."}, {"description": "Requires protected attribute labels for the adversary and plenty of data; it targets only known biases and may not generalize to other fairness concerns."}, {"description": "High computational cost due to training multiple networks simultaneously and possibly longer training times to reach equilibrium between model and adversary."}],[],3,3
56,"Prejudice Remover Regulariser","The Prejudice Remover regularizer is a fairness technique that adds a penalty term to the model's loss function to discourage dependence on protected attributes. During training, this regularization term penalizes outcomes where predictions carry information about a sensitive attribute, effectively guiding the model to 'remove prejudice' by treating the protected attribute as irrelevant in its decisions.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Regularization"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "Training a logistic regression for hiring decisions with an additional term in the loss that penalizes the model whenever its predictions differ significantly between male and female candidates of similar qualifications, thereby reducing gender bias.", "goal": "Fairness"}, {"description": "Applying a prejudice remover penalty to a classifier so that it minimizes classification error while also minimizing any correlation between the predicted outcome and the sensitive attribute (like race) in the training data.", "goal": "Fairness"}],[{"description": "Requires formally defining the fairness penalty; an improperly formulated penalty might not capture the fairness objective or could be too restrictive."}, {"description": "Can be difficult to calibrate – too much regularization may cause underfitting and drop in accuracy, whereas too little may not sufficiently mitigate bias."}, {"description": "Primarily developed for simpler models (like logistic regression); incorporating similar regularizers into complex models or multiple protected attributes scenarios can be complex."}],[],3,2
57,"Meta Fair Classifier","Meta Fair Classifier is a strategy that wraps around a standard classifier to directly optimize a chosen fairness metric along with accuracy. It uses a meta-learning approach to adjust the classifier's parameters (or decision threshold) in response to fairness constraints, effectively treating fairness as part of the optimization objective. This allows it to modify any underlying model to improve fairness outcomes like equal opportunity or demographic parity.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Meta-Algorithm"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "Taking a pre-trained classifier and iteratively adjusting its decision boundary using a meta algorithm so that the false positive rate for different racial groups becomes equal (satisfying equalized odds) while maintaining as much overall accuracy as possible.", "goal": "Fairness"}, {"description": "Applying a meta-learning loop on a random forest model to optimize not only for prediction accuracy on credit risk, but also to minimize the difference in loan approval rates between protected and unprotected groups.", "goal": "Fairness"}],[{"description": "Introduces additional complexity and computation in training, since it may involve multiple passes or an outer loop to adjust for fairness."}, {"description": "Effectiveness is tied to the specific fairness metric chosen; improving one fairness measure can sometimes worsen another or even overall accuracy beyond acceptable levels."}, {"description": "As a general approach, it may require careful tuning to achieve a good balance, and there's no one-size-fits-all guarantee – it provides a framework but still relies on the specifics of data and model."}],[],3,2
58,"Exponentiated Gradient Reduction","Exponentiated Gradient Reduction is an algorithm for finding a classifier that best balances accuracy and fairness constraints. It formulates the problem as a constrained optimization (maximize accuracy subject to fairness conditions, like bound on discrimination) and uses an iterative, multiplicative update approach (exponentiated gradients) to efficiently search over possible models or weighting of models. The output is often an optimal randomized classifier or a weighted combination of models that satisfies the fairness constraints as much as possible.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Constrained Optimization"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "Using exponentiated gradient to adjust the outputs of a set of classifiers such that the final decision rule meets equalized odds: for instance, combining two classifiers with different biases into a composite decision that equalizes true positive rates across groups.", "goal": "Fairness"}, {"description": "Applying this method in a lending scenario to find the best trade-off model that maintains high predictive accuracy while ensuring that the loan approval rate for a protected group is within a desired range of the rate for the unprotected group.", "goal": "Fairness"}],[{"description": "The solution can be a randomized classifier (a distribution over models), which can be harder to implement in practice as it may require randomization at prediction time."}, {"description": "Requires clearly defined fairness constraints and protected attribute labels to enforce during training, which might not capture all nuances of fairness."}, {"description": "Though efficient, the method is more mathematically involved; users need to understand its formulation or rely on existing implementations to use it correctly."}],[],3,2
59,"Fair Transfer Learning","Fair transfer learning ensures that when a model trained on one dataset or domain is adapted to another, it carries over fairness considerations. This means the adaptation process explicitly checks and preserves fairness metrics so that the transferred model remains fair in the new context, not just accurate. Techniques include re-weighting or fine-tuning with fairness constraints to prevent a model from becoming biased in the target domain.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Transfer Learning"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "Fine-tuning a facial recognition model trained predominantly on lighter-skinned faces for deployment in a diverse population, and during fine-tuning, including fairness constraints or data augmentation to improve accuracy on darker-skinned faces and reduce bias.", "goal": "Fairness"}, {"description": "Transferring a language toxicity detection model from one online community to another, while re-balancing or re-calibrating it to ensure that slang or dialect from a particular demographic in the new community is not unfairly flagged as toxic.", "goal": "Fairness"}],[{"description": "If the source model or data was highly biased, it may be very difficult to 'unlearn' those biases just through fine-tuning or transfer adjustments."}, {"description": "There is no single standard technique for fair transfer; it often involves custom adjustments that depend on understanding both source and target domain biases."}, {"description": "Assumes you have knowledge of protected attributes in the target domain to guide the fairness adjustments; without that, transferring fairness is difficult."}],[],3,2
60,"Adaptive Sensitive Reweighting","Adaptive Sensitive Reweighting is a training-time method that keeps track of how well the model is doing for each group and adjusts the emphasis on each group's data accordingly. If the model is performing worse on a certain group, this technique will give more weight to that group's data in the next training iterations, ensuring that the model pays more attention and improves performance for that group.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Adaptive Training"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "Training a classifier for a medical diagnosis where early on it has higher error for older patients; the algorithm increases the weight of those patients' records in subsequent training iterations to reduce the age-related performance gap.", "goal": "Fairness"}, {"description": "In a speech recognition model, noticing lower accuracy for a particular accent and automatically boosting the influence of training samples from that accent to encourage the model to learn those patterns better and equalize accuracy.", "goal": "Fairness"}],[{"description": "Needs ongoing measurement of performance per group during training; for very small groups, these estimates can be unstable or noisy."}, {"description": "Over-weighting a challenging group too much can degrade overall accuracy or lead to overfitting that group's specifics."}, {"description": "Adds complexity to the training procedure with a feedback loop that must be carefully tuned to actually improve balance without oscillation or divergence."}],[],3,2
61,"Multi-Accuracy Boosting","Multi-Accuracy Boosting is about making sure the model is accurate for all groups, not just overall. It identifies cases where the model is making mistakes disproportionately for a particular group, and then specifically focuses on fixing those errors (for example, by adding additional weak learners or adjusting the model) so that accuracy improves for that group without hurting others. The idea is similar to boosting, but with attention to subgroup performance.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Adaptive Training"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "After training an initial model for loan default prediction, using multi-accuracy boosting to train additional small models that specifically correct the errors made for, say, younger applicants or a certain income bracket, and combining them for a more uniformly accurate predictor.", "goal": "Fairness"}, {"description": "Improving an image classifier that has lower accuracy on images with darker lighting by focusing subsequent boosting rounds on those failure cases, thus raising performance on that subset without significantly affecting other subsets.", "goal": "Fairness"}],[{"description": "Requires pinpointing the subsets or patterns where accuracy is low; if the subgroups are not pre-defined (like protected groups), one must detect them from the error distribution, which can be complex."}, {"description": "The resulting model may be an ensemble, which is more complex than a single model, potentially making it harder to interpret and deploy."}, {"description": "Overemphasizing niche cases to improve subgroup accuracy could lead to overfitting those cases if not balanced, and it might yield diminishing returns if base model is already strong."}],[],3,2
62,"Equalised Odds Post-Processing","Equalized Odds post-processing adjusts the outputs of a trained classifier to satisfy equalized odds, meaning it aims to equalize true positive rates and false positive rates across protected groups. It typically works by using different decision thresholds for each group or introducing randomized decisions for some outputs, such that each group ends up with similar error rates, all while leaving the classifier's scores unchanged.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Post-Processing Techniques"}],[{"category": "Post-Processing Techniques", "subcategory": "Outcome Adjustment"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Postprocessing"}],[{"description": "After training a risk prediction model, applying a post-processor that, for example, raises the acceptance threshold for a group with a higher false positive rate until their error rates match those of other groups, ensuring fairness in false alarms and misses.", "goal": "Fairness"}, {"description": "Adjusting the outputs of a face recognition system by calibrating confidence score cutoffs per demographic so that the frequency of incorrect identifications (false positives) is uniform across races.", "goal": "Fairness"}],[{"description": "Requires protected attribute information at prediction time to apply group-specific adjustments, which might not be feasible or desirable in deployment."}, {"description": "Can slightly reduce overall accuracy, as some predictions are intentionally changed (or randomized) to meet fairness criteria rather than strictly follow the model's confidence."}, {"description": "Introduces different operating thresholds for different groups, which may raise legal or ethical questions about applying varying standards, even if for fairness reasons."}],[],3,2
63,"Threshold Optimiser","Threshold optimization in fairness involves finding distinct decision thresholds for different groups on a model's score output to satisfy a fairness constraint (such as demographic parity or equal opportunity). Instead of one cutoff for all, each group might have its own cutoff score for the positive class. These thresholds are chosen so that a specified fairness metric is achieved, effectively post-processing the model's decisions group-wise.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Post-Processing Techniques"}],[{"category": "Post-Processing Techniques", "subcategory": "Outcome Adjustment"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Postprocessing"}],[{"description": "Setting a slightly lower acceptance score threshold for a group that historically had lower positive rates in a loan model, and a higher threshold for a group with higher positive rates, until the approved loan percentages are equalized between groups.", "goal": "Fairness"}, {"description": "Determining two different probability cutoffs for a disease prediction (one for male patients, one for female patients) such that the true positive detection rate of the disease is the same for both genders.", "goal": "Fairness"}],[{"description": "Requires explicit use of protected group identifiers at inference time to apply different thresholds, raising similar deployment concerns as other group-specific post-processing."}, {"description": "If there are many groups or intersectional groups to consider, managing distinct thresholds becomes impractical or could lead to inconsistency."}, {"description": "Optimizing thresholds for fairness might sacrifice some individual-level accuracy or consistency (someone just below threshold in one group might have been above it in another group for same score)."}],[],3,2
64,"Reject Option Classification","Reject Option Classification is a post-processing method where the classifier is allowed to abstain or alter decisions in an uncertainty band to improve fairness. Typically, for instances where the model's confidence is not high (near the decision boundary), if the instance belongs to a disadvantaged group, the method will change the outcome in favor of that group. In other words, within a 'reject option' range of scores, decisions are flipped to the favorable outcome for the protected group to mitigate bias in those borderline cases.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Post-Processing Techniques"}],[{"category": "Post-Processing Techniques", "subcategory": "Outcome Adjustment"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Postprocessing"}],[{"description": "In a credit scoring system, for applicants whose scores are around the approval cutoff, if the applicant is from a minority group that faces bias, the system will opt to approve rather than reject, to counteract potential bias in uncertain cases.", "goal": "Fairness"}, {"description": "For a predictive policing model, if a case is on the threshold of being flagged (low confidence), and it involves a community that has historically been over-targeted, the reject option method would decide not to flag it to avoid reinforcing bias in borderline situations.", "goal": "Fairness"}],[{"description": "Only affects cases near the decision boundary; it doesn't correct decisions where the model is very confident (either correctly or incorrectly)."}, {"description": "Requires identifying the 'reject option' range and the disadvantaged group; if these are not well-chosen, it might do little or even introduce new bias."}, {"description": "From an operational perspective, it explicitly treats similar cases differently based on group, which can be contentious and must be justified by the bias reduction in those ambiguous cases."}],[],3,2
65,"Calibration with Equality of Opportunity","This approach adjusts a model's probability estimates and decision threshold in each group so that the model is well-calibrated within each group and satisfies the equal opportunity criterion (equal true positive rates across groups). In practice, it means aligning the predicted positive rates with actual outcomes for each score range per group, while also ensuring each group’s qualified members are selected at similar rates.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Post-Processing Techniques"}],[{"category": "Post-Processing Techniques", "subcategory": "Outcome Adjustment"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Postprocessing"}],[{"description": "Tuning a loan default probability model such that a predicted risk score corresponds to the same default likelihood for different ethnic groups, and setting group-specific cutoffs so that the percentage of actual defaulters approved (true positive rate complement) is equal across groups.", "goal": "Fairness"}, {"description": "Adjusting a medical diagnostic test's scoring for different demographic groups to ensure that a given risk score has the same meaning (chance of disease) across groups and that the test catches the same fraction of true cases in each group.", "goal": "Fairness"}],[{"description": "Requires group-specific calibration and thresholds, which means the model must explicitly account for protected attributes when making final decisions."}, {"description": "Achieving both perfect calibration and equal opportunity simultaneously might be infeasible if the original model has disparities; often a compromise must be made."}, {"description": "Involves a complex balancing act between accuracy, calibration, and fairness; focusing on these criteria may slightly reduce overall accuracy or calibration on the combined population."}],[],3,2
66,"Statistical Parity Difference","Statistical Parity Difference is a fairness metric that measures the difference in the rate of positive outcomes between a protected group and an unprotected group. A value of 0 indicates both groups receive positive outcomes at equal rates (demographic parity). A large difference suggests potential bias in selection rates.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Group Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "Computing that a company hires 50% of male applicants but only 30% of female applicants, yielding a statistical parity difference of 20 percentage points (female minus male = -0.20), which flags a bias in hiring rates.", "goal": "Fairness"}, {"description": "Evaluating a loan algorithm by comparing approval rates: if Group A is approved 40% of the time and Group B 45%, the statistical parity difference is -0.05 for Group A (suggesting Group A is slightly disadvantaged).", "goal": "Fairness"}],[{"description": "It's a coarse measure that ignores qualifications; equalizing selection rates might require selecting less qualified individuals from one group or rejecting qualified individuals from another, potentially conflicting with merit-based outcomes."}, {"description": "Focuses only on outcome rates, not error rates or model correctness; a process can have equal selection rates but still be unfair in how errors are distributed."}, {"description": "Doesn't indicate which direction the bias is without context (positive or negative difference), and enforcing zero difference might not always be feasible or desirable depending on context."}],[],1,1
67,"Disparate Impact","Disparate Impact is a metric often used in legal contexts to assess if a protected group receives positive outcomes at a rate less than a certain fraction (usually 80%) of the rate of an advantaged group. It is essentially the ratio of outcome rates between groups. If the ratio falls below the threshold (e.g., 0.8), it suggests potentially unlawful discrimination absent a business necessity justification.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Group Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "Checking a promotion rate: if 30% of Group X and 50% of Group Y get promoted, the disparate impact ratio is 0.6 (30/50), failing the 80% rule and indicating a potential bias against Group X.", "goal": "Fairness"}, {"description": "Auditing a credit model by calculating that the approval rate for minority applicants is 85% of that of white applicants, which passes the 80% test (since 0.85 > 0.8) suggesting no strong disparate impact under that rule.", "goal": "Fairness"}],[{"description": "The 80% rule is somewhat arbitrary and a blunt instrument; a ratio slightly below 0.8 might or might not be practically significant, and above 0.8 doesn't guarantee fairness."}, {"description": "Doesn't consider the qualifications or base rates of the groups; focusing solely on the ratio could incentivize hiring or selecting by quota to avoid violations, regardless of merit."}, {"description": "As a diagnostic, it flags imbalance but doesn't provide guidance on how to fix it without potentially complex trade-offs."}],[],1,1
68,"Demographic Parity","Demographic Parity (also called Statistical Parity) is a fairness criterion that requires the decision outcome to be independent of the protected attribute. In practice, it means each demographic group should have the same probability of receiving the positive outcome. A classifier satisfies demographic parity if P(Ĥ = 1 | Group A) = P(Ĥ = 1 | Group B), for all groups.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Group Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "Designing a college admission process under demographic parity: if 10% of applicants are admitted overall, then ideally 10% of applicants from each racial group would be admitted as well.", "goal": "Fairness"}, {"description": "Evaluating an algorithm by checking if the loan approval rate is the same for minority and majority applicants, which would indicate demographic parity in lending decisions.", "goal": "Fairness"}],[{"description": "Ignores differences in actual qualifications or rates of positive outcomes in reality; forcing demographic parity might require interventions that conflict with accuracy or business objectives."}, {"description": "Does not address how errors are distributed; it's possible to satisfy demographic parity but still have one group experience more false negatives or false positives."}, {"description": "If a protected attribute truly correlates with the outcome due to external factors (not bias), achieving parity could lead to less useful or even harmful predictions (e.g., giving loans where default risk is higher just to balance rates)."}],[],1,1
69,"Equal Opportunity Difference","Equal Opportunity is a fairness concept requiring that true positive rates (the probability of correctly giving a positive outcome to those who deserve it) are the same across groups. The Equal Opportunity Difference measures the disparity in true positive rates between a protected and unprotected group. A difference of 0 means both groups' qualified members are selected at equal rates.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Group Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "In a cancer screening test, ensuring that if 90% of sick patients from Group A are correctly identified, then about 90% of sick patients from Group B are also identified (equal opportunity), and measuring any gap as the difference in these percentages.", "goal": "Fairness"}, {"description": "Evaluating a hiring algorithm by comparing the hire rate among top-qualified male candidates to the hire rate among top-qualified female candidates; a small difference indicates the model meets the equal opportunity criterion closely.", "goal": "Fairness"}],[{"description": "Focuses only on true positive rates (people who actually qualify); it does not consider false positive disparities (which group might get more unqualified selections)."}, {"description": "Requires ground truth labels of who actually qualifies or deserves a positive outcome, which might not be straightforward to define in some contexts."}, {"description": "Optimizing for equal opportunity might result in adjusting thresholds in a way that could increase false positives for the group with lower TPR, so it's often considered alongside other metrics like false positive rate differences (equalized odds)."}],[],1,1
70,"Average Odds Difference","Average Odds Difference is a composite fairness metric that averages the differences in true positive rates and false positive rates between a protected and unprotected group. It essentially measures how far a classifier is from achieving equalized odds (equal TPR and FPR across groups). A value of 0 indicates the model has equal accuracy and error rates for both groups.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Group Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "Evaluating a recidivism risk model by calculating the gap in true positive rates and false positive rates for defendants of different races, and averaging them to get a single fairness score (with 0 meaning both error rates and hit rates are the same for all races).", "goal": "Fairness"}, {"description": "Comparing two credit models: one has an average odds difference of 0.02 and another 0.15 between demographics, indicating the first model is much closer to fair (small disparity in both approval of creditworthy applicants and denial of non-creditworthy applicants).", "goal": "Fairness"}],[{"description": "By averaging TPR and FPR differences, it might mask a scenario where one is high and the other is low; two models with different kinds of unfairness could get the same average score."}, {"description": "It requires known protected group labels to compute, and it's a diagnostic measure rather than a direct fix - one still needs to adjust the model to improve it."}, {"description": "As a single number, it doesn't convey which aspect (false positives or false negatives disparity) is contributing more to the unfairness, so further analysis is needed to pinpoint issues."}],[],1,1
71,"Individual Fairness Metric (Consistency)","This metric evaluates how consistent a model's decisions are for similar individuals. Typically, one computes the extent to which the model gives the same prediction to individuals who are alike in all relevant aspects. A perfectly individually fair model would treat every pair of sufficiently similar individuals identically in terms of outcomes.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Individual Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Individual Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "For a lending model, measuring consistency by taking each loan applicant and comparing the model's decision with decisions for other applicants with very similar credit profiles. A high rate of identical outcomes among these neighbors would indicate strong individual fairness.", "goal": "Fairness"}, {"description": "Auditing a resume screening tool by checking sets of resumes that are nearly identical (except maybe name or minor details) to see if the tool's decisions differ. Consistency would be high if such similar resumes always yield the same hiring recommendation.", "goal": "Fairness"}],[{"description": "Requires a definition of 'similarity' between individuals, which can be subjective and difficult to formalize without inadvertently including protected characteristics."}, {"description": "High-dimensional data can make it hard to find truly similar individuals, and the metric might be noisy if nearest neighbors are not that close in important features."}, {"description": "It's a diagnostic metric; ensuring individual fairness often requires fundamentally different modeling approaches or constraints, not just post-hoc checks."}],[],1,1
72,"Algorithmic Fairness using K-NN","This approach uses the idea of K-Nearest Neighbors to assess individual fairness. It looks at each data point's nearest neighbors (based on relevant features) and compares the model's decisions within these small clusters. If people who are very similar (neighbors) are getting different outcomes, the model may be individually unfair. The K-NN fairness check quantifies how often this happens.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Individual Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Individual Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "Analyzing a university admissions algorithm by taking each applicant and checking the admission decisions of the 5 most similar other applicants (in terms of grades, scores, etc.). If many such groups contain both admits and rejects inconsistently, it signals an individual fairness problem.", "goal": "Fairness"}, {"description": "Using a K-NN approach on a mortgage approval model: for each approved applicant, see if there are almost identical applicants who were denied (or vice versa). A low incidence of these inconsistencies would indicate the model is treating similar applicants similarly.", "goal": "Fairness"}],[{"description": "The notion of 'nearest neighbors' depends on the feature space and distance metric chosen, which might not capture true similarity in terms of what should be relevant for decisions."}, {"description": "Computationally, checking K-NN for every data point can be expensive for large datasets, although sampling or approximate methods can be used."}, {"description": "This method identifies potential individual fairness issues but doesn't directly tell how to fix them; it's more a diagnostic tool."}],[],2,1
73,"Counterfactual Fairness (Causal Modeling)","Counterfactual fairness is the idea that if you took a person and changed their protected attribute (like race or gender) but everything else remained the same, the model's prediction should not change. In practice, testing this often requires a causal model of the data to generate these 'what if' scenarios. A model is counterfactually fair if its decisions are the same in these counterfactual worlds where only the protected attribute differs.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Individual Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Individual Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "Developing a hiring model and validating it by using a causal model: e.g., simulate a candidate being of a different gender (while holding qualifications constant as defined by the causal graph) and check that the hiring decision doesn't change. If it remains the same for all candidates, the model is counterfactually fair with respect to gender.", "goal": "Fairness"}, {"description": "In a credit scoring context, building a structural causal model of factors like income, education (and how these might be influenced by race), then ensuring the credit model's output for a person is the same in a counterfactual scenario where that person's race is changed (but their financial history stays consistent with the new race's typical context).", "goal": "Fairness"}],[{"description": "Requires a well-specified causal model of the domain, which is often difficult to obtain or validate; mistakes in the causal model can lead to incorrect fairness conclusions."}, {"description": "In complex systems, it's challenging to change a protected attribute 'and only its legitimate effects' – separating causal pathways into acceptable and unacceptable influences can be contentious."}, {"description": "Achieving strict counterfactual fairness might be impossible if the protected attribute has pervasive influence on many features (some of which might be considered legitimately relevant), potentially making the fairness criterion too strict for practical use."}],[],3,3
74,"Path-Specific Counterfactual Fairness","Path-specific counterfactual fairness is a nuanced approach in causal fairness that allows certain causal pathways from a protected attribute to influence the decision, while blocking others deemed unfair. It defines fairness constraints on specific paths in the causal model. For example, gender might influence job performance (a fair path) which influences promotion, but not directly influence promotion decisions; the model is fair if gender affects output only through allowed paths like performance.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Individual Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Individual Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "In a college admissions scenario, using a causal model to allow race to influence admission only through paths like socioeconomic disadvantage (if one deems compensatory consideration as fair) but not through paths like bias in test scores. The admission model is then constrained to be fair in this path-specific sense.", "goal": "Fairness"}, {"description": "Designing a lending model where a protected attribute (say, age) can affect creditworthiness via income history (allowed path), but should not affect the decision directly or via irrelevant factors. The model is tested and adjusted so that age's effect on approval exists only through income (as specified in the causal graph).", "goal": "Fairness"}],[{"description": "Even more complex than basic counterfactual fairness; it demands consensus on which causal paths are 'fair' versus 'unfair' – a subjective and context-dependent judgment."}, {"description": "Building and validating a detailed causal model with path annotations is labor-intensive and requires expertise; any omission or error in the model can undermine the fairness guarantees."}, {"description": "Hard to implement in practice for large-scale models; often feasible only in simplified simulations or scenarios where causal relations are reasonably well-understood."}],[],3,3
75,"Causal Fairness Assessment with Do-Calculus","This approach uses formal causal inference (Pearl's do-calculus) to assess and ensure fairness. It involves constructing a causal model of the factors leading to decisions and then using interventions (the do-operator) to simulate changes in protected attributes. By comparing outcomes under these interventions, one can quantify bias and adjust the model or decision policy to satisfy fairness criteria (like ensuring P(outcome | do(Group=A)) is similar to P(outcome | do(Group=B))).",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Metrics"}],[{"category": "Metrics", "subcategory": "Individual Fairness Metrics"}],[{"type": "Fairness Approach", "value": "Individual Fairness"}, {"type": "Project Lifecycle Stage", "value": "Evaluation"}],[{"description": "Building a causal graph for a hiring process (including nodes for skills, bias in interview, gender, etc.) and using do-calculus to calculate the probability of being hired when setting gender to male vs female. If there's a difference, the process is unfair; the model or process is then modified until these do-intervention outcomes are equal.", "goal": "Fairness"}, {"description": "Using a structural causal model for credit approvals to test fairness: computing approval odds under an intervention that changes an applicant's race while keeping all other factors (like income, debt) causally consistent. This helps identify unfair race effects which can be addressed in the model.", "goal": "Fairness"}],[{"description": "Relies on having a correct and comprehensive causal model; any missing confounders or incorrect causal assumptions can lead to wrong conclusions about fairness."}, {"description": "Do-calculus computations can become intractable in complex models with many variables, limiting this approach to relatively simpler or well-structured problems."}, {"description": "Highly technical and not commonly used in industry due to the expertise required; mostly seen in academic research or very controlled decision settings."}],[],3,3
76,"Diversity Constraints in Recommendations","This technique introduces fairness or diversity requirements into recommendation systems. The idea is to ensure that recommended items are not homogeneous or biased – for example, content from different creators or catering to different user groups gets fair exposure. Constraints or re-ranking algorithms can enforce that the set of recommendations covers a variety of sources or topics, preventing minority groups or niche content from being systematically under-represented.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Post-Processing Techniques"}],[{"category": "Post-Processing Techniques", "subcategory": "Re-ranking"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Postprocessing"}],[{"description": "A news recommendation platform implementing a constraint that not all recommended articles come from the same political viewpoint or the same few publishers, ensuring users see a mix (which also gives smaller outlets a chance to be seen).", "goal": "Fairness"}, {"description": "A music streaming service adjusting its playlist generation so that artists from underrepresented genres appear alongside mainstream artists, increasing the diversity of music exposure for listeners.", "goal": "Fairness"}],[{"description": "Enforcing diversity might lower the immediate relevance or user satisfaction if users have very specific tastes, as they may see some content outside their usual preference."}, {"description": "Needs careful tuning to avoid tokenism (just adding diversity for its own sake without regard to quality) – there's a trade-off between diversity and personalization accuracy."}, {"description": "What 'diversity' means must be defined (e.g., diversity in genre, provider, demographics, etc.), and incorrect assumptions could lead to unintended results or perceived bias in another form."}],[],2,2
77,"Bayesian Fairness Regularization","Bayesian Fairness Regularization incorporates fairness constraints or priors into a Bayesian modeling framework. For example, one can impose a prior that strongly favors models with small differences in outcomes across groups, or include fairness as a term in the Bayesian objective. This way, the posterior distribution of model parameters will naturally tend to satisfy fairness criteria, and one can quantify uncertainty about fairness as well.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "In-Processing Techniques"}],[{"category": "In-Processing Techniques", "subcategory": "Regularization"}],[{"type": "Fairness Approach", "value": "Group Fairness"}, {"type": "Project Lifecycle Stage", "value": "Modeling"}],[{"description": "Training a Bayesian logistic regression for hiring, with a prior that weights on 'gender' or its proxies are centered near zero with low variance. The posterior then reflects models that likely have negligible gender influence unless data overwhelmingly suggests otherwise, thus promoting fairness.", "goal": "Fairness"}, {"description": "Applying a Bayesian framework to a credit scoring model where a fairness penalty (like differences in group default predictions) is encoded in the prior or as a regularization term. The result is a distribution over models that trade off fit to data with adherence to fairness, giving a probabilistic view of fairness compliance.", "goal": "Fairness"}],[{"description": "Selecting appropriate priors or constraints is non-trivial and can be subjective; a mis-specified prior might bias the model in undesirable ways or be too weak to ensure fairness."}, {"description": "Bayesian methods can be computationally intensive, especially for complex models or large datasets, which may limit their practicality."}, {"description": "Interpreting and communicating a probabilistic fairness guarantee is more complex than a single model output; stakeholders might find it harder to understand 'fairness with uncertainty' compared to hard constraints."}],[],3,3
78,"SHAP Values for Fairness","This involves using SHAP (SHapley Additive exPlanations) to interpret model predictions for different demographic groups and identify potential biases. By examining the average SHAP contributions of protected attributes or correlated features to the model's output, one can diagnose whether and how much those features are influencing decisions. It helps highlight bias by attributing prediction disparities to specific features.",Model-Agnostic,Fairness,[{"goal": "Fairness", "category": "Fairness Explanations"}],[{"category": "Fairness Explanations", "subcategory": "Feature Attribution"}],[{"type": "Fairness Approach", "value": "Transparency"}, {"type": "Project Lifecycle Stage", "value": "Postprocessing"}],[{"description": "Auditing a lending model by computing SHAP values for each feature across applicants. If 'ZIP code' (a proxy for race) consistently has a large negative SHAP value for minority applicants, it reveals a source of bias in the model's decisions.", "goal": "Fairness"}, {"description": "Using SHAP plots to compare feature importance for male vs female candidates in a hiring model. For example, if 'years of experience' is weighted differently depending on gender, as evidenced by different SHAP value distributions, it signals an inconsistency that might be addressed.", "goal": "Fairness"}],[{"description": "SHAP is an explanation tool, not a debiasing method; it helps detect and understand bias but does not automatically fix it."}, {"description": "Interpretation of SHAP results requires care – correlated features can share importance, and seeing a protected feature with low SHAP value doesn't guarantee fairness if proxies exist."}, {"description": "Computing SHAP can be computationally expensive for large models or datasets, and results can be complex to aggregate for group-level conclusions."}],[],3,2
