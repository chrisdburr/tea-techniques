{
  "slug": "empirical-calibration",
  "name": "Empirical Calibration",
  "acronym": null,
  "description": "Empirical calibration adjusts a model's predicted probabilities to match observed frequencies. For example, if events predicted with 80% confidence only occur 60% of the time, calibration would correct this overconfidence. Common techniques include Platt scaling and isotonic regression, which learn transformations that map the model's raw scores to well-calibrated probabilities, improving the reliability of confidence measures for downstream decisions.",
  "complexity_rating": 2,
  "computational_cost_rating": 2,
  "assurance_goals": [
    {
      "id": 3,
      "name": "Reliability"
    },
    {
      "id": 6,
      "name": "Transparency"
    },
    {
      "id": 2,
      "name": "Fairness"
    }
  ],
  "tags": [
    {
      "name": "applicable-models/agnostic",
      "id": 1
    },
    {
      "name": "assurance-goal-category/reliability",
      "id": 27
    },
    {
      "name": "assurance-goal-category/transparency",
      "id": 31
    },
    {
      "name": "assurance-goal-category/fairness",
      "id": 20
    },
    {
      "name": "data-requirements/calibration-set",
      "id": 36
    },
    {
      "name": "data-type/any",
      "id": 46
    },
    {
      "name": "evidence-type/quantitative-metric",
      "id": 59
    },
    {
      "name": "expertise-needed/statistics",
      "id": 79
    },
    {
      "name": "explanatory-scope/global",
      "id": 80
    },
    {
      "name": "lifecycle-stage/system-deployment-and-use",
      "id": 103
    },
    {
      "name": "technique-type/algorithmic",
      "id": 107
    }
  ],
  "related_techniques": ["temperature-scaling"],
  "related_technique_slugs": ["temperature-scaling"],
  "resources": [
    {
      "title": "google/empirical_calibration",
      "url": "https://github.com/google/empirical_calibration",
      "authors": "",
      "publication_date": null,
      "source_type": "software_package",
      "resource_type": "documentation"
    },
    {
      "title": "A Python Library For Empirical Calibration",
      "url": "http://arxiv.org/pdf/1906.11920v2",
      "authors": ["Xiaojing Wang", "Jingang Miao", "Yunting Sun"],
      "publication_date": "2019-07-25",
      "source_type": "technical_paper",
      "resource_type": "documentation"
    },
    {
      "title": "Assessing the effectiveness of empirical calibration under different bias scenarios",
      "url": "http://arxiv.org/pdf/2111.04233v2",
      "authors": ["Hon Hwang", "Juan C Quiroz", "Blanca Gallego"],
      "publication_date": "2021-11-08",
      "source_type": "technical_paper",
      "resource_type": "documentation"
    }
  ],
  "example_use_cases": [
    {
      "description": "Adjusting a credit default prediction model's probabilities to ensure that loan applicants with a predicted 30% default risk actually default 30% of the time, improving decision-making.",
      "goal": "Reliability"
    },
    {
      "description": "Calibrating a medical diagnosis model's confidence scores so that stakeholders can meaningfully interpret probability outputs, enabling doctors to make informed decisions about treatment urgency based on reliable confidence estimates.",
      "goal": "Transparency"
    },
    {
      "description": "Ensuring that a hiring algorithm's confidence scores are equally well-calibrated across different demographic groups, preventing systematically overconfident predictions for certain populations that could lead to biased decision-making.",
      "goal": "Fairness"
    }
  ],
  "limitations": [
    {
      "description": "Requires a separate held-out calibration dataset, which reduces the amount of data available for model training."
    },
    {
      "description": "Calibration performance can degrade over time if the underlying data distribution shifts, requiring periodic recalibration."
    },
    {
      "description": "May sacrifice some discriminative power in favour of calibration, potentially reducing the model's ability to distinguish between classes."
    },
    {
      "description": "Calibration methods assume that the calibration set is representative of future data, which may not hold in dynamic environments."
    }
  ]
}
