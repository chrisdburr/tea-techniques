{
  "tag": {
    "name": "technique-type/process",
    "slug": "technique-type-process",
    "count": 4,
    "category": "technique-type"
  },
  "techniques": [
    {
      "slug": "internal-review-boards",
      "name": "Internal Review Boards",
      "description": "Internal Review Boards (IRBs) provide independent, systematic evaluation of AI/ML projects throughout their lifecycle to identify ethical, safety, and societal risks before they materialise. Typically composed of multidisciplinary experts including ethicists, domain specialists, legal counsel, community representatives, and technical staff, IRBs review project proposals, assess potential harms to individuals and communities, evaluate mitigation strategies, and establish ongoing monitoring requirements. Unlike traditional research ethics committees, AI-focused IRBs address algorithmic bias, fairness concerns, privacy implications, and societal impact at scale, providing essential governance for responsible AI development and deployment.",
      "assurance_goals": [
        "Safety",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/safety",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/qualitative-report",
        "evidence-type/governance-framework",
        "expertise-needed/ethics",
        "expertise-needed/legal",
        "expertise-needed/domain-expertise",
        "lifecycle-stage/project-planning",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/process"
      ],
      "example_use_cases": [
        {
          "description": "Reviewing a proposed criminal risk assessment tool to evaluate potential discriminatory impacts, privacy implications, and societal consequences before development begins, ensuring vulnerable communities are protected from algorithmic harm.",
          "goal": "Safety"
        },
        {
          "description": "Evaluating a hiring algorithm for bias across demographic groups, requiring algorithmic audits and ongoing monitoring to ensure equitable treatment of all candidates and compliance with employment law.",
          "goal": "Fairness"
        },
        {
          "description": "Establishing transparent governance processes for a healthcare AI system, requiring clear documentation of decision-making criteria, model limitations, and performance metrics that can be communicated to patients and regulators.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Can significantly slow development timelines and increase project costs, potentially making organisations less competitive or delaying beneficial AI applications from reaching users."
        },
        {
          "description": "Effectiveness heavily depends on board composition, with inadequate diversity or expertise leading to blind spots in risk assessment and biased decision-making."
        },
        {
          "description": "May face internal pressure to approve revenue-generating projects or strategic initiatives, compromising independence and rigorous ethical evaluation."
        },
        {
          "description": "Limited authority or enforcement mechanisms can result in recommendations being ignored, particularly when they conflict with business objectives or technical constraints."
        },
        {
          "description": "Risk of becoming bureaucratic or box-ticking exercises rather than substantive evaluations, especially in organisations without strong ethical leadership or clear accountability structures."
        }
      ],
      "resources": [
        {
          "title": "Investigating Algorithm Review Boards for Organizational Responsible Artificial Intelligence Governance",
          "url": "https://link.springer.com/article/10.1007/s43681-024-00574-8",
          "source_type": "technical_paper",
          "authors": [
            "Emily Hadley",
            "Alan Blatecky",
            "Megan Comfort"
          ],
          "publication_date": "2024-09-16",
          "description": "Research on how organizations can establish algorithm review boards to govern and mitigate risks in AI deployment across sectors"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 1,
      "acronym": "IRBs",
      "related_techniques": [
        "red-teaming",
        "human-in-the-loop-safeguards",
        "confidence-thresholding",
        "runtime-monitoring-and-circuit-breakers"
      ]
    },
    {
      "slug": "human-in-the-loop-safeguards",
      "name": "Human-in-the-Loop Safeguards",
      "description": "Human-in-the-loop safeguards establish systematic checkpoints where human experts review, validate, or override AI/ML system decisions before they take effect. This governance approach combines automated efficiency with human judgement by defining clear intervention criteria (such as uncertainty thresholds, risk levels, or sensitive contexts) that trigger mandatory human oversight. By incorporating domain expertise, ethical considerations, and contextual understanding that machines may lack, these safeguards help ensure that critical decisions maintain appropriate human accountability whilst preserving the benefits of automated processing for routine cases.",
      "assurance_goals": [
        "Safety",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/safety",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/qualitative-report",
        "expertise-needed/domain-knowledge",
        "expertise-needed/stakeholder-engagement",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/process"
      ],
      "example_use_cases": [
        {
          "description": "Implementing mandatory human physician review for any medical AI diagnostic recommendation before treatment decisions are made, especially for complex cases or when the system confidence is below established thresholds, ensuring patient safety through expert oversight.",
          "goal": "Safety"
        },
        {
          "description": "Requiring human review of automated loan approval decisions when applicants request explanations or appeal rejections, allowing human underwriters to provide clear reasoning and ensure customers understand the decision-making process behind their application outcomes.",
          "goal": "Transparency"
        },
        {
          "description": "Mandating human oversight when hiring algorithms flag candidates from underrepresented groups for rejection, enabling recruiters to verify that decisions are based on legitimate job-relevant criteria rather than potential algorithmic bias, and providing fair recourse mechanisms.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Scales poorly with high request volumes, creating bottlenecks that can delay critical decisions and potentially overwhelm human reviewers with excessive workload."
        },
        {
          "description": "Introduces significant latency into automated processes, potentially making time-sensitive applications impractical or reducing user satisfaction with slower response times."
        },
        {
          "description": "Human reviewers may experience decision fatigue, leading to decreased attention quality over time and potential inconsistency in review standards across different cases or time periods."
        },
        {
          "description": "Risk of automation bias where humans defer too readily to AI recommendations rather than providing meaningful independent review, undermining the safeguard's effectiveness."
        },
        {
          "description": "Requires significant ongoing investment in human resources, training, and expertise maintenance, making it expensive to implement and sustain across large-scale systems."
        }
      ],
      "resources": [
        {
          "title": "Human-in-the-Loop AI: A Comprehensive Guide",
          "url": "https://www.holisticai.com/blog/human-in-the-loop-ai",
          "source_type": "tutorial",
          "description": "Comprehensive guide covering HITL AI collaborative approach, including human oversight throughout AI lifecycle, bias mitigation, ethical alignment, and applications across healthcare, manufacturing, and finance"
        },
        {
          "title": "Improving the Applicability of AI for Psychiatric Applications through Human-in-the-loop Methodologies",
          "url": "https://core.ac.uk/download/544064129.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Chandler, Chelsea",
            "Elvev√•g, Brita",
            "Foltz, Peter W."
          ],
          "publication_date": "2022-01-01",
          "description": "Technical paper exploring HITL methodologies for psychiatric AI applications, focusing on improving applicability and clinical effectiveness through human oversight integration"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "related_techniques": [
        "internal-review-boards",
        "red-teaming",
        "confidence-thresholding",
        "runtime-monitoring-and-circuit-breakers"
      ]
    },
    {
      "slug": "mlflow-experiment-tracking",
      "name": "MLflow Experiment Tracking",
      "description": "MLflow is an open-source platform that tracks machine learning experiments by automatically logging parameters, metrics, models, and artifacts throughout the ML lifecycle. It provides a centralised repository for comparing different experimental runs, reproducing results, and managing model versions. Teams can track hyperparameters, evaluation metrics, model files, and execution environment details, creating a comprehensive audit trail that supports collaboration, reproducibility, and regulatory compliance across the entire machine learning development process.",
      "assurance_goals": [
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/documentation",
        "expertise-needed/ml-engineering",
        "expertise-needed/software-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/process"
      ],
      "example_use_cases": [
        {
          "description": "Tracking medical diagnosis model experiments across different hospitals, logging hyperparameters, performance metrics, and model artifacts to ensure reproducible research and enable regulatory audits of model development processes.",
          "goal": "Transparency"
        },
        {
          "description": "Managing fraud detection model versions in production, tracking which specific model configuration and training data version is deployed, enabling quick rollback and performance comparison when system reliability issues arise.",
          "goal": "Reliability"
        },
        {
          "description": "Documenting loan approval model experiments with complete parameter tracking and performance logging across demographic groups, supporting fair lending compliance by providing transparent records of model development and validation processes.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires teams to adopt disciplined logging practices and may introduce overhead to development workflows if not properly integrated into existing processes."
        },
        {
          "description": "Storage costs can grow substantially with extensive artifact logging, especially for large models or high-frequency experimentation."
        },
        {
          "description": "Tracking quality depends on developers consistently logging relevant information, with incomplete logging leading to gaps in experimental records."
        },
        {
          "description": "Complex multi-stage pipelines may require custom instrumentation to capture dependencies and data flow relationships effectively."
        },
        {
          "description": "Security and access control configurations require careful setup to protect sensitive model information and experimental data in shared environments."
        }
      ],
      "resources": [
        {
          "title": "MLflow Documentation",
          "url": "https://mlflow.org/docs/latest/index.html",
          "source_type": "documentation",
          "description": "Comprehensive official documentation covering MLflow setup, tracking APIs, model management, and deployment workflows with examples and best practices"
        },
        {
          "title": "mlflow/mlflow",
          "url": "https://github.com/mlflow/mlflow",
          "source_type": "software_package",
          "description": "Official MLflow open-source repository containing the complete platform for ML experiment tracking, model management, and deployment"
        },
        {
          "title": "An MLOps Framework for Explainable Network Intrusion Detection with MLflow",
          "url": "https://ieeexplore.ieee.org/abstract/document/10733700",
          "source_type": "technical_paper",
          "authors": [
            "Vincenzo Spadari",
            "Francesco Cerasuolo",
            "Giampaolo Bovenzi",
            "Antonio Pescap√®"
          ],
          "publication_date": "2024-06-26",
          "description": "Research paper demonstrating MLflow framework application for managing machine learning pipelines in network intrusion detection, covering experiment tracking, model deployment, and monitoring across security datasets"
        },
        {
          "title": "MLflow Tutorial - Machine Learning Lifecycle Management",
          "url": "https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html",
          "source_type": "tutorial",
          "description": "Step-by-step tutorial demonstrating MLflow experiment tracking, model packaging, and deployment using real machine learning examples"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "related_techniques": [
        "model-cards",
        "datasheets-for-datasets",
        "data-version-control",
        "automated-documentation-generation"
      ]
    },
    {
      "slug": "data-version-control",
      "name": "Data Version Control",
      "description": "Data Version Control (DVC) is a Git-like version control system specifically designed for machine learning data, models, and experiments. It tracks changes to large data files, maintains reproducible ML pipelines, and creates a complete audit trail of data transformations, model training, and evaluation processes. DVC works alongside Git to provide end-to-end lineage tracking from raw data through preprocessing, training, and deployment, enabling teams to reproduce any model version and understand exactly how datasets evolved throughout the ML lifecycle.",
      "assurance_goals": [
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/documentation",
        "expertise-needed/software-engineering",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/data-handling",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/process"
      ],
      "example_use_cases": [
        {
          "description": "Tracking medical imaging dataset versions and model training pipelines to ensure reproducible research results, enabling hospitals to verify which specific data version and preprocessing steps were used for regulatory submissions.",
          "goal": "Transparency"
        },
        {
          "description": "Managing credit scoring model data pipelines with complete version control of training datasets, feature engineering steps, and model artifacts, ensuring reliable model reproduction and rollback capabilities when performance issues arise.",
          "goal": "Reliability"
        },
        {
          "description": "Maintaining pharmaceutical drug discovery data lineage across multiple research teams, tracking compound datasets, feature extraction processes, and model versions to support FDA submissions with complete experimental provenance.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires learning Git-like workflows and CLI commands, which may have a steep learning curve for teams unfamiliar with version control systems."
        },
        {
          "description": "Storage costs can be substantial for large datasets with frequent changes, especially when maintaining multiple versions and branches of data."
        },
        {
          "description": "Complex data pipelines with many interdependencies may require significant setup time and careful configuration to track properly."
        },
        {
          "description": "Performance can degrade with very large files or datasets due to checksumming and synchronisation overhead during operations."
        },
        {
          "description": "Team coordination becomes essential as improper branch management or merge conflicts can disrupt collaborative workflows."
        }
      ],
      "resources": [
        {
          "title": "DVC Documentation",
          "url": "https://dvc.org/doc",
          "source_type": "documentation",
          "description": "Comprehensive official documentation covering DVC installation, data versioning, pipeline creation, and collaborative workflows with tutorials and best practices"
        },
        {
          "title": "iterative/dvc",
          "url": "https://github.com/iterative/dvc",
          "source_type": "software_package",
          "description": "Official DVC open-source repository containing the complete data version control system for machine learning with Git integration"
        },
        {
          "title": "DVC Tutorial - Data Version Control for Machine Learning",
          "url": "https://dvc.org/doc/start",
          "source_type": "tutorial",
          "description": "Step-by-step getting started guide demonstrating DVC basics including data tracking, pipeline creation, and experiment management"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "DVC",
      "related_techniques": [
        "model-cards",
        "datasheets-for-datasets",
        "mlflow-experiment-tracking",
        "automated-documentation-generation"
      ]
    }
  ],
  "count": 4
}