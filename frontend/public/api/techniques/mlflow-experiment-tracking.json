{
  "slug": "mlflow-experiment-tracking",
  "name": "MLflow Experiment Tracking",
  "acronym": null,
  "description": "MLflow is an open-source platform that tracks machine learning experiments by automatically logging parameters, metrics, models, and artifacts throughout the ML lifecycle. It provides a centralised repository for comparing different experimental runs, reproducing results, and managing model versions. Teams can track hyperparameters, evaluation metrics, model files, and execution environment details, creating a comprehensive audit trail that supports collaboration, reproducibility, and regulatory compliance across the entire machine learning development process.",
  "complexity_rating": 2,
  "computational_cost_rating": 2,
  "assurance_goals": [
    {
      "id": 6,
      "name": "Transparency"
    },
    {
      "id": 3,
      "name": "Reliability"
    }
  ],
  "tags": [
    {
      "name": "applicable-models/agnostic",
      "id": 1
    },
    {
      "name": "assurance-goal-category/transparency",
      "id": 31
    },
    {
      "name": "assurance-goal-category/reliability",
      "id": 27
    },
    {
      "name": "data-requirements/no-special-requirements",
      "id": 39
    },
    {
      "name": "data-type/any",
      "id": 46
    },
    {
      "name": "evidence-type/documentation",
      "id": 53
    },
    {
      "name": "expertise-needed/ml-engineering",
      "id": 73
    },
    {
      "name": "expertise-needed/software-engineering",
      "id": 77
    },
    {
      "name": "lifecycle-stage/model-development",
      "id": 92
    },
    {
      "name": "lifecycle-stage/system-deployment-and-use",
      "id": 103
    },
    {
      "name": "technique-type/process",
      "id": 114
    }
  ],
  "related_techniques": [
    "model-cards",
    "datasheets-for-datasets",
    "data-version-control",
    "automated-documentation-generation"
  ],
  "related_technique_slugs": [
    "model-cards",
    "datasheets-for-datasets",
    "data-version-control",
    "automated-documentation-generation"
  ],
  "resources": [
    {
      "title": "MLflow Documentation",
      "url": "https://mlflow.org/docs/latest/index.html",
      "description": "Comprehensive official documentation covering MLflow setup, tracking APIs, model management, and deployment workflows with examples and best practices",
      "authors": "",
      "publication_date": null,
      "source_type": "documentation",
      "resource_type": "documentation"
    },
    {
      "title": "mlflow/mlflow",
      "url": "https://github.com/mlflow/mlflow",
      "description": "Official MLflow open-source repository containing the complete platform for ML experiment tracking, model management, and deployment",
      "authors": "",
      "publication_date": null,
      "source_type": "software_package",
      "resource_type": "documentation"
    },
    {
      "title": "An MLOps Framework for Explainable Network Intrusion Detection with MLflow",
      "url": "https://ieeexplore.ieee.org/abstract/document/10733700",
      "description": "Research paper demonstrating MLflow framework application for managing machine learning pipelines in network intrusion detection, covering experiment tracking, model deployment, and monitoring across security datasets",
      "authors": [
        "Vincenzo Spadari",
        "Francesco Cerasuolo",
        "Giampaolo Bovenzi",
        "Antonio Pescap√®"
      ],
      "publication_date": "2024-06-26",
      "source_type": "technical_paper",
      "resource_type": "documentation"
    },
    {
      "title": "MLflow Tutorial - Machine Learning Lifecycle Management",
      "url": "https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html",
      "description": "Step-by-step tutorial demonstrating MLflow experiment tracking, model packaging, and deployment using real machine learning examples",
      "authors": "",
      "publication_date": null,
      "source_type": "tutorial",
      "resource_type": "documentation"
    }
  ],
  "example_use_cases": [
    {
      "description": "Tracking medical diagnosis model experiments across different hospitals, logging hyperparameters, performance metrics, and model artifacts to ensure reproducible research and enable regulatory audits of model development processes.",
      "goal": "Transparency"
    },
    {
      "description": "Managing fraud detection model versions in production, tracking which specific model configuration and training data version is deployed, enabling quick rollback and performance comparison when system reliability issues arise.",
      "goal": "Reliability"
    },
    {
      "description": "Documenting loan approval model experiments with complete parameter tracking and performance logging across demographic groups, supporting fair lending compliance by providing transparent records of model development and validation processes.",
      "goal": "Transparency"
    }
  ],
  "limitations": [
    {
      "description": "Requires teams to adopt disciplined logging practices and may introduce overhead to development workflows if not properly integrated into existing processes."
    },
    {
      "description": "Storage costs can grow substantially with extensive artifact logging, especially for large models or high-frequency experimentation."
    },
    {
      "description": "Tracking quality depends on developers consistently logging relevant information, with incomplete logging leading to gaps in experimental records."
    },
    {
      "description": "Complex multi-stage pipelines may require custom instrumentation to capture dependencies and data flow relationships effectively."
    },
    {
      "description": "Security and access control configurations require careful setup to protect sensitive model information and experimental data in shared environments."
    }
  ]
}
