[
  {
    "id": 1,
    "name": "SHapley Additive exPlanations (SHAP)",
    "description": "SHAP provides explanations by calculating how much each input feature contributes to a model's prediction. It assigns an importance score to each feature, showing whether the feature pushes the prediction toward or away from the expected outcome. The method works by systematically evaluating how predictions change when features are included or excluded, similar to calculating fair contributions in team scenarios.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/feature-analysis",
      "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining why a complex model (like a neural network) made a specific prediction by showing how each feature influenced that prediction.",
        "goal": "Explainability"
      },
      {
        "description": "Identifying which factors contributed most to a loan approval decision in a financial model using SHAP values.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Assumes feature independence, so results can be less reliable when features are highly correlated."
      },
      {
        "description": "Computationally intensive for large models or datasets (calculating many Shapley values is slow)."
      }
    ],
    "resources": [
      {
        "title": "shap/shap",
        "url": "https://github.com/shap/shap",
        "source_type": "software_package"
      },
      {
        "title": "Introduction to SHapley Additive exPlanations (SHAP) \u2014 XAI Tutorials",
        "url": "https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/shap.html",
        "source_type": "tutorial"
      },
      {
        "title": "An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models",
        "url": "http://arxiv.org/pdf/2204.11351v3",
        "source_type": "technical_paper",
        "authors": [
          "Han Yuan",
          "Mingxuan Liu",
          "Lican Kang",
          "Chenkui Miao",
          "Ying Wu"
        ],
        "publication_date": "2022-04-24"
      },
      {
        "title": "SHAP: Shapley Additive Explanations | Towards Data Science",
        "url": "https://towardsdatascience.com/shap-shapley-additive-explanations-5a2a271ed9c3/",
        "source_type": "tutorial"
      },
      {
        "title": "MAIF/shapash",
        "url": "https://github.com/MAIF/shapash",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      5,
      6,
      7,
      12,
      27,
      28
    ]
  },
  {
    "id": 2,
    "name": "Permutation Importance",
    "description": "Permutation importance measures a feature's importance by shuffling its values and observing the impact on model performance. If the model's accuracy drops significantly when a feature is permuted, that feature is considered important.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/feature-analysis",
      "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/feature-importance",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Assessing which features are most important in a trained Random Forest by measuring performance drop when each feature's values are shuffled.",
        "goal": "Explainability"
      },
      {
        "description": "Checking that a model is not relying on an irrelevant field by confirming that shuffling that field doesn't change the model's accuracy.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Can be misleading when features are correlated, and computationally expensive on high-dimensional datasets due to repetitive model evaluations."
      }
    ],
    "resources": [
      {
        "title": "Asymptotic Unbiasedness of the Permutation Importance Measure in Random Forest Models",
        "url": "http://arxiv.org/pdf/1912.03306v1",
        "source_type": "technical_paper",
        "authors": [
          "Burim Ramosaj",
          "Markus Pauly"
        ],
        "publication_date": "2019-12-05"
      },
      {
        "title": "eli5.permutation_importance \u2014 ELI5 0.15.0 documentation",
        "url": "https://eli5.readthedocs.io/en/latest/autodocs/permutation_importance.html",
        "source_type": "documentation"
      },
      {
        "title": "Permutation Importance \u2014 PermutationImportance 1.2.1.5 ...",
        "url": "https://permutationimportance.readthedocs.io/en/latest/permutation.html",
        "source_type": "documentation"
      },
      {
        "title": "parrt/random-forest-importances",
        "url": "https://github.com/parrt/random-forest-importances",
        "source_type": "software_package"
      },
      {
        "title": "Statistically Valid Variable Importance Assessment through Conditional Permutations",
        "url": "http://arxiv.org/pdf/2309.07593v2",
        "source_type": "technical_paper",
        "authors": [
          "Ahmad Chamma",
          "Denis A. Engemann",
          "Bertrand Thirion"
        ],
        "publication_date": "2023-09-14"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      3,
      4,
      10
    ]
  },
  {
    "id": 3,
    "name": "Mean Decrease Impurity",
    "description": "Mean Decrease Impurity measures how much each feature helps improve decision-making in tree-based models like Random Forest. It calculates how much 'disorder' each feature reduces when making splits in the decision trees. Features that create cleaner, more organized splits receive higher importance scores.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/tree-based",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Determining important features in Random Forest classification tasks.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Biased towards features with more splits or categories; only applicable to tree-based models and can overestimate importance of high-cardinality features."
      }
    ],
    "resources": [
      {
        "title": "Trees, forests, and impurity-based variable importance",
        "url": "http://arxiv.org/pdf/2001.04295v3",
        "source_type": "technical_paper",
        "authors": [
          "Erwan Scornet"
        ],
        "publication_date": "2020-01-13"
      },
      {
        "title": "A Debiased MDI Feature Importance Measure for Random Forests",
        "url": "http://arxiv.org/pdf/1906.10845v2",
        "source_type": "technical_paper",
        "authors": [
          "Xiao Li",
          "Yu Wang",
          "Sumanta Basu",
          "Karl Kumbier",
          "Bin Yu"
        ],
        "publication_date": "2019-06-26"
      },
      {
        "title": "Variable Importance in Random Forests | Towards Data Science",
        "url": "https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0/",
        "source_type": "tutorial"
      },
      {
        "title": "Interpreting Deep Forest through Feature Contribution and MDI Feature Importance",
        "url": "http://arxiv.org/pdf/2305.00805v1",
        "source_type": "technical_paper",
        "authors": [
          "Yi-Xiao He",
          "Shen-Huan Lyu",
          "Yuan Jiang"
        ],
        "publication_date": "2023-05-01"
      },
      {
        "title": "optuna.importance.MeanDecreaseImpurityImportanceEvaluator ...",
        "url": "https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.MeanDecreaseImpurityImportanceEvaluator.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      2,
      4,
      10
    ]
  },
  {
    "id": 4,
    "name": "Coefficient Magnitudes (in Linear Models)",
    "description": "Examines the absolute values of coefficients in linear models to judge feature influence. Features with larger absolute coefficients have a stronger impact on the prediction, and the sign of a coefficient shows if it pushes the outcome up or down.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/linear-model",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Interpreting which features influence housing price predictions in linear regression.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Only valid for linear relationships; can be affected by feature scaling and multicollinearity, and does not capture non-linear importance."
      }
    ],
    "resources": [],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [
      2,
      3,
      10
    ]
  },
  {
    "id": 5,
    "name": "Integrated Gradients",
    "description": "Integrated Gradients explains predictions by calculating how much each input feature contributes to the final output. It works by gradually changing each feature from a neutral starting point (baseline) to its actual value, measuring the model's sensitivity to these changes step by step. This produces importance scores showing which features most influenced the prediction.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
		"applicable-models/neural-network",
		"assurance-goal-category/explainability",
		"data-requirements/labelled-data",
		"data-requirements/reference-dataset",
		"data-type/tabular",
		"data-type/image",
		"evidence-type/quantitative-metric",
		"expertise-needed/ml-engineering",
		"expertise-needed/statistics",
		"explanatory-scope/local",
		"lifecycle-stage/model-development",
		"technique-type/algorithmic"
	],
    "example_use_cases": [
      {
        "description": "Understanding pixel contributions in image classification with deep neural networks.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires a meaningful baseline input; results can be sensitive to the choice of baseline and model must be differentiable, limiting use with non-differentiable components."
      }
    ],
    "resources": [
      {
        "title": "ankurtaly/Integrated-Gradients",
        "url": "https://github.com/ankurtaly/Integrated-Gradients",
        "source_type": "software_package"
      },
      {
        "title": "pytorch/captum",
        "url": "https://github.com/pytorch/captum",
        "source_type": "software_package"
      },
      {
        "title": "Maximum Entropy Baseline for Integrated Gradients",
        "url": "http://arxiv.org/pdf/2204.05948v1",
        "source_type": "technical_paper",
        "authors": [
          "Hanxiao Tan"
        ],
        "publication_date": "2022-04-12"
      },
      {
        "title": "Integrated Gradients from Scratch | Towards Data Science",
        "url": "https://towardsdatascience.com/integrated-gradients-from-scratch-b46311e4ab4/",
        "source_type": "tutorial"
      },
      {
        "title": "Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution",
        "url": "http://arxiv.org/pdf/2004.10484v2",
        "source_type": "technical_paper",
        "authors": [
          "Gary S. W. Goh",
          "Sebastian Lapuschkin",
          "Leander Weber",
          "Wojciech Samek",
          "Alexander Binder"
        ],
        "publication_date": "2020-04-22"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      1,
      6,
      7,
      12,
      27,
      28
    ]
  },
  {
    "id": 6,
    "name": "DeepLIFT",
    "description": "DeepLIFT assigns credit (or blame) to each input feature by comparing a neuron's activation to a reference (baseline) and tracking the difference backward through the network. It attributes the change in the output to changes in each input feature relative to what the output would be at the reference input.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-requirements/access-to-training-data",
      "data-requirements/labelled-data",
      "data-requirements/reference-dataset",
      "data-type/graph",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining why a neural network classifies an image as a specific object by tracing neuron activations.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Needs careful configuration for each network architecture; may produce inconsistent scores if multiple reference points are possible, and not all model types supported."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      1,
      5,
      7,
      12,
      27,
      28
    ]
  },
  {
    "id": 7,
    "name": "Layer-wise Relevance Propagation (LRP)",
    "description": "LRP explains a model's prediction by propagating the result backwards through the network and assigning a relevance score to each input feature. Starting from the predicted output, it distributes the 'relevance' of that output to neurons in each preceding layer, all the way back to the input features, indicating how each feature contributed.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-requirements/labelled-data",
      "data-type/graph",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Visualising important regions in medical images for disease diagnosis using deep learning models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires tailored rules for each layer type; results can sometimes be hard to interpret (negative relevances), and implementation is complex for new architectures."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [
      1,
      5,
      6,
      12,
      27,
      28
    ]
  },
  {
    "id": 8,
    "name": "Contextual Decomposition",
    "description": "Contextual Decomposition interprets neural network predictions by breaking down the model's output into parts attributed to specific input features or groups of features. It isolates the contribution of a particular input (or a combination of inputs) to the final prediction, taking into account the context provided by the other inputs.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "data-requirements/labelled-data",
      "data-type/graph",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining sentiment predictions in text by attributing scores to words or phrases.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Primarily designed for LSTMs; not widely implemented in standard libraries, requiring custom code, and may not scale well to very deep or different model types."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [
      9,
      11,
      26
    ]
  },
  {
    "id": 9,
    "name": "Taylor Decomposition",
    "description": "Taylor Decomposition explains predictions by breaking them down into mathematical components that show how each input feature contributes. It uses calculus to approximate how the model's output changes based on small variations in each feature. This creates a mathematical breakdown showing which features and feature combinations most influence the final prediction.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Attributing feature contributions in complex models for specific predictions.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "The approach is mathematically complex and can be computationally intensive; approximations may introduce error and it's mainly theoretical with limited tooling."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      8,
      11,
      26
    ]
  },
  {
    "id": 10,
    "name": "Sobol Indices",
    "description": "Sobol Indices measure how much each input feature affects the model's output variability across the entire dataset. They calculate what percentage of the model's output changes can be attributed to each individual feature and to combinations of features working together. This helps identify which inputs have the most influence on prediction uncertainty.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Understanding parameter impacts in environmental modeling outputs.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires a large number of model evaluations for accurate estimation; assumes independent input distributions and may be difficult to apply to high-dimensional inputs."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 5,
    "related_techniques": [
      2,
      3,
      4
    ]
  },
  {
    "id": 11,
    "name": "Feature Interaction Detection (H-statistic)",
    "description": "The H-statistic measures the strength of interaction between two features in a model by comparing their combined effect on the prediction to the sum of their individual effects. A higher H-statistic for a feature pair means the model's prediction cannot be explained by additive effects of the two features alone, indicating a significant interaction.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Identifying significant interactions in healthcare predictive models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Only captures pairwise interactions (or one feature vs all); relies on partial dependence which can be misleading if features are correlated or have complex interactions."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "related_techniques": [
      8,
      9,
      26
    ]
  },
  {
    "id": 12,
    "name": "Local Interpretable Model-Agnostic Explanations (LIME)",
    "description": "LIME explains an individual prediction by training a simple surrogate model around that specific data point. It perturbs the input data point to create synthetic data, gets the complex model's predictions for these new points, and then fits an interpretable model (like a small linear model) on this local dataset. The surrogate's parameters (or rules) then highlight which features of the original data point influenced the prediction the most in that locality.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-training-data",
      "data-type/tabular",
      "evidence-type/qualitative-report",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining why a customer was denied a loan by approximating the model's decision locally.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Explanations can vary with repeated runs due to randomness; the linear surrogate may not be faithful if the model behavior is highly non-linear in that locality."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 3,
    "related_techniques": [
      1,
      5,
      6,
      7,
      27,
      28
    ]
  },
  {
    "id": 13,
    "name": "Ridge Regression Surrogates",
    "description": "This technique approximates a complex model by training a ridge regression (a linear model with L2 regularization) on the original model's predictions. The ridge regression serves as a global surrogate that balances fidelity and interpretability, capturing the main linear relationships that the complex model learned while ignoring noise due to regularization.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/linear-model",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Summarizing complex model behavior for regulatory reporting in finance.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "The surrogate only approximates the original model, potentially losing important non-linear behavior; requires a representative dataset to train the surrogate model."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      29,
      64,
      65
    ]
  },
  {
    "id": 14,
    "name": "Partial Dependence Plots (PDP)",
    "description": "Partial Dependence Plots illustrate how the predicted outcome changes as one (or two) features vary, averaging out the effects of all other features. They show the marginal effect of selected features on the model prediction by plotting the average prediction as a function of those features.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Understanding how changes in age affect predicted disease risk in medical models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Assumes features are independent of others (due to averaging); can be misleading when features are correlated, and only shows average effects, not instance-specific."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      15,
      16
    ]
  },
  {
    "id": 15,
    "name": "Accumulated Local Effects Plots (ALE)",
    "description": "ALE plots show how features influence predictions, like PDPs, but they do so by calculating local changes in the prediction as the feature moves through its range. They accumulate these local effects and account for the presence of other features, making ALE plots more robust than PDPs when features are correlated.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/visualization",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Exploring the effect of house size on price predictions in real estate models with correlated features.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "More complex to compute than PDP; still provides only average effects and can be harder to interpret for higher-order interactions beyond pairs."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      14,
      16
    ]
  },
  {
    "id": 16,
    "name": "Individual Conditional Expectation Plots (ICE)",
    "description": "ICE plots display the predicted output for individual instances as a function of a feature, with all other features held fixed for each instance. Each line on an ICE plot represents one instance's prediction trajectory as the feature of interest changes, revealing whether different instances are affected differently by that feature.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/visualization",
      "expertise-needed/low",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/visualization"
    ],
    "example_use_cases": [
      {
        "description": "Visualizing how customers' predicted spending changes with income in consumer behavior models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Plots can become cluttered with many instances; does not inherently summarize the overall effect without visual inspection, and still assumes fixed other features."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      14,
      15
    ]
  },
  {
    "id": 17,
    "name": "Saliency Maps",
    "description": "Saliency maps highlight parts of an input (such as pixels in an image) that strongly influence the model's prediction. Typically computed via the gradient of the output with respect to the input, they produce a heatmap where brighter regions indicate greater influence on the model's decision.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-requirements/labelled-data",
      "data-type/image",
      "evidence-type/visualization",
      "expertise-needed/statistics",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Identifying regions contributing to tumor diagnosis in medical images.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Often noisy and sensitive to small perturbations; highlights may not correspond to human-understandable features, and they only indicate local gradient, not causal importance."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      18,
      19
    ]
  },
  {
    "id": 18,
    "name": "Gradient-weighted Class Activation Mapping (Grad-CAM)",
    "description": "Grad-CAM produces a coarse localization map highlighting image regions important for a classification. It works by using the gradients of a target class flowing into the last convolutional layer of a CNN to weight the feature maps, and then projects these weights back to the image space as a heatmap of influential regions.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/cnn",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-requirements/labelled-data",
      "data-type/image",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Visualizing parts of an image leading to a 'dog' classification in image recognition models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires access to internal feature maps; resolution is limited to coarse feature map size, and it is specific to CNN-based vision models with recognizable layers."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      17,
      19
    ]
  },
  {
    "id": 19,
    "name": "Occlusion Sensitivity",
    "description": "Occlusion sensitivity tests which parts of the input are important by occluding (masking or removing) them and seeing how the model's prediction changes. For example, portions of an image can be covered up in a sliding window fashion; if the model's confidence drops significantly when a certain region is occluded, that region was important for the prediction.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/image",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Understanding which words affect sentiment prediction by masking them in NLP models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive if many parts need to be occluded; choice of occlusion size can bias results, and it may not capture interactions if multiple parts jointly matter."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 4,
    "related_techniques": [
      17,
      18
    ]
  },
  {
    "id": 20,
    "name": "Attention Mechanisms in Neural Networks",
    "description": "Attention mechanisms allow models, especially in sequence tasks like NLP, to weight different parts of the input when making predictions. Visualizing or examining the learned attention weights can provide insight into which parts of the input the model found most relevant for a given output. By looking at these attention heatmaps, we can interpret which parts of the input were most influential for the model's decision.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/transformer",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-requirements/labelled-data",
      "data-type/text",
      "evidence-type/visualization",
      "expertise-needed/low",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analyzing which words a transformer model focuses on during machine translation tasks.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Attention weights are not always strongly correlated with importance; focusing solely on attention can be misleading ('attention is not explanation' debate) and only applies to models with attention layers."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      72
    ]
  },
  {
    "id": 21,
    "name": "Factor Analysis",
    "description": "Factor analysis identifies hidden themes or patterns that explain why certain features in data tend to vary together. It works by finding a smaller number of underlying factors that can explain the relationships between many observed features. For example, it might discover that several test scores are all influenced by a single underlying 'intelligence' factor.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Discovering underlying factors in psychological survey data for social science research.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Assumes linear relationships and normality; results (factors) can be abstract and not directly interpretable, and requires deciding on number of factors and rotation method."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      22,
      23,
      24
    ]
  },
  {
    "id": 22,
    "name": "Principal Component Analysis (PCA)",
    "description": "PCA is a dimensionality reduction method that transforms the original features into a new set of orthogonal components (principal components) ordered by how much variance they explain in the data. By keeping only the top components, PCA provides a simpler representation of the data that captures the most important variance, which can aid visualization or reduce complexity.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Visualizing high-dimensional gene expression data in bioinformatics.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Principal components are linear combinations that may not correspond to clear real-world concepts; only captures linear variance and can be affected by scaling of features."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      21,
      23,
      24
    ]
  },
  {
    "id": 23,
    "name": "t-SNE",
    "description": "t-SNE creates visual maps of complex, high-dimensional data by compressing it into 2D or 3D plots that humans can easily view. It works by ensuring that data points that are similar in the original complex space appear close together in the simplified visualization. This reveals hidden patterns, clusters, and groupings in data that would be impossible to see otherwise.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-training-data",
      "data-type/image",
      "evidence-type/visualization",
      "expertise-needed/user-experience",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Visualizing clusters in high-dimensional word embeddings.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Results can vary between runs; may distort global structure in favor of local clustering and requires tuning (perplexity, iterations) to avoid misleading patterns."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      21,
      22,
      24
    ]
  },
  {
    "id": 24,
    "name": "UMAP",
    "description": "UMAP is a non-linear dimensionality reduction technique, similar to t-SNE, aimed at visualizing high-dimensional data in 2D or 3D. It tends to preserve both local structure and some global structure of the data better than t-SNE, often producing more interpretable overall layouts of clusters.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Visualizing patterns in user behavior data for marketing analysis.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Choice of parameters (neighbors, min distance) affects outcome; like t-SNE, it can sometimes be difficult to interpret distances in the reduced space in terms of original features."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      21,
      22,
      23
    ]
  },
  {
    "id": 25,
    "name": "Prototype and Criticism Models",
    "description": "This approach identifies representative examples (prototypes) of the dataset and outlier or hard-to-represent examples (criticisms). Prototypes capture the typical patterns the model or data exhibits, while criticisms highlight where those patterns fail to cover, giving insight into the diversity and exceptions in the data or model behavior.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Selecting representative customer profiles for targeted marketing.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Determining prototypes/criticisms can be computationally complex for large datasets; results depend on the metric chosen and might not capture all important aspects of data variability."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      73,
      77
    ]
  },
  {
    "id": 26,
    "name": "Influence Functions",
    "description": "Influence functions identify which training examples most strongly affected a model's specific predictions. They work by estimating how much a prediction would change if individual training data points were removed from the dataset. This reveals which training examples pushed the model toward or away from its current prediction, helping trace predictions back to their most influential training data.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-training-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Debugging model predictions by identifying influential training data points.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires access to and differentiation through the training process; can be intractable for large models due to needing Hessian computations, and may not be accurate if model is highly non-convex."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "related_techniques": [
      8,
      9,
      11
    ]
  },
  {
    "id": 27,
    "name": "Contrastive Explanation Method (CEM)",
    "description": "CEM explains model decisions by producing contrastive examples: it finds the minimal changes to an input that would switch the model's prediction. It outputs pertinent negatives (what could be removed from the input to change the prediction) and pertinent positives (what minimal additional features would be needed to reach the same decision) as a way to highlight what is essential in the input for that prediction.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining loan rejections by showing what changes would lead to approval.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Solving for pertinent positives/negatives requires iterative optimization per instance; results can be sensitive to parameter settings and might yield unrealistic contrastive inputs if constraints are not tight."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      1,
      5,
      6,
      7,
      12,
      28
    ]
  },
  {
    "id": 28,
    "name": "ANCHOR",
    "description": "ANCHOR generates precision if-then rules as explanations for individual predictions. It finds a minimal set of conditions (on input features) that 'anchor' the prediction, meaning that if those conditions are met, the model will almost always give the same prediction. These anchor rules are designed to be easily understood and highly predictive for that specific instance.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Generating rules to explain individual predictions in text classification.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Only provides explanations for individual predictions (local rules); may struggle with continuous features (usually requires discretization) and might not find an anchor if conditions are too strict."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      1,
      5,
      6,
      7,
      12,
      27
    ]
  },
  {
    "id": 29,
    "name": "RuleFit",
    "description": "RuleFit is a method that creates an interpretable model by combining linear terms with decision rules. It first extracts potential rules from ensemble trees, then builds a sparse linear model where those rules (binary conditions) and original features are used as predictors, with regularization to keep the model simple. The final model is a linear combination of a small set of rules and original features, balancing interpretability with predictive power.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Building interpretable models for predicting customer churn with rule-based explanations.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "The resulting model can still have many rules, complicating interpretability; performance may lag behind black-box models if too few rules are allowed for simplicity."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      13,
      64,
      65
    ]
  },
  {
    "id": 30,
    "name": "Monte Carlo Dropout",
    "description": "Monte Carlo Dropout uses dropout at prediction time to estimate model uncertainty. By running multiple forward passes with random dropout activated and observing the variation in outputs, it provides a distribution of predictions. A consistent prediction across runs indicates high confidence, while widely varying predictions indicate uncertainty.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Estimating prediction uncertainty in medical diagnosis models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Only captures model uncertainty, not data uncertainty; requires multiple forward passes and results depend on dropout rate, which must be same as training to be meaningful."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      42,
      43,
      44,
      47,
      48,
      49
    ]
  },
  {
    "id": 31,
    "name": "Out-of-DIstribution detector for Neural networks (ODIN)",
    "description": "ODIN detects when a neural network receives unusual inputs that are very different from its training data. It works by making small, controlled changes to inputs and adjusting the model's confidence scores to better distinguish between familiar and unfamiliar data. When the adjusted confidence score is low, ODIN flags the input as potentially problematic or out-of-scope for the model.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-training-data",
      "data-type/graph",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Detecting unusual inputs that a classifier wasn't trained on, like identifying when an image recognition model is shown a completely new object class.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires tuning (temperature and perturbation magnitude) which may vary across different out-of-distribution types; may still struggle with adversarial examples or inputs very similar to training data."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      55,
      103
    ]
  },
  {
    "id": 32,
    "name": "Permutation Tests",
    "description": "Permutation tests assess the significance of an observed result (like model accuracy or feature importance) by comparing it to what would happen purely by chance. This is done by randomly shuffling labels or data many times and calculating the result each time, building a distribution of outcomes under the null hypothesis (no real relationship). If the actual result is far out in the tail of this distribution, it is deemed statistically significant.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Validating that a feature is truly important by comparing its importance score to what would be expected from random noise.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive, especially for large datasets or models with long inference times; requires many permutations to get reliable p-values for strict significance thresholds."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [
      50,
      51,
      52
    ]
  },
  {
    "id": 33,
    "name": "Demographic Parity Assessment",
    "description": "Measures whether prediction rates are equal across different demographic groups (e.g., loan approval rates). This is often done by calculating the difference in positive outcome rates (Statistical Parity Difference) or the ratio of rates (Disparate Impact ratio) between groups. This technique helps identify potential group-level biases by comparing outcome distributions.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Checking if a job candidate screening algorithm selects candidates from different gender groups at equal rates.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Can force ignoring relevant features that legitimately correlate with protected attributes; may reduce accuracy and can conflict with individual fairness notions."
      }
    ],
    "resources": [],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [
      101,
      102
    ]
  },
  {
    "id": 34,
    "name": "Adversarial Debiasing",
    "description": "Adversarial debiasing reduces bias by training a model using a competitive setup. The main model learns to make accurate predictions while a competing 'bias detector' tries to identify protected characteristics (like race or gender) from the model's internal processing. The main model learns to hide these characteristics from the bias detector, forcing it to make decisions without relying on demographic information.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Training a resume screening model that makes hiring recommendations while ensuring representations don't enable prediction of applicants' gender or ethnicity.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "More complex to implement than standard models; may require careful tuning to balance task performance and fairness objectives, and effectiveness depends on adversary quality."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      87,
      88,
      89,
      90,
      91,
      92,
      93,
      94
    ]
  },
  {
    "id": 35,
    "name": "Counterfactual Fairness Assessment",
    "description": "Counterfactual fairness ensures that a model's prediction for an individual would remain the same in a counterfactual world where their sensitive attributes were different. It uses causal modeling to distinguish between legitimate and discriminatory influences of protected characteristics. This approach addresses fairness at an individual level, ensuring people aren't treated differently based solely on demographic factors that should be irrelevant to the decision.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "fairness-approach/causal",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring a credit scoring model gives the same result for a person regardless of what their race would have been, while still accounting for legitimate factors affected by systemic inequalities.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires specifying causal relationships, which can be subjective; may be mathematically impossible to satisfy along with other fairness definitions, and implementation is complex."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [
      105,
      106
    ]
  },
  {
    "id": 37,
    "name": "Sensitivity Analysis for Fairness",
    "description": "Sensitivity analysis for fairness examines how model outputs change when sensitive attributes or their correlates are varied. By systematically altering certain inputs and observing prediction changes, it reveals if and how much protected characteristics influence decisions. This approach helps identify potential discrimination even in models that don't explicitly use sensitive attributes but might rely on correlated proxies.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Testing whether a lending model's decisions change significantly when only the applicant's zip code (which may correlate with race) is altered, while keeping all other factors constant.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "May require domain expertise to identify relevant proxies for sensitive attributes; can be computationally intensive for complex models, and choosing comparison points involves subjective judgment."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      82,
      86,
      104,
      107,
      108
    ]
  },
  {
    "id": 38,
    "name": "Synthetic Data Generation",
    "description": "Synthetic data generation creates artificial datasets that maintain the statistical properties and patterns of real data without containing actual information from real individuals. By using techniques like differential privacy or generative models, teams can develop, test, and share ML models without exposing sensitive information. This approach balances data utility with privacy protection, enabling collaboration while minimizing re-identification risks.",
    "assurance_goals": [
      "Privacy"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/privacy",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/statistical-test",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Creating realistic but synthetic electronic health records for developing and testing medical diagnosis algorithms without exposing real patient data.",
        "goal": "Privacy"
      }
    ],
    "limitations": [
      {
        "description": "May not capture all nuances of real data, potentially reducing model performance; generating high-quality synthetic data can be challenging, and some approaches may still leak information."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "related_techniques": [
      39,
      40,
      41
    ]
  },
  {
    "id": 39,
    "name": "Federated Learning",
    "description": "Federated learning trains models across multiple devices or servers without exchanging the underlying data, only sharing model updates. Data remains on the local devices, preserving privacy while still enabling collaborative learning from distributed datasets. By keeping sensitive information local and only sharing model parameters or gradients, organizations can build effective models while respecting privacy boundaries and data sovereignty requirements.",
    "assurance_goals": [
      "Privacy"
    ],
    "tags": [
      "assurance-goal-category/privacy",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Developing a smartphone keyboard prediction model by learning from users' typing patterns without their text ever leaving their devices.",
        "goal": "Privacy"
      }
    ],
    "limitations": [
      {
        "description": "Communication overhead can be significant; heterogeneous client data may lead to training instability, and the approach still has potential privacy vulnerabilities to inference attacks."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 5,
    "related_techniques": [
      38,
      40,
      41
    ]
  },
  {
    "id": 40,
    "name": "Applying Differential Privacy Mechanisms",
    "description": "Differential privacy protects individual privacy by adding controlled random noise to data or model results. This makes it mathematically impossible to determine whether any specific person's information was included in the dataset. Users can adjust the amount of noise added to balance privacy protection with data usefulness - more noise provides stronger privacy but may reduce accuracy.",
    "assurance_goals": [
      "Privacy"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/privacy",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adding privacy protection to a census data analysis model to ensure individual households cannot be identified while still obtaining accurate population statistics.",
        "goal": "Privacy"
      }
    ],
    "limitations": [
      {
        "description": "Adds noise that reduces model accuracy; setting the privacy budget requires careful consideration, and strong privacy guarantees may significantly impact utility."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [
      38,
      39,
      41
    ]
  },
  {
    "id": 41,
    "name": "Homomorphic Encryption",
    "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
    "assurance_goals": [
      "Privacy"
    ],
    "tags": [
      "assurance-goal-category/privacy",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Enabling a cloud-based medical diagnosis service to process encrypted patient data and return encrypted results without ever seeing the actual medical information.",
        "goal": "Privacy"
      }
    ],
    "limitations": [
      {
        "description": "Extremely computationally expensive, often orders of magnitude slower than unencrypted computation; limited operations supported efficiently, and implementation requires cryptographic expertise."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "related_techniques": [
      38,
      39,
      40
    ]
  },
  {
    "id": 42,
    "name": "Prediction Intervals",
    "description": "Prediction intervals provide a range within which a future observation is likely to fall with a specified probability. Unlike single-point predictions, they quantify the uncertainty in a model's predictions, giving upper and lower bounds that account for both the inherent noise in the data and the model's uncertainty. This helps users understand how precise and reliable a prediction really is.",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Providing realistic ranges for sales forecasts to help business planning under uncertainty.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Requires assumptions about error distribution; can be overconfident if model is miscalibrated, data distribution shifts, or assumptions are violated."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      30,
      43,
      44,
      47,
      48,
      49
    ]
  },
  {
    "id": 43,
    "name": "Quantile Regression",
    "description": "Quantile regression estimates different percentiles of the prediction's conditional distribution rather than just the mean. By modeling multiple quantiles (e.g., 10th, 50th, and 90th percentiles), it provides insights into the full range of possible outcomes and how features differently affect various parts of the outcome distribution. This helps understand prediction uncertainty and how the relationships might vary across different segments of the data.",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Modeling how housing prices vary across different market segments, by estimating how factors affect low-end, median, and luxury properties differently.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Can be computationally intensive to fit multiple quantiles; may yield crossing quantiles without constraints, creating logically inconsistent prediction intervals."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      30,
      42,
      44,
      47,
      48,
      49
    ]
  },
  {
    "id": 44,
    "name": "Conformal Prediction",
    "description": "Conformal prediction creates prediction sets that contain the true outcome with a guaranteed coverage probability. Unlike traditional confidence intervals, conformal prediction makes minimal assumptions and works with any model type. It provides rigorous uncertainty quantification by using past prediction errors on similar examples to calibrate the size of prediction intervals, ensuring they have the specified coverage rate (e.g., 95% of intervals contain the true value).",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Creating prediction intervals for medical test results that have a provable 95% coverage rate, regardless of the underlying model complexity.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Intervals can be unnecessarily wide if nonconformity scores vary greatly across the data; requires a held-out calibration set which reduces data available for training."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 2,
    "related_techniques": [
      30,
      42,
      43,
      47,
      48,
      49
    ]
  },
  {
    "id": 45,
    "name": "Empirical Calibration",
    "description": "Empirical calibration adjusts a model's predicted probabilities to match observed frequencies. For example, if events predicted with 80% confidence only occur 60% of the time, calibration would correct this overconfidence. Common techniques include Platt scaling and isotonic regression, which learn transformations that map the model's raw scores to well-calibrated probabilities, improving the reliability of confidence measures for downstream decisions.",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/calibration-set",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adjusting a credit default prediction model's probabilities to ensure that loan applicants with a predicted 30% default risk actually default 30% of the time, improving decision-making.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Requires held-out calibration data; calibration can degrade over time if data distribution shifts, and might sacrifice discrimination power for calibration in some cases."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      46
    ]
  },
  {
    "id": 46,
    "name": "Temperature Scaling",
    "description": "Temperature scaling is a simple but effective calibration method for neural networks that divides the logits (pre-softmax outputs) by a single scalar parameter called temperature. This parameter is optimized on a validation set to minimize calibration error. Higher temperatures smooth out confidence, making the model less overconfident. It preserves the model's accuracy while improving its calibration, ensuring probability estimates better reflect true likelihoods.",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "assurance-goal-category/reliability",
      "data-requirements/access-to-model-internals",
      "data-requirements/calibration-set",
      "data-requirements/validation-set",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adjusting a deep learning image classifier's confidence scores to be realistic, ensuring that when it's 90% confident, it's right 90% of the time.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Only addresses calibration at the dataset level; doesn't fix miscalibration that varies across subgroups or feature values, and does not improve the rank ordering of predictions."
      }
    ],
    "resources": [],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [
      45
    ]
  },
  {
    "id": 47,
    "name": "Deep Ensembles",
    "description": "Deep ensembles combine predictions from multiple neural networks trained independently with different random initializations. By capturing model uncertainty through the diversity of the ensemble's predictions, they provide more reliable uncertainty estimates and better out-of-distribution detection than single models. The disagreement between ensemble members naturally indicates prediction uncertainty, improving both accuracy and calibration.",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "assurance-goal-category/reliability",
      "data-requirements/calibration-set",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Improving self-driving car safety by using multiple neural networks to detect obstacles, where disagreement between models signals uncertainty and triggers extra caution.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive to train and deploy multiple complete models; may still provide overconfident predictions for inputs far from the training distribution."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 5,
    "related_techniques": [
      30,
      42,
      43,
      44,
      48,
      49
    ]
  },
  {
    "id": 48,
    "name": "Bootstrapping",
    "description": "Bootstrapping estimates uncertainty by resampling data with replacement many times, training a model on each sample, and analyzing the variation in predictions. This approach provides confidence intervals and stability measures without making strong statistical assumptions. By showing how predictions change with different data samples, it reveals how sensitive the model is to the specific training examples it sees.",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Estimating uncertainty in financial risk models by resampling historical data to understand how predictions might vary under different historical scenarios.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive to train multiple models; does not account for uncertainty in model structure or for systematically missing data patterns."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [
      30,
      42,
      43,
      44,
      47,
      49
    ]
  },
  {
    "id": 49,
    "name": "Jackknife Resampling",
    "description": "Jackknife resampling assesses model stability by systematically leaving out one (or a few) data points at a time and retraining the model. This approach reveals how individual points influence results and provides an estimate of the standard error. By examining the distribution of predictions across these leave-one-out models, it identifies unusually influential points and characterizes prediction uncertainty.",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating how removing individual countries from a global climate model affects predictions, to identify which regions have outsized influence on the results.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Extremely computationally intensive for large datasets, as it requires training n models for n data points; may underestimate uncertainty compared to other methods."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 5,
    "related_techniques": [
      30,
      42,
      43,
      44,
      47,
      48
    ]
  },
  {
    "id": 50,
    "name": "Cross-validation",
    "description": "Cross-validation evaluates model performance and stability by partitioning data into multiple subsets. The model is trained and tested repeatedly on different train-test splits, revealing how performance varies across different subsamples of data. This provides a realistic assessment of how well the model will generalize to new data and helps quantify prediction uncertainty, making it a fundamental technique for reliable model evaluation.",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/validation-set",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Using 10-fold cross-validation to estimate a healthcare prediction model's true accuracy and assess if performance is consistent across different patient subgroups.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Time-consuming for large datasets or complex models; standard cross-validation can give optimistic estimates if there are dependencies in the data (e.g., time series)."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 3,
    "related_techniques": [
      32,
      51,
      52
    ]
  },
  {
    "id": 51,
    "name": "Area Under Precision-Recall Curve (AUPRC)",
    "description": "AUPRC measures model performance by plotting precision against recall at various classification thresholds and calculating the area under the resulting curve. Unlike accuracy or AUC-ROC, it's particularly valuable for imbalanced datasets where the minority class is of primary interest. By focusing on the trade-off between precision and recall, it provides a more informative assessment for use cases where false positives and false negatives have different costs.",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/labelled-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating fraud detection models where genuine transactions far outnumber fraudulent ones, to ensure fraud is caught without overwhelming reviewers with false alarms.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "More sensitive to class distribution than ROC curves; comparing models across datasets with different class balances can be misleading."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      32,
      50,
      52
    ]
  },
  {
    "id": 52,
    "name": "Precision Metrics for High-Risk Domains",
    "description": "In high-risk domains, specialized metrics focus on worst-case performance and extreme error cases rather than averages. These include metrics like maximum error, 99th percentile error, and failure rate above critical thresholds. By explicitly measuring performance in the most challenging situations, these metrics ensure models meet safety requirements and help identify potentially catastrophic failure modes.",
    "assurance_goals": [
      "Reliability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating an autonomous vehicle's detection system by measuring its worst-case performance in challenging visibility conditions, rather than average-case performance.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "May require more test cases and data to accurately estimate tail performance; optimizing for extreme cases can sometimes harm average performance."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      32,
      50,
      51
    ]
  },
  {
    "id": 53,
    "name": "Internal Review Boards",
    "description": "Internal review boards evaluate ML projects for ethical and safety concerns before development or deployment. Comprised of cross-functional experts (technical, legal, ethics, domain specialists), they review use cases, potential harms, mitigation strategies, and monitoring plans. By formalizing ethical review processes, they help organizations identify risks early and ensure responsible AI development practices.",
    "assurance_goals": [
      "Safety"
    ],
    "tags": [
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/qualitative-report",
      "expertise-needed/ml-engineering",
      "expertise-needed/regulatory-compliance",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "example_use_cases": [
      {
        "description": "Reviewing a proposed criminal risk assessment tool to evaluate potential discriminatory impacts and privacy implications before development begins.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Can slow development timelines; effectiveness depends on board composition and authority, and organizations may face pressure to approve revenue-generating projects."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 1,
    "related_techniques": [
      54,
      56,
      57,
      58
    ]
  },
  {
    "id": 54,
    "name": "Red Teaming",
    "description": "Red teaming involves dedicated adversarial testing of ML systems by specialists who try to find flaws, vulnerabilities, harmful outputs, or ways to circumvent safety measures. Drawing on security practices, red teams probe systems from multiple angles, including prompt engineering, adversarial examples, and edge case testing. This approach reveals non-obvious risks before deployment and helps build more robust, safer AI systems.",
    "assurance_goals": [
      "Safety"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/graph",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Having a specialized team attempt to make a content moderation AI generate harmful outputs by using creative prompting techniques.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Requires specialized expertise and significant resources; can only find issues that testers think to look for, and systems may remain vulnerable to novel attack types."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      53,
      56,
      57,
      58
    ]
  },
  {
    "id": 55,
    "name": "Anomaly Detection",
    "description": "Anomaly detection identifies unusual behaviors, inputs, or outputs that deviate significantly from normal patterns. Applied to ML systems, it can flag unexpected model predictions, suspicious inputs, or drift in behavior. By continuously monitoring for anomalies in production, organizations can catch potential issues early, investigate causes, and prevent harm from system misuse or malfunction.",
    "assurance_goals": [
      "Safety"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Detecting unusual patterns of API calls to a machine translation service that might indicate attempts to extract harmful outputs or attack the model.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Setting appropriate thresholds is challenging; may generate false alarms in legitimate edge cases, and novel anomalies might not match patterns the system is trained to detect."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      31,
      103
    ]
  },
  {
    "id": 56,
    "name": "Human-in-the-Loop Safeguards",
    "description": "Human-in-the-loop safeguards integrate human oversight into model operations, requiring human review of high-risk or uncertain predictions before actions are taken. This approach combines model efficiency with human judgment for sensitive decisions. By designating appropriate intervention points and routing challenging cases to human experts, organizations can better manage the risks of automated decisions while still benefiting from ML capabilities.",
    "assurance_goals": [
      "Safety"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Requiring human moderator approval before taking down content that an AI system has flagged for potential policy violations.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Scales poorly with high request volumes; introduces latency into the decision process, and humans may experience fatigue or defer too easily to automation."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      53,
      54,
      57,
      58
    ]
  },
  {
    "id": 57,
    "name": "Confidence Thresholding",
    "description": "Confidence thresholding routes predictions to different handling paths based on the model's confidence level. High-confidence predictions proceed automatically, while low-confidence cases receive extra scrutiny, human review, or fallback handling. By establishing appropriate thresholds for different risk levels, organizations can balance automation benefits with prudent safeguards, ensuring greater oversight where uncertainty is high.",
    "assurance_goals": [
      "Safety"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Only allowing a medical diagnosis system to make recommendations automatically when its confidence exceeds 95%, routing less certain cases to human physicians.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Many models have poorly calibrated confidence scores; threshold selection can be challenging, and some risky predictions may still have high confidence."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      53,
      54,
      56,
      58
    ]
  },
  {
    "id": 58,
    "name": "Runtime Monitoring and Circuit Breakers",
    "description": "Runtime monitoring tracks key system metrics and prediction patterns in production, with automated circuit breakers that can throttle, disable, or revert ML systems when anomalies exceed thresholds. This approach provides real-time protection against unexpected behavior. By continuously monitoring inputs, outputs, and system health, organizations can quickly detect and respond to potential issues before they cause significant harm.",
    "assurance_goals": [
      "Safety"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Automatically disabling an automated trading system if it starts making a volume of trades that exceeds historical patterns by a large margin.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Setting appropriate thresholds requires careful calibration; false alarms can disrupt service unnecessarily, and some harmful behaviors may still fall within monitored metrics ranges."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      53,
      54,
      56,
      57
    ]
  },
  {
    "id": 59,
    "name": "Model Cards",
    "description": "Model cards are standardized documentation templates that provide essential information about ML models, including their intended uses, performance metrics across different conditions and demographic groups, training data characteristics, and known limitations. By creating transparency about a model's behavior and appropriate contexts, they help prevent misuse and enable users to make informed decisions about when and how to apply the model.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/regulatory-compliance",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Creating comprehensive documentation for a facial recognition API that clearly describes performance differences across skin tones and lighting conditions.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Creating thorough model cards requires significant effort; information may become outdated as models are updated, and some organizations may provide incomplete information."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      60,
      61,
      62,
      63
    ]
  },
  {
    "id": 60,
    "name": "Datasheets for Datasets",
    "description": "Datasheets for datasets document a dataset's creation, composition, intended uses, and maintenance. They include information about collection methods, preprocessing steps, recommended uses, potential biases, and legal/ethical considerations. By providing this context, datasheets improve transparency, help users make informed decisions about dataset suitability, and encourage dataset creators to reflect on responsible practices throughout the data lifecycle.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/documentation",
      "expertise-needed/low",
      "expertise-needed/regulatory-compliance",
      "lifecycle-stage/data-handling",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Creating comprehensive documentation for a public facial image dataset, detailing consent procedures, demographic representation, and appropriate use guidelines.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Requires time and effort to create and maintain; information may become outdated if the dataset evolves, and there's no enforced standard for completeness."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      59,
      61,
      62,
      63
    ]
  },
  {
    "id": 61,
    "name": "System Documentation Templates",
    "description": "System documentation templates provide standardized frameworks for describing ML systems beyond just models and datasets. They capture information about infrastructure, deployment environments, monitoring systems, failure modes, and maintenance procedures. By ensuring comprehensive documentation of the entire ML pipeline, they support better governance, reproducibility, and safety, serving both technical and non-technical stakeholders.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/low",
      "lifecycle-stage/project-design",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Documenting an automated trading system's components, monitoring protocols, and emergency shutdown procedures to support operational safety and regulatory compliance.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Maintaining comprehensive documentation can be time-consuming; different stakeholders may require different levels of detail, and templates may not fit all types of ML systems equally well."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 1,
    "related_techniques": [
      59,
      60,
      62,
      63
    ]
  },
  {
    "id": 62,
    "name": "Lineage Tracking for ML Systems",
    "description": "ML system lineage tracks the complete history of models, datasets, and experiments through a system's lifecycle. It records which datasets were used to train each model version, what hyperparameters were set, who approved changes, and when models were deployed. By maintaining this audit trail, organizations can reproduce past results, track sources of performance issues, and demonstrate regulatory compliance when needed.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/regulatory-compliance",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "example_use_cases": [
      {
        "description": "Maintaining complete records of model versions and datasets for a medical diagnosis system to support regulatory audits and trace the origin of any prediction issues.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Implementing comprehensive lineage tracking requires dedicated infrastructure; can generate large volumes of metadata, and requires discipline from all team members to maintain."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      59,
      60,
      61,
      63
    ]
  },
  {
    "id": 63,
    "name": "Automated Documentation Generation",
    "description": "Automated documentation generation extracts information directly from code, models, and data pipelines to create and maintain up-to-date documentation. It can capture model architectures, data schemas, feature importance, performance metrics, and lineage information without manual writing. By reducing the burden of documentation maintenance, it helps teams keep comprehensive records that remain accurate as systems evolve.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/documentation",
      "expertise-needed/software-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Automatically generating model cards each time a new model version is trained, with updated performance metrics and data statistics.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "May miss context and insights that human documentation would include; quality depends on instrumentation comprehensiveness, and unstructured information is harder to capture."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      59,
      60,
      61,
      62
    ]
  },
  {
    "id": 64,
    "name": "Model Distillation",
    "description": "Model distillation compresses a large, complex model (the teacher) into a smaller, simpler model (the student) that approximates the original's behavior. The student model learns from the teacher's outputs rather than the raw data. This technique makes models more interpretable, deployable, and efficient while preserving most of the original performance. It helps balance the benefits of complex models with the practical requirements of responsible deployment.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Compressing a large computer vision model with billions of parameters into a smaller model that can run on mobile devices while being easier to inspect.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Student models typically lose some performance compared to teacher models; the distillation process may still produce black-box models if not combined with interpretable architectures."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [
      13,
      29,
      65,
      71
    ]
  },
  {
    "id": 65,
    "name": "Model Extraction",
    "description": "Model extraction creates a simpler, interpretable model (like a decision tree) that approximates a complex black-box model's behavior. Unlike distillation which transfers knowledge during training, extraction works with already-trained models by analyzing their inputs and outputs. This technique helps understand what patterns a black-box model has learned and provides explanations that stakeholders can understand without technical expertise.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Converting a complex neural network credit score model into a set of human-readable rules to explain to regulators and applicants how decisions are made.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Extracted models may not faithfully represent the original if it's highly complex; accuracy and fidelity trade off against simplicity, and training data for extraction may not be diverse enough."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      13,
      29,
      64
    ]
  },
  {
    "id": 66,
    "name": "Monotonicity Constraints",
    "description": "Monotonicity constraints ensure that a model's predictions always increase (or decrease) as a specific feature increases, enforcing a consistent directional relationship. For instance, income can only positively affect loan approval chances. This technique makes models more intuitive and transparent, as users can understand how changing a particular input will affect the output, without unexpected reversals or non-linear behaviors.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring a credit scoring model always treats higher income as a positive factor (or at least never as a negative factor) for creditworthiness assessment.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "May reduce model accuracy if true relationships are non-monotonic; requires domain knowledge to identify which features should be monotonic, and increases training complexity."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      67,
      68,
      69,
      70
    ]
  },
  {
    "id": 67,
    "name": "Decision Trees and Rule Lists",
    "description": "Decision trees and rule lists create models that make predictions through a series of interpretable if-then rules arranged in a flowchart-like structure. Each decision point is based on a clear condition about a feature, and the path to any prediction can be traced and explained in natural language. These inherently transparent models allow stakeholders to understand exactly how inputs lead to outputs.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "applicable-models/tree-based",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/structured-output",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/project-design",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Building a loan approval system using a decision tree with clear rules based on income, credit history, and debt ratio that can be explained to applicants.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Often less accurate than complex models for difficult problems; deep trees can still be hard to interpret, and training can be unstable with small data changes."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      66,
      68,
      69,
      70
    ]
  },
  {
    "id": 68,
    "name": "Linear/Logistic Models with Few Features",
    "description": "Linear and logistic regression models with a small number of carefully selected features provide transparent predictions through simple, weighted combinations of inputs. Each coefficient represents a feature's impact, and the entire model can be expressed as a single equation. By prioritizing simplicity and selecting the most important features, these models balance interpretability with adequate performance for many applications.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Using a simple logistic regression with 5-10 key features for a medical screening test, where clinicians need to understand and explain the factors behind each risk assessment.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "May miss complex nonlinear relationships or interactions; predictive performance often lower than more complex models for difficult problems."
      }
    ],
    "resources": [],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [
      66,
      67,
      69,
      70
    ]
  },
  {
    "id": 69,
    "name": "Generalized Additive Models (GAMs)",
    "description": "GAMs extend linear models by allowing flexible, nonlinear relationships between individual features and the target while maintaining the additive structure that keeps them interpretable. Each feature's effect is modeled separately as a smooth function, visualized as a curve showing how the feature influences predictions across its range. This approach balances the transparency of linear models with the ability to capture more complex patterns.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Predicting hospital readmission risk with a GAM that shows how the risk varies nonlinearly with patient age while still keeping the model transparent enough for clinical use.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Cannot capture complex interactions between features unless explicitly modeled; setup requires deciding which features need nonlinear treatment, and fitting process is more complex than linear models."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      66,
      67,
      68,
      70
    ]
  },
  {
    "id": 70,
    "name": "Naive Bayes and Probabilistic Models",
    "description": "Naive Bayes and related probabilistic models make predictions based on Bayes' theorem and conditional probabilities, with an assumption that features are conditionally independent. Their transparency comes from the straightforward probabilistic reasoning, where each feature's contribution to the prediction is clearly defined as a likelihood. By explicitly modeling the probability of outcomes given evidence, these models provide intuitive explanations for how inputs affect predictions.",
    "assurance_goals": [
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/project-design",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Using a Naive Bayes classifier for email spam detection that can explain its decisions by showing which words increased the probability of the spam classification.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Independence assumption often violated in real data, leading to miscalibrated probabilities; performance typically lower than more complex models for tasks with intricate patterns."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      66,
      67,
      68,
      69
    ]
  },
  {
    "id": 71,
    "name": "Model Pruning",
    "description": "Simplifies neural networks by removing less important weights or neurons, reducing complexity while retaining performance.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Reducing model size for deployment on mobile devices without significant loss in accuracy.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Over-pruning can significantly reduce accuracy; finding the right pruning threshold is trial-and-error, and pruned models may still be complex to interpret if remaining structure is not simple."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      64
    ]
  },
  {
    "id": 72,
    "name": "Attention Visualisation in Transformers",
    "description": "Visualizes attention weights in transformer-based models to show how the model focuses on different input parts during prediction.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/transformer",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Understanding which words a transformer model focuses on during machine translation tasks.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Similar to other attention interpretations: not always clear if high attention means importance; only applicable to transformer-based models and doesn't explain the model's reasoning beyond attention weights."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      20
    ]
  },
  {
    "id": 73,
    "name": "Neuron Activation Analysis",
    "description": "Analyzes activation patterns of neurons in large language models (LLMs) to interpret their roles and the concepts they represent.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Identifying neurons responsible for syntax or semantics in language models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Interpreting individual neurons requires analyzing large numbers of activations; insights are often qualitative, and important behavior may be distributed across many neurons rather than single ones."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      25,
      77
    ]
  },
  {
    "id": 74,
    "name": "Prompt Sensitivity Analysis",
    "description": "Studies how variations in input prompts affect LLM outputs to understand model behavior and sensitivity.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating how different phrasings influence an LLM's answers in question-answering tasks.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Only surfaces sensitivity to tested prompt variations; may not cover all aspects of model behavior, and systematic prompt generation can be time-consuming or incomplete."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      75,
      76,
      78
    ]
  },
  {
    "id": 75,
    "name": "Causal Mediation Analysis in Language Models",
    "description": "Investigates causal relationships within LLMs by assessing how interventions on specific components affect outputs.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Understanding how adjusting embeddings changes model responses in language generation tasks.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires defining interventions on internal components, which needs expert knowledge of model architecture; results depend on correctness of causal assumptions and can be challenging to interpret conclusively."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      74,
      76,
      78
    ]
  },
  {
    "id": 76,
    "name": "Feature Attribution with Integrated Gradients in NLP",
    "description": "Applies Integrated Gradients to attribute importance of input tokens in LLMs for specific predictions, often producing visualizations.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Identifying words influencing text sentiment classification or topic modeling.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Long text inputs mean integrating over many steps which is slow; attributions can be diffuse across many tokens, and choosing a neutral baseline (e.g., empty or padding text) is non-trivial."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      74,
      75,
      78
    ]
  },
  {
    "id": 77,
    "name": "Concept Activation Vectors (CAVs)",
    "description": "Concept Activation Vectors (CAVs) help explain how neural networks make decisions by identifying specific concepts that the model uses internally. They work by finding mathematical directions in the model's internal representation space that correspond to human-understandable concepts (like 'stripes' or 'young'). This allows researchers to test how much these concepts influence the model's predictions.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Assessing how concepts like 'negativity' affect language model outputs.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Relies on having clearly defined concept examples; concept directions might not exist clearly in the model's internal space, and one must choose which layer to examine, affecting results."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      25,
      73
    ]
  },
  {
    "id": 78,
    "name": "In-Context Learning Analysis",
    "description": "Examines how LLMs learn from examples provided in the input prompt, revealing capacity for few-shot learning.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analyzing the effect of examples on an LLM's ability to perform a new task like translation.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Observational analysis without guaranteed causal insight; any findings can be specific to the tested tasks or prompts, making general conclusions about model behavior difficult."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 2,
    "related_techniques": [
      74,
      75,
      76
    ]
  },
  {
    "id": 79,
    "name": "Reweighing",
    "description": "Assigns weights to instances in the training data to ensure different groups are equally represented in all labels.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Balancing gender representation in credit approval datasets before training a classifier.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Does not remove bias present in features; simply changing weights can lead to higher variance if some groups are underrepresented, and it assumes labels are unbiased which might not hold."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      80,
      81,
      84,
      85
    ]
  },
  {
    "id": 80,
    "name": "Disparate Impact Remover",
    "description": "Edits feature values to reduce dependence between features and protected attributes, aiming to mitigate disparate impact.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Adjusting salary features to reduce gender bias in income prediction models.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Altering features could reduce model accuracy if important information is removed; addresses only measured attributes and might not eliminate bias through proxies."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      79,
      81,
      84,
      85
    ]
  },
  {
    "id": 81,
    "name": "Learning Fair Representations",
    "description": "Learns latent representations that encode data well but obfuscate information about protected attributes.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Creating unbiased data representations for hiring algorithms.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires training a complex model (encoder) with adversarial or constrained objectives; balancing reconstruction and fairness is tricky and may lead to loss of useful information."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "related_techniques": [
      79,
      80,
      84,
      85
    ]
  },
  {
    "id": 82,
    "name": "Fairness GAN",
    "description": "Employs Generative Adversarial Networks to generate fair representations of data that obfuscate protected attributes.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Creating unbiased datasets for training fair image recognition models.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "GAN training is notoriously difficult to stabilize; ensuring fairness might come at the cost of utility, and it needs a large dataset to train the generator and discriminator effectively."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "related_techniques": [
      37,
      86,
      104,
      107,
      108
    ]
  },
  {
    "id": 84,
    "name": "Relabelling",
    "description": "Changes labels of certain instances in training data to reduce bias, often based on fairness constraints.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Modifying labels in loan default datasets to mitigate historical biases.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Altering labels risks introducing new biases or reducing prediction accuracy; deciding which instances to relabel can require a fairness criterion and ground truth fairness assessment."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      79,
      80,
      81,
      85
    ]
  },
  {
    "id": 85,
    "name": "Preferential Sampling",
    "description": "Preferential Sampling addresses dataset imbalances by re-sampling training data with preference for underrepresented groups to achieve fair representation. This preprocessing technique modifies the training distribution to ensure that all demographic groups have adequate representation, helping prevent models from learning biased patterns due to skewed data. The approach can involve oversampling minority groups, undersampling majority groups, or using sophisticated sampling strategies that balance representation while maintaining data quality.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Oversampling minority groups in medical data to train unbiased models.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Over-sampling can cause overfitting to minority examples; under-sampling can drop important data from majority group, and it doesn't adjust the model if it inherently treats groups differently."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      79,
      80,
      81,
      84
    ]
  },
  {
    "id": 86,
    "name": "Attribute Removal (Fairness Through Unawareness)",
    "description": "Attribute Removal (Fairness Through Unawareness) ensures fairness by completely excluding protected attributes such as race, gender, or age from the model's input features. While this approach prevents direct discrimination, it may not eliminate bias if other features are correlated with protected attributes (proxy discrimination). This technique represents the most basic fairness intervention but often needs to be combined with other approaches to address indirect bias through seemingly neutral features.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Removing gender as a feature in employee promotion predictions.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Ignoring protected attributes doesn't guarantee fairness; proxies for the protected attribute in other features can still lead to biased outcomes."
      }
    ],
    "resources": [],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [
      37,
      82,
      104,
      107,
      108
    ]
  },
  {
    "id": 87,
    "name": "Adversarial Debiasing for Text",
    "description": "Applies adversarial debiasing techniques specifically to textual data to mitigate biases in language models.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Reducing gender bias in sentiment analysis models by adversarial training on text data.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Text data can carry subtle biases in language; adversarial removal of bias might strip out important linguistic context, and the technique inherits all challenges of adversarial training."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "related_techniques": [
      34,
      88,
      89,
      90,
      91,
      92,
      93,
      94
    ]
  },
  {
    "id": 88,
    "name": "Fair Adversarial Networks",
    "description": "Fair Adversarial Networks extend the adversarial debiasing framework specifically for deep learning architectures. The technique uses adversarial training where a main network learns to make predictions while an adversarial network attempts to predict sensitive attributes from the main network's representations. This competitive process forces the main network to learn representations that are predictive for the task but uninformative about protected attributes, thereby reducing bias in deep learning models.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Reducing bias in facial recognition systems with adversarial networks.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Extending adversarial debiasing to deep networks can be very complex to implement; requires careful tuning of loss trade-offs, and interpretations of fairness improvement can be opaque."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      34,
      87,
      89,
      90,
      91,
      92,
      93,
      94
    ]
  },
  {
    "id": 89,
    "name": "Prejudice Remover Regulariser",
    "description": "Incorporates a fairness penalty into the learning objective to penalize models that encode biases with respect to protected attributes.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Training logistic regression models with fairness constraints for university admissions.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires setting a hyperparameter for fairness penalty; too high can severely hurt accuracy, too low has little effect. Only applicable to models that can incorporate such a regularizer (e.g., logistic regression)."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      34,
      87,
      88,
      90,
      91,
      92,
      93,
      94
    ]
  },
  {
    "id": 90,
    "name": "Meta Fair Classifier",
    "description": "Meta Fair Classifier is a meta-learning approach that can modify any existing classifier to optimize for fairness metrics while maintaining predictive performance. The technique learns how to adjust model parameters or decision boundaries to satisfy fairness constraints such as demographic parity or equalized odds. This approach is particularly useful when you have a pre-trained model that performs well but exhibits bias, as it can retrofit fairness without requiring complete retraining.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Applying fairness optimization to models in employee evaluation systems.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Meta-learning approach can be complex to implement and require extensive hyperparameter tuning; may result in longer training times and complexity that makes the method less accessible."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      34,
      87,
      88,
      89,
      91,
      92,
      93,
      94
    ]
  },
  {
    "id": 91,
    "name": "Exponentiated Gradient Reduction",
    "description": "Exponentiated Gradient Reduction creates fair machine learning models by treating fairness as a mathematical constraint that must be satisfied during training. It uses a specific optimization algorithm that gradually adjusts the model to balance both accuracy and fairness requirements. The method systematically finds the best possible trade-off between making correct predictions and treating different groups fairly.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Training fair classifiers for employment screening processes.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Involves iterative retraining of a classifier with adjusted weights; might require a convex base learner for theoretical guarantees, and can be sensitive to convergence criteria."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      34,
      87,
      88,
      89,
      90,
      92,
      93,
      94
    ]
  },
  {
    "id": 92,
    "name": "Fair Transfer Learning",
    "description": "Fair Transfer Learning adapts pre-trained models from one domain to another while explicitly preserving fairness constraints across different contexts or populations. This technique addresses the challenge that fairness properties may not transfer when adapting models to new domains with different demographic compositions or data distributions. The approach typically involves constraint-aware fine-tuning or domain adaptation techniques that maintain equitable performance across groups in the target domain.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Transferring fairness-aware models from one region's data to another in healthcare analytics.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Fairness achieved in one domain might not directly translate to another if data distributions differ; approach can be complicated to design and may need careful tuning to preserve fairness across domains."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      34,
      87,
      88,
      89,
      90,
      91,
      93,
      94
    ]
  },
  {
    "id": 93,
    "name": "Adaptive Sensitive Reweighting",
    "description": "Adaptive Sensitive Reweighting dynamically adjusts instance weights during the training process based on the model's current performance across different demographic groups. Unlike static reweighting approaches, this technique continuously monitors bias indicators and adaptively modifies the importance of training examples to reduce disparities. The method helps prevent the model from learning biased patterns by emphasizing examples from underperforming groups as training progresses.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Balancing performance in speech recognition across accents and dialects.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires monitoring model performance across groups in training, which can introduce instability; if not tuned properly, could oscillate or focus too much on one group at a time."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      34,
      87,
      88,
      89,
      90,
      91,
      92,
      94
    ]
  },
  {
    "id": 94,
    "name": "Multi-Accuracy Boosting",
    "description": "Improves accuracy uniformly across groups by correcting errors where the model performs poorly for certain groups.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Enhancing model performance for underrepresented groups in disease prediction.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Targets error patterns in subgroups which requires identifying those groups or error regions; could increase complexity of the model and may overfit if very granular corrections are made."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      34,
      87,
      88,
      89,
      90,
      91,
      92,
      93
    ]
  },
  {
    "id": 95,
    "name": "Equalised Odds Post-Processing",
    "description": "Equalized Odds Post-Processing is a technique that modifies a model's output probabilities after training to ensure equal true positive rates and false positive rates across different demographic groups. This post-processing approach allows practitioners to achieve fairness without retraining the model, making it particularly useful when the original training process cannot be modified. The technique typically involves finding optimal threshold adjustments for each group.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring fairness in recidivism risk assessments used in judicial decisions.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Adjusting outputs can reduce model confidence or require randomization in decisions; may sacrifice individual consistency (similar cases get different outcomes to satisfy group rates)."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      96,
      97,
      98
    ]
  },
  {
    "id": 96,
    "name": "Threshold Optimiser",
    "description": "Threshold Optimizer adjusts decision thresholds for different demographic groups after model training to satisfy specific fairness constraints. This post-processing technique allows practitioners to achieve fairness goals like demographic parity or equalized opportunity without modifying the underlying model. The optimizer finds optimal threshold values for each group that balance fairness requirements with overall model performance, making it particularly useful when fairness considerations arise after model deployment.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/process"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring equal acceptance rates in college admissions across demographics.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires a held-out set to determine thresholds per group; if distributions shift, thresholds may need recalibration. Also, using different thresholds per group can raise legal or ethical concerns in deployment."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 1,
    "related_techniques": [
      95,
      97,
      98
    ]
  },
  {
    "id": 97,
    "name": "Reject Option Classification",
    "description": "Changes decisions where the model is least certain, favoring the disadvantaged group within this uncertain region.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Mitigating bias in hiring decisions by adjusting uncertain predictions.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Only applicable when model uncertainty can be estimated; choosing the 'reject' region and how to reassign decisions can be subjective and might reject too many instances if tuned conservatively."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      95,
      96,
      98
    ]
  },
  {
    "id": 98,
    "name": "Calibration with Equality of Opportunity",
    "description": "Adjusts probabilities to achieve equal true positive rates across groups while maintaining calibration within each group.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/calibration-set",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Balancing opportunity in credit scoring across different ethnic groups.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Achieving calibration within each group while equalizing true positive rates can be at odds with overall calibration; it may involve solving for probabilities in a way that lowers overall model calibration or accuracy."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      95,
      96,
      97
    ]
  },
  {
    "id": 101,
    "name": "Equal Opportunity Difference",
    "description": "Computes the difference in true positive rates between groups, assessing fairness in terms of equal opportunity.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Assessing fairness in medical diagnosis models across age groups.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Addresses only true positive rates, ignoring false positive disparities; requires accurate ground truth labels for the positive class, and improving TPR for one group might increase FPR for that group."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      33,
      102
    ]
  },
  {
    "id": 102,
    "name": "Average Odds Difference",
    "description": "Average Odds Difference measures fairness by calculating the average difference in both false positive rates and true positive rates between different demographic groups. This metric captures how consistently a model performs across groups for both positive and negative predictions. A value of 0 indicates perfect fairness under the equalized odds criterion, while larger absolute values indicate greater disparities in model performance between groups.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Measuring bias in criminal risk assessment tools.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Combines multiple error rates which may obscure specific issues (a model could have one high and one low disparity and still average out); still needs ground truth labels and a balanced trade-off with accuracy."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      33,
      101
    ]
  },
  {
    "id": 103,
    "name": "Measuring Individual Fairness (Consistency)",
    "description": "Evaluates whether similar individuals receive similar predictions, assessing fairness at an individual level.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/individual",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring similar credit applicants receive similar loan decisions.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires a domain-specific similarity metric between individuals; hard to define and validate, and ensuring consistency can conflict with achieving good group-level metrics."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      31,
      55
    ]
  },
  {
    "id": 104,
    "name": "Algorithmic Fairness using K-NN",
    "description": "Uses K-nearest neighbors to assess individual fairness by comparing predictions among similar instances.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/individual",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating fairness in personalized recommendation systems.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Relies on the quality of the distance metric; high-dimensional data can make nearest neighbor comparisons noisy (curse of dimensionality), and it doesn't directly fix the model, just evaluates it."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      37,
      82,
      86,
      107,
      108
    ]
  },
  {
    "id": 105,
    "name": "Path-Specific Counterfactual Fairness Assessment",
    "description": "Path-Specific Counterfactual Fairness Assessment evaluates fairness by examining specific causal pathways in a model's decision-making process. Unlike general counterfactual fairness, this approach allows practitioners to identify and intervene on particular causal paths that may introduce bias, while preserving other legitimate pathways. The technique helps distinguish between direct discrimination (through protected attributes) and indirect discrimination (through seemingly neutral factors that correlate with protected attributes).",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "fairness-approach/causal",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Modeling fair decisions in advertising without altering legitimate causal effects.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires identifying which causal pathways are 'allowable' and which are not\u2014a subjective decision; analyzing specific paths adds complexity to the causal model and the fairness criterion."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      35,
      106
    ]
  },
  {
    "id": 106,
    "name": "Causal Fairness Assessment with Do-Calculus",
    "description": "Utilizes causal inference techniques to assess and mitigate bias by computing interventional distributions.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "fairness-approach/causal",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Understanding bias in hiring decisions through causal relationships.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Strongly dependent on having a correct causal graph; using do-calculus in practice can be computationally intense and challenging outside of relatively simple models or well-specified causal relationships."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      35,
      105
    ]
  },
  {
    "id": 107,
    "name": "Diversity Constraints in Recommendations",
    "description": "Incorporates diversity and fairness constraints in recommendation systems for varied and fair content exposure.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring fair representation of genres in music recommendation platforms.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "May reduce accuracy or relevance of recommendations if forced diversity conflicts with user preferences; implementing constraints can complicate the recommendation algorithm and objective function."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      37,
      82,
      86,
      104,
      108
    ]
  },
  {
    "id": 108,
    "name": "Bayesian Fairness Regularization",
    "description": "Applies Bayesian methods to include fairness as a prior, allowing probabilistic interpretation of fairness constraints.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Applying fairness regularization in Bayesian models for credit risk assessment.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Selecting appropriate prior distributions for fairness is non-trivial; Bayesian methods can be computationally intensive (e.g., requiring sampling) and outcomes can be sensitive to prior assumptions."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      37,
      82,
      86,
      104,
      107
    ]
  }
]