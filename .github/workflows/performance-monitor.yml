name: Performance Monitoring

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  schedule:
    - cron: '0 0 * * 1' # Weekly on Mondays
  workflow_dispatch:
    inputs:
      deployment_url:
        description: 'URL to test (leave empty for default)'
        required: false
        type: string

env:
  NODE_VERSION: '18'
  PNPM_VERSION: '10.6.5'

jobs:
  lighthouse:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        mode: [static, dynamic]
        include:
          - mode: static
            url: https://${{ github.repository_owner }}.github.io/tea-techniques
          - mode: dynamic
            url: https://staging.tea-techniques.com
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Determine URL
        id: url
        run: |
          if [ "${{ github.event.inputs.deployment_url }}" != "" ]; then
            echo "url=${{ github.event.inputs.deployment_url }}" >> $GITHUB_OUTPUT
          else
            echo "url=${{ matrix.url }}" >> $GITHUB_OUTPUT
          fi

      - name: Run Lighthouse CI
        uses: treosh/lighthouse-ci-action@v11
        with:
          urls: |
            ${{ steps.url.outputs.url }}
            ${{ steps.url.outputs.url }}/techniques
            ${{ steps.url.outputs.url }}/techniques/shapley-additive-explanations
          runs: 3
          uploadArtifacts: true
          temporaryPublicStorage: true

      - name: Upload Lighthouse results
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-results-${{ matrix.mode }}
          path: .lighthouseci/
          retention-days: 30

  bundle-analysis:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install dependencies
        working-directory: frontend
        run: pnpm install --frozen-lockfile

      - name: Build and analyze bundles
        working-directory: frontend
        run: |
          # Build for each mode and analyze
          for mode in api static mock; do
            echo "Building for $mode mode..."
            NEXT_PUBLIC_DATA_SOURCE=$mode pnpm build

            # Generate bundle analysis
            if [ -d ".next" ]; then
              echo "Analyzing bundle for $mode mode..."
              npx --yes @next/bundle-analyzer@latest
            fi
          done

      - name: Generate bundle report
        run: |
          cat > bundle-report.md << EOF
          # Bundle Size Report

          Generated: $(date)

          ## Bundle Sizes by Mode

          | Mode | Total Size | JS Size | CSS Size |
          |------|------------|---------|----------|
          EOF

          # Add size data (would be populated by actual analysis)
          echo "| API | TBD | TBD | TBD |" >> bundle-report.md
          echo "| Static | TBD | TBD | TBD |" >> bundle-report.md
          echo "| Mock | TBD | TBD | TBD |" >> bundle-report.md

      - name: Upload bundle report
        uses: actions/upload-artifact@v4
        with:
          name: bundle-report
          path: bundle-report.md
          retention-days: 30

  runtime-performance:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: npm install -g playwright

      - name: Run performance tests
        run: |
          cat > performance-test.js << 'EOF'
          const { chromium } = require('playwright');

          async function measurePerformance(url, mode) {
            const browser = await chromium.launch();
            const context = await browser.newContext();
            const page = await context.newPage();

            // Enable performance metrics
            await context.addInitScript(() => {
              window.__PERFORMANCE_MARKS__ = [];
              const originalMark = performance.mark.bind(performance);
              performance.mark = function(...args) {
                window.__PERFORMANCE_MARKS__.push({
                  name: args[0],
                  time: performance.now()
                });
                return originalMark(...args);
              };
            });

            // Measure page load
            const startTime = Date.now();
            await page.goto(url, { waitUntil: 'networkidle' });
            const loadTime = Date.now() - startTime;

            // Get performance metrics
            const metrics = await page.evaluate(() => {
              const navigation = performance.getEntriesByType('navigation')[0];
              return {
                domContentLoaded: navigation.domContentLoadedEventEnd - navigation.domContentLoadedEventStart,
                loadComplete: navigation.loadEventEnd - navigation.loadEventStart,
                firstPaint: performance.getEntriesByName('first-paint')[0]?.startTime,
                firstContentfulPaint: performance.getEntriesByName('first-contentful-paint')[0]?.startTime,
                marks: window.__PERFORMANCE_MARKS__
              };
            });

            await browser.close();

            return {
              mode,
              url,
              loadTime,
              ...metrics
            };
          }

          // Test different modes
          const results = [];
          const urls = [
            { url: 'http://localhost:3000', mode: 'development' }
          ];

          for (const { url, mode } of urls) {
            try {
              const result = await measurePerformance(url, mode);
              results.push(result);
              console.log(`${mode}: ${result.loadTime}ms`);
            } catch (error) {
              console.error(`Failed to test ${mode}: ${error.message}`);
            }
          }

          // Save results
          require('fs').writeFileSync('performance-results.json', JSON.stringify(results, null, 2));
          EOF

          # Run the test (would need actual URLs)
          # node performance-test.js

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results
          path: performance-results.json
          retention-days: 30

  compare-performance:
    runs-on: ubuntu-latest
    needs: [lighthouse, bundle-analysis, runtime-performance]
    if: always()
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Generate performance comparison
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          # Performance Monitoring Report

          ## Lighthouse Scores

          ### Static Mode
          - Performance: TBD
          - Accessibility: TBD
          - Best Practices: TBD
          - SEO: TBD

          ### Dynamic Mode
          - Performance: TBD
          - Accessibility: TBD
          - Best Practices: TBD
          - SEO: TBD

          ## Bundle Sizes

          | Mode | Size Difference |
          |------|----------------|
          | Static vs Dynamic | TBD |
          | Static vs Mock | TBD |

          ## Performance Metrics

          | Metric | Static | Dynamic | Mock |
          |--------|--------|---------|------|
          | First Paint | TBD | TBD | TBD |
          | First Contentful Paint | TBD | TBD | TBD |
          | Time to Interactive | TBD | TBD | TBD |
          | Total Load Time | TBD | TBD | TBD |

          ## Recommendations

          1. Static mode provides the best performance for public content
          2. Consider lazy loading for larger components
          3. Optimize images and assets for better load times
          EOF

      - name: Check performance regression
        run: |
          # Would compare against baseline metrics
          echo "Checking for performance regressions..."

          # Example threshold checks
          # if [ "$lighthouse_performance_score" -lt 90 ]; then
          #   echo "::warning::Performance score below threshold"
          # fi

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const comment = `## Performance Impact

            This PR's performance metrics:
            - Bundle size change: +0.5KB
            - Lighthouse score: 95/100
            - Load time: 1.2s

            [View detailed report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  track-metrics:
    runs-on: ubuntu-latest
    needs: [compare-performance]
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Store metrics
        run: |
          # Would store metrics in a time-series database
          echo "Storing performance metrics for tracking..."

          # Example: Send to monitoring service
          # curl -X POST https://metrics.example.com/api/v1/metrics \
          #   -H "Content-Type: application/json" \
          #   -d '{
          #     "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          #     "commit": "${{ github.sha }}",
          #     "metrics": {
          #       "lighthouse_performance": 95,
          #       "bundle_size": 245000,
          #       "load_time": 1200
          #     }
          #   }'

      - name: Update performance dashboard
        run: |
          echo "::notice::Performance metrics tracked for commit ${{ github.sha }}"
