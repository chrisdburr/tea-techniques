{
  "tag": {
    "name": "applicable-models/cnn",
    "slug": "applicable-models-cnn",
    "count": 5,
    "category": "applicable-models"
  },
  "techniques": [
    {
      "slug": "taylor-decomposition",
      "name": "Taylor Decomposition",
      "description": "Taylor Decomposition is a mathematical technique that explains neural network predictions by computing first-order and higher-order derivatives of the network's output with respect to input features. It decomposes the prediction into relevance scores that indicate how much each input feature and feature interaction contributes to the final decision. The method uses Layer-wise Relevance Propagation (LRP) principles to trace prediction contributions backwards through the network layers, providing precise mathematical attributions for each input element.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/explainability/property/fidelity",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analyzing which pixels in an image contribute most to a convolutional neural network's classification decision, showing both positive and negative relevance scores for different regions of the input image.",
          "goal": "Explainability"
        },
        {
          "description": "Understanding how different word embeddings in a sentiment analysis model contribute to the final sentiment score, revealing which terms drive positive vs negative predictions.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Mathematically complex requiring deep understanding of calculus and neural network architectures."
        },
        {
          "description": "Computationally intensive as it requires computing gradients and higher-order derivatives through the entire network."
        },
        {
          "description": "Approximations used in practice may introduce errors that affect attribution accuracy."
        },
        {
          "description": "Limited tooling availability compared to other explainability methods, with most implementations being research-focused rather than production-ready."
        }
      ],
      "resources": [
        {
          "title": "Explaining NonLinear Classification Decisions with Deep Taylor Decomposition",
          "url": "http://arxiv.org/pdf/1512.02479v1",
          "source_type": "technical_paper",
          "authors": [
            "Grégoire Montavon",
            "Sebastian Bach",
            "Alexander Binder",
            "Wojciech Samek",
            "Klaus-Robert Müller"
          ],
          "publication_date": "2015-12-08"
        },
        {
          "title": "A Rigorous Study Of The Deep Taylor Decomposition",
          "url": "http://arxiv.org/pdf/2211.08425v1",
          "source_type": "technical_paper",
          "authors": [
            "Leon Sixt",
            "Tim Landgraf"
          ],
          "publication_date": "2022-11-14"
        },
        {
          "title": "sebastian-lapuschkin/lrp_toolbox",
          "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "contextual-decomposition",
        "influence-functions"
      ]
    },
    {
      "slug": "gradient-weighted-class-activation-mapping",
      "name": "Gradient-weighted Class Activation Mapping",
      "description": "Grad-CAM creates visual heatmaps showing which regions of an image a convolutional neural network focuses on when making a specific classification. Unlike pixel-level techniques, Grad-CAM produces coarser region-based explanations by using gradients from the predicted class to weight the CNN's final feature maps, then projecting these weighted activations back to create an overlay on the original image. This provides intuitive visual explanations of where the model is 'looking' for evidence of different classes.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/visualization-methods/activation-maps",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Validating that a melanoma detection model focuses on the actual skin lesion rather than surrounding healthy skin, medical equipment, or artifacts when making cancer/benign classifications.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging an autonomous vehicle's traffic sign recognition system by visualising whether the model correctly focuses on the sign itself rather than background objects, shadows, or irrelevant visual elements.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a medical imaging system for racial bias by examining whether diagnostic predictions inappropriately focus on skin tone regions rather than actual pathological features, ensuring equitable healthcare AI deployment.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires access to the CNN's internal feature maps and gradients, limiting use to white-box scenarios."
        },
        {
          "description": "Resolution is constrained by the final convolutional layer's feature map size, producing coarser localisation than pixel-level methods."
        },
        {
          "description": "Only applicable to CNN architectures with clearly defined convolutional layers, not suitable for other neural network types."
        },
        {
          "description": "May highlight regions that correlate with the class but aren't causally important for the model's decision-making process."
        }
      ],
      "resources": [
        {
          "title": "jacobgil/pytorch-grad-cam",
          "url": "https://github.com/jacobgil/pytorch-grad-cam",
          "source_type": "software_package"
        },
        {
          "title": "Grad-CAM: Visualize class activation maps with Keras, TensorFlow ...",
          "url": "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/",
          "source_type": "tutorial"
        },
        {
          "title": "kazuto1011/grad-cam-pytorch",
          "url": "https://github.com/kazuto1011/grad-cam-pytorch",
          "source_type": "software_package"
        },
        {
          "title": "A Tutorial on Explainable Image Classification for Dementia Stages Using Convolutional Neural Network and Gradient-weighted Class Activation Mapping",
          "url": "https://www.semanticscholar.org/paper/8b1139cb06cfe5ba69e2fd05e1450b43df031a02",
          "source_type": "documentation",
          "authors": [
            "Kevin Kam Fung Yuen"
          ]
        },
        {
          "title": "A Guide to Grad-CAM in Deep Learning - Analytics Vidhya",
          "url": "https://www.analyticsvidhya.com/blog/2023/12/grad-cam-in-deep-learning/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "Grad-CAM",
      "related_techniques": [
        "saliency-maps",
        "occlusion-sensitivity"
      ]
    },
    {
      "slug": "classical-attention-analysis-in-neural-networks",
      "name": "Classical Attention Analysis in Neural Networks",
      "description": "Classical attention mechanisms in RNNs and CNNs create alignment matrices and temporal attention patterns that show how models focus on different input elements over time or space. This technique analyses these traditional attention patterns, particularly in encoder-decoder architectures and sequence-to-sequence models, where attention weights reveal which source elements influence each output step. Unlike transformer self-attention analysis, this focuses on understanding alignment patterns, temporal dependencies, and encoder-decoder attention dynamics in classical neural architectures.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/rnn",
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/visualization-methods/attention-patterns",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/fidelity",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing encoder-decoder attention in a neural machine translation model to verify the alignment between source and target words, ensuring the model learns proper translation correspondences rather than positional biases.",
          "goal": "Explainability"
        },
        {
          "description": "Examining temporal attention patterns in an RNN-based image captioning model to understand how attention moves across different image regions as it generates each word of the caption description.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Attention weights are not always strongly correlated with feature importance for the final prediction."
        },
        {
          "description": "High attention does not necessarily imply causal influence - models can attend to irrelevant but correlated features."
        },
        {
          "description": "Only applicable to neural network architectures that explicitly use attention mechanisms."
        },
        {
          "description": "Interpretation can be misleading without understanding the specific attention mechanism implementation and training dynamics."
        }
      ],
      "resources": [
        {
          "title": "An Attentive Survey of Attention Models",
          "url": "https://www.semanticscholar.org/paper/a8427ce5aee6d62800c725588e89940ed4910e0d",
          "source_type": "documentation",
          "authors": [
            "S. Chaudhari",
            "Gungor Polatkan",
            "R. Ramanath",
            "Varun Mithal"
          ]
        },
        {
          "title": "Attention, please! A survey of neural attention models in deep learning",
          "url": "https://www.semanticscholar.org/paper/44930df2a3186edb58c4d6f6e5ed828c5d6a0089",
          "source_type": "documentation",
          "authors": [
            "Alana de Santana Correia",
            "E. Colombini"
          ]
        },
        {
          "title": "ecco - Explain, Analyze, and Visualize NLP Language Models",
          "url": "https://github.com/jalammar/ecco",
          "source_type": "software_package"
        },
        {
          "title": "Enhancing Sentiment Analysis of Twitter Data Using Recurrent Neural Networks with Attention Mechanism",
          "url": "https://www.semanticscholar.org/paper/c59e0158280a567114ae8ca64a932eefd127e0aa",
          "source_type": "technical_paper",
          "authors": [
            "S. Nithya",
            "X. A. Presskila",
            "B. Sakthivel",
            "R. Krishnan",
            "K. Narayanan",
            "S. Sundararajan"
          ]
        },
        {
          "title": "Can Neural Networks Develop Attention? Google Thinks they Can ...",
          "url": "https://www.kdnuggets.com/2019/11/neural-networks-develop-attention-google.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "attention-visualisation-in-transformers"
      ]
    },
    {
      "slug": "concept-activation-vectors",
      "name": "Concept Activation Vectors",
      "description": "Concept Activation Vectors (CAVs), also known as Testing with Concept Activation Vectors (TCAV), identify mathematical directions in neural network representation space that correspond to human-understandable concepts such as 'stripes', 'young', or 'medical equipment'. The technique works by finding linear directions that separate activations of concept examples from non-concept examples, then measuring how much these concept directions influence the model's predictions. This provides quantitative answers to questions like 'How much does the concept of youth affect this model's hiring decisions?' enabling systematic bias detection and model understanding.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/transformer",
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/visualization",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-knowledge",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use/auditing",
        "technique-type/algorithmic",
        "assurance-goal-category/explainability/representation-analysis/concept-identification",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/causal-pathways",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/causality"
      ],
      "example_use_cases": [
        {
          "description": "Auditing a medical imaging model to verify it focuses on diagnostic features (like 'tumour characteristics') rather than irrelevant concepts (like 'scanner type' or 'patient positioning') when classifying chest X-rays, ensuring clinical decisions rely on medically relevant information.",
          "goal": "Explainability"
        },
        {
          "description": "Testing whether a hiring algorithm's resume screening decisions are influenced by concepts related to protected characteristics such as 'gender-associated names', 'prestigious universities', or 'employment gaps', enabling systematic bias detection and compliance verification.",
          "goal": "Fairness"
        },
        {
          "description": "Providing regulatory-compliant explanations for financial lending decisions by quantifying how concepts like 'debt-to-income ratio', 'employment stability', and 'credit history length' influence loan approval models, with precise sensitivity scores for audit documentation.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires clearly defined concept examples and non-concept examples, which can be challenging to obtain for abstract or subjective concepts."
        },
        {
          "description": "Assumes that meaningful concept directions exist as linear separable directions in the model's internal representation space, which may not hold for all concepts."
        },
        {
          "description": "Results depend heavily on which network layer is examined, as different layers capture different levels of abstraction and concept representation."
        },
        {
          "description": "Computational cost grows significantly with model size and number of concepts tested, though recent advances like FastCAV address this limitation."
        },
        {
          "description": "Interpretation requires domain expertise to define meaningful concepts and understand the significance of sensitivity scores in practical contexts."
        }
      ],
      "resources": [
        {
          "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks",
          "url": "http://arxiv.org/pdf/2505.17883v1",
          "source_type": "technical_paper",
          "authors": [
            "Laines Schmalwasser",
            "Niklas Penzel",
            "Joachim Denzler",
            "Julia Niebling"
          ],
          "publication_date": "2025-05-23"
        },
        {
          "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
          "url": "http://arxiv.org/pdf/2311.15303v1",
          "source_type": "technical_paper",
          "authors": [
            "Avani Gupta",
            "Saurabh Saini",
            "P J Narayanan"
          ],
          "publication_date": "2023-11-26"
        },
        {
          "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations",
          "url": "http://arxiv.org/pdf/2503.05522v1",
          "source_type": "technical_paper",
          "authors": [
            "Eren Erogullari",
            "Sebastian Lapuschkin",
            "Wojciech Samek",
            "Frederik Pahde"
          ],
          "publication_date": "2025-03-07"
        },
        {
          "title": "Concept Gradient: Concept-based Interpretation Without Linear Assumption",
          "url": "http://arxiv.org/pdf/2208.14966v2",
          "source_type": "technical_paper",
          "authors": [
            "Andrew Bai",
            "Chih-Kuan Yeh",
            "Pradeep Ravikumar",
            "Neil Y. C. Lin",
            "Cho-Jui Hsieh"
          ],
          "publication_date": "2022-08-31"
        },
        {
          "title": "SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation",
          "url": "http://arxiv.org/pdf/2310.07698v1",
          "source_type": "technical_paper",
          "authors": [
            "Bo Pan",
            "Zhenke Liu",
            "Yifei Zhang",
            "Liang Zhao"
          ],
          "publication_date": "2023-10-11"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "acronym": "CAVs",
      "related_techniques": [
        "prototype-and-criticism-models",
        "neuron-activation-analysis"
      ]
    },
    {
      "slug": "fair-adversarial-networks",
      "name": "Fair Adversarial Networks",
      "description": "An in-processing fairness technique that employs adversarial training with dual neural networks to learn fair representations. The method consists of a predictor network that learns the main task whilst an adversarial discriminator network simultaneously attempts to predict sensitive attributes from the predictor's hidden representations. Through this adversarial min-max game, the predictor is incentivised to learn features that are informative for the task but statistically independent of protected attributes, effectively removing bias at the representation level in deep learning models.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/cnn",
        "applicable-models/gan",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training a facial recognition system that maintains high accuracy for person identification whilst ensuring equal performance across different ethnic groups, using adversarial training to remove race-related features from learned representations.",
          "goal": "Fairness"
        },
        {
          "description": "Developing a resume screening neural network that provides transparent evidence of bias mitigation by demonstrating that learned features cannot predict gender, whilst maintaining predictive performance for job suitability assessment.",
          "goal": "Transparency"
        },
        {
          "description": "Creating a medical image analysis model that achieves reliable diagnostic performance across patient demographics by using adversarial debiasing to ensure age and gender information cannot be extracted from diagnostic features.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Implementation complexity is high, requiring careful design of adversarial loss functions and balancing multiple competing objectives during training."
        },
        {
          "description": "Sensitive to hyperparameter choices, particularly the trade-off weights between prediction accuracy and adversarial loss, which require extensive tuning."
        },
        {
          "description": "Adversarial training can be unstable, with potential for mode collapse or failure to converge, especially in complex deep learning architectures."
        },
        {
          "description": "Interpretability of fairness improvements can be limited, as it may be difficult to verify that sensitive attributes are truly removed from learned representations."
        },
        {
          "description": "Computational overhead is significant due to training two networks simultaneously, increasing both training time and resource requirements."
        }
      ],
      "resources": [
        {
          "title": "Fair Adversarial Networks",
          "url": "http://arxiv.org/pdf/2002.12144v1",
          "source_type": "technical_paper",
          "authors": [
            "George Cevora"
          ],
          "publication_date": "2020-02-23"
        },
        {
          "title": "Demonstrating Rosa: the fairness solution for any Data Analytic pipeline",
          "url": "http://arxiv.org/pdf/2003.00899v2",
          "source_type": "technical_paper",
          "authors": [
            "Kate Wilkinson",
            "George Cevora"
          ],
          "publication_date": "2020-02-28"
        },
        {
          "title": "Triangular Trade-off between Robustness, Accuracy, and Fairness in Deep Neural Networks: A Survey",
          "url": "https://www.semanticscholar.org/paper/13b0444d079bea1c8c57a6082200b67ab5f4616e",
          "source_type": "documentation",
          "authors": [
            "Jingyang Li",
            "Guoqiang Li"
          ],
          "publication_date": "2025-02-10"
        },
        {
          "title": "Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks",
          "url": "https://www.semanticscholar.org/paper/6995779ac582c5f2436cfb82a3c8cf5ca72bae2f",
          "source_type": "technical_paper",
          "authors": [
            "Resmi Ramachandranpillai",
            "Md Fahim Sikder",
            "David Bergström",
            "Fredrik Heintz"
          ],
          "publication_date": "2023-12-14"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "adversarial-debiasing",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    }
  ],
  "count": 5
}