id,name,description,model_dependency,assurance_goals,categories,subcategories,attributes,example_use_cases,limitations,resources,complexity_rating,computational_cost_rating
80,Adversarial Training,Trains models with adversarially perturbed inputs to make them robust against malicious attacks. By including crafted adversarial examples during training, the model learns to correct its predictions even when the input is slightly manipulated, thereby improving its security against such inputs.,Model-Specific,Security,[{"goal": "Security", "category": "Adversarial Robustness"}],[{"category": "Adversarial Robustness", "subcategory": "Robust Training"}],[{"type": "Security Approach", "value": "Adversarial Defense"}, {"type": "Project Lifecycle Stage", "value": "Training"}],[{"description": "Training an image classifier on both normal and adversarially noised images so it correctly classifies objects even when an attacker adds small perturbations to the input.", "goal": "Security"}, {"description": "Hardening a spam detection model by generating adversarial spam examples (that evade the original model) and retraining on them so that the model learns to catch cleverly disguised spam.", "goal": "Security"}],[{"description": "Significantly increases training time as it requires generating adversarial examples at each iteration and then optimizing against them."}, {"description": "Models remain vulnerable to types of attacks not seen during training; new or stronger adversarial techniques might still succeed."}, {"description": "Can reduce natural accuracy (on unperturbed data) if not carefully balanced, as the model might over-focus on adversarial patterns."}],[],3,3
81,Differential Privacy,Incorporates random noise into model training or outputs to protect individual data points from being identified or exploited. Using techniques like DP-SGD (Differentially Private Stochastic Gradient Descent), models can learn patterns in data without memorizing details about any specific example, ensuring that sensitive personal information remains private even if model parameters are exposed.,Model-Agnostic,Security,[{"goal": "Security", "category": "Privacy Preservation"}],[{"category": "Privacy Preservation", "subcategory": "Data Privacy"}],[{"type": "Security Approach", "value": "Data Privacy"}, {"type": "Project Lifecycle Stage", "value": "Training"}],[{"description": "Training a hospital ML model (e.g., for disease prediction) with differential privacy so that it can be published or shared without revealing any individual patient\u2019s medical record details.", "goal": "Security"}, {"description": "Using differential privacy in an online recommendation system so that the model cannot inadvertently leak a particular user\u2019s preferences or transaction history when queried or inspected.", "goal": "Security"}],[{"description": "Models trained with strong privacy guarantees often have lower accuracy because the added noise obscures some signal in the data."}, {"description": "Setting the privacy parameters (like epsilon in differential privacy) involves a trade-off between privacy and utility, and understanding the implications can be complex."}, {"description": "Differential privacy techniques can be complicated to implement and require careful tuning to truly guarantee privacy without unnecessarily sacrificing performance."}],[],3,3
82,Formal Verification of Neural Networks,Uses mathematical methods to prove that a neural network satisfies certain safety properties (like never producing an output above a threshold for inputs in a given range). Through techniques like SMT solving or abstract interpretation, one can rigorously check that within specified bounds (e.g., pixel perturbations, sensor noise), the network\u2019s decisions won\u2019t violate safety constraints.,Model-Specific,Safety,[{"goal": "Safety", "category": "Verification"}],[{"category": "Verification", "subcategory": "Formal Methods"}],[{"type": "Verification Approach", "value": "Formal Methods"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "Verifying an autonomous vehicle\u2019s neural network controller to ensure that for any possible sensor reading within normal operating ranges, the steering output won\u2019t cause a drastic unsafe maneuver.", "goal": "Safety"}, {"description": "Proving that a simple neural network used in aircraft control will always output a throttle value within safe limits for all inputs within the expected flight envelope.", "goal": "Safety"}],[{"description": "Scalability is a major issue: formal verification methods often can\u2019t handle large networks or complex models, so they may only be applied to smaller or simplified versions."}, {"description": "Requires precise specification of safety properties and input bounds, which might not capture every real-world scenario or subtle failure mode."}, {"description": "Formal proofs guarantee safety only within the checked conditions; they do not account for unknown unknowns or inputs outside the considered range, which could still break the system."}],[],3,3
83,Deep Ensembles,Trains multiple models (often neural networks) independently and then combines their predictions (e.g., by averaging). This approach not only often improves accuracy but also provides a way to estimate uncertainty: if all models agree, the prediction is confident; if they diverge, it signals uncertainty. Deep ensembles thus enhance reliability and robustness, helping detect when a model might be unsure or facing an out-of-distribution input.,Model-Agnostic,Safety,[{"goal": "Safety", "category": "Uncertainty and Reliability"}],[{"category": "Uncertainty and Reliability", "subcategory": "Ensembles & Uncertainty"}],[{"type": "Safety Approach", "value": "Robustness and Uncertainty"}, {"type": "Project Lifecycle Stage", "value": "Training and Inference"}],[{"description": "Deploying an ensemble of five neural networks for medical image diagnosis so that if one model is uncertain or errors, the others can compensate, and the system can flag cases where the models disagree for human review.", "goal": "Safety"}, {"description": "Using deep ensembles in a weather forecasting model to provide not just a single prediction but a range (with confidence intervals), alerting forecasters when there is high model uncertainty for a particular forecast.", "goal": "Safety"}],[{"description": "Increases computational resources needed: training and running multiple models is significantly more expensive than a single model."}, {"description": "Ensembles still might not cover certain systematic biases if all models are trained on the same data in similar ways; they can fail together on certain out-of-distribution cases."}, {"description": "Interpretability can suffer, as it\u2019s not just one model\u2019s reasoning but an aggregate of many, and debugging an ensemble can be more complex."}],[],2,3
84,Model Calibration,Adjusts a model so that its predicted probabilities reflect true probabilities. After training, techniques like Platt scaling or isotonic regression can be applied to calibration sets to ensure that, for example, among all instances the model says have a 70% chance, roughly 70% actually occur. Calibration improves trust in probabilities, which is critical for safety-related decisions and downstream decision-making processes.,Model-Agnostic,Safety,[{"goal": "Safety", "category": "Uncertainty and Reliability"}],[{"category": "Uncertainty and Reliability", "subcategory": "Calibration"}],[{"type": "Safety Approach", "value": "Uncertainty and Reliability"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "Calibrating a probability-of-default model so that when it predicts a 0.2 default risk for loans, historically about 20% of those loans actually default, thus the bank can trust the probabilities to make risk decisions.", "goal": "Safety"}, {"description": "Adjusting the output of a medical diagnosis neural network via Platt scaling on a validation set, so that its output scores can be interpreted as true likelihoods of disease, enabling doctors to make informed decisions based on those probabilities.", "goal": "Safety"}],[{"description": "Calibration procedures might slightly degrade raw accuracy or AUC if not done carefully, as they adjust the model outputs to better align with observed frequencies."}, {"description": "They require extra validation data for tuning; if the data is not representative, the calibration might be off for deployment populations."}, {"description": "Calibration usually assumes the data distribution remains the same; if it shifts, probabilities can become miscalibrated again, necessitating recalibration."}],[],1,1
85,"Adversarial Testing (Red Teaming)",Involves actively probing and attacking a trained model to find weaknesses, much like a "red team" would attempt to breach a security system. By generating adversarial inputs or exploring edge cases (including nonsensical or worst-case scenarios), testers identify situations where the model fails or behaves unexpectedly. This helps in evaluating the model's safety and security, and informs improvements or guardrails.,Model-Agnostic,Security,[{"goal": "Security", "category": "Robustness Evaluation"}],[{"category": "Robustness Evaluation", "subcategory": "Red Teaming"}],[{"type": "Security Approach", "value": "Robustness Evaluation"}, {"type": "Project Lifecycle Stage", "value": "Post-training"}],[{"description": "A team of experts deliberately crafting unusual or slightly perturbed inputs (adversarial examples) for an image classifier \u2013 like subtly altering pixels or adding irrelevant objects \u2013 to see if the classifier can be tricked into mislabeling (e.g., making a stop sign image be classified as a speed limit sign).", "goal": "Security"}, {"description": "Subjecting a language model chatbot to a suite of malicious or tricky queries (containing code injection patterns, biased statements, etc.) to uncover if it produces any unsafe or biased responses, and then using those findings to tighten the model\u2019s filters.", "goal": "Security"}],[{"description": "Adversarial testing can never exhaust all possible attacks; passing known tests doesn\u2019t guarantee the model is safe from all unknown or future attack strategies."}, {"description": "Requires expertise to design effective attacks or edge-case scenarios; not all organizations have the capability to thoroughly red-team their AI systems."}, {"description": "It can be time-consuming and expensive to perform extensive adversarial testing, and it often needs to be repeated whenever the model or environment changes."}],[],2,2
86,"Safe Reinforcement Learning",Applies additional constraints or safety-focused reward modifications to reinforcement learning (RL) algorithms so that agents avoid unsafe actions during learning and deployment. Techniques include constrained Markov Decision Processes (CMDPs), reward shaping to include penalty for unsafe actions, or shielding (preventing certain actions that lead to unsafe states). The goal is to ensure an RL agent explores and operates without causing unacceptable harm or violations.,Model-Specific,Safety,[{"goal": "Safety", "category": "Safe Reinforcement Learning"}],[{"category": "Safe Reinforcement Learning", "subcategory": "Constrained Learning"}],[{"type": "Safety Approach", "value": "Constrained Exploration"}, {"type": "Project Lifecycle Stage", "value": "Training"}],[{"description": "Training a robot with an RL algorithm that includes a safety buffer \u2013 if the robot gets too close to a human or edge, an overriding mechanism kicks in or a large negative reward is given, thereby teaching the robot to avoid dangerous proximity in its policy.", "goal": "Safety"}, {"description": "Using safe RL for an autonomous drone by adding constraints so it never flies above a certain altitude over crowds and incorporating those as explicit constraints in the learning algorithm (e.g., via Lagrange multiplier methods in the reward), thus it learns a policy that respects no-fly zones.", "goal": "Safety"}],[{"description": "Adding safety constraints can make the learning problem harder to solve; the agent might learn more slowly or struggle to find any policy that satisfies strict constraints."}, {"description": "Defining what is \"safe\" often requires manual specification of constraints or unsafe states, which might not cover all cases or might be overly conservative."}, {"description": "Even with safe RL, there is no absolute guarantee; the agent might still find edge-case behaviors that technically satisfy constraints but are undesired, so human oversight remains important."}],[],3,3
87,"Feature Visualization (Activation Maximization)",Generates visual or interpretable representations of what a neural network has learned by finding an input that maximally activates certain neurons or output categories. For vision models, this often means creating an artificial image (starting from noise) that the network would strongly classify as a particular concept (e.g., \"dog\"). The resulting visualization reveals what features the model considers indicative of that concept, providing insight into the model's internal representation.,Model-Specific,Explainability,[{"goal": "Explainability", "category": "Visualization Techniques"}],[{"category": "Visualization Techniques", "subcategory": "Activation Maximization"}],[{"type": "Scope", "value": "Global"}],[{"description": "Understanding what causes a CNN\u2019s \"Neuron 123\" to fire by generating an image that highly excites that neuron, revealing it responds to, say, round, orange textures (suggesting it detects oranges or balls).", "goal": "Explainability"}, {"description": "Visualizing the concept of \"dumbbell\" as learned by an image classifier by creating an artificial image \u2013 the result might show a hand attached to a weight, indicating the model associates dumbbells with human hands, which could explain a bias in misclassification.", "goal": "Explainability"}],[{"description": "The produced images can be abstract or have artifacts and thus may be hard to interpret definitively (they might look like a strange mix of features rather than a clear object)."}, {"description": "This technique mostly applies to vision or maybe audio (in a transformed domain); for abstract features or for models outside of sensory domains, it\u2019s less straightforward to visualize what maximizes an activation."}, {"description": "Optimization to generate these visuals can converge to unusual patterns that do excite the network but are not recognizable to humans, so human intuition is not always matched by these results."}],[],2,2
88,Counterfactual Explanations,Provides explanations for individual model decisions by identifying the minimal changes to an input that would flip the model's prediction. In other words, it answers \"What needs to be different for a different outcome?\" For instance, a counterfactual explanation for a loan denial might be: \"If the applicant's income were $5,000 higher, the loan would have been approved.\" These explanations are intuitive as they align with how humans reason about \"what-if\" scenarios.,Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Example-Based Methods"}],[{"category": "Example-Based Methods", "subcategory": "Counterfactuals"}],[{"type": "Scope", "value": "Local"}],[{"description": "Explaining why an applicant was rejected by a credit model by presenting a scenario (counterfactual) in which they would be accepted \u2013 e.g., \"If your credit score had been 50 points higher, you would have been approved.\" This gives the applicant actionable insight.", "goal": "Explainability"}, {"description": "For a medical diagnosis AI that predicted no disease, providing a counterfactual like \"If this patient had these specific symptoms (which they do not currently have), the model would predict a high risk of disease,\" highlighting what factors drive a positive prediction.", "goal": "Explainability"}],[{"description": "Finding true minimal counterfactuals can be computationally intense, as it often requires iterative searches or solving an optimization problem for each explanation."}, {"description": "The counterfactual suggested might be implausible or not actionable in real life (e.g., \"if you were 5 years older\" might flip a decision, but age cannot be changed)."}, {"description": "There can be many possible counterfactuals; choosing which one to present (some that change one feature vs. many features slightly) can influence user perception and may require additional criteria to select."}],[],3,2
89,Decision Tree Surrogate Model,Creates an interpretable approximation of a complex model by training a decision tree on the inputs and outputs of that model. The decision tree (being much simpler and rule-based) serves as a surrogate that mimics the behavior of the original model as closely as possible. Analysts can then examine the tree's structure (rules and splits) to gain insight into how the complex model might be making decisions globally.,Model-Agnostic,Explainability,[{"goal": "Explainability", "category": "Model Approximation"}],[{"category": "Model Approximation", "subcategory": "Global Surrogates"}],[{"type": "Scope", "value": "Global"}],[{"description": "After training a black-box ensemble for fraud detection, fitting a decision tree on the ensemble's predictions to produce a set of if-then rules (surrogate) that approximate its logic, allowing auditors to see key factors and thresholds the model uses for flagging fraud.", "goal": "Explainability"}, {"description": "Using a surrogate decision tree to explain a deep neural network for loan approval, yielding simple rules like \"IF income > X and credit_history = good THEN approved\" which roughly capture the neural net's decision patterns, offering stakeholders a high-level understanding.", "goal": "Explainability"}],[{"description": "The surrogate tree is an approximation and might not capture all nuances of the original model, especially if the model's behavior is too complex to be faithfully represented by a tree of reasonable size."}, {"description": "If forced to keep the tree simple for interpretability, its fidelity to the original model can be low (meaning the explanations might be misleading for certain cases where the model behaves differently)."}, {"description": "Surrogates provide a global view, which may miss local irregularities in the model's decision boundary; combining this with local explanation methods can give a fuller picture."}],[],2,1
