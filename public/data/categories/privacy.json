{
  "goal": {
    "name": "Privacy",
    "slug": "privacy",
    "count": 7,
    "description": "Protecting personal data and maintaining confidentiality in AI systems."
  },
  "techniques": [
    {
      "slug": "influence-functions",
      "name": "Influence Functions",
      "description": "Influence functions quantify how much each training example influenced a model's predictions by computing the change in prediction that would occur if that training example were removed and the model retrained. Using calculus and the implicit function theorem, they approximate this 'leave-one-out' effect without actually retraining the model by computing gradients and Hessian information. This mathematical approach reveals which specific training examples were most responsible for pushing the model toward or away from particular predictions, enabling practitioners to trace problematic outputs back to their root causes in the training data.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Privacy"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "assurance-goal-category/privacy",
        "data-requirements/access-to-training-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Investigating why a medical diagnosis model misclassified a patient by identifying which specific training cases most influenced the incorrect prediction, revealing potential mislabelled examples or problematic patterns in the training data.",
          "goal": "Explainability"
        },
        {
          "description": "Analysing a spam detection system that falsely flagged legitimate emails by tracing the prediction back to influential training examples, discovering that certain training emails contained misleading patterns that caused the model to overfit.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a loan approval model for discriminatory patterns by identifying which training examples most influenced rejections of minority applicants, revealing whether biased historical decisions are driving current unfair outcomes.",
          "goal": "Fairness"
        },
        {
          "description": "Assessing membership inference risks in a medical model by identifying whether certain patient records have disproportionate influence on predictions, indicating potential data leakage vulnerabilities.",
          "goal": "Privacy"
        }
      ],
      "limitations": [
        {
          "description": "Computationally intensive, requiring Hessian matrix computations that become intractable for very large models with millions of parameters."
        },
        {
          "description": "Requires access to the complete training dataset and training process, making it impossible to apply to pre-trained models without access to original training data."
        },
        {
          "description": "Accuracy degrades for highly non-convex models where the linear approximation underlying influence functions breaks down."
        },
        {
          "description": "Results can be sensitive to hyperparameter choices and may not generalise well across different model architectures or training procedures."
        }
      ],
      "resources": [
        {
          "title": "Understanding Black-box Predictions via Influence Functions",
          "url": "https://www.semanticscholar.org/paper/08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
          "source_type": "technical_paper",
          "authors": [
            "Pang Wei Koh",
            "Percy Liang"
          ]
        },
        {
          "title": "nimarb/pytorch_influence_functions",
          "url": "https://github.com/nimarb/pytorch_influence_functions",
          "source_type": "software_package"
        },
        {
          "title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions",
          "url": "https://www.semanticscholar.org/paper/f33f3dece9f34c1ec5417dccf9e0acf592d8e8cb",
          "source_type": "technical_paper",
          "authors": [
            "Sang Keun Choe",
            "Hwijeen Ahn",
            "Juhan Bae",
            "Kewen Zhao",
            "Minsoo Kang",
            "Youngseog Chung",
            "Adithya Pratapa",
            "W. Neiswanger",
            "Emma Strubell",
            "Teruko Mitamura",
            "Jeff G. Schneider",
            "Eduard H. Hovy",
            "Roger B. Grosse",
            "Eric P. Xing"
          ]
        },
        {
          "title": "Scaling Up Influence Functions",
          "url": "https://www.semanticscholar.org/paper/ef2a773c3c7848a6cc16b18164be5f8876a310af",
          "source_type": "technical_paper",
          "authors": [
            "Andrea Schioppa",
            "Polina Zablotskaia",
            "David Vilar",
            "Artem Sokolov"
          ]
        },
        {
          "title": "Welcome to torch-influence's API Reference! â€” torch-influence 0.1.0 ...",
          "url": "https://torch-influence.readthedocs.io/",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "contextual-decomposition",
        "taylor-decomposition"
      ]
    },
    {
      "slug": "synthetic-data-generation",
      "name": "Synthetic Data Generation",
      "description": "Synthetic data generation creates artificial datasets that aim to preserve the statistical properties, distributions, and relationships of real data whilst containing no actual records from real individuals. The technique encompasses various approaches including generative adversarial networks (GANs), variational autoencoders (VAEs), statistical sampling methods, and privacy-preserving techniques like differential privacy. Beyond privacy protection, synthetic data serves multiple purposes: augmenting limited datasets, balancing class distributions, testing model robustness, enabling data sharing across organisations, and supporting fairness assessments by generating representative samples for underrepresented groups.",
      "assurance_goals": [
        "Privacy",
        "Fairness",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/privacy",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/structured-output",
        "expertise-needed/cryptography",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/data-handling",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Creating realistic but synthetic electronic health records for developing and testing medical diagnosis algorithms without exposing real patient data, enabling secure collaboration between healthcare institutions.",
          "goal": "Privacy"
        },
        {
          "description": "Generating synthetic samples for underrepresented demographic groups in a hiring dataset to train fair recruitment models, ensuring all groups have sufficient representation for bias testing and mitigation.",
          "goal": "Fairness"
        },
        {
          "description": "Augmenting limited training data for rare medical conditions by generating synthetic patient records, improving model reliability and performance on edge cases where real data is insufficient.",
          "goal": "Reliability"
        },
        {
          "description": "Creating synthetic financial transaction data for testing fraud detection systems in development environments, avoiding exposure of real customer financial information whilst maintaining realistic attack patterns.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "May not capture all subtle patterns, correlations, and edge cases present in real data, potentially leading to reduced model performance when deployed on actual data with different characteristics."
        },
        {
          "description": "Generating high-quality synthetic data that maintains both statistical fidelity and utility requires sophisticated techniques and substantial computational resources, especially for complex, high-dimensional datasets."
        },
        {
          "description": "Privacy-preserving approaches may still risk information leakage through statistical inference attacks, membership inference, or model inversion, requiring careful privacy budget management and validation."
        },
        {
          "description": "Synthetic data may inadvertently amplify existing biases in the original data or introduce new biases through the generation process, particularly in generative models trained on biased datasets."
        },
        {
          "description": "Validation and quality assessment of synthetic data is challenging, as traditional metrics may not adequately capture whether the synthetic data preserves the relationships and patterns needed for specific downstream tasks."
        }
      ],
      "resources": [
        {
          "title": "sdv-dev/SDV",
          "url": "https://github.com/sdv-dev/SDV",
          "source_type": "software_package"
        },
        {
          "title": "An evaluation framework for synthetic data generation models",
          "url": "http://arxiv.org/pdf/2404.08866v1",
          "source_type": "technical_paper",
          "authors": [
            "Ioannis E. Livieris",
            "Nikos Alimpertis",
            "George Domalis",
            "Dimitris Tsakalidis"
          ],
          "publication_date": "2024-04-13"
        },
        {
          "title": "Synthetic Data â€” SecureML 0.2.2 documentation",
          "url": "https://secureml.readthedocs.io/en/latest/user_guide/synthetic_data.html",
          "source_type": "documentation"
        },
        {
          "title": "How to Generate Real-World Synthetic Data with CTGAN | Towards ...",
          "url": "https://towardsdatascience.com/how-to-generate-real-world-synthetic-data-with-ctgan-af41b4d60fde/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 4,
      "related_techniques": [
        "federated-learning",
        "differential-privacy",
        "homomorphic-encryption"
      ]
    },
    {
      "slug": "federated-learning",
      "name": "Federated Learning",
      "description": "Federated learning enables collaborative model training across multiple distributed parties (devices, organisations, or data centres) without requiring centralised data sharing. Participants train models locally on their private datasets and only share model updates (gradients, weights, or aggregated statistics) with a central coordinator. This distributed approach serves multiple purposes: preserving data privacy and sovereignty, reducing communication costs, enabling learning from diverse data sources, improving model robustness through heterogeneous training, and facilitating compliance with data protection regulations whilst maintaining model performance comparable to centralised training.",
      "assurance_goals": [
        "Privacy",
        "Reliability",
        "Safety",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/privacy",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/software-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Developing a smartphone keyboard prediction model by learning from users' typing patterns without their text ever leaving their devices, enabling personalised predictions whilst maintaining complete data privacy.",
          "goal": "Privacy"
        },
        {
          "description": "Training a medical diagnosis model across multiple hospitals without sharing patient records, ensuring model robustness by learning from diverse patient populations and clinical practices across different institutions.",
          "goal": "Reliability"
        },
        {
          "description": "Creating a cybersecurity threat detection model by federating learning across financial institutions without exposing sensitive transaction data, reducing systemic risk whilst maintaining competitive confidentiality.",
          "goal": "Safety"
        },
        {
          "description": "Building a fair credit scoring model by training across multiple regions and demographics without centralising sensitive financial data, ensuring representation from diverse populations whilst respecting local data sovereignty laws.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Communication overhead can be substantial, especially with frequent model updates and large models, potentially limiting scalability and increasing training time compared to centralised approaches."
        },
        {
          "description": "Statistical heterogeneity across participants (non-IID data distributions) can lead to training instability, slower convergence, and reduced model performance compared to centralised training on pooled data."
        },
        {
          "description": "System heterogeneity in computational capabilities, network connectivity, and availability of participating devices can create bottlenecks and introduce bias towards more capable participants."
        },
        {
          "description": "Privacy vulnerabilities remain through gradient leakage attacks, model inversion, and membership inference attacks that can potentially reconstruct sensitive information from shared model updates."
        },
        {
          "description": "Coordination complexity increases with the number of participants, requiring sophisticated aggregation protocols, fault tolerance mechanisms, and secure communication infrastructure."
        }
      ],
      "resources": [
        {
          "title": "Open Federated Learning (OpenFL) Documentation",
          "url": "https://openfl.readthedocs.io/en/stable/",
          "source_type": "documentation"
        },
        {
          "title": "Federated Learning - DeepLearning.AI",
          "url": "https://www.deeplearning.ai/short-courses/intro-to-federated-learning/",
          "source_type": "tutorial"
        },
        {
          "title": "A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection",
          "url": "http://arxiv.org/pdf/1907.09693v7",
          "source_type": "documentation",
          "authors": [
            "Qinbin Li",
            "Zeyi Wen",
            "Zhaomin Wu",
            "Sixu Hu",
            "Naibo Wang",
            "Yuan Li",
            "Xu Liu",
            "Bingsheng He"
          ],
          "publication_date": "2019-07-23"
        },
        {
          "title": "Federated learning with hybrid differential privacy for secure and reliable cross-IoT platform knowledge sharing",
          "url": "https://core.ac.uk/download/603345619.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Algburi, S.",
            "Algburi, S.",
            "Anupallavi, S.",
            "Anupallavi, S.",
            "Ashokkumar, S. R.",
            "Ashokkumar, S. R.",
            "Elmedany, W.",
            "Elmedany, W.",
            "Khalaf, O. I.",
            "Khalaf, O. I.",
            "Selvaraj, D.",
            "Selvaraj, D.",
            "Sharif, M. S.",
            "Sharif, M. S."
          ],
          "publication_date": "2024-01-01"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 5,
      "related_techniques": [
        "synthetic-data-generation",
        "differential-privacy",
        "homomorphic-encryption"
      ]
    },
    {
      "slug": "differential-privacy",
      "name": "Differential Privacy",
      "description": "Differential privacy provides mathematically rigorous privacy protection by adding carefully calibrated random noise to data queries, statistical computations, or machine learning outputs. The technique works by ensuring that the presence or absence of any individual's data has minimal impact on the results - specifically, any query result should be nearly indistinguishable whether or not a particular person's data is included. This is achieved through controlled noise addition that scales with the query's sensitivity and a privacy budget (epsilon) that quantifies the privacy-utility trade-off. The smaller the epsilon, the more noise is added and the stronger the privacy guarantee, but at the cost of reduced accuracy.",
      "assurance_goals": [
        "Privacy",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/privacy",
        "assurance-goal-category/privacy/formal-guarantee/differential-privacy",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/privacy-guarantee",
        "expertise-needed/cryptography",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/data-handling",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Protecting individual privacy in census data analysis by adding calibrated noise to demographic statistics, ensuring households cannot be re-identified whilst maintaining accurate population insights for policy planning.",
          "goal": "Privacy"
        },
        {
          "description": "Publishing differentially private aggregate statistics about model performance across different demographic groups, enabling transparent bias auditing without exposing sensitive individual prediction details or group membership.",
          "goal": "Transparency"
        },
        {
          "description": "Enabling fair evaluation of lending algorithms by releasing differentially private performance metrics across protected groups, allowing regulatory compliance checking whilst protecting individual applicant privacy.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Adding noise inherently reduces the accuracy and utility of results, with stronger privacy guarantees (smaller epsilon values) leading to more significant degradation in data quality."
        },
        {
          "description": "Setting the privacy budget (epsilon) requires expertise and careful consideration of the privacy-utility trade-off, with no universal guidelines for appropriate values across different applications."
        },
        {
          "description": "Sequential queries consume the privacy budget cumulatively, potentially requiring careful query planning and potentially prohibiting future analyses once the budget is exhausted."
        },
        {
          "description": "Implementation complexity is high, requiring deep understanding of sensitivity analysis, noise mechanisms, and composition theorems to avoid inadvertent privacy violations."
        },
        {
          "description": "May not protect against all privacy attacks, particularly sophisticated adversaries with auxiliary information or when combined with other data sources that could aid re-identification."
        }
      ],
      "resources": [
        {
          "title": "Google Differential Privacy Library",
          "url": "https://github.com/google/differential-privacy",
          "source_type": "software_package",
          "description": "Open-source library providing implementations of differential privacy algorithms and utilities"
        },
        {
          "title": "The Algorithmic Foundations of Differential Privacy",
          "url": "https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Cynthia Dwork",
            "Aaron Roth"
          ],
          "description": "Foundational monograph on differential privacy theory and algorithms"
        },
        {
          "title": "Opacus: User-Friendly Differential Privacy Library in PyTorch",
          "url": "https://github.com/pytorch/opacus",
          "source_type": "software_package",
          "description": "PyTorch library for training neural networks with differential privacy"
        },
        {
          "title": "Programming Differential Privacy",
          "url": "https://programming-dp.com/",
          "source_type": "tutorial",
          "description": "Comprehensive online book and tutorial for learning differential privacy programming"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 3,
      "related_techniques": [
        "synthetic-data-generation",
        "federated-learning",
        "homomorphic-encryption"
      ]
    },
    {
      "slug": "homomorphic-encryption",
      "name": "Homomorphic Encryption",
      "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
      "assurance_goals": [
        "Privacy",
        "Safety",
        "Transparency",
        "Security"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/privacy",
        "assurance-goal-category/privacy/formal-guarantee",
        "assurance-goal-category/safety",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/privacy-guarantee",
        "expertise-needed/cryptography",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Enabling a cloud-based medical diagnosis service to process encrypted patient data and return encrypted results without the cloud provider ever accessing actual medical information, ensuring complete patient privacy during outsourced computation.",
          "goal": "Privacy"
        },
        {
          "description": "Securing financial risk assessment computations by allowing banks to jointly analyse encrypted transaction patterns for fraud detection without exposing individual customer data, reducing systemic security risks.",
          "goal": "Safety"
        },
        {
          "description": "Enabling transparent audit of algorithmic decision-making by allowing regulators to verify model computations on encrypted data, providing accountability whilst protecting the proprietary nature of both the algorithm and the underlying data.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Extremely computationally expensive, often 100-1000x slower than unencrypted computation, making it impractical for real-time applications or large-scale data processing."
        },
        {
          "description": "Limited range of operations supported efficiently, with complex operations like divisions, comparisons, and non-polynomial functions being particularly challenging or impossible to implement."
        },
        {
          "description": "Implementation requires deep cryptographic expertise to avoid security vulnerabilities, choose appropriate parameters, and optimise performance for specific use cases."
        },
        {
          "description": "Memory and storage requirements are significantly higher than traditional computation, as encrypted data typically requires much more space than plaintext equivalents."
        },
        {
          "description": "Current fully homomorphic encryption schemes have practical limitations on computation depth before noise accumulation requires expensive bootstrapping operations to refresh ciphertexts."
        }
      ],
      "resources": [
        {
          "title": "zama-ai/concrete-ml",
          "url": "https://github.com/zama-ai/concrete-ml",
          "source_type": "software_package",
          "description": "Privacy-preserving machine learning library that enables data scientists to run ML models on encrypted data using FHE without cryptography expertise"
        },
        {
          "title": "Survey on Fully Homomorphic Encryption, Theory, and Applications",
          "url": "https://core.ac.uk/download/579858842.pdf",
          "source_type": "documentation",
          "authors": [
            "Chiara Marcolla",
            "Frank H.P. Fitzek",
            "Marc Manzano",
            "Najwa Aaraj",
            "Riccardo Bassoli",
            "Victor Sucasas"
          ],
          "publication_date": "2022-10-06",
          "description": "Comprehensive survey covering FHE theory, cryptographic schemes, and practical applications across different domains"
        },
        {
          "title": "Welcome to OpenFHE's documentation! â€” OpenFHE documentation",
          "url": "https://openfhe-development.readthedocs.io/",
          "source_type": "documentation",
          "description": "Documentation for open-source C++ library supporting multiple FHE schemes including BFV, BGV, CKKS, and Boolean circuits"
        },
        {
          "title": "Evaluation of Privacy-Preserving Support Vector Machine (SVM) Learning Using Homomorphic Encryption",
          "url": "https://core.ac.uk/download/656115203.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Ali, Hisham",
            "Buchanan, William J."
          ],
          "publication_date": "2025-01-01",
          "description": "Technical paper evaluating performance overhead of SVM learning with homomorphic encryption for privacy-preserving ML"
        },
        {
          "title": "microsoft/SEAL",
          "url": "https://github.com/microsoft/SEAL",
          "source_type": "software_package",
          "description": "Easy-to-use homomorphic encryption library enabling computations on encrypted integers and real numbers"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "synthetic-data-generation",
        "federated-learning",
        "differential-privacy"
      ]
    },
    {
      "slug": "datasheets-for-datasets",
      "name": "Datasheets for Datasets",
      "description": "Datasheets for datasets establish comprehensive documentation standards for datasets, systematically recording creation methodology, data composition, collection procedures, preprocessing transformations, intended applications, potential biases, privacy considerations, and maintenance protocols. These structured documents enhance dataset transparency by providing essential context for appropriate usage, enabling informed decisions about dataset suitability for specific tasks, supporting bias detection and mitigation efforts, ensuring compliance with data protection regulations, and promoting responsible data stewardship throughout the entire data lifecycle from collection to disposal.",
      "assurance_goals": [
        "Transparency",
        "Fairness",
        "Privacy"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "assurance-goal-category/privacy",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/documentation",
        "expertise-needed/domain-knowledge",
        "expertise-needed/regulatory-compliance",
        "lifecycle-stage/data-handling",
        "lifecycle-stage/data-handling/collection",
        "lifecycle-stage/data-handling/preparation",
        "technique-type/documentation"
      ],
      "example_use_cases": [
        {
          "description": "Documenting a medical imaging dataset with detailed information about patient privacy protections, anonymisation procedures, and data sharing constraints to ensure sensitive health information is handled appropriately and regulatory compliance is maintained.",
          "goal": "Privacy"
        },
        {
          "description": "Creating comprehensive datasheets for recruitment datasets that document demographic representation across different job categories, helping developers identify potential bias in training data and develop more equitable hiring algorithms.",
          "goal": "Fairness"
        },
        {
          "description": "Establishing transparent documentation for financial transaction datasets that clearly describes data collection methodology, preprocessing steps, and intended use cases, enabling researchers to make informed decisions about dataset appropriateness for their specific applications.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Creating thorough datasheets requires significant time investment and domain expertise to properly document collection methods, biases, and ethical considerations, potentially delaying dataset release or publication."
        },
        {
          "description": "Information may become outdated as datasets undergo preprocessing, cleaning, or augmentation, requiring ongoing maintenance to ensure documentation accuracy throughout the data lifecycle."
        },
        {
          "description": "Absence of standardised templates and enforcement mechanisms leads to inconsistent documentation quality and completeness across different organisations and research communities."
        },
        {
          "description": "Dataset creators may intentionally omit sensitive information about collection methods, participant consent, or potential biases to avoid legal liability or competitive disadvantage."
        },
        {
          "description": "Limited adoption and awareness means many existing datasets lack proper documentation, creating gaps in the historical record and making legacy dataset assessment difficult."
        }
      ],
      "resources": [
        {
          "title": "Datasheets for Datasets",
          "url": "https://arxiv.org/abs/1803.09010",
          "source_type": "technical_paper",
          "authors": [
            "Timnit Gebru",
            "Jamie Morgenstern",
            "Briana Vecchione",
            "Jennifer Wortman Vaughan",
            "Hanna Wallach",
            "Hal DaumÃ© III",
            "Kate Crawford"
          ],
          "publication_date": "2018-03-23",
          "description": "Foundational paper proposing standardised documentation for machine learning datasets to facilitate transparency, accountability, and better communication between dataset creators and consumers"
        },
        {
          "title": "Datasheets for AI and medical datasets (DAIMS): a data validation and documentation framework before machine learning analysis in medical research",
          "url": "http://arxiv.org/pdf/2501.14094v1",
          "source_type": "technical_paper",
          "authors": [
            "Ramtin Zargari Marandi",
            "Anne Svane Frahm",
            "Maja Milojevic"
          ],
          "publication_date": "2025-01-23",
          "description": "Recent framework extending datasheets specifically for medical AI datasets, providing validation and documentation standards for healthcare machine learning research"
        },
        {
          "title": "MT-Adapted Datasheets for Datasets: Template and Repository",
          "url": "http://arxiv.org/pdf/2005.13156v1",
          "source_type": "technical_paper",
          "authors": [
            "Marta R. Costa-jussÃ ",
            "Roger Creus",
            "Oriol Domingo",
            "Albert DomÃ­nguez",
            "Miquel Escobar",
            "Cayetana LÃ³pez",
            "Marina Garcia",
            "Margarita Geleta"
          ],
          "publication_date": "2020-05-27"
        },
        {
          "title": "Understanding Machine Learning Practitioners' Data Documentation Perceptions, Needs, Challenges, and Desiderata",
          "url": "http://arxiv.org/pdf/2206.02923v2",
          "source_type": "technical_paper",
          "authors": [
            "Amy K. Heger",
            "Liz B. Marquis",
            "Mihaela Vorvoreanu",
            "Hanna Wallach",
            "Jennifer Wortman Vaughan"
          ],
          "publication_date": "2022-06-06"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "model-cards",
        "mlflow-experiment-tracking",
        "data-version-control",
        "automated-documentation-generation"
      ]
    },
    {
      "slug": "fairness-gan",
      "name": "Fairness GAN",
      "description": "A data generation technique that employs Generative Adversarial Networks (GANs) to create fair synthetic datasets by learning to generate data representations that preserve utility whilst obfuscating protected attributes. Unlike traditional GANs, Fairness GANs incorporate fairness constraints into the training objective, ensuring that the generated data maintains statistical parity across demographic groups. The technique can be used for data augmentation to balance underrepresented groups or to create privacy-preserving synthetic datasets that remove demographic bias from training data.",
      "assurance_goals": [
        "Fairness",
        "Privacy",
        "Reliability"
      ],
      "tags": [
        "applicable-models/gan",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/privacy",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "evidence-type/synthetic-data",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/data-collection/data-augmentation",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Generating balanced synthetic datasets for medical research by creating additional samples from underrepresented demographic groups, ensuring equal representation across ethnicity and gender whilst maintaining the statistical properties needed for robust model training.",
          "goal": "Fairness"
        },
        {
          "description": "Creating privacy-preserving synthetic datasets for financial services that remove demographic identifiers whilst preserving the underlying patterns needed for credit risk assessment, allowing secure data sharing between institutions without exposing sensitive customer information.",
          "goal": "Privacy"
        },
        {
          "description": "Augmenting recruitment datasets by generating synthetic candidate profiles that balance gender and ethnicity representation, ensuring reliable model performance across all demographic groups when real-world data exhibits significant imbalances.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "GAN training is notoriously difficult to stabilise, with potential for mode collapse or failure to converge, especially when additional fairness constraints are imposed."
        },
        {
          "description": "Ensuring fairness in generated data may come at the cost of data utility, potentially reducing the quality or realism of synthetic samples."
        },
        {
          "description": "Requires large datasets to train both generator and discriminator networks effectively, limiting applicability in data-scarce domains."
        },
        {
          "description": "Evaluation complexity is high, as it requires assessing both the quality of generated data and the preservation of fairness properties across demographic groups."
        },
        {
          "description": "May inadvertently introduce new biases if the fairness constraints are not properly specified or if the training data itself contains subtle biases."
        }
      ],
      "resources": [
        {
          "title": "Fairness GAN",
          "url": "http://arxiv.org/pdf/1805.09910v1",
          "source_type": "technical_paper",
          "authors": [
            "Prasanna Sattigeri",
            "Samuel C. Hoffman",
            "Vijil Chenthamarakshan",
            "Kush R. Varshney"
          ],
          "publication_date": "2018-05-24"
        },
        {
          "title": "Fair GANs through model rebalancing for extremely imbalanced class distributions",
          "url": "http://arxiv.org/pdf/2308.08638v2",
          "source_type": "technical_paper",
          "authors": [
            "Anubhav Jain",
            "Nasir Memon",
            "Julian Togelius"
          ],
          "publication_date": "2023-08-16"
        },
        {
          "title": "Inclusive GAN: Improving Data and Minority Coverage in Generative Models",
          "url": "http://arxiv.org/abs/2004.03355",
          "source_type": "technical_paper",
          "authors": [
            "Ning Yu",
            "Ke Li",
            "Peng Zhou",
            "Jitendra Malik",
            "Larry Davis",
            "Mario Fritz"
          ],
          "publication_date": "2020-04-07"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "sensitivity-analysis-for-fairness",
        "attribute-removal-fairness-through-unawareness",
        "bayesian-fairness-regularization"
      ]
    }
  ],
  "count": 7
}