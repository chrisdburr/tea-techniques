{
  "tag": {
    "name": "applicable-models/architecture/linear-models",
    "slug": "applicable-models-architecture-linear-models",
    "count": 5,
    "category": "applicable-models"
  },
  "techniques": [
    {
      "slug": "coefficient-magnitudes-in-linear-models",
      "name": "Coefficient Magnitudes (in Linear Models)",
      "description": "Coefficient Magnitudes assess feature influence in linear models by examining the absolute values of their coefficients. Features with larger absolute coefficients are considered to have a stronger impact on the prediction, while the sign of the coefficient indicates the direction of that influence (positive or negative). This technique provides a straightforward and transparent way to understand the direct linear relationship between each input feature and the model's output.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/model-specific",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "expertise-needed/low",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/metric",
        "applicable-models/architecture/linear-models",
        "applicable-models/paradigm/parametric",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/white-box"
      ],
      "example_use_cases": [
        {
          "description": "Interpreting which features influence housing price predictions in linear regression, such as identifying that 'number of bedrooms' has a larger positive impact than 'distance to city centre' based on coefficient magnitudes.",
          "goal": "Explainability"
        },
        {
          "description": "Explaining the factors contributing to customer lifetime value (CLV) in a linear model, showing how 'average monthly spend' has a strong positive coefficient, making the model transparent for business stakeholders.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Only valid for linear relationships; it cannot capture complex non-linear patterns or interactions between features."
        },
        {
          "description": "Highly sensitive to feature scaling; features with larger numerical ranges can appear more important even if their true impact is smaller."
        },
        {
          "description": "Can be misleading in the presence of multicollinearity, where correlated features may split importance or have unstable coefficients."
        },
        {
          "description": "Does not imply causation; a strong correlation (large coefficient) does not necessarily mean a causal relationship."
        }
      ],
      "resources": [],
      "complexity_rating": 1,
      "computational_cost_rating": 1,
      "related_techniques": [
        "permutation-importance",
        "mean-decrease-impurity",
        "sobol-indices"
      ]
    },
    {
      "slug": "influence-functions",
      "name": "Influence Functions",
      "description": "Influence functions quantify how much each training example influenced a model's predictions by computing the change in prediction that would occur if that training example were removed and the model retrained. Using calculus and the implicit function theorem, they approximate this 'leave-one-out' effect without actually retraining the model by computing gradients and Hessian information. This mathematical approach reveals which specific training examples were most responsible for pushing the model toward or away from particular predictions, enabling practitioners to trace problematic outputs back to their root causes in the training data.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Privacy"
      ],
      "tags": [
        "applicable-models/architecture/linear-models",
        "applicable-models/architecture/neural-networks",
        "applicable-models/paradigm/parametric",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/differentiable",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/training-data",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/causal-pathways",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/instance-based/influence-analysis",
        "assurance-goal-category/explainability/property/causality",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/fairness",
        "assurance-goal-category/privacy",
        "data-requirements/access-to-training-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Investigating why a medical diagnosis model misclassified a patient by identifying which specific training cases most influenced the incorrect prediction, revealing potential mislabelled examples or problematic patterns in the training data.",
          "goal": "Explainability"
        },
        {
          "description": "Analysing a spam detection system that falsely flagged legitimate emails by tracing the prediction back to influential training examples, discovering that certain training emails contained misleading patterns that caused the model to overfit.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a loan approval model for discriminatory patterns by identifying which training examples most influenced rejections of minority applicants, revealing whether biased historical decisions are driving current unfair outcomes.",
          "goal": "Fairness"
        },
        {
          "description": "Assessing membership inference risks in a medical model by identifying whether certain patient records have disproportionate influence on predictions, indicating potential data leakage vulnerabilities.",
          "goal": "Privacy"
        }
      ],
      "limitations": [
        {
          "description": "Computationally intensive, requiring Hessian matrix computations that become intractable for very large models with millions of parameters."
        },
        {
          "description": "Requires access to the complete training dataset and training process, making it impossible to apply to pre-trained models without access to original training data."
        },
        {
          "description": "Accuracy degrades for highly non-convex models where the linear approximation underlying influence functions breaks down."
        },
        {
          "description": "Results can be sensitive to hyperparameter choices and may not generalise well across different model architectures or training procedures."
        }
      ],
      "resources": [
        {
          "title": "Understanding Black-box Predictions via Influence Functions",
          "url": "https://www.semanticscholar.org/paper/08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
          "source_type": "technical_paper",
          "authors": [
            "Pang Wei Koh",
            "Percy Liang"
          ]
        },
        {
          "title": "nimarb/pytorch_influence_functions",
          "url": "https://github.com/nimarb/pytorch_influence_functions",
          "source_type": "software_package"
        },
        {
          "title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions",
          "url": "https://www.semanticscholar.org/paper/f33f3dece9f34c1ec5417dccf9e0acf592d8e8cb",
          "source_type": "technical_paper",
          "authors": [
            "Sang Keun Choe",
            "Hwijeen Ahn",
            "Juhan Bae",
            "Kewen Zhao",
            "Minsoo Kang",
            "Youngseog Chung",
            "Adithya Pratapa",
            "W. Neiswanger",
            "Emma Strubell",
            "Teruko Mitamura",
            "Jeff G. Schneider",
            "Eduard H. Hovy",
            "Roger B. Grosse",
            "Eric P. Xing"
          ]
        },
        {
          "title": "Scaling Up Influence Functions",
          "url": "https://www.semanticscholar.org/paper/ef2a773c3c7848a6cc16b18164be5f8876a310af",
          "source_type": "technical_paper",
          "authors": [
            "Andrea Schioppa",
            "Polina Zablotskaia",
            "David Vilar",
            "Artem Sokolov"
          ]
        },
        {
          "title": "Welcome to torch-influence's API Reference! — torch-influence 0.1.0 ...",
          "url": "https://torch-influence.readthedocs.io/",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "contextual-decomposition",
        "taylor-decomposition"
      ]
    },
    {
      "slug": "federated-learning",
      "name": "Federated Learning",
      "description": "Federated learning enables collaborative model training across multiple distributed parties (devices, organisations, or data centres) without requiring centralised data sharing. Participants train models locally on their private datasets and only share model updates (gradients, weights, or aggregated statistics) with a central coordinator. This distributed approach serves multiple purposes: preserving data privacy and sovereignty, reducing communication costs, enabling learning from diverse data sources, improving model robustness through heterogeneous training, and facilitating compliance with data protection regulations whilst maintaining model performance comparable to centralised training.",
      "assurance_goals": [
        "Privacy",
        "Reliability",
        "Safety",
        "Fairness"
      ],
      "tags": [
        "applicable-models/architecture/linear-models",
        "applicable-models/architecture/neural-networks",
        "applicable-models/paradigm/parametric",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/fairness",
        "assurance-goal-category/privacy",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/software-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Developing a smartphone keyboard prediction model by learning from users' typing patterns without their text ever leaving their devices, enabling personalised predictions whilst maintaining complete data privacy.",
          "goal": "Privacy"
        },
        {
          "description": "Training a medical diagnosis model across multiple hospitals without sharing patient records, ensuring model robustness by learning from diverse patient populations and clinical practices across different institutions.",
          "goal": "Reliability"
        },
        {
          "description": "Creating a cybersecurity threat detection model by federating learning across financial institutions without exposing sensitive transaction data, reducing systemic risk whilst maintaining competitive confidentiality.",
          "goal": "Safety"
        },
        {
          "description": "Building a fair credit scoring model by training across multiple regions and demographics without centralising sensitive financial data, ensuring representation from diverse populations whilst respecting local data sovereignty laws.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Communication overhead can be substantial, especially with frequent model updates and large models, potentially limiting scalability and increasing training time compared to centralised approaches."
        },
        {
          "description": "Statistical heterogeneity across participants (non-IID data distributions) can lead to training instability, slower convergence, and reduced model performance compared to centralised training on pooled data."
        },
        {
          "description": "System heterogeneity in computational capabilities, network connectivity, and availability of participating devices can create bottlenecks and introduce bias towards more capable participants."
        },
        {
          "description": "Privacy vulnerabilities remain through gradient leakage attacks, model inversion, and membership inference attacks that can potentially reconstruct sensitive information from shared model updates."
        },
        {
          "description": "Coordination complexity increases with the number of participants, requiring sophisticated aggregation protocols, fault tolerance mechanisms, and secure communication infrastructure."
        }
      ],
      "resources": [
        {
          "title": "Open Federated Learning (OpenFL) Documentation",
          "url": "https://openfl.readthedocs.io/en/stable/",
          "source_type": "documentation"
        },
        {
          "title": "Federated Learning - DeepLearning.AI",
          "url": "https://www.deeplearning.ai/short-courses/intro-to-federated-learning/",
          "source_type": "tutorial"
        },
        {
          "title": "A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection",
          "url": "http://arxiv.org/pdf/1907.09693v7",
          "source_type": "documentation",
          "authors": [
            "Qinbin Li",
            "Zeyi Wen",
            "Zhaomin Wu",
            "Sixu Hu",
            "Naibo Wang",
            "Yuan Li",
            "Xu Liu",
            "Bingsheng He"
          ],
          "publication_date": "2019-07-23"
        },
        {
          "title": "Federated learning with hybrid differential privacy for secure and reliable cross-IoT platform knowledge sharing",
          "url": "https://core.ac.uk/download/603345619.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Algburi, S.",
            "Algburi, S.",
            "Anupallavi, S.",
            "Anupallavi, S.",
            "Ashokkumar, S. R.",
            "Ashokkumar, S. R.",
            "Elmedany, W.",
            "Elmedany, W.",
            "Khalaf, O. I.",
            "Khalaf, O. I.",
            "Selvaraj, D.",
            "Selvaraj, D.",
            "Sharif, M. S.",
            "Sharif, M. S."
          ],
          "publication_date": "2024-01-01"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 5,
      "related_techniques": [
        "synthetic-data-generation",
        "differential-privacy",
        "homomorphic-encryption"
      ]
    },
    {
      "slug": "homomorphic-encryption",
      "name": "Homomorphic Encryption",
      "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
      "assurance_goals": [
        "Privacy",
        "Safety",
        "Transparency",
        "Security"
      ],
      "tags": [
        "applicable-models/architecture/linear-models",
        "applicable-models/architecture/neural-networks/feedforward",
        "applicable-models/paradigm/discriminative",
        "applicable-models/paradigm/parametric",
        "applicable-models/requirements/differentiable",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/privacy",
        "assurance-goal-category/privacy/formal-guarantee",
        "assurance-goal-category/safety",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/privacy-guarantee",
        "evidence-type/quantitative-metric",
        "expertise-needed/cryptography",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Enabling a cloud-based medical diagnosis service to process encrypted patient data and return encrypted results without the cloud provider ever accessing actual medical information, ensuring complete patient privacy during outsourced computation.",
          "goal": "Privacy"
        },
        {
          "description": "Securing financial risk assessment computations by allowing banks to jointly analyse encrypted transaction patterns for fraud detection without exposing individual customer data, reducing systemic security risks.",
          "goal": "Safety"
        },
        {
          "description": "Enabling transparent audit of algorithmic decision-making by allowing regulators to verify model computations on encrypted data, providing accountability whilst protecting the proprietary nature of both the algorithm and the underlying data.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Extremely computationally expensive, often 100-1000x slower than unencrypted computation, making it impractical for real-time applications or large-scale data processing."
        },
        {
          "description": "Limited range of operations supported efficiently, with complex operations like divisions, comparisons, and non-polynomial functions being particularly challenging or impossible to implement."
        },
        {
          "description": "Implementation requires deep cryptographic expertise to avoid security vulnerabilities, choose appropriate parameters, and optimise performance for specific use cases."
        },
        {
          "description": "Memory and storage requirements are significantly higher than traditional computation, as encrypted data typically requires much more space than plaintext equivalents."
        },
        {
          "description": "Current fully homomorphic encryption schemes have practical limitations on computation depth before noise accumulation requires expensive bootstrapping operations to refresh ciphertexts."
        }
      ],
      "resources": [
        {
          "title": "zama-ai/concrete-ml",
          "url": "https://github.com/zama-ai/concrete-ml",
          "source_type": "software_package",
          "description": "Privacy-preserving machine learning library that enables data scientists to run ML models on encrypted data using FHE without cryptography expertise"
        },
        {
          "title": "Survey on Fully Homomorphic Encryption, Theory, and Applications",
          "url": "https://core.ac.uk/download/579858842.pdf",
          "source_type": "documentation",
          "authors": [
            "Chiara Marcolla",
            "Frank H.P. Fitzek",
            "Marc Manzano",
            "Najwa Aaraj",
            "Riccardo Bassoli",
            "Victor Sucasas"
          ],
          "publication_date": "2022-10-06",
          "description": "Comprehensive survey covering FHE theory, cryptographic schemes, and practical applications across different domains"
        },
        {
          "title": "Welcome to OpenFHE's documentation! — OpenFHE documentation",
          "url": "https://openfhe-development.readthedocs.io/",
          "source_type": "documentation",
          "description": "Documentation for open-source C++ library supporting multiple FHE schemes including BFV, BGV, CKKS, and Boolean circuits"
        },
        {
          "title": "Evaluation of Privacy-Preserving Support Vector Machine (SVM) Learning Using Homomorphic Encryption",
          "url": "https://core.ac.uk/download/656115203.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Ali, Hisham",
            "Buchanan, William J."
          ],
          "publication_date": "2025-01-01",
          "description": "Technical paper evaluating performance overhead of SVM learning with homomorphic encryption for privacy-preserving ML"
        },
        {
          "title": "microsoft/SEAL",
          "url": "https://github.com/microsoft/SEAL",
          "source_type": "software_package",
          "description": "Easy-to-use homomorphic encryption library enabling computations on encrypted integers and real numbers"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "synthetic-data-generation",
        "federated-learning",
        "differential-privacy"
      ]
    },
    {
      "slug": "intrinsically-interpretable-models",
      "name": "Intrinsically Interpretable Models",
      "description": "Intrinsically interpretable models are machine learning algorithms that are transparent by design, allowing users to understand their decision-making process without requiring additional explanation techniques. This category includes decision trees and rule lists (which use if-then logic), linear and logistic regression models (which use weighted feature combinations), and other simple algorithms where the model structure itself provides interpretability. These models prioritise transparency over complexity, making them ideal when stakeholder understanding and regulatory compliance are paramount.",
      "assurance_goals": [
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/linear-models",
        "applicable-models/architecture/tree-based",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/structured-output",
        "expertise-needed/low",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/project-design",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Developing a medical diagnosis support system using a decision tree with clear if-then rules based on symptoms and test results, allowing healthcare professionals to trace the reasoning path and explain diagnoses to patients whilst ensuring clinical transparency and accountability.",
          "goal": "Transparency"
        },
        {
          "description": "Creating a fraud detection model using logistic regression with carefully selected features (transaction amount, location, time patterns) where each coefficient's contribution can be understood and validated, ensuring reliable performance that financial institutions can audit and regulatory bodies can approve.",
          "goal": "Reliability"
        },
        {
          "description": "Implementing a hiring decision support tool using rule lists that explicitly state qualification criteria and scoring logic, providing transparent candidate evaluation that can be explained to applicants and reviewed for fairness whilst meeting legal requirements for employment decision documentation.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Generally achieve lower predictive accuracy than complex models (neural networks, ensembles) for difficult problems involving high-dimensional data, non-linear relationships, or complex feature interactions."
        },
        {
          "description": "Linear models cannot capture non-linear relationships or feature interactions without manual feature engineering, limiting their applicability to inherently non-linear domains like image recognition or natural language processing."
        },
        {
          "description": "Decision trees can become unstable with small changes in training data, potentially leading to completely different tree structures and predictions, affecting model reliability in dynamic environments."
        },
        {
          "description": "Deep decision trees may lose interpretability despite being inherently transparent, as human cognitive limits make it difficult to follow complex branching logic with many levels and conditions."
        },
        {
          "description": "Feature selection becomes critical for maintaining interpretability, requiring domain expertise to identify the most relevant variables whilst potentially missing important but subtle predictive signals."
        }
      ],
      "resources": [
        {
          "title": "scikit-learn Decision Trees",
          "url": "https://scikit-learn.org/stable/modules/tree.html",
          "source_type": "documentation",
          "description": "Comprehensive documentation for decision tree implementation in scikit-learn, including classification and regression trees with interpretability guidelines and visualisation tools."
        },
        {
          "title": "scikit-learn Linear Models",
          "url": "https://scikit-learn.org/stable/modules/linear_model.html",
          "source_type": "documentation",
          "description": "Complete guide to linear and logistic regression models in scikit-learn, covering implementation, feature selection, and coefficient interpretation for transparent modeling."
        },
        {
          "title": "Interpretable Machine Learning",
          "url": "https://christophm.github.io/interpretable-ml-book/",
          "source_type": "tutorial",
          "description": "Open-source book providing comprehensive coverage of interpretable machine learning models including decision trees, linear models, and rule-based systems with practical examples."
        },
        {
          "title": "R package 'rpart' for Recursive Partitioning",
          "url": "https://cran.r-project.org/web/packages/rpart/index.html",
          "source_type": "software_package",
          "description": "R implementation of recursive partitioning for classification, regression and survival trees with extensive documentation and plotting capabilities for interpretable tree models."
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "monotonicity-constraints",
        "generalized-additive-models"
      ]
    }
  ],
  "count": 5
}