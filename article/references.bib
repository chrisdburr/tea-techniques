@article{brundage2020toward,
  title = {Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims},
  journal = {arXiv:2004.07213 [cs]},
  url = {http://arxiv.org/abs/2004.07213},
  abstract = {With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.},
  author = {Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth and Maharaj, Tegan and Koh, Pang Wei and Hooker, Sara and Leung, Jade and Trask, Andrew and Bluemke, Emma and Lebensbold, Jonathan and O'Keefe, Cullen and Koren, Mark and Ryffel, Th�o and Rubinovitz, J. B. and Besiroglu, Tamay and Carugati, Federica and Clark, Jack and Eckersley, Peter and de Haas, Sarah and Johnson, Maritza and Laurie, Ben and Ingerman, Alex and Krawczuk, Igor and Askell, Amanda and Cammarota, Rosario and Lohn, Andrew and Krueger, David and Stix, Charlotte and Henderson, Peter and Graham, Logan and Prunkl, Carina and Martin, Bianca and Seger, Elizabeth and Zilberman, Noa and h�igeartaigh, Se�n � and Kroeger, Frens and Sastry, Girish and Kagan, Rebecca and Weller, Adrian and Tse, Brian and Barnes, Elizabeth and Dafoe, Allan and Scharre, Paul and Herbert-Voss, Ariel and Rasser, Martijn and Sodhani, Shagun and Flynn, Carrick and Gilbert, Thomas Krendl and Dyer, Lisa and Khan, Saif and Bengio, Yoshua and Anderljung, Markus},
  year = {2020}
}

@article{raji2020closing,
  title = {Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing},
  pages = {12},
  abstract = {Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.},
  author = {Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and Smith-Loud, Jamila and Theron, Daniel and Barnes, Parker},
  year = {2020}
}

@article{mehrabi2019survey,
  title = {A Survey on Bias and Fairness in Machine Learning},
  journal = {arXiv:1908.09635 [cs]},
  url = {http://arxiv.org/abs/1908.09635},
  abstract = {With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  year = {2019}
}

@book{barocas2019fairness,
  title = {Fairness and machine learning},
  publisher = {fairmlbook.org},
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  year = {2019}
}

@article{linardatos2020explainable,
  title = {Explainable AI: A Review of Machine Learning Interpretability Methods},
  journal = {Entropy},
  volume = {23},
  number = {1},
  pages = {18},
  doi = {10.3390/e23010018},
  url = {https://www.mdpi.com/1099-4300/23/1/18},
  abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into "black box" approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
  author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  year = {2020}
}

@techreport{phillips2021four,
  title = {Four Principles of Explainable Artificial Intelligence},
  url = {https://nvlpubs.nist.gov/nistpubs/ir/2021/NIST.IR.8312.pdf},
  author = {Phillips, P. Jonathon and Hahn, Carina A. and Fontana, Peter C. and Yates, Amy N. and Greene, Kristen and Broniatowski, David A. and Przybocki, Mark A.},
  year = {2021}
}

@article{burr2022ethical,
  title = {Ethical assurance: a practical approach to the responsible design, development, and deployment of data-driven technologies},
  journal = {AI and Ethics},
  doi = {10.1007/s43681-022-00178-0},
  url = {https://link.springer.com/10.1007/s43681-022-00178-0},
  author = {Burr, Christopher and Leslie, David},
  year = {2022}
}

@inproceedings{ribeiro2016should,
  title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages = {1135--1144},
  year = {2016},
  doi = {10.1145/2939672.2939778}
}

@inproceedings{lundberg2017unified,
  title = {A Unified Approach to Interpreting Model Predictions},
  author = {Lundberg, Scott M. and Lee, Su-In},
  booktitle = {Advances in Neural Information Processing Systems 30},
  pages = {4765--4774},
  year = {2017},
  url = {https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions}
}

@article{barocas2016big,
  title = {Big Data's Disparate Impact},
  author = {Barocas, Solon and Selbst, Andrew D.},
  journal = {California Law Review},
  volume = {104},
  number = {3},
  pages = {671--732},
  year = {2016},
  doi = {10.2139/ssrn.2477899}
}