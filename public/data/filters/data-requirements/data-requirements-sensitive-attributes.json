{
  "tag": {
    "name": "data-requirements/sensitive-attributes",
    "slug": "data-requirements-sensitive-attributes",
    "count": 26,
    "category": "data-requirements"
  },
  "techniques": [
    {
      "slug": "demographic-parity-assessment",
      "name": "Demographic Parity Assessment",
      "description": "Demographic Parity Assessment evaluates whether a model produces equal positive prediction rates across different demographic groups, regardless of underlying differences in qualifications or base rates. It quantifies fairness using metrics like Statistical Parity Difference (the absolute difference in positive outcome rates between groups) or Disparate Impact ratio (the ratio of positive rates). Unlike techniques that modify data or models, this is purely a measurement approach that highlights when protected groups receive favourable outcomes at different rates, helping organisations identify and document potential discrimination.",
      "assurance_goals": [
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/fairness-metric",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating credit approval algorithms by calculating that loan approval rates for different racial groups must be within 20% of each other (0.8 disparate impact ratio), ensuring compliance with anti-discrimination regulations.",
          "goal": "Fairness"
        },
        {
          "description": "Monitoring hiring platforms by measuring that job recommendation rates for male vs female candidates remain statistically equivalent (Statistical Parity Difference < 0.05), preventing systemic gender bias in career opportunities.",
          "goal": "Fairness"
        },
        {
          "description": "Auditing healthcare triage systems to verify that urgent care assignment rates are equal across ethnic groups, ensuring that automated medical prioritisation doesn't disadvantage minority patients.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Purely observational - identifies discrimination but doesn't provide solutions for remediation or bias mitigation."
        },
        {
          "description": "May penalise models for legitimate differences in base rates between groups, potentially forcing artificial equality where none should exist."
        },
        {
          "description": "Can conflict with individual fairness principles, where similarly qualified individuals might receive different treatment to achieve group parity."
        },
        {
          "description": "Doesn't account for quality of outcomes or consider whether equal rates are actually desirable given different group needs or preferences."
        }
      ],
      "resources": [
        {
          "title": "Fairness through awareness",
          "url": "http://arxiv.org/pdf/1104.3913v1",
          "source_type": "technical_paper",
          "authors": [
            "Cynthia Dwork",
            "Moritz Hardt",
            "Toniann Pitassi",
            "Omer Reingold",
            "Richard Zemel"
          ],
          "publication_date": "2011-04-20"
        },
        {
          "title": "AI Fairness 360 Toolkit",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package"
        },
        {
          "title": "Fairlearn - Demographic Parity",
          "url": "https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html#demographic-parity",
          "source_type": "documentation"
        },
        {
          "title": "A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection",
          "url": "https://www.semanticscholar.org/paper/ce96e451a2685485c05f06fb0d991e29a9c43dae",
          "source_type": "technical_paper",
          "authors": [
            "Julia Ive",
            "Paulina Bondaronek",
            "Vishal Yadav",
            "D. Santel",
            "Tracy Glauser",
            "Tina Cheng",
            "Jeffrey R. Strawn",
            "G. Agasthya",
            "Jordan Tschida",
            "Sanghyun Choo",
            "Mayanka Chandrashekar",
            "Anuj J. Kapadia",
            "J. Pestian"
          ]
        }
      ],
      "complexity_rating": 1,
      "computational_cost_rating": 1,
      "related_techniques": [
        "equal-opportunity-difference",
        "average-odds-difference"
      ]
    },
    {
      "slug": "adversarial-debiasing",
      "name": "Adversarial Debiasing",
      "description": "Adversarial debiasing reduces bias by training models using a competitive adversarial setup, similar to Generative Adversarial Networks (GANs). The technique involves two neural networks: a predictor that learns to make accurate predictions on the main task, and an adversary (bias detector) that attempts to predict protected attributes (such as race, gender, or age) from the predictor's internal representations. Through adversarial training, the predictor learns to produce representations that retain predictive power for the main task whilst being uninformative about protected characteristics, thereby reducing discriminatory bias.",
      "assurance_goals": [
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training a resume screening model for a technology company that evaluates candidates based on skills and experience whilst preventing the internal representations from encoding gender or ethnicity information, ensuring hiring decisions cannot be influenced by protected characteristics even indirectly through correlated features.",
          "goal": "Fairness"
        },
        {
          "description": "Developing a credit scoring model for loan approvals that accurately predicts default risk whilst ensuring the model's internal features cannot be used to infer applicants' race or age, thereby preventing discriminatory lending practices whilst maintaining predictive accuracy.",
          "goal": "Fairness"
        },
        {
          "description": "Creating a medical diagnosis model that makes accurate predictions about patient conditions whilst ensuring that the learned representations cannot reveal sensitive demographic information like gender or ethnicity, protecting patient privacy whilst maintaining clinical effectiveness.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Significantly more complex to implement than standard models, requiring expertise in adversarial training techniques and careful architecture design for both predictor and adversary networks."
        },
        {
          "description": "Requires careful hyperparameter tuning to balance the competing objectives of task performance and bias mitigation, as overly strong adversarial training can harm predictive accuracy."
        },
        {
          "description": "Effectiveness heavily depends on the quality and design of the adversary network - a weak adversary may fail to detect subtle biases, whilst an overly strong adversary may eliminate useful information."
        },
        {
          "description": "Training can be unstable and may suffer from convergence issues common to adversarial training, requiring careful learning rate scheduling and regularisation techniques."
        },
        {
          "description": "Provides no formal guarantees about bias elimination and may not prevent all forms of discrimination, particularly when protected attributes can be inferred from other available features."
        }
      ],
      "resources": [
        {
          "title": "AI Fairness 360 (AIF360)",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package",
          "description": "Comprehensive toolkit for bias detection and mitigation including adversarial debiasing implementations"
        },
        {
          "title": "On the Fairness ROAD: Robust Optimization for Adversarial Debiasing",
          "url": "https://www.semanticscholar.org/paper/0c887592d781538a1b5c2168eae541b563c0ba9a",
          "source_type": "technical_paper",
          "authors": [
            "Vincent Grari",
            "Thibault Laugel",
            "Tatsunori B. Hashimoto",
            "S. Lamprier",
            "Marcin Detyniecki"
          ]
        },
        {
          "title": "aif360.sklearn.inprocessing.AdversarialDebiasing — aif360 0.6.1 ...",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.inprocessing.AdversarialDebiasing.html",
          "source_type": "documentation"
        },
        {
          "title": "Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing",
          "url": "http://arxiv.org/pdf/2103.06179v1",
          "source_type": "technical_paper",
          "authors": [
            "Christian Reimers",
            "Paul Bodesheim",
            "Jakob Runge",
            "Joachim Denzler"
          ],
          "publication_date": "2021-03-10"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "counterfactual-fairness-assessment",
      "name": "Counterfactual Fairness Assessment",
      "description": "Counterfactual Fairness Assessment evaluates whether a model's predictions would remain unchanged if an individual's protected attributes (race, gender, age) were different, whilst keeping all other causally legitimate factors constant. The technique requires constructing a causal graph that maps relationships between variables, then using do-calculus or structural causal models to simulate counterfactual scenarios. For example, it asks: 'Would this loan application still be approved if the applicant were a different race, holding constant their actual qualifications and economic circumstances?' This individual-level fairness criterion helps identify when decisions depend improperly on protected characteristics.",
      "assurance_goals": [
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/causal-inference",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "fairness-approach/causal",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating a hiring algorithm by testing whether qualified candidates would receive the same evaluation scores if their gender were different, whilst controlling for actual skills, experience, and education, revealing whether gender bias affects recruitment decisions.",
          "goal": "Fairness"
        },
        {
          "description": "Assessing a criminal sentencing model by examining whether defendants with identical criminal histories and case circumstances would receive the same sentence recommendations regardless of their race, identifying potential discriminatory patterns in judicial AI systems.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires explicit specification of causal relationships between variables, which involves subjective assumptions about what constitutes legitimate versus illegitimate causal pathways."
        },
        {
          "description": "May be mathematically impossible to satisfy simultaneously with other fairness criteria (like statistical parity), forcing practitioners to choose between competing fairness definitions."
        },
        {
          "description": "Implementation complexity is high, requiring sophisticated causal inference techniques and structural causal models that are difficult to construct and validate."
        },
        {
          "description": "Depends heavily on the quality and completeness of the causal graph, which may be incorrect or missing important confounding variables."
        }
      ],
      "resources": [
        {
          "title": "Counterfactual Fairness",
          "url": "https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Matt J. Kusner",
            "Joshua Loftus",
            "Chris Russell",
            "Ricardo Silva"
          ],
          "publication_date": "2017-12-04"
        },
        {
          "title": "fairlearn/fairlearn",
          "url": "https://github.com/fairlearn/fairlearn",
          "source_type": "software_package"
        },
        {
          "title": "Counterfactual Fairness in Text Classification through Robustness",
          "url": "http://arxiv.org/pdf/1809.10610v2",
          "source_type": "technical_paper",
          "authors": [
            "Sahaj Garg",
            "Vincent Perot",
            "Nicole Limtiaco",
            "Ankur Taly",
            "Ed H. Chi",
            "Alex Beutel"
          ],
          "publication_date": "2018-09-27"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 3,
      "related_techniques": [
        "path-specific-counterfactual-fairness-assessment"
      ]
    },
    {
      "slug": "sensitivity-analysis-for-fairness",
      "name": "Sensitivity Analysis for Fairness",
      "description": "Sensitivity Analysis for Fairness systematically evaluates how model predictions change when sensitive attributes or their proxies are perturbed whilst holding other factors constant. The technique involves creating counterfactual instances by modifying potentially discriminatory features (race, gender, age) or their correlates (zip code, names, education institutions) and measuring the resulting prediction differences. This controlled perturbation approach quantifies the degree to which protected characteristics influence model decisions, helping detect both direct discrimination and indirect bias through proxy variables even when sensitive attributes are not explicitly used as model inputs.",
      "assurance_goals": [
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/local",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Testing whether a lending model's decisions change significantly when only the applicant's zip code (which may correlate with race) is altered, while keeping all other factors constant.",
          "goal": "Fairness"
        },
        {
          "description": "Evaluating a recruitment algorithm by systematically changing candidate names from stereotypically male to female names (whilst keeping qualifications identical) to measure whether gender bias affects hiring recommendations, revealing discrimination through name-based proxies.",
          "goal": "Fairness"
        },
        {
          "description": "Assessing a healthcare resource allocation model by varying patient zip codes across different socioeconomic areas to determine whether geographic proxies for race and income inappropriately influence treatment recommendations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires domain expertise to identify relevant proxy variables for sensitive attributes, which may not be obvious or comprehensive."
        },
        {
          "description": "Computationally intensive for complex models when testing many feature combinations or perturbation ranges."
        },
        {
          "description": "Choice of perturbation ranges and comparison points involves subjective decisions that can significantly affect results and conclusions."
        },
        {
          "description": "May miss subtle or interaction-based forms of discrimination that only manifest under specific combinations of features."
        }
      ],
      "resources": [
        {
          "title": "The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning",
          "url": "http://arxiv.org/pdf/2410.09600v2",
          "source_type": "technical_paper",
          "authors": [
            "Jake Fawkes",
            "Nic Fishman",
            "Mel Andrews",
            "Zachary C. Lipton"
          ],
          "publication_date": "2024-10-12"
        },
        {
          "title": "Fair SA: Sensitivity Analysis for Fairness in Face Recognition",
          "url": "http://arxiv.org/pdf/2202.03586v2",
          "source_type": "technical_paper",
          "authors": [
            "Aparna R. Joshi",
            "Xavier Suau",
            "Nivedha Sivakumar",
            "Luca Zappella",
            "Nicholas Apostoloff"
          ],
          "publication_date": "2022-02-08"
        },
        {
          "title": "fairlearn/fairlearn",
          "url": "https://github.com/fairlearn/fairlearn",
          "source_type": "software_package"
        },
        {
          "title": "Aequitas: Bias Audit Toolkit",
          "url": "https://github.com/dssg/aequitas",
          "source_type": "software_package"
        },
        {
          "title": "Fairness Through Sensitivity Analysis - Towards Data Science",
          "url": "https://towardsdatascience.com/fairness-through-sensitivity-analysis-3ea1b4d79e6c",
          "source_type": "tutorial"
        },
        {
          "title": "User Guide - Fairlearn documentation",
          "url": "https://fairlearn.org/v0.8.0/user_guide/assessment.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "fairness-gan",
        "attribute-removal-fairness-through-unawareness",
        "bayesian-fairness-regularization"
      ]
    },
    {
      "slug": "model-cards",
      "name": "Model Cards",
      "description": "Model cards are standardised documentation frameworks that systematically document machine learning models through structured templates. The templates cover intended use cases, performance metrics across different demographic groups and operating conditions, training data characteristics, evaluation procedures, limitations, and ethical considerations. They serve as comprehensive technical specifications that enable informed model selection, prevent inappropriate deployment, support regulatory compliance, and facilitate fair assessment by providing transparent reporting of model capabilities and constraints across diverse populations and scenarios.",
      "assurance_goals": [
        "Transparency",
        "Fairness",
        "Safety"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/transparency",
        "assurance-goal-category/transparency/documentation/model-card",
        "assurance-goal-category/fairness",
        "assurance-goal-category/safety",
        "data-requirements/access-to-training-data",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/documentation",
        "expertise-needed/ml-engineering",
        "expertise-needed/regulatory-compliance",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/documentation"
      ],
      "example_use_cases": [
        {
          "description": "Documenting a medical diagnosis AI with detailed performance metrics across different patient demographics, age groups, and clinical conditions, enabling healthcare providers to understand when the model should be trusted and when additional expert consultation is needed for patient safety.",
          "goal": "Safety"
        },
        {
          "description": "Creating comprehensive model cards for hiring algorithms that transparently report performance differences across demographic groups, helping HR departments identify potential bias issues and ensure equitable candidate evaluation processes.",
          "goal": "Fairness"
        },
        {
          "description": "Publishing detailed model documentation for a credit scoring API that clearly describes training data sources, evaluation methodologies, and performance limitations, enabling financial institutions to make informed decisions about model deployment and regulatory compliance.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Creating comprehensive model cards requires substantial time, expertise, and resources to gather performance data across diverse conditions and demographic groups, potentially delaying model deployment timelines."
        },
        {
          "description": "Information can become outdated quickly as models are retrained, updated, or deployed in new contexts, requiring ongoing maintenance and version control to remain accurate and useful."
        },
        {
          "description": "Organisations may provide incomplete or superficial documentation to avoid revealing competitive advantages or potential liabilities, undermining the transparency goals of model cards."
        },
        {
          "description": "Lack of standardised formats and enforcement mechanisms means model card quality and completeness vary significantly across different organisations and use cases."
        },
        {
          "description": "Technical complexity of documenting model behaviour across all relevant dimensions may exceed the expertise of some development teams, leading to gaps in critical information."
        }
      ],
      "resources": [
        {
          "title": "Model Cards for Model Reporting",
          "url": "http://arxiv.org/pdf/1810.03993v2",
          "source_type": "technical_paper",
          "authors": [
            "Margaret Mitchell",
            "Simone Wu",
            "Andrew Zaldivar",
            "Parker Barnes",
            "Lucy Vasserman",
            "Ben Hutchinson",
            "Elena Spitzer",
            "Inioluwa Deborah Raji",
            "Timnit Gebru"
          ],
          "publication_date": "2018-10-05",
          "description": "Foundational paper introducing model cards as a framework for transparent model reporting and responsible AI documentation"
        },
        {
          "title": "Model Card Guidebook",
          "url": "https://huggingface.co/docs/hub/en/model-card-guidebook",
          "source_type": "tutorial",
          "description": "Comprehensive guide providing updated model card templates, creator tools, and practical insights for implementing model documentation across diverse stakeholder needs"
        },
        {
          "title": "scikit-learn model cards documentation",
          "url": "https://skops.readthedocs.io/en/stable/auto_examples/plot_model_card.html",
          "source_type": "tutorial",
          "description": "Practical tutorial demonstrating how to create comprehensive model cards for scikit-learn models using the skops library with metrics, visualisations, and metadata"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "datasheets-for-datasets",
        "data-version-control",
        "automated-documentation-generation"
      ]
    },
    {
      "slug": "reweighing",
      "name": "Reweighing",
      "description": "Reweighing is a pre-processing technique that mitigates bias by assigning different weights to training examples based on their group membership and class label. The weights are calculated to ensure that privileged and unprivileged groups have equal influence on the model's training process, effectively balancing the dataset without altering the feature values themselves. This helps to train fairer models by correcting for historical imbalances in how different groups are represented in the data.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group/statistical-parity",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/access-to-training-data",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/fairness-metric",
        "expertise-needed/low",
        "fairness-approach/group",
        "lifecycle-stage/data-handling/preprocessing",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "In a loan application system, if historical data shows that a higher proportion of applicants from a minority group were denied loans (negative outcome), reweighing would assign higher weights to these instances. This forces the model to pay more attention to correctly classifying the underrepresented group, aiming to correct for historical bias and improve fairness metrics like equal opportunity.",
          "goal": "Fairness"
        },
        {
          "description": "When developing a hiring model, if the training data contains fewer female applicants for senior roles, reweighing can be applied to increase the importance of these instances. This helps to prevent the model from learning a spurious correlation between gender and seniority, ensuring that female candidates are evaluated more equitably during the screening process.",
          "goal": "Fairness"
        },
        {
          "description": "In a medical diagnosis system, reweighing provides transparency by explicitly showing which demographic groups required adjustment for balanced representation. The computed weights serve as documentation of historical bias patterns in medical data, helping clinicians understand potential disparities and ensuring the model's decisions are based on medical evidence rather than demographic correlations.",
          "goal": "Transparency"
        },
        {
          "description": "For a credit scoring model deployed across different regions, reweighing improves reliability by ensuring consistent performance across demographic groups. By balancing the training data representation, the model maintains stable accuracy metrics across different population segments, reducing the risk of performance degradation when deployed in areas with different demographic compositions.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "The technique only adjusts the overall influence of demographic groups and does not address biases that may be encoded within the features themselves."
        },
        {
          "description": "Assigning very high weights to a small number of instances from an underrepresented group can increase the model's variance and make it sensitive to outliers, potentially harming generalisation."
        },
        {
          "description": "The effectiveness of reweighing depends on the assumption that the labels in the training data are accurate; it cannot correct for label bias where outcomes were themselves the result of historical discrimination."
        },
        {
          "description": "It may not be effective if the feature distributions for different groups are fundamentally different, as it cannot change the underlying data relationships."
        }
      ],
      "resources": [
        {
          "title": "Achieving Fairness at No Utility Cost via Data Reweighing with Influence",
          "url": "http://arxiv.org/pdf/2202.00787v2",
          "source_type": "technical_paper",
          "authors": [
            "Peizhao Li",
            "Hongfu Liu"
          ],
          "publication_date": "2022-02-01",
          "description": "Presents a novel reweighing approach that computes individual sample weights based on influence functions to achieve fairness without sacrificing model utility"
        },
        {
          "title": "aif360.sklearn.preprocessing.Reweighing — aif360 0.6.1 ...",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.preprocessing.Reweighing.html",
          "source_type": "documentation",
          "description": "Documentation for scikit-learn compatible implementation of the reweighing preprocessing technique in the AI Fairness 360 library"
        },
        {
          "title": "brandeis-machine-learning/influence-fairness",
          "url": "https://github.com/brandeis-machine-learning/influence-fairness",
          "source_type": "software_package",
          "description": "Python implementation of influence-based data reweighing for achieving cost-free fairness with experiments on tabular datasets"
        },
        {
          "title": "Boosting Fair Classifier Generalization through Adaptive Priority Reweighing",
          "url": "http://arxiv.org/pdf/2309.08375v3",
          "source_type": "technical_paper",
          "authors": [
            "Zhihao Hu",
            "Yiran Xu",
            "Mengnan Du",
            "Jindong Gu",
            "Xinmei Tian",
            "Fengxiang He"
          ],
          "publication_date": "2023-09-15",
          "description": "Advanced reweighing technique that adaptively prioritises samples near decision boundaries to improve fairness generalisation across different demographic groups"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "disparate-impact-remover",
        "relabelling",
        "preferential-sampling"
      ]
    },
    {
      "slug": "disparate-impact-remover",
      "name": "Disparate Impact Remover",
      "description": "Disparate Impact Remover is a preprocessing technique that transforms feature values in a dataset to reduce statistical dependence between features and protected attributes (like race or gender). The method modifies non-protected features through mathematical transformations that preserve the utility of the data whilst reducing correlations that could lead to discriminatory outcomes. This approach specifically targets the '80% rule' disparate impact threshold by adjusting feature distributions to ensure more equitable treatment across demographic groups in downstream model predictions.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Transforming features in a credit scoring dataset where variables like 'years of employment' and 'education level' are correlated with race, applying mathematical transformations to reduce these correlations whilst preserving the predictive value for creditworthiness assessment.",
          "goal": "Fairness"
        },
        {
          "description": "Preprocessing a recruitment dataset where features like 'previous job titles' and 'university attended' correlate with gender, modifying these features to ensure the '80% rule' is met whilst maintaining useful information for predicting job performance.",
          "goal": "Fairness"
        },
        {
          "description": "Preprocessing financial lending data to provide transparent bias metrics showing the quantified reduction in correlation between protected attributes and creditworthiness features, enabling institutions to demonstrate compliance with the 80% rule and explain their fairness interventions to regulators.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring consistent model performance across demographic groups in healthcare risk assessment by mathematically transforming features to reduce protected attribute correlations, improving reliability of predictions for minority populations who may have been systematically under-served.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Feature transformations may reduce model accuracy by removing or distorting important predictive information during the debiasing process."
        },
        {
          "description": "Only addresses measured protected attributes and cannot eliminate bias that operates through unmeasured proxy variables."
        },
        {
          "description": "Effectiveness depends on the specific transformation method chosen and may not generalise well to different datasets or domains."
        },
        {
          "description": "May create artificial feature distributions that don't reflect real-world data patterns, potentially causing issues in model deployment."
        }
      ],
      "resources": [
        {
          "title": "holistic-ai/holisticai",
          "url": "https://github.com/holistic-ai/holisticai",
          "source_type": "software_package",
          "description": "Comprehensive open-source toolkit for AI fairness with bias measurement, mitigation techniques, and visualisation tools"
        },
        {
          "title": "Disparate Impact Remover — holisticai documentation",
          "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/preprocessing/bc_disparate_impact_remover_disparate_impact_remover.html",
          "source_type": "tutorial",
          "description": "Comprehensive tutorial covering theoretical background, methodology, and practical implementation of disparate impact removal"
        },
        {
          "title": "Trusted-AI/AIF360",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package",
          "description": "IBM Research's extensible open-source library for detecting and mitigating algorithmic bias across multiple domains"
        },
        {
          "title": "aif360.algorithms.preprocessing.DisparateImpactRemover — aif360 ...",
          "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.DisparateImpactRemover.html",
          "source_type": "documentation",
          "description": "Technical API documentation for AIF360's DisparateImpactRemover class with parameters, methods, and usage examples"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "reweighing",
        "relabelling",
        "preferential-sampling"
      ]
    },
    {
      "slug": "fairness-gan",
      "name": "Fairness GAN",
      "description": "A data generation technique that employs Generative Adversarial Networks (GANs) to create fair synthetic datasets by learning to generate data representations that preserve utility whilst obfuscating protected attributes. Unlike traditional GANs, Fairness GANs incorporate fairness constraints into the training objective, ensuring that the generated data maintains statistical parity across demographic groups. The technique can be used for data augmentation to balance underrepresented groups or to create privacy-preserving synthetic datasets that remove demographic bias from training data.",
      "assurance_goals": [
        "Fairness",
        "Privacy",
        "Reliability"
      ],
      "tags": [
        "applicable-models/gan",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/privacy",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "evidence-type/synthetic-data",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/data-collection/data-augmentation",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Generating balanced synthetic datasets for medical research by creating additional samples from underrepresented demographic groups, ensuring equal representation across ethnicity and gender whilst maintaining the statistical properties needed for robust model training.",
          "goal": "Fairness"
        },
        {
          "description": "Creating privacy-preserving synthetic datasets for financial services that remove demographic identifiers whilst preserving the underlying patterns needed for credit risk assessment, allowing secure data sharing between institutions without exposing sensitive customer information.",
          "goal": "Privacy"
        },
        {
          "description": "Augmenting recruitment datasets by generating synthetic candidate profiles that balance gender and ethnicity representation, ensuring reliable model performance across all demographic groups when real-world data exhibits significant imbalances.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "GAN training is notoriously difficult to stabilise, with potential for mode collapse or failure to converge, especially when additional fairness constraints are imposed."
        },
        {
          "description": "Ensuring fairness in generated data may come at the cost of data utility, potentially reducing the quality or realism of synthetic samples."
        },
        {
          "description": "Requires large datasets to train both generator and discriminator networks effectively, limiting applicability in data-scarce domains."
        },
        {
          "description": "Evaluation complexity is high, as it requires assessing both the quality of generated data and the preservation of fairness properties across demographic groups."
        },
        {
          "description": "May inadvertently introduce new biases if the fairness constraints are not properly specified or if the training data itself contains subtle biases."
        }
      ],
      "resources": [
        {
          "title": "Fairness GAN",
          "url": "http://arxiv.org/pdf/1805.09910v1",
          "source_type": "technical_paper",
          "authors": [
            "Prasanna Sattigeri",
            "Samuel C. Hoffman",
            "Vijil Chenthamarakshan",
            "Kush R. Varshney"
          ],
          "publication_date": "2018-05-24"
        },
        {
          "title": "Fair GANs through model rebalancing for extremely imbalanced class distributions",
          "url": "http://arxiv.org/pdf/2308.08638v2",
          "source_type": "technical_paper",
          "authors": [
            "Anubhav Jain",
            "Nasir Memon",
            "Julian Togelius"
          ],
          "publication_date": "2023-08-16"
        },
        {
          "title": "Inclusive GAN: Improving Data and Minority Coverage in Generative Models",
          "url": "http://arxiv.org/abs/2004.03355",
          "source_type": "technical_paper",
          "authors": [
            "Ning Yu",
            "Ke Li",
            "Peng Zhou",
            "Jitendra Malik",
            "Larry Davis",
            "Mario Fritz"
          ],
          "publication_date": "2020-04-07"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "sensitivity-analysis-for-fairness",
        "attribute-removal-fairness-through-unawareness",
        "bayesian-fairness-regularization"
      ]
    },
    {
      "slug": "relabelling",
      "name": "Relabelling",
      "description": "A preprocessing fairness technique that modifies class labels in training data to achieve equal positive outcome rates across protected groups. Also known as 'data massaging', this method identifies instances that contribute to discriminatory patterns and flips their labels (from positive to negative or vice versa) to balance the proportion of positive outcomes between demographic groups. The technique aims to remove historical bias from training data whilst preserving the overall class distribution, enabling standard classifiers to learn from discrimination-free datasets.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/access-to-training-data",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/dataset-analysis",
        "expertise-needed/statistics",
        "expertise-needed/domain-knowledge",
        "fairness-approach/group",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/data-collection/data-preprocessing",
        "lifecycle-stage/model-development",
        "technique-type/procedural"
      ],
      "example_use_cases": [
        {
          "description": "Preprocessing historical hiring datasets by relabelling borderline cases to ensure equal hiring rates across gender and ethnicity groups, correcting for past discriminatory practices whilst maintaining overall qualification standards for fair recruitment model training.",
          "goal": "Fairness"
        },
        {
          "description": "Creating transparent credit scoring datasets by documenting which loan applications had labels modified to address historical lending discrimination, providing clear audit trails showing how training data bias was systematically corrected before model development.",
          "goal": "Transparency"
        },
        {
          "description": "Improving reliability of medical diagnosis training data by relabelling cases where demographic bias may have influenced historical diagnoses, ensuring models learn from corrected labels that reflect true medical conditions rather than biased historical treatment patterns.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Altering training labels risks introducing new biases or artificial patterns that may not reflect genuine relationships in the data."
        },
        {
          "description": "Deciding which instances to relabel requires careful selection criteria and domain expertise to avoid inappropriate label changes."
        },
        {
          "description": "May reduce prediction accuracy if too many labels are changed, particularly when the modifications conflict with genuine patterns in the data."
        },
        {
          "description": "Requires access to ground truth or expert knowledge to determine whether original labels reflect genuine outcomes or discriminatory bias."
        },
        {
          "description": "Effectiveness depends on accurate identification of discriminatory instances, which can be challenging when bias patterns are subtle or complex."
        }
      ],
      "resources": [
        {
          "title": "Data preprocessing techniques for classification without discrimination",
          "url": "https://link.springer.com/article/10.1007/s10115-011-0463-8",
          "source_type": "technical_paper",
          "authors": [
            "Faisal Kamiran",
            "Toon Calders"
          ],
          "publication_date": "2012-06-01"
        },
        {
          "title": "Classifying without discriminating",
          "url": "https://www.researchgate.net/publication/224440330_Classifying_without_discriminating",
          "source_type": "technical_paper",
          "authors": [
            "Toon Calders",
            "Sicco Verwer"
          ],
          "publication_date": "2010-02-01"
        },
        {
          "title": "Data Pre-Processing for Discrimination Prevention",
          "url": "https://krvarshney.github.io/pubs/CalmonWVRV_jstsp2018.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Flavio Calmon",
            "Dennis Wei",
            "Bhanukiran Vinzamuri",
            "Karthikeyan Natesan Ramamurthy",
            "Kush R. Varshney"
          ],
          "publication_date": "2018-01-01"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "reweighing",
        "disparate-impact-remover",
        "preferential-sampling"
      ]
    },
    {
      "slug": "preferential-sampling",
      "name": "Preferential Sampling",
      "description": "A preprocessing fairness technique developed by Kamiran and Calders that addresses dataset imbalances by re-sampling training data with preference for underrepresented groups to achieve discrimination-free classification. This method modifies the training distribution by prioritising borderline objects (instances near decision boundaries) from underrepresented groups for duplication whilst potentially removing instances from overrepresented groups. Unlike relabelling approaches, preferential sampling maintains original class labels whilst creating a more balanced dataset that prevents models from learning biased patterns due to skewed group representation.",
      "assurance_goals": [
        "Fairness",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/dataset-analysis",
        "expertise-needed/low",
        "fairness-approach/group",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/data-collection/data-preprocessing",
        "lifecycle-stage/model-development",
        "technique-type/procedural"
      ],
      "example_use_cases": [
        {
          "description": "Preprocessing hiring datasets by preferentially sampling candidates from underrepresented gender and ethnic groups, particularly focusing on borderline cases near decision boundaries, to ensure fair representation whilst maintaining original qualifications and labels for unbiased recruitment model training.",
          "goal": "Fairness"
        },
        {
          "description": "Balancing medical training datasets by oversampling patients from underrepresented demographic groups to ensure reliable diagnostic performance across all populations, preventing models from exhibiting reduced accuracy for minority patient groups due to insufficient training examples.",
          "goal": "Reliability"
        },
        {
          "description": "Creating transparent credit scoring datasets by documenting and adjusting the sampling process to ensure equal representation across demographic groups, providing clear evidence to regulators that training data imbalances have been addressed without altering original creditworthiness labels.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Oversampling minority groups can cause overfitting to duplicated examples, particularly when borderline instances are repeatedly sampled, potentially reducing model generalisation."
        },
        {
          "description": "Undersampling majority groups may remove important examples that contain valuable information, potentially degrading overall model performance."
        },
        {
          "description": "Does not address inherent algorithmic bias in the learning process itself, only correcting for representation imbalances in the training data."
        },
        {
          "description": "Selection of borderline objects requires careful threshold tuning and may be sensitive to the choice of distance metrics or similarity measures used."
        },
        {
          "description": "May not address intersectional fairness issues when multiple protected attributes create complex group combinations that require nuanced sampling strategies."
        }
      ],
      "resources": [
        {
          "title": "Data preprocessing techniques for classification without discrimination",
          "url": "https://link.springer.com/article/10.1007/s10115-011-0463-8",
          "source_type": "technical_paper",
          "authors": [
            "Faisal Kamiran",
            "Toon Calders"
          ],
          "publication_date": "2012-06-01"
        },
        {
          "title": "Classification with no discrimination by preferential sampling",
          "url": "https://research.tue.nl/en/publications/classification-with-no-discrimination-by-preferential-sampling",
          "source_type": "technical_paper",
          "authors": [
            "Faisal Kamiran",
            "Toon Calders"
          ],
          "publication_date": "2010-05-27"
        },
        {
          "title": "A Survey on Bias and Fairness in Machine Learning",
          "url": "https://arxiv.org/abs/1908.09635",
          "source_type": "documentation",
          "authors": [
            "Ninareh Mehrabi",
            "Fred Morstatter",
            "Nripsuta Saxena",
            "Kristina Lerman",
            "Aram Galstyan"
          ],
          "publication_date": "2019-08-25"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "related_techniques": [
        "reweighing",
        "disparate-impact-remover",
        "relabelling"
      ]
    },
    {
      "slug": "attribute-removal-fairness-through-unawareness",
      "name": "Attribute Removal (Fairness Through Unawareness)",
      "description": "Attribute Removal (Fairness Through Unawareness) ensures fairness by completely excluding protected attributes such as race, gender, or age from the model's input features. While this approach prevents direct discrimination, it may not eliminate bias if other features are correlated with protected attributes (proxy discrimination). This technique represents the most basic fairness intervention but often needs to be combined with other approaches to address indirect bias through seemingly neutral features.",
      "assurance_goals": [
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/low",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Removing gender, race, and age attributes from hiring algorithms to prevent direct discrimination, whilst acknowledging that indirect bias may persist through correlated features like education institution or postal code.",
          "goal": "Fairness"
        },
        {
          "description": "Excluding protected demographic attributes from credit scoring models to comply with fair lending regulations, ensuring no explicit consideration of race, gender, or ethnicity in loan approval decisions.",
          "goal": "Fairness"
        },
        {
          "description": "Building medical diagnosis models that exclude patient race and ethnicity to prevent biased treatment recommendations, whilst ensuring clinical decisions are based solely on medical indicators and symptoms.",
          "goal": "Fairness"
        },
        {
          "description": "Creating transparent regulatory reporting systems that demonstrate compliance by explicitly documenting which protected attributes have been excluded from decision-making algorithms, providing clear audit trails for regulatory review.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Proxy discrimination remains a major concern as seemingly neutral features (education, postal code, previous employment) may strongly correlate with protected attributes, perpetuating indirect bias."
        },
        {
          "description": "Intersectional bias cannot be addressed through simple attribute removal, as complex interactions between multiple demographic characteristics may create compounding discrimination effects."
        },
        {
          "description": "Legal and regulatory compliance may be insufficient, as many jurisdictions require demonstrating disparate impact absence rather than simply removing protected attributes from models."
        },
        {
          "description": "Identifying all potential proxy variables is practically impossible, especially with high-dimensional data where subtle correlations with protected attributes may exist in unexpected features."
        },
        {
          "description": "Performance degradation may occur if removed attributes contain legitimate predictive information, creating tension between fairness objectives and model accuracy requirements."
        }
      ],
      "resources": [
        {
          "title": "Fairness Through Awareness",
          "url": "https://arxiv.org/abs/1104.3913",
          "source_type": "technical_paper",
          "description": "Foundational paper introducing fairness through awareness concept and demonstrating limitations of fairness through unawareness",
          "authors": [
            "Dwork, Cynthia",
            "Hardt, Moritz",
            "Pitassi, Toniann",
            "Reingold, Omer",
            "Zemel, Richard"
          ],
          "publication_date": "2012-01-01"
        },
        {
          "title": "Fairness Constraints: Mechanisms for Fair Classification",
          "url": "https://arxiv.org/abs/1507.05259",
          "source_type": "technical_paper",
          "description": "Comprehensive analysis of fairness approaches including attribute removal limitations and proxy discrimination challenges",
          "authors": [
            "Zafar, Muhammad Bilal",
            "Valera, Isabel",
            "Rodriguez, Manuel Gomez",
            "Gummadi, Krishna P."
          ],
          "publication_date": "2015-07-19"
        },
        {
          "title": "Fairlearn: A toolkit for assessing and improving fairness in machine learning",
          "url": "https://github.com/fairlearn/fairlearn",
          "source_type": "software_package",
          "description": "Microsoft's comprehensive fairness toolkit with preprocessing methods including attribute removal and proxy detection tools"
        },
        {
          "title": "The Ethical Algorithm: The Science of Socially Aware Algorithm Design",
          "url": "https://global.oup.com/academic/product/the-ethical-algorithm-9780190948207",
          "source_type": "documentation",
          "description": "Accessible book covering fairness through unawareness concepts and practical considerations for practitioners",
          "authors": [
            "Kearns, Michael",
            "Roth, Aaron"
          ],
          "publication_date": "2019-11-01"
        }
      ],
      "complexity_rating": 1,
      "computational_cost_rating": 1,
      "related_techniques": [
        "sensitivity-analysis-for-fairness",
        "fairness-gan",
        "bayesian-fairness-regularization"
      ]
    },
    {
      "slug": "fair-adversarial-networks",
      "name": "Fair Adversarial Networks",
      "description": "An in-processing fairness technique that employs adversarial training with dual neural networks to learn fair representations. The method consists of a predictor network that learns the main task whilst an adversarial discriminator network simultaneously attempts to predict sensitive attributes from the predictor's hidden representations. Through this adversarial min-max game, the predictor is incentivised to learn features that are informative for the task but statistically independent of protected attributes, effectively removing bias at the representation level in deep learning models.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/cnn",
        "applicable-models/gan",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training a facial recognition system that maintains high accuracy for person identification whilst ensuring equal performance across different ethnic groups, using adversarial training to remove race-related features from learned representations.",
          "goal": "Fairness"
        },
        {
          "description": "Developing a resume screening neural network that provides transparent evidence of bias mitigation by demonstrating that learned features cannot predict gender, whilst maintaining predictive performance for job suitability assessment.",
          "goal": "Transparency"
        },
        {
          "description": "Creating a medical image analysis model that achieves reliable diagnostic performance across patient demographics by using adversarial debiasing to ensure age and gender information cannot be extracted from diagnostic features.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Implementation complexity is high, requiring careful design of adversarial loss functions and balancing multiple competing objectives during training."
        },
        {
          "description": "Sensitive to hyperparameter choices, particularly the trade-off weights between prediction accuracy and adversarial loss, which require extensive tuning."
        },
        {
          "description": "Adversarial training can be unstable, with potential for mode collapse or failure to converge, especially in complex deep learning architectures."
        },
        {
          "description": "Interpretability of fairness improvements can be limited, as it may be difficult to verify that sensitive attributes are truly removed from learned representations."
        },
        {
          "description": "Computational overhead is significant due to training two networks simultaneously, increasing both training time and resource requirements."
        }
      ],
      "resources": [
        {
          "title": "Fair Adversarial Networks",
          "url": "http://arxiv.org/pdf/2002.12144v1",
          "source_type": "technical_paper",
          "authors": [
            "George Cevora"
          ],
          "publication_date": "2020-02-23"
        },
        {
          "title": "Demonstrating Rosa: the fairness solution for any Data Analytic pipeline",
          "url": "http://arxiv.org/pdf/2003.00899v2",
          "source_type": "technical_paper",
          "authors": [
            "Kate Wilkinson",
            "George Cevora"
          ],
          "publication_date": "2020-02-28"
        },
        {
          "title": "Triangular Trade-off between Robustness, Accuracy, and Fairness in Deep Neural Networks: A Survey",
          "url": "https://www.semanticscholar.org/paper/13b0444d079bea1c8c57a6082200b67ab5f4616e",
          "source_type": "documentation",
          "authors": [
            "Jingyang Li",
            "Guoqiang Li"
          ],
          "publication_date": "2025-02-10"
        },
        {
          "title": "Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks",
          "url": "https://www.semanticscholar.org/paper/6995779ac582c5f2436cfb82a3c8cf5ca72bae2f",
          "source_type": "technical_paper",
          "authors": [
            "Resmi Ramachandranpillai",
            "Md Fahim Sikder",
            "David Bergström",
            "Fredrik Heintz"
          ],
          "publication_date": "2023-12-14"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "adversarial-debiasing",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "prejudice-remover-regulariser",
      "name": "Prejudice Remover Regulariser",
      "description": "An in-processing fairness technique that adds a fairness penalty to machine learning models to reduce bias against protected groups. The method works by minimising 'mutual information' - essentially reducing how much the model's predictions reveal about sensitive attributes like race or gender. By adding this penalty term to the learning objective (typically in logistic regression), the technique ensures predictions become less dependent on protected features. This addresses not only direct discrimination but also indirect bias through correlated features. Practitioners can adjust a tuning parameter to balance between maintaining accuracy and removing prejudice from the model.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/logistic-regression",
        "applicable-models/probabilistic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training credit scoring models with prejudice remover regularisation to ensure loan approval decisions are not influenced by gender or ethnicity, minimising mutual information between predictions and protected attributes whilst maintaining accurate risk assessment.",
          "goal": "Fairness"
        },
        {
          "description": "Developing transparent university admission models that provide clear evidence of bias mitigation by demonstrating reduced statistical dependence between acceptance decisions and protected characteristics, enabling regulatory compliance reporting.",
          "goal": "Transparency"
        },
        {
          "description": "Building reliable recruitment screening models that maintain consistent performance across demographic groups by regularising against indirect prejudice through correlated features like school names or postal codes that might proxy for protected attributes.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Requires careful tuning of the fairness penalty hyperparameter, where too high values severely degrade accuracy whilst too low values provide insufficient bias mitigation."
        },
        {
          "description": "Primarily applicable to probabilistic discriminative models like logistic regression, limiting its use with other model architectures such as deep neural networks or tree-based methods."
        },
        {
          "description": "Computational complexity increases with the calculation of mutual information between predictions and sensitive attributes, particularly for high-dimensional data."
        },
        {
          "description": "May not fully eliminate all forms of discrimination, particularly when complex interactions between multiple sensitive attributes create intersectional biases."
        },
        {
          "description": "Effectiveness depends on accurate identification and inclusion of all sensitive attributes, potentially missing hidden biases from unobserved protected characteristics."
        }
      ],
      "resources": [
        {
          "title": "Fairness-Aware Classifier with Prejudice Remover Regularizer",
          "url": "https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3",
          "source_type": "technical_paper",
          "authors": [
            "Toshihiro Kamishima",
            "Shotaro Akaho",
            "Hideki Asoh",
            "Jun Sakuma"
          ],
          "publication_date": "2012-09-24"
        },
        {
          "title": "Fairness-Aware Machine Learning and Data Mining",
          "url": "https://www.kamishima.net/faml/",
          "source_type": "documentation"
        },
        {
          "title": "Fairness-aware Classifier (faclass)",
          "url": "https://www.kamishima.net/faclass/",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "meta-fair-classifier",
      "name": "Meta Fair Classifier",
      "description": "An in-processing fairness technique that employs meta-learning to modify any existing classifier for optimising fairness metrics whilst maintaining predictive performance. The method learns how to adjust model parameters or decision boundaries to satisfy fairness constraints such as demographic parity or equalised odds through iterative optimisation. This approach is particularly valuable when retrofitting fairness to pre-trained models that perform well but exhibit bias, as it can incorporate fairness without requiring complete retraining from scratch.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "lifecycle-stage/model-optimization",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Retrofitting an existing hiring algorithm to achieve demographic parity across gender and ethnicity groups by using meta-learning to adjust decision boundaries, ensuring equitable candidate selection whilst maintaining the model's ability to identify qualified applicants.",
          "goal": "Fairness"
        },
        {
          "description": "Modifying a pre-trained credit scoring model to provide transparent fairness guarantees by learning optimal parameter adjustments that satisfy equalised odds constraints, enabling clear reporting on fair lending compliance to regulatory authorities.",
          "goal": "Transparency"
        },
        {
          "description": "Adapting a medical diagnosis model to ensure reliable performance across patient demographics by meta-learning fairness-aware adjustments that maintain diagnostic accuracy whilst reducing disparities in treatment recommendations across age and socioeconomic groups.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Meta-learning approach can be complex to implement, requiring expertise in both the underlying classifier and meta-optimisation techniques."
        },
        {
          "description": "Requires extensive hyperparameter tuning to balance fairness constraints with predictive performance, making optimisation challenging."
        },
        {
          "description": "May result in longer training times compared to simpler fairness techniques due to the iterative meta-learning process."
        },
        {
          "description": "Performance depends heavily on the quality and characteristics of the base classifier being modified, limiting effectiveness with poorly-performing models."
        },
        {
          "description": "Theoretical guarantees about fairness-accuracy trade-offs may not hold in practice due to finite sample effects and optimisation challenges."
        }
      ],
      "resources": [
        {
          "title": "ρ-Fair Method — holisticai documentation",
          "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/inprocessing/bc_meta_fair_classifier_rho_fair.html",
          "source_type": "documentation"
        },
        {
          "title": "aif360.algorithms.inprocessing — aif360 0.1.0 documentation",
          "url": "https://aif360.readthedocs.io/en/v0.2.3/modules/inprocessing.html",
          "source_type": "documentation"
        },
        {
          "title": "Welcome to AI Fairness 360's documentation! — aif360 0.1.0 ...",
          "url": "https://aif360.readthedocs.io/en/v0.2.3/",
          "source_type": "documentation"
        },
        {
          "title": "Algorithmic decision making methods for fair credit scoring",
          "url": "http://arxiv.org/abs/2209.07912",
          "source_type": "technical_paper",
          "authors": [
            "Moldovan, Darie"
          ],
          "publication_date": "2022-09-16"
        },
        {
          "title": "The Importance of Modeling Data Missingness in Algorithmic Fairness: A\n  Causal Perspective",
          "url": "http://arxiv.org/abs/2012.11448",
          "source_type": "technical_paper",
          "authors": [
            "Amayuelas, Alfonso",
            "Deshpande, Amit",
            "Goel, Naman",
            "Sharma, Amit"
          ],
          "publication_date": "2020-12-21"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "exponentiated-gradient-reduction",
      "name": "Exponentiated Gradient Reduction",
      "description": "An in-processing fairness technique based on Agarwal et al.'s reductions approach that transforms fair classification into a sequence of cost-sensitive classification problems. The method uses an exponentiated gradient algorithm to iteratively reweight training data, returning a randomised classifier that achieves the lowest empirical error whilst satisfying fairness constraints. This reduction-based framework provides theoretical guarantees about both accuracy and constraint violation, making it suitable for various fairness criteria including demographic parity and equalised odds.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/labelled-data",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training a hiring algorithm with demographic parity constraints to ensure equal selection rates across gender groups, using iterative reweighting to balance fairness and predictive accuracy whilst maintaining legal compliance.",
          "goal": "Fairness"
        },
        {
          "description": "Developing a loan approval model with equalised odds constraints, providing transparent documentation of the theoretical guarantees about both error rates and fairness constraint violations achieved by the reduction approach.",
          "goal": "Transparency"
        },
        {
          "description": "Creating a medical diagnosis classifier that maintains reliable performance across demographic groups by using randomised prediction averaging, ensuring consistent healthcare delivery whilst monitoring constraint satisfaction over time.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Requires convex base learners for theoretical guarantees about convergence and optimality, limiting the choice of underlying models."
        },
        {
          "description": "Produces randomised classifiers that may give different predictions for identical inputs, which can be problematic in applications requiring consistent decisions."
        },
        {
          "description": "Convergence can be slow and sensitive to hyperparameter choices, particularly the learning rate and tolerance settings."
        },
        {
          "description": "Involves iterative retraining with adjusted weights, which can be computationally expensive for large datasets or complex models."
        },
        {
          "description": "Fairness constraints may significantly reduce model accuracy, and the trade-off between fairness and performance is not always transparent to practitioners."
        }
      ],
      "resources": [
        {
          "title": "A Reductions Approach to Fair Classification",
          "url": "https://arxiv.org/abs/1803.02453",
          "source_type": "technical_paper",
          "description": "Foundational paper by Agarwal et al. introducing the exponentiated gradient reduction approach for fair classification with theoretical guarantees.",
          "authors": [
            "Alekh Agarwal",
            "Alina Beygelzimer",
            "Miroslav Dudík",
            "John Langford",
            "Hanna Wallach"
          ],
          "publication_date": "2018-03-06"
        },
        {
          "title": "Fairlearn: ExponentiatedGradient",
          "url": "https://fairlearn.org/v0.10/api_reference/generated/fairlearn.reductions.ExponentiatedGradient.html",
          "source_type": "documentation",
          "description": "Microsoft's Fairlearn implementation of the Agarwal et al. algorithm with comprehensive API documentation and examples."
        },
        {
          "title": "IBM AIF360: ExponentiatedGradientReduction",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.inprocessing.ExponentiatedGradientReduction.html",
          "source_type": "documentation",
          "description": "IBM's AIF360 implementation with scikit-learn compatible API for in-processing fairness constraints during model training."
        },
        {
          "title": "Fairlearn Reductions Guide",
          "url": "https://fairlearn.org/main/user_guide/mitigation/reductions.html",
          "source_type": "tutorial",
          "description": "Comprehensive guide to using reduction-based approaches for fairness, including practical examples of exponentiated gradient methods and fairness constraints."
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "fair-transfer-learning",
      "name": "Fair Transfer Learning",
      "description": "An in-processing fairness technique that adapts pre-trained models from one domain to another whilst explicitly preserving fairness constraints across different contexts or populations. The method addresses the challenge that fairness properties may not transfer when adapting models to new domains with different demographic compositions or data distributions. Fair transfer learning typically involves constraint-aware fine-tuning, domain adaptation techniques, or adversarial training that maintains equitable performance across groups in the target domain, ensuring that bias mitigation efforts carry over from source to target domains.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/pre-trained-model",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "lifecycle-stage/model-development/fine-tuning",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adapting a hiring algorithm trained on one country's recruitment data to another region whilst maintaining fairness across gender and ethnicity groups, ensuring equitable candidate evaluation despite different local demographic distributions and cultural contexts.",
          "goal": "Fairness"
        },
        {
          "description": "Transferring a medical diagnosis model from urban hospital data to rural clinics whilst providing transparent evidence that fairness constraints are preserved across age, gender, and socioeconomic groups despite different patient populations and healthcare infrastructure.",
          "goal": "Transparency"
        },
        {
          "description": "Adapting a fraud detection system from one financial market to another whilst ensuring reliable performance across customer demographics, maintaining consistent accuracy and fairness even when transaction patterns and customer characteristics differ between markets.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Fairness properties achieved in the source domain may not translate directly to the target domain if demographic distributions or data characteristics differ significantly."
        },
        {
          "description": "Requires careful hyperparameter tuning and constraint specification to balance fairness preservation with model performance in the new domain."
        },
        {
          "description": "Implementation complexity is high, requiring expertise in both transfer learning techniques and fairness constraint optimisation methods."
        },
        {
          "description": "May suffer from negative transfer effects where fairness constraints that worked well in the source domain actually harm performance in the target domain."
        },
        {
          "description": "Evaluation challenges arise from needing to validate fairness across multiple domains and demographic groups simultaneously."
        }
      ],
      "resources": [
        {
          "title": "Segmenting across places: The need for fair transfer learning with satellite imagery",
          "url": "http://arxiv.org/pdf/2204.04358v3",
          "source_type": "technical_paper",
          "authors": [
            "Miao Zhang",
            "Harvineet Singh",
            "Lazarus Chok",
            "Rumi Chunara"
          ],
          "publication_date": "2022-04-09"
        },
        {
          "title": "Trustworthy Transfer Learning: A Survey",
          "url": "https://www.semanticscholar.org/paper/7ee5c5b58ed0b4e585e0c30790c206bea07faacf",
          "source_type": "documentation",
          "authors": [
            "Jun Wu",
            "Jingrui He"
          ],
          "publication_date": "2024-12-18"
        },
        {
          "title": "Cross-Institutional Transfer Learning for Educational Models: Implications for Model Performance, Fairness, and Equity",
          "url": "https://arxiv.org/abs/2305.00927",
          "source_type": "technical_paper",
          "authors": [
            "Josh Gardner",
            "Renzhe Yu",
            "Quan Nguyen",
            "Christopher Brooks",
            "Rene Kizilcec"
          ],
          "publication_date": "2023-05-01"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "adaptive-sensitive-reweighting",
      "name": "Adaptive Sensitive Reweighting",
      "description": "Adaptive Sensitive Reweighting dynamically adjusts the importance of training examples during model training based on real-time performance across different demographic groups. Unlike traditional static reweighting that fixes weights at the start, this technique continuously monitors fairness metrics and automatically increases the weight of examples from underperforming groups whilst decreasing weights for overrepresented groups. The adaptive mechanism prevents models from perpetuating historical biases by ensuring balanced learning across all demographics throughout the training process.",
      "assurance_goals": [
        "Fairness",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "data-requirements/access-to-model-internals",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training speech recognition systems that adapt weights during training to ensure consistent accuracy across different accents, dialects, and linguistic backgrounds, preventing models from favouring dominant accent groups in the training data.",
          "goal": "Fairness"
        },
        {
          "description": "Developing hiring algorithms that dynamically adjust training example weights to maintain consistent evaluation performance across demographic groups, ensuring the model doesn't learn to favour candidates from overrepresented backgrounds.",
          "goal": "Fairness"
        },
        {
          "description": "Building medical diagnostic models that adaptively reweight patient examples during training to ensure reliable performance across different age groups, ethnicities, and socioeconomic backgrounds, preventing healthcare disparities.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Training instability can occur when adaptive weight adjustments cause oscillations between demographic groups, potentially preventing convergence if reweighting parameters are not carefully tuned."
        },
        {
          "description": "Computational overhead increases significantly due to continuous monitoring of fairness metrics across groups during training, requiring additional memory and processing time."
        },
        {
          "description": "Risk of overfitting to specific demographic subgroups if the adaptation mechanism becomes too aggressive in correcting for observed performance disparities during training."
        },
        {
          "description": "Requires careful hyperparameter tuning for adaptation rates and fairness thresholds, making the technique sensitive to configuration choices that may not generalise across different datasets."
        },
        {
          "description": "May inadvertently harm overall model performance if the reweighting process prioritises fairness at the expense of learning important patterns that benefit all groups."
        }
      ],
      "resources": [
        {
          "title": "Adaptive Sensitive Reweighting to Mitigate Bias in Fairness-aware Classification",
          "url": "https://dl.acm.org/doi/10.1145/3178876.3186133",
          "source_type": "technical_paper",
          "description": "Original paper introducing adaptive sensitive reweighting technique using CULEP model for bias mitigation in classification tasks",
          "authors": [
            "Krasanakis, Emmanouil",
            "Spyromitros-Xioufis, Eleftherios",
            "Papadopoulos, Symeon",
            "Kompatsiaris, Yiannis"
          ],
          "publication_date": "2018-04-23"
        },
        {
          "title": "AIF360: A comprehensive set of fairness metrics for datasets and machine learning models",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package",
          "description": "IBM's comprehensive fairness toolkit including implementations of various reweighting techniques and bias mitigation methods"
        },
        {
          "title": "Fairlearn: A toolkit for assessing and improving fairness in machine learning",
          "url": "https://github.com/fairlearn/fairlearn",
          "source_type": "software_package",
          "description": "Microsoft's open-source toolkit providing reweighting and other bias mitigation algorithms with comprehensive documentation"
        },
        {
          "title": "Causal Fairness-Guided Dataset Reweighting using Neural Networks",
          "url": "https://arxiv.org/abs/2311.10512",
          "source_type": "technical_paper",
          "description": "Recent research on causal fairness-guided dataset reweighting using neural networks to address fairness from causal perspective",
          "authors": [
            "Zhao, Xuan",
            "Broelemann, Klaus",
            "Ruggieri, Salvatore",
            "Kasneci, Gjergji"
          ],
          "publication_date": "2023-11-17"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "multi-accuracy-boosting",
      "name": "Multi-Accuracy Boosting",
      "description": "An in-processing fairness technique that employs boosting algorithms to improve accuracy uniformly across demographic groups by iteratively correcting errors where the model performs poorly for certain subgroups. The method uses a multi-calibration approach that trains weak learners to focus on prediction errors for underperforming groups, ensuring that no group experiences systematically worse accuracy. This iterative boosting process continues until accuracy parity is achieved across all groups whilst maintaining overall model performance.",
      "assurance_goals": [
        "Fairness",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "applicable-models/ensemble",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training a medical diagnosis model that achieves equal accuracy across age, gender, and ethnicity groups by using boosting to specifically target prediction errors for underrepresented patient demographics, ensuring equitable healthcare outcomes for all populations.",
          "goal": "Fairness"
        },
        {
          "description": "Building a robust fraud detection system that maintains consistent accuracy across different customer segments by iteratively correcting errors where the model performs poorly for specific demographic or geographic groups, ensuring reliable fraud prevention across all user types.",
          "goal": "Reliability"
        },
        {
          "description": "Developing a transparent hiring algorithm that provides clear evidence of equal performance across candidate demographics by using multi-accuracy boosting to systematically address group-specific prediction errors, enabling auditable fair recruitment processes.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires identifying and defining relevant subgroups or error regions, which may be challenging when group boundaries are unclear or overlapping."
        },
        {
          "description": "Could increase model complexity significantly as the boosting process adds multiple weak learners, potentially affecting interpretability and computational efficiency."
        },
        {
          "description": "May overfit to training data if very granular corrections are made, particularly when subgroups are small or the boosting process continues for too many iterations."
        },
        {
          "description": "Performance depends on the quality of subgroup identification, and may fail to achieve fairness if important demographic intersections are not properly captured."
        },
        {
          "description": "Convergence to equal accuracy across groups is not guaranteed, especially when there are fundamental differences in data distributions between groups."
        }
      ],
      "resources": [
        {
          "title": "mcboost: Multi-Calibration Boosting for R",
          "url": "https://joss.theoj.org/papers/10.21105/joss.03453",
          "source_type": "technical_paper",
          "authors": [
            "Bernd Bischl",
            "Susanne Dandl",
            "Christoph Kern",
            "Michael P. Kim",
            "Florian Pfisterer",
            "Matthew Sun"
          ],
          "publication_date": "2021-08-24"
        },
        {
          "title": "mlr-org/mcboost",
          "url": "https://github.com/mlr-org/mcboost",
          "source_type": "software_package"
        },
        {
          "title": "Multigroup Robustness",
          "url": "http://arxiv.org/abs/2405.00614",
          "source_type": "technical_paper",
          "authors": [
            "Lunjia Hu",
            "Charlotte Peale",
            "Judy Hanwen Shen"
          ],
          "publication_date": "2024-05-01"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting"
      ]
    },
    {
      "slug": "equalised-odds-post-processing",
      "name": "Equalised Odds Post-Processing",
      "description": "A post-processing fairness technique based on Hardt et al.'s seminal work that adjusts classification thresholds after model training to achieve equal true positive rates and false positive rates across demographic groups. The method uses group-specific decision thresholds, potentially with randomisation, to satisfy the equalised odds constraint whilst preserving model utility. This approach enables fairness mitigation without retraining, making it applicable to existing deployed models or when training data access is restricted.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/labelled-data",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/testing",
        "lifecycle-stage/system-deployment-and-use/monitoring",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Post-processing a criminal recidivism risk assessment model to ensure equal error rates across racial groups, using group-specific thresholds to achieve equal TPR and FPR whilst maintaining predictive accuracy for judicial decision support.",
          "goal": "Fairness"
        },
        {
          "description": "Adjusting a hiring algorithm's decision thresholds to ensure equal opportunities for qualified candidates across gender groups, providing transparent evidence that the screening process treats all demographics equitably.",
          "goal": "Transparency"
        },
        {
          "description": "Calibrating a medical diagnosis model's outputs to maintain equal detection rates across age groups, ensuring reliable performance monitoring and consistent healthcare delivery regardless of patient demographics.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "May require randomisation in decision-making, leading to inconsistent outcomes for similar individuals to achieve group-level fairness constraints."
        },
        {
          "description": "Post-processing can reduce overall model accuracy or confidence scores, particularly when group-specific ROC curves do not intersect favourably."
        },
        {
          "description": "Violates calibration properties of the original model, creating a trade-off between equalised odds and predictive rate parity."
        },
        {
          "description": "Limited to combinations of error rates that lie on the intersection of group-specific ROC curves, which may represent poor trade-offs."
        },
        {
          "description": "Requires access to sensitive attributes during deployment, which may not be available or legally permissible in all contexts."
        }
      ],
      "resources": [
        {
          "title": "Equality of Opportunity in Supervised Learning",
          "url": "https://arxiv.org/abs/1610.02413",
          "source_type": "technical_paper",
          "description": "Foundational paper by Hardt et al. introducing the equalised odds post-processing algorithm and mathematical framework for fairness constraints.",
          "authors": [
            "Moritz Hardt",
            "Eric Price",
            "Nathan Srebro"
          ],
          "publication_date": "2016-10-07"
        },
        {
          "title": "Equalized odds postprocessing under imperfect group information",
          "url": "https://arxiv.org/abs/1906.03284",
          "source_type": "technical_paper",
          "description": "Extension of Hardt et al.'s method examining robustness when protected attribute information is imperfect or noisy.",
          "authors": [
            "Pranjal Awasthi",
            "Matthäus Kleindessner",
            "Jamie Morgenstern"
          ],
          "publication_date": "2019-06-07"
        },
        {
          "title": "Fairlearn: ThresholdOptimizer",
          "url": "https://fairlearn.org/v0.10/api_reference/generated/fairlearn.postprocessing.ThresholdOptimizer.html",
          "source_type": "documentation",
          "description": "Microsoft's Fairlearn implementation of the Hardt et al. algorithm with API documentation and usage examples for equalised odds constraints."
        },
        {
          "title": "IBM AIF360",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package",
          "description": "Comprehensive fairness toolkit including EqualizedOddsPostprocessing implementation based on Hardt et al.'s original algorithm."
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "threshold-optimiser",
        "reject-option-classification",
        "calibration-with-equality-of-opportunity"
      ]
    },
    {
      "slug": "threshold-optimiser",
      "name": "Threshold Optimiser",
      "description": "Threshold Optimiser adjusts decision thresholds for different demographic groups after model training to satisfy specific fairness constraints. This post-processing technique optimises group-specific thresholds by analysing the probability distribution of model outputs, allowing practitioners to achieve fairness goals like demographic parity or equalised opportunity without modifying the underlying model. The optimiser finds optimal threshold values for each group that balance fairness requirements with overall model performance, making it particularly useful when fairness considerations arise after model deployment.",
      "assurance_goals": [
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/post-processing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting hiring decision thresholds in a recruitment system to ensure equal opportunity rates across gender and ethnicity groups, where the model outputs probability scores but different demographic groups require different thresholds to achieve equitable outcomes.",
          "goal": "Fairness"
        },
        {
          "description": "Optimising credit approval thresholds for different demographic groups in loan applications to satisfy regulatory requirements for equal treatment whilst maintaining acceptable default rates across all groups.",
          "goal": "Fairness"
        },
        {
          "description": "Calibrating medical diagnosis thresholds across age and gender groups to ensure diagnostic accuracy is maintained whilst preventing systematic over-diagnosis or under-diagnosis in specific populations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires a held-out dataset with known group memberships to determine optimal thresholds for each demographic group."
        },
        {
          "description": "Threshold values may need recalibration when input data distributions shift or model performance changes over time."
        },
        {
          "description": "Using different decision thresholds per group can raise legal or ethical concerns in deployment contexts where equal treatment is mandated."
        },
        {
          "description": "Performance depends on the quality and representativeness of the calibration dataset for each demographic group."
        },
        {
          "description": "May lead to reduced overall accuracy as the optimisation trades off individual accuracy for group fairness."
        }
      ],
      "resources": [
        {
          "title": "Group-Aware Threshold Adaptation for Fair Classification",
          "url": "https://arxiv.org/abs/2111.04271",
          "source_type": "technical_paper",
          "authors": [
            "Jang, Taeuk",
            "Shi, Pengyi",
            "Wang, Xiaoqian"
          ],
          "publication_date": "2021-11-08",
          "description": "Introduces a novel post-processing method for learning adaptive classification thresholds for each demographic group by optimising confusion matrices estimated from model probability distributions."
        },
        {
          "title": "Equality of Opportunity in Supervised Learning",
          "url": "https://arxiv.org/abs/1610.02413",
          "source_type": "technical_paper",
          "authors": [
            "Hardt, Moritz",
            "Price, Eric",
            "Srebro, Nathan"
          ],
          "publication_date": "2016-10-07",
          "description": "Foundational work introducing threshold optimisation techniques to achieve equalized opportunity and demographic parity in supervised learning."
        },
        {
          "title": "AIF360: A comprehensive set of fairness metrics and algorithms",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package",
          "description": "Open-source library containing threshold optimisation implementations for various fairness constraints including equalized odds and demographic parity."
        },
        {
          "title": "Fairlearn: A toolkit for assessing and improving fairness in AI",
          "url": "https://github.com/fairlearn/fairlearn",
          "source_type": "software_package",
          "description": "Python library providing threshold optimisation methods and post-processing algorithms for achieving fairness in machine learning models."
        },
        {
          "title": "HolisticAI: Randomized Threshold Optimizer",
          "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/postprocessing/bc_ml_debiaser_rto.html",
          "source_type": "documentation",
          "description": "Documentation for the Randomized Threshold Optimizer implementation that achieves statistical parity through group-aware threshold adjustment with randomization."
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "equalised-odds-post-processing",
        "reject-option-classification",
        "calibration-with-equality-of-opportunity"
      ]
    },
    {
      "slug": "reject-option-classification",
      "name": "Reject Option Classification",
      "description": "A post-processing fairness technique that modifies predictions in regions of high uncertainty to favour disadvantaged groups and achieve fairness objectives. The method identifies a 'rejection region' where the model's confidence is low (typically near the decision boundary) and reassigns predictions within this region to benefit underrepresented groups. By leveraging model uncertainty, this approach can improve fairness metrics like demographic parity or equalised odds whilst minimising changes to confident predictions, thus preserving overall accuracy for cases where the model is certain.",
      "assurance_goals": [
        "Fairness",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-requirements/prediction-probabilities",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "fairness-approach/group",
        "lifecycle-stage/post-processing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting hiring algorithm predictions in the uncertainty region where candidate scores are close to the threshold, reassigning borderline cases to ensure equal selection rates across gender and ethnicity groups whilst maintaining decisions for clearly qualified or unqualified candidates.",
          "goal": "Fairness"
        },
        {
          "description": "Improving reliability of loan approval systems by identifying applications where the model is uncertain and adjusting these edge cases to ensure consistent approval rates across demographic groups, reducing the risk of systematic discrimination in borderline creditworthiness assessments.",
          "goal": "Reliability"
        },
        {
          "description": "Creating transparent bail decision systems that clearly document which predictions fall within the rejection region and how adjustments are made, providing courts with explainable fairness interventions that show exactly when and why decisions were modified for equity.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires models that provide reliable uncertainty estimates or probability scores, limiting applicability to deterministic classifiers without confidence outputs."
        },
        {
          "description": "Selection of the rejection region threshold is subjective and requires careful tuning to balance fairness improvements with accuracy preservation."
        },
        {
          "description": "May reject too many instances if tuned conservatively, potentially affecting a large portion of predictions and reducing the model's practical utility."
        },
        {
          "description": "Cannot address bias in confident predictions outside the rejection region, limiting effectiveness when discrimination occurs in high-certainty cases."
        },
        {
          "description": "Performance depends on the quality of uncertainty estimates, which may be poorly calibrated in some models, leading to inappropriate rejection regions."
        }
      ],
      "resources": [
        {
          "title": "Machine Learning with a Reject Option: A survey",
          "url": "https://www.semanticscholar.org/paper/24864a7f899718477c04ede9c0bea906c5dc2667",
          "source_type": "documentation",
          "authors": [
            "Kilian Hendrickx",
            "Lorenzo Perini",
            "Dries Van der Plas",
            "Wannes Meert",
            "Jesse Davis"
          ]
        },
        {
          "title": "aif360.algorithms.postprocessing.RejectOptionClassification ...",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html",
          "source_type": "documentation"
        },
        {
          "title": "Survey on Leveraging Uncertainty Estimation Towards Trustworthy Deep Neural Networks: The Case of Reject Option and Post-training Processing",
          "url": "https://www.semanticscholar.org/paper/e939c6ac58e08b539ae8a7dc54216bceb775b085",
          "source_type": "documentation",
          "authors": [
            "M. Hasan",
            "Moloud Abdar",
            "A. Khosravi",
            "U. Aickelin",
            "Pietro Lio'",
            "Ibrahim Hossain",
            "Ashikur Rahman",
            "Saeid Nahavandi"
          ]
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "equalised-odds-post-processing",
        "threshold-optimiser",
        "calibration-with-equality-of-opportunity"
      ]
    },
    {
      "slug": "calibration-with-equality-of-opportunity",
      "name": "Calibration with Equality of Opportunity",
      "description": "A post-processing fairness technique that adjusts model predictions to achieve equal true positive rates across protected groups whilst maintaining calibration within each group. The method addresses fairness by ensuring that qualified individuals from different demographic groups have equal chances of receiving positive predictions, whilst preserving the meaning of probability scores within each group. This technique attempts to balance the competing objectives of group fairness and accurate probability estimation.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/calibration-set",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/testing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting a loan approval model to ensure that qualified applicants from different ethnic backgrounds have equal approval rates, whilst maintaining that a 70% predicted repayment probability means the same thing for each ethnic group in practice.",
          "goal": "Fairness"
        },
        {
          "description": "Post-processing a university admissions algorithm to equalise acceptance rates for qualified students across gender groups, whilst ensuring the predicted success scores remain well-calibrated within each gender to support transparent decision-making.",
          "goal": "Transparency"
        },
        {
          "description": "Calibrating a medical diagnosis model to maintain equal detection rates for a disease across different age groups whilst preserving the reliability of risk scores, ensuring that a 30% risk prediction accurately reflects actual disease occurrence within each age group.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Fundamental mathematical incompatibility exists between perfect calibration and exact equality of opportunity, except in highly constrained special cases."
        },
        {
          "description": "May reduce overall model accuracy or calibration when forcing equal true positive rates across groups with genuinely different base rates."
        },
        {
          "description": "Requires access to sensitive attributes during post-processing, which may not be available or legally permissible in all contexts."
        },
        {
          "description": "The technique only addresses one aspect of fairness (true positive rates) and may allow disparities in false positive rates between groups."
        },
        {
          "description": "Post-processing approaches cannot address biases inherent in the training data or model architecture, only adjust final predictions."
        }
      ],
      "resources": [
        {
          "title": "On Fairness and Calibration",
          "url": "https://arxiv.org/abs/1709.02012",
          "source_type": "technical_paper",
          "description": "Foundational paper demonstrating the mathematical tension between calibration and equalised odds fairness constraints.",
          "authors": [
            "Geoff Pleiss",
            "Manish Raghavan",
            "Felix Wu",
            "Jon Kleinberg",
            "Kilian Q. Weinberger"
          ],
          "publication_date": "2017-09-06"
        },
        {
          "title": "equalized_odds_and_calibration",
          "url": "https://github.com/gpleiss/equalized_odds_and_calibration",
          "source_type": "software_package",
          "description": "Python implementation of post-processing methods for achieving calibration with equality of opportunity constraints."
        },
        {
          "title": "Equality of Opportunity in Supervised Learning",
          "url": "https://arxiv.org/abs/1610.02413",
          "source_type": "technical_paper",
          "description": "Original paper introducing the equality of opportunity fairness criterion and post-processing algorithms.",
          "authors": [
            "Moritz Hardt",
            "Eric Price",
            "Nathan Srebro"
          ],
          "publication_date": "2016-10-07"
        },
        {
          "title": "Fairlearn: Postprocessing Methods",
          "url": "https://fairlearn.org/v0.10/user_guide/mitigation/postprocessing.html",
          "source_type": "documentation",
          "description": "Documentation for implementing threshold optimisation and calibration methods to achieve fairness constraints."
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 2,
      "related_techniques": [
        "equalised-odds-post-processing",
        "threshold-optimiser",
        "reject-option-classification"
      ]
    },
    {
      "slug": "equal-opportunity-difference",
      "name": "Equal Opportunity Difference",
      "description": "A fairness metric that quantifies discrimination by measuring the difference in true positive rates (recall) between protected and privileged groups. Based on Hardt et al.'s equality of opportunity framework, this metric computes the maximum difference in TPR across demographic groups, with a value of 0 indicating perfect fairness. The technique provides a mathematical measure of whether qualified individuals from different groups have equal chances of receiving positive predictions.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/low",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/testing",
        "lifecycle-stage/system-deployment-and-use/monitoring",
        "technique-type/metric"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating a loan approval model to ensure that qualified applicants from different ethnic backgrounds have equal approval rates, measuring whether a 90% TPR for qualified white applicants is matched by similar rates for qualified minority applicants.",
          "goal": "Fairness"
        },
        {
          "description": "Auditing a medical diagnosis system to verify that patients with a particular condition are detected at equal rates across age groups, providing transparent evidence that diagnostic accuracy is consistent regardless of patient demographics.",
          "goal": "Transparency"
        },
        {
          "description": "Monitoring a university admissions algorithm in production to ensure that qualified students from different socioeconomic backgrounds have equal acceptance rates, validating the reliability of the fairness properties over time.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Only considers true positive rates and ignores false positive rate disparities, potentially allowing discrimination in the form of unequal false alarm rates between groups."
        },
        {
          "description": "Requires accurate ground truth labels for the positive class, which may be biased or unavailable in some domains."
        },
        {
          "description": "Improving TPR for one group might increase FPR for that group, creating trade-offs between different types of fairness."
        },
        {
          "description": "Does not account for intersectional fairness across multiple protected attributes simultaneously."
        },
        {
          "description": "May not capture fairness concerns for the negative class or individuals who should not receive positive predictions."
        }
      ],
      "resources": [
        {
          "title": "aif360.sklearn.metrics.equal_opportunity_difference — aif360 0.6.1 ...",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.metrics.equal_opportunity_difference.html",
          "source_type": "documentation"
        },
        {
          "title": "Welcome to TransparentAI's documentation! — TransparentAI 0.1.0 ...",
          "url": "https://transparentai.readthedocs.io/en/latest/",
          "source_type": "documentation"
        },
        {
          "title": "lale.lib.aif360.util module — LALE 0.9.0-dev documentation",
          "url": "https://lale.readthedocs.io/en/latest/modules/lale.lib.aif360.util.html",
          "source_type": "documentation"
        },
        {
          "title": "IBM/bias-mitigation-of-machine-learning-models-using-aif360",
          "url": "https://github.com/IBM/bias-mitigation-of-machine-learning-models-using-aif360",
          "source_type": "software_package"
        },
        {
          "title": "aif360.algorithms.postprocessing.RejectOptionClassification ...",
          "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "demographic-parity-assessment",
        "average-odds-difference"
      ]
    },
    {
      "slug": "average-odds-difference",
      "name": "Average Odds Difference",
      "description": "Average Odds Difference measures fairness by calculating the average difference in both false positive rates and true positive rates between different demographic groups. This metric captures how consistently a model performs across groups for both positive and negative predictions. A value of 0 indicates perfect fairness under the equalized odds criterion, while larger absolute values indicate greater disparities in model performance between groups.",
      "assurance_goals": [
        "Fairness",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/low",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "technique-type/metric"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating criminal risk assessment tools to ensure equal false positive rates (wrongly flagging low-risk individuals as high-risk) and true positive rates (correctly identifying high-risk individuals) across racial and ethnic groups.",
          "goal": "Fairness"
        },
        {
          "description": "Auditing hiring algorithms to verify that both the rate of correctly identifying qualified candidates and the rate of incorrectly rejecting qualified candidates remain consistent across gender and demographic groups.",
          "goal": "Fairness"
        },
        {
          "description": "Monitoring loan approval systems to ensure reliable performance by checking that both approval rates for creditworthy applicants and rejection rates for non-creditworthy applicants are consistent across protected demographic categories.",
          "goal": "Reliability"
        },
        {
          "description": "Testing medical diagnostic models to validate that diagnostic accuracy (both correctly identifying disease and correctly ruling out disease) remains consistent across patient demographics, ensuring reliable healthcare delivery.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Averaging effect can mask important disparities when false positive and true positive rate differences compensate for each other, potentially hiding significant bias in one direction."
        },
        {
          "description": "Requires access to ground truth labels and sensitive attribute information, which may not be available in all deployment scenarios or may be subject to privacy constraints."
        },
        {
          "description": "Does not account for base rate differences between groups, meaning equal error rates may not translate to equal treatment when group prevalences differ significantly."
        },
        {
          "description": "Focuses solely on prediction accuracy disparities without considering whether the underlying decision-making process or feature selection introduces systematic bias against certain groups."
        },
        {
          "description": "May encourage optimization for fairness metrics at the expense of overall model performance, potentially reducing utility for the primary prediction task."
        }
      ],
      "resources": [
        {
          "title": "Equality of Opportunity in Supervised Learning",
          "url": "https://arxiv.org/abs/1610.02413",
          "source_type": "technical_paper",
          "description": "Foundational paper introducing equalized odds and related fairness metrics including average odds difference",
          "authors": [
            "Hardt, Moritz",
            "Price, Eric",
            "Srebro, Nathan"
          ],
          "publication_date": "2016-10-07"
        },
        {
          "title": "FairBalance: How to Achieve Equalized Odds With Data Pre-processing",
          "url": "https://arxiv.org/abs/2107.08310",
          "source_type": "technical_paper",
          "description": "Research on achieving equalized odds through data preprocessing techniques with practical implementation guidance",
          "authors": [
            "Yu, Zhe",
            "Chakraborty, Joymallya",
            "Menzies, Tim"
          ],
          "publication_date": "2021-07-17"
        },
        {
          "title": "AIF360: Average Odds Difference Documentation",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.metrics.average_odds_difference.html",
          "source_type": "documentation",
          "description": "IBM AIF360 toolkit implementation and documentation for computing average odds difference metrics"
        },
        {
          "title": "Fairlearn: A toolkit for assessing and improving fairness in machine learning",
          "url": "https://github.com/fairlearn/fairlearn",
          "source_type": "software_package",
          "description": "Microsoft's comprehensive fairness toolkit with implementations of various fairness metrics including average odds difference"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "demographic-parity-assessment",
        "equal-opportunity-difference"
      ]
    },
    {
      "slug": "path-specific-counterfactual-fairness-assessment",
      "name": "Path-Specific Counterfactual Fairness Assessment",
      "description": "A causal fairness evaluation technique that assesses algorithmic discrimination by examining specific causal pathways in a model's decision-making process. Unlike general counterfactual fairness, this approach enables practitioners to identify and intervene on particular causal paths that may introduce bias whilst preserving other legitimate pathways. The method uses causal graphs to distinguish between direct discrimination (through protected attributes) and indirect discrimination (through seemingly neutral factors that correlate with protected attributes), allowing for more nuanced fairness assessments in complex causal settings.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/causal",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/causal-graph",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/causal-analysis",
        "expertise-needed/causal-inference",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "fairness-approach/causal",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "technique-type/metric"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating hiring algorithms by identifying which causal pathways from education and experience legitimately affect job performance versus those that introduce gender or racial bias, enabling targeted interventions that preserve merit-based selection whilst eliminating discriminatory pathways.",
          "goal": "Fairness"
        },
        {
          "description": "Analysing loan approval models to provide transparent evidence of which factors legitimately influence creditworthiness versus those that create indirect discrimination, enabling clear explanations to regulators about causal mechanisms underlying fair lending decisions.",
          "goal": "Transparency"
        },
        {
          "description": "Assessing medical diagnosis systems to ensure reliable performance by distinguishing between clinically relevant causal pathways (symptoms to diagnosis) and potentially biased pathways (demographics to diagnosis), maintaining diagnostic accuracy whilst preventing healthcare disparities.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Requires identifying which causal pathways are 'allowable' and which are not—a subjective decision; analyzing specific paths adds complexity to the causal model and the fairness criterion."
        }
      ],
      "resources": [
        {
          "title": "Path-Specific Counterfactual Fairness via Dividend Correction",
          "url": "https://www.semanticscholar.org/paper/197367ee337e8838fd2ef1a887101ddc84eb0612",
          "source_type": "technical_paper",
          "authors": [
            "Daisuke Hatano",
            "Satoshi Hara",
            "Hiromi Arai"
          ]
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "counterfactual-fairness-assessment"
      ]
    },
    {
      "slug": "bayesian-fairness-regularization",
      "name": "Bayesian Fairness Regularization",
      "description": "Bayesian Fairness Regularization incorporates fairness constraints into machine learning models through Bayesian methods, treating fairness as a prior distribution or regularization term. This approach includes techniques like Fair Bayesian Optimization that use constrained optimization to tune model hyperparameters whilst enforcing fairness constraints, and methods that add regularization terms to objective functions to penalize discriminatory predictions. The technique allows for probabilistic interpretation of fairness constraints and can account for uncertainty in both model parameters and fairness requirements.",
      "assurance_goals": [
        "Fairness",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Using Fair Bayesian Optimization to tune hyperparameters of credit risk models, automatically balancing predictive accuracy with fairness constraints across different demographic groups whilst accounting for uncertainty in both model performance and fairness requirements.",
          "goal": "Fairness"
        },
        {
          "description": "Implementing Bayesian neural networks with fairness-aware priors for hiring recommendation systems, where uncertainty in fairness constraints is modeled probabilistically to ensure robust fair decision-making across different candidate populations.",
          "goal": "Fairness"
        },
        {
          "description": "Applying Bayesian regularization techniques to medical diagnosis models to ensure reliable performance across patient demographics, using probabilistic constraints to maintain consistent diagnostic accuracy whilst preventing algorithmic bias in healthcare delivery.",
          "goal": "Reliability"
        },
        {
          "description": "Developing insurance premium calculation models using Bayesian fairness regularization to ensure actuarially sound pricing that meets regulatory fairness requirements, with probabilistic modeling of both risk assessment accuracy and demographic equity.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Prior selection challenges make it difficult to specify appropriate prior distributions for fairness constraints, requiring domain expertise and potentially leading to suboptimal or biased outcomes if priors are poorly chosen."
        },
        {
          "description": "Computational complexity increases significantly due to Bayesian inference requirements, including sampling methods, variational inference, or optimization over probability distributions, making the approach less scalable for large datasets."
        },
        {
          "description": "Sensitivity to hyperparameters affects both the Bayesian inference process and fairness regularization terms, requiring careful tuning of multiple parameters that control the trade-off between accuracy, fairness, and computational efficiency."
        },
        {
          "description": "Convergence and stability issues may arise in Bayesian optimization with fairness constraints, particularly when fairness objectives conflict with performance objectives or when the constraint space becomes highly complex."
        },
        {
          "description": "Limited theoretical understanding exists for the interaction between Bayesian uncertainty quantification and fairness constraints, making it challenging to provide guarantees about both predictive performance and fairness under uncertainty."
        }
      ],
      "resources": [
        {
          "title": "Bayesian fairness",
          "url": "https://arxiv.org/abs/1706.00119",
          "source_type": "technical_paper",
          "description": "Foundational paper introducing Bayesian approaches to fairness under parameter uncertainty, demonstrating how Bayesian perspectives lead to fair decision rules",
          "authors": [
            "Dimitrakakis, Christos",
            "Liu, Yang",
            "Parkes, David",
            "Radanovic, Goran"
          ],
          "publication_date": "2017-05-31"
        },
        {
          "title": "Fair Bayesian Optimization",
          "url": "https://arxiv.org/abs/2006.05109",
          "source_type": "technical_paper",
          "description": "Constrained Bayesian optimization framework for optimizing ML model performance while enforcing fairness constraints through hyperparameter tuning",
          "authors": [
            "Perrone, Valerio",
            "Donini, Michele",
            "Zafar, Muhammad Bilal",
            "Schmucker, Robin",
            "Kenthapadi, Krishnaram",
            "Archambeau, Cédric"
          ],
          "publication_date": "2020-06-09"
        },
        {
          "title": "Fair Gaussian Processes",
          "url": "https://github.com/ztanml/fgp",
          "source_type": "software_package",
          "description": "MATLAB implementation of Fair Gaussian Processes with multiple fairness criteria support including statistical parity, equality of opportunity, and equalized odds"
        },
        {
          "title": "Fairness-Aware Classifier with Prejudice Remover Regularizer",
          "url": "https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3",
          "source_type": "technical_paper",
          "description": "Seminal paper introducing regularization-based approach to fairness in probabilistic discriminative models with mathematical framework for fairness constraints",
          "authors": [
            "Kamishima, Toshihiro",
            "Akaho, Shotaro",
            "Asoh, Hideki",
            "Sakuma, Jun"
          ],
          "publication_date": "2012-09-24"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "sensitivity-analysis-for-fairness",
        "fairness-gan",
        "attribute-removal-fairness-through-unawareness"
      ]
    }
  ],
  "count": 26
}