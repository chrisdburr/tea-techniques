id,name,description,model_dependency,assurance_goals,categories,subcategories,attributes,example_use_cases,limitations,resources,complexity_rating,computational_cost_rating
1,SHAP (SHapley Additive exPlanations),"Assigns importance values to each feature by computing their contribution to individual predictions, based on Shapley values from cooperative game theory.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Global""}, {""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Explaining individual predictions in complex models like neural networks or ensemble models."", ""goal"": ""Explainability""}]","[{""description"": ""Computationally expensive for large models or many features; assumptions like feature independence can affect accuracy of explanations.""}]","[{""type"": ""Paper"", ""title"": ""A Unified Approach to Interpreting Model Predictions (NIPS 2017)"", ""url"": ""https://arxiv.org/abs/1705.07874""}, {""type"": ""GitHub"", ""title"": ""SHAP Library"", ""url"": ""https://github.com/slundberg/shap""}]",3,3
2,Permutation Importance,Evaluates feature importance by measuring the decrease in model accuracy when a feature's values are randomly shuffled.,Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Assessing feature importance in models where coefficients are not available, such as tree-based models."", ""goal"": ""Explainability""}]","[{""description"": ""Can be misleading when features are correlated, since importance may be shared; requires retraining or multiple model evaluations per feature.""}]","[{""type"": ""Documentation"", ""title"": ""Permutation Feature Importance - Scikit-learn Guide"", ""url"": ""https://scikit-learn.org/stable/modules/permutation_importance.html""}]",2,2
3,Mean Decrease Impurity (MDI),Calculates feature importance in tree-based models by measuring how much each feature decreases impurity across all trees.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Determining important features in Random Forest classification tasks."", ""goal"": ""Explainability""}]","[{""description"": ""Biased towards features with more splits or categories; only applicable to tree-based models and can overestimate importance of high-cardinality features.""}]","[{""type"": ""Documentation"", ""title"": ""Feature Importance Measures in Random Forests (Scikit-learn)"", ""url"": ""https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html""}]",2,1
4,Gini Importance,Measures the total reduction of Gini impurity brought by a feature across all nodes and trees in decision trees and Random Forests.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Selecting important features when building tree-based classification models."", ""goal"": ""Explainability""}]","[{""description"": ""Tends to inflate importance for features with many values; like MDI, it is specific to tree models and not reliable for comparing across different model types.""}]","[{""type"": ""Documentation"", ""title"": ""Feature Importance Measures in Random Forests (Scikit-learn)"", ""url"": ""https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html""}]",2,1
5,Coefficient Magnitudes (in Linear Models),"Examines coefficients in linear models to gauge feature influence; larger absolute values imply stronger impact on the outcome (with coefficient sign indicating direction).",Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Interpreting which features influence housing price predictions in linear regression."", ""goal"": ""Explainability""}]","[{""description"": ""Only valid for linear relationships; can be affected by feature scaling and multicollinearity, and does not capture non-linear importance.""}]","[{""type"": ""Tutorial"", ""title"": ""Interpreting Regression Coefficients"", ""url"": ""https://www.theanalysisfactor.com/interpreting-regression-coefficients/""}]",1,1
6,Integrated Gradients,Attributes feature importance by integrating gradients of the model's output with respect to inputs along a path from a baseline to the actual input.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Understanding pixel contributions in image classification with deep neural networks."", ""goal"": ""Explainability""}]","[{""description"": ""Requires a meaningful baseline input; results can be sensitive to the choice of baseline and model must be differentiable, limiting use with non-differentiable components.""}]","[{""type"": ""Paper"", ""title"": ""Axiomatic Attribution for Deep Networks (Integrated Gradients, 2017)"", ""url"": ""https://arxiv.org/abs/1703.01365""}]",4,3
7,DeepLIFT,"Tracks changes in the output relative to a reference input, decomposing contributions from individual neurons to the final prediction in deep learning models.",Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Explaining why a neural network classifies an image as a specific object by tracing neuron activations."", ""goal"": ""Explainability""}]","[{""description"": ""Needs careful configuration for each network architecture; may produce inconsistent scores if multiple reference points are possible, and not all model types supported.""}]","[{""type"": ""Paper"", ""title"": ""Learning Important Features Through Propagating Activation Differences (DeepLIFT, 2017)"", ""url"": ""https://arxiv.org/abs/1704.02685""}]",4,3
8,Layer-wise Relevance Propagation (LRP),"Explains predictions by backpropagating relevance scores from the output layer to input features, distributing the prediction score layer by layer.",Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Visualising important regions in medical images for disease diagnosis using deep learning models."", ""goal"": ""Explainability""}]","[{""description"": ""Requires tailored rules for each layer type; results can sometimes be hard to interpret (negative relevances), and implementation is complex for new architectures.""}]","[{""type"": ""Paper"", ""title"": ""On Pixel-Wise Explanations by Layer-Wise Relevance Propagation (Bach et al., 2015)"", ""url"": ""https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140""}]",5,3
9,"Variable Importance in Random Forests (MDA, MDG)",Calculates feature importance by measuring the Mean Decrease Accuracy or Mean Decrease Gini when a feature is excluded from Random Forest models.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Identifying key predictors in a Random Forest model for credit scoring."", ""goal"": ""Explainability""}]","[{""description"": ""Permutation importance (MDA) shares limitations with correlated features; Gini importance (MDG) biases towards continuous or high-cardinality features.""}]","[{""type"": ""Documentation"", ""title"": ""Feature Importance Measures in Random Forests (Scikit-learn)"", ""url"": ""https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html""}]",2,2
10,Contextual Decomposition,Interprets neural networks by decomposing activations to explain predictions based on contributions of individual features or groups.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Explaining sentiment predictions in text by attributing scores to words or phrases."", ""goal"": ""Explainability""}]","[{""description"": ""Primarily designed for LSTMs; not widely implemented in standard libraries, requiring custom code, and may not scale well to very deep or different model types.""}]","[{""type"": ""Paper"", ""title"": ""Beyond Word Importance: Contextual Decomposition (Murdoch et al., 2018)"", ""url"": ""https://openreview.net/forum?id=rkRwGg-0Z""}, {""type"": ""GitHub"", ""title"": ""ContextualDecomposition Repository"", ""url"": ""https://github.com/jamie-murdoch/ContextualDecomposition""}]",5,3
11,Taylor Decomposition,Decomposes predictions into contributions from individual features using a Taylor series expansion of the model's prediction function.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Attributing feature contributions in complex models for specific predictions."", ""goal"": ""Explainability""}]","[{""description"": ""The approach is mathematically complex and can be computationally intensive; approximations may introduce error and it's mainly theoretical with limited tooling.""}]","[{""type"": ""Paper"", ""title"": ""Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition (Montavon et al., 2017)"", ""url"": ""https://arxiv.org/abs/1512.02479""}]",5,4
12,Sobol Indices,Quantifies the contribution of individual variables and their interactions to the output variance in sensitivity analysis.,Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Interaction Analysis""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Understanding parameter impacts in environmental modeling outputs."", ""goal"": ""Explainability""}]","[{""description"": ""Requires a large number of model evaluations for accurate estimation; assumes independent input distributions and may be difficult to apply to high-dimensional inputs.""}]","[{""type"": ""Library"", ""title"": ""SALib (Python Sensitivity Analysis Library)"", ""url"": ""https://github.com/SALib/SALib""}]",4,5
13,Feature Interaction Detection (H-statistic),Measures feature interaction by comparing joint contributions to the model with the sum of individual contributions.,Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Interaction Analysis""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Identifying significant interactions in healthcare predictive models."", ""goal"": ""Explainability""}]","[{""description"": ""Only captures pairwise interactions (or one feature vs all); relies on partial dependence which can be misleading if features are correlated or have complex interactions.""}]","[{""type"": ""Article"", ""title"": ""Interpretable ML Book – Friedman's H-Statistic"", ""url"": ""https://christophm.github.io/interpretable-ml-book/interaction.html""}]",4,4
14,LIME (Local Interpretable Model-Agnostic Explanations),Generates local surrogate models that approximate complex model behavior around a specific instance using interpretable models like linear models.,Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Model Approximation""}]","[{""category"": ""Model Approximation"", ""subcategory"": ""Local Surrogates""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Explaining why a customer was denied a loan by approximating the model's decision locally."", ""goal"": ""Explainability""}]","[{""description"": ""Explanations can vary with repeated runs due to randomness; the linear surrogate may not be faithful if the model behavior is highly non-linear in that locality.""}]","[{""type"": ""Paper"", ""title"": ""Why Should I Trust You? (Ribeiro et al., 2016)"", ""url"": ""https://dl.acm.org/doi/10.1145/2939672.2939778""}, {""type"": ""GitHub"", ""title"": ""LIME Library"", ""url"": ""https://github.com/marcotcr/lime""}]",2,3
15,Ridge Regression Surrogates,"Uses Ridge Regression as a surrogate to approximate global behavior of complex models, balancing simplicity and interpretability with regularization.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Model Approximation""}]","[{""category"": ""Model Approximation"", ""subcategory"": ""Global Surrogates""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Summarizing complex model behavior for regulatory reporting in finance."", ""goal"": ""Explainability""}]","[{""description"": ""The surrogate only approximates the original model, potentially losing important non-linear behavior; requires a representative dataset to train the surrogate model.""}]","[{""type"": ""Article"", ""title"": ""Global Surrogate Models for Explainability"", ""url"": ""https://christophm.github.io/interpretable-ml-book/global.html""}]",3,2
16,Partial Dependence Plots (PDP),"Visualizes the relationship between one or two features and the predicted outcome, averaging out effects of other features.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Feature Visualization""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Understanding how changes in age affect predicted disease risk in medical models."", ""goal"": ""Explainability""}]","[{""description"": ""Assumes features are independent of others (due to averaging); can be misleading when features are correlated, and only shows average effects, not instance-specific.""}]","[{""type"": ""Documentation"", ""title"": ""Partial Dependence Plot Tutorial (Scikit-learn)"", ""url"": ""https://scikit-learn.org/stable/modules/partial_dependence.html""}]",2,2
17,Accumulated Local Effects (ALE) Plots,"Similar to PDPs but account for feature interactions, providing accurate insights when features are correlated.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Feature Visualization""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Exploring the effect of house size on price predictions in real estate models with correlated features."", ""goal"": ""Explainability""}]","[{""description"": ""More complex to compute than PDP; still provides only average effects and can be harder to interpret for higher-order interactions beyond pairs.""}]","[{""type"": ""Article"", ""title"": ""ALE Plots – A Better PDP (Medium)"", ""url"": ""https://medium.com/@parrt/accumulated-local-effects-ale-plots-better-than-pdp-72a72fc88749""}]",3,2
18,Individual Conditional Expectation (ICE) Plots,"Shows how a feature affects predictions for individual instances, highlighting heterogeneous effects across data points.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Feature Visualization""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Visualizing how customers' predicted spending changes with income in consumer behavior models."", ""goal"": ""Explainability""}]","[{""description"": ""Plots can become cluttered with many instances; does not inherently summarize the overall effect without visual inspection, and still assumes fixed other features.""}]","[{""type"": ""Article"", ""title"": ""Interpreting ICE Plots"", ""url"": ""https://christophm.github.io/interpretable-ml-book/ice.html""}]",2,2
19,Saliency Maps,Highlights important pixels in input images that most influence the output prediction in computer vision models.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Model Behavior Visualization""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Identifying regions contributing to tumor diagnosis in medical images."", ""goal"": ""Explainability""}]","[{""description"": ""Often noisy and sensitive to small perturbations; highlights may not correspond to human-understandable features, and they only indicate local gradient, not causal importance.""}]","[{""type"": ""Paper"", ""title"": ""Deep Inside Convolutional Networks (Simonyan et al., 2014)"", ""url"": ""https://arxiv.org/abs/1312.6034""}]",3,2
20,Grad-CAM (Gradient-weighted Class Activation Mapping),Uses gradients to produce heatmaps highlighting image regions that contribute most to the model's output.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Model Behavior Visualization""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Visualizing parts of an image leading to a 'dog' classification in image recognition models."", ""goal"": ""Explainability""}]","[{""description"": ""Requires access to internal feature maps; resolution is limited to coarse feature map size, and it is specific to CNN-based vision models with recognizable layers.""}]","[{""type"": ""Paper"", ""title"": ""Grad-CAM: Visual Explanations (Selvaraju et al., 2017)"", ""url"": ""https://arxiv.org/abs/1610.02391""}]",3,2
21,Occlusion Sensitivity,Measures prediction changes by systematically occluding parts of the input to identify important regions or features.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Model Behavior Visualization""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Understanding which words affect sentiment prediction by masking them in NLP models."", ""goal"": ""Explainability""}]","[{""description"": ""Computationally expensive if many parts need to be occluded; choice of occlusion size can bias results, and it may not capture interactions if multiple parts jointly matter.""}]","[{""type"": ""Paper"", ""title"": ""Visualizing and Understanding CNNs (Zeiler & Fergus, 2014)"", ""url"": ""https://arxiv.org/abs/1311.2901""}]",2,4
22,Attention Mechanisms in Neural Networks,"Visualizes attention weights in models like transformers, highlighting important input parts for predictions.",Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Model Behavior Visualization""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Analyzing which words a transformer model focuses on during machine translation tasks."", ""goal"": ""Explainability""}]","[{""description"": ""Attention weights are not always strongly correlated with importance; focusing solely on attention can be misleading ('attention is not explanation' debate) and only applies to models with attention layers.""}]","[{""type"": ""Article"", ""title"": ""Attention is not Explanation (Serrano & Smith, 2019)"", ""url"": ""https://arxiv.org/abs/1902.10186""}]",2,1
23,Factor Analysis,"Reduces dimensionality by identifying latent factors explaining observed variability, aiding in data interpretation.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Model Simplification""}]","[{""category"": ""Model Simplification"", ""subcategory"": ""Dimensionality Reduction""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Discovering underlying factors in psychological survey data for social science research."", ""goal"": ""Explainability""}]","[{""description"": ""Assumes linear relationships and normality; results (factors) can be abstract and not directly interpretable, and requires deciding on number of factors and rotation method.""}]","[{""type"": ""Book"", ""title"": ""Factor Analysis Explanation (Statistics Textbook)"", ""url"": ""https://www.statsoft.com/Textbook/Factor-Analysis""}]",3,3
24,Principal Component Analysis (PCA),"Reduces dimensionality by projecting data onto directions of maximum variance, simplifying data while retaining important information.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Dimensionality Reduction Visualization""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Visualizing high-dimensional gene expression data in bioinformatics."", ""goal"": ""Explainability""}]","[{""description"": ""Principal components are linear combinations that may not correspond to clear real-world concepts; only captures linear variance and can be affected by scaling of features.""}]","[{""type"": ""Documentation"", ""title"": ""PCA - Sklearn User Guide"", ""url"": ""https://scikit-learn.org/stable/modules/decomposition.html#pca""}]",2,2
25,t-SNE,"A non-linear technique that visualizes high-dimensional data in 2D or 3D, preserving local relationships between points.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Dimensionality Reduction Visualization""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Visualizing clusters in high-dimensional word embeddings."", ""goal"": ""Explainability""}]","[{""description"": ""Results can vary between runs; may distort global structure in favor of local clustering and requires tuning (perplexity, iterations) to avoid misleading patterns.""}]","[{""type"": ""Article"", ""title"": ""How to Use t-SNE Effectively (Distill.pub)"", ""url"": ""https://distill.pub/2016/misread-tsne""}]",3,3
26,UMAP,A non-linear technique similar to t-SNE but better at preserving global data structure.,Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Dimensionality Reduction Visualization""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Visualizing patterns in user behavior data for marketing analysis."", ""goal"": ""Explainability""}]","[{""description"": ""Choice of parameters (neighbors, min distance) affects outcome; like t-SNE, it can sometimes be difficult to interpret distances in the reduced space in terms of original features.""}]","[{""type"": ""Documentation"", ""title"": ""UMAP How-To Guide"", ""url"": ""https://umap-learn.readthedocs.io/en/latest/""}]",3,3
27,Prototype and Criticism Models,Identifies representative (prototypes) and non-representative (criticisms) examples to summarize and highlight model behavior.,Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Example-Based Methods""}]","[{""category"": ""Example-Based Methods"", ""subcategory"": ""Prototype and Criticism Methods""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Selecting representative customer profiles for targeted marketing."", ""goal"": ""Explainability""}]","[{""description"": ""Determining prototypes/criticisms can be computationally complex for large datasets; results depend on the metric chosen and might not capture all important aspects of data variability.""}]","[{""type"": ""Paper"", ""title"": ""MMD-Critic: Prototypes and Criticisms (Kim et al., 2016)"", ""url"": ""https://papers.nips.cc/paper/2016/file/5680522b8e2bb01943234bceabde35a1-Paper.pdf""}]",4,3
28,Influence Functions,Measures the impact of training examples on model predictions to identify influential data points.,Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Example-Based Methods""}]","[{""category"": ""Example-Based Methods"", ""subcategory"": ""Prototype and Criticism Methods""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Debugging model predictions by identifying influential training data points."", ""goal"": ""Explainability""}]","[{""description"": ""Requires access to and differentiation through the training process; can be intractable for large models due to needing Hessian computations, and may not be accurate if model is highly non-convex.""}]","[{""type"": ""Paper"", ""title"": ""Understanding Black-box Predictions via Influence Functions (Koh & Liang, 2017)"", ""url"": ""https://arxiv.org/abs/1703.04730""}]",5,5
29,Contrastive Explanation Method (CEM),"Generates explanations by identifying minimal input changes that result in different outcomes, offering counterfactual reasoning.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Example-Based Methods""}]","[{""category"": ""Example-Based Methods"", ""subcategory"": ""Counterfactual Explanations""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Explaining loan rejections by showing what changes would lead to approval."", ""goal"": ""Explainability""}]","[{""description"": ""Solving for pertinent positives/negatives requires iterative optimization per instance; results can be sensitive to parameter settings and might yield unrealistic contrastive inputs if constraints are not tight.""}]","[{""type"": ""Paper"", ""title"": ""Towards Contrastive Explanations with Pertinent Negatives (Dhurandhar et al., 2018)"", ""url"": ""https://papers.nips.cc/paper/2018/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf""}]",5,4
30,"Bayesian Networks (e.g., bnlearn)",Probabilistic graphical models representing variables and their conditional dependencies for causal reasoning.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Example-Based Methods""}]","[{""category"": ""Example-Based Methods"", ""subcategory"": ""Causal Analysis""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Modeling causal relationships in gene regulatory networks."", ""goal"": ""Explainability""}]","[{""description"": ""Learning the network structure from data is NP-hard for many variables; causal conclusions require correct model specification and may be invalid if important variables are omitted.""}]","[{""type"": ""Library"", ""title"": ""bnlearn Package (Bayesian Networks in R)"", ""url"": ""https://cran.r-project.org/web/packages/bnlearn/index.html""}]",4,4
31,ANCHOR,"Provides high-precision if-then rules for specific predictions, explaining which features are responsible for the decision.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Rule Extraction""}]","[{""category"": ""Rule Extraction"", ""subcategory"": ""Decision Rules""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Generating rules to explain individual predictions in text classification."", ""goal"": ""Explainability""}]","[{""description"": ""Only provides explanations for individual predictions (local rules); may struggle with continuous features (usually requires discretization) and might not find an anchor if conditions are too strict.""}]","[{""type"": ""Paper"", ""title"": ""Anchors: High-Precision Model-Agnostic Explanations (Ribeiro et al., 2018)"", ""url"": ""https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982/16015""}]",3,3
32,RuleFit,Combines decision rules with linear models to provide interpretable models capturing non-linear patterns.,Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Rule Extraction""}]","[{""category"": ""Rule Extraction"", ""subcategory"": ""Decision Rules""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Building interpretable models for predicting customer churn with rule-based explanations."", ""goal"": ""Explainability""}]","[{""description"": ""The resulting model can still have many rules, complicating interpretability; performance may lag behind black-box models if too few rules are allowed for simplicity.""}]","[{""type"": ""Paper"", ""title"": ""RuleFit: An Interpretable Model (Friedman & Popescu, 2008)"", ""url"": ""http://statweb.stanford.edu/~jhf/ftp/RuleFit.pdf""}]",3,3
33,Monte Carlo Dropout,Uses dropout at inference time in deep learning models to estimate uncertainty by approximating Bayesian inference.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Uncertainty and Reliability""}]","[{""category"": ""Uncertainty and Reliability"", ""subcategory"": ""Confidence Estimation""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Estimating prediction uncertainty in medical diagnosis models."", ""goal"": ""Explainability""}]","[{""description"": ""Only captures model uncertainty, not data uncertainty; requires multiple forward passes and results depend on dropout rate, which must be same as training to be meaningful.""}]","[{""type"": ""Paper"", ""title"": ""Dropout as a Bayesian Approximation (Gal & Ghahramani, 2016)"", ""url"": ""https://arxiv.org/abs/1506.02142""}]",3,3
34,ODIN (Out-of-DIstribution detector for Neural networks),Detects out-of-distribution samples in neural networks by applying temperature scaling and input perturbations.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Uncertainty and Reliability""}]","[{""category"": ""Uncertainty and Reliability"", ""subcategory"": ""Out-of-Distribution Detection""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Identifying when an image classifier encounters novel inputs."", ""goal"": ""Explainability""}]","[{""description"": ""Needs tuning of temperature and noise magnitude via a calibration set; method effectiveness can vary across datasets and it adds a small overhead to inference.""}]","[{""type"": ""Paper"", ""title"": ""ODIN: Out-of-Distribution Detector (Liang et al., 2018)"", ""url"": ""https://arxiv.org/abs/1706.02690""}]",4,2
35,Permutation Tests,Estimates uncertainty by permuting data labels and calculating test statistics to create a null distribution in non-parametric methods.,Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Uncertainty and Reliability""}]","[{""category"": ""Uncertainty and Reliability"", ""subcategory"": ""Uncertainty Quantification""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Assessing the significance of model predictions in hypothesis testing."", ""goal"": ""Explainability""}]","[{""description"": ""Requires many permutations for reliable p-values, especially for small effects; does not pinpoint cause of model issues, only provides statistical significance of performance.""}]","[{""type"": ""Article"", ""title"": ""Permutation Tests in ML (Blog)"", ""url"": ""https://towardsdatascience.com/understanding-permutation-tests-391aff0aff7d""}]",3,4
36,"Fairness Metrics (e.g., Equalized Odds, Demographic Parity)",Evaluates models for fairness by measuring disparities in predictions across different demographic groups.,Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Fairness Explanations""}]","[{""category"": ""Fairness Explanations"", ""subcategory"": ""Bias Detection and Mitigation""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Ensuring a hiring model does not discriminate based on gender or ethnicity."", ""goal"": ""Explainability""}]","[{""description"": ""Each metric addresses a specific notion of fairness and may conflict with others; they require true outcome labels for evaluation and do not directly tell how to fix bias.""}]","[{""type"": ""Survey"", ""title"": ""Fairness Definitions Explained (Verma & Rubin, 2018)"", ""url"": ""https://arxiv.org/abs/1802.04896""}]",2,1
37,Model Pruning,"Simplifies neural networks by removing less important weights or neurons, reducing complexity while retaining performance.",Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Model Simplification""}]","[{""category"": ""Model Simplification"", ""subcategory"": ""Model Pruning""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Reducing model size for deployment on mobile devices without significant loss in accuracy."", ""goal"": ""Explainability""}]","[{""description"": ""Over-pruning can significantly reduce accuracy; finding the right pruning threshold is trial-and-error, and pruned models may still be complex to interpret if remaining structure is not simple.""}]","[{""type"": ""Paper"", ""title"": ""Learning to Prune Deep Neural Networks (Han et al., 2015)"", ""url"": ""https://arxiv.org/abs/1506.02626""}]",3,2
38,Knowledge Distillation,"Trains a simpler 'student' model to replicate the behavior of a complex 'teacher' model, resulting in a more interpretable model.",Model-Agnostic,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Model Simplification""}]","[{""category"": ""Model Simplification"", ""subcategory"": ""Model Distillation""}]","[{""type"": ""Scope"", ""value"": ""Global""}]","[{""description"": ""Simplifying a deep neural network for faster inference in real-time applications."", ""goal"": ""Explainability""}]","[{""description"": ""Student model performance depends on teacher quality and training technique; distilled model might still be a black box (though simpler) and requires additional training data or time.""}]","[{""type"": ""Paper"", ""title"": ""Distilling the Knowledge in a Neural Network (Hinton et al., 2015)"", ""url"": ""https://arxiv.org/abs/1503.02531""}]",3,4
39,Attention Visualisation in Transformers,Visualizes attention weights in transformer-based models to show how the model focuses on different input parts during prediction.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Visualization Techniques""}]","[{""category"": ""Visualization Techniques"", ""subcategory"": ""Model Behavior Visualization""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Understanding which words a transformer model focuses on during machine translation tasks."", ""goal"": ""Explainability""}]","[{""description"": ""Similar to other attention interpretations: not always clear if high attention means importance; only applicable to transformer-based models and doesn't explain the model's reasoning beyond attention weights.""}]","[{""type"": ""Tool"", ""title"": ""BERTViz (Transformer Attention Visualizer)"", ""url"": ""https://github.com/jessevig/bertviz""}]",2,1
40,Neuron Activation Analysis,Analyzes activation patterns of neurons in large language models (LLMs) to interpret their roles and the concepts they represent.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Global""}, {""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Identifying neurons responsible for syntax or semantics in language models."", ""goal"": ""Explainability""}]","[{""description"": ""Interpreting individual neurons requires analyzing large numbers of activations; insights are often qualitative, and important behavior may be distributed across many neurons rather than single ones.""}]","[{""type"": ""Paper"", ""title"": ""Interpretability in the Wild: Neural Network Foci (Bau et al., 2020)"", ""url"": ""https://arxiv.org/abs/2007.09710""}, {""type"": ""Blog"", ""title"": ""OpenAI: Circuits & Neurons Analysis"", ""url"": ""https://openai.com/blog/automated-interpretability/""}]",5,4
41,Prompt Sensitivity Analysis,Studies how variations in input prompts affect LLM outputs to understand model behavior and sensitivity.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Example-Based Methods""}]","[{""category"": ""Example-Based Methods"", ""subcategory"": ""Prototype and Criticism Methods""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Evaluating how different phrasings influence an LLM's answers in question-answering tasks."", ""goal"": ""Explainability""}]","[{""description"": ""Only surfaces sensitivity to tested prompt variations; may not cover all aspects of model behavior, and systematic prompt generation can be time-consuming or incomplete.""}]","[{""type"": ""Paper"", ""title"": ""Prompt Sensitivity Index (Zhang et al., 2022)"", ""url"": ""https://aclanthology.org/2022.findings-emnlp.167.pdf""}]",3,3
42,Causal Mediation Analysis in Language Models,Investigates causal relationships within LLMs by assessing how interventions on specific components affect outputs.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Example-Based Methods""}]","[{""category"": ""Example-Based Methods"", ""subcategory"": ""Causal Analysis""}]","[{""type"": ""Scope"", ""value"": ""Global""}, {""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Understanding how adjusting embeddings changes model responses in language generation tasks."", ""goal"": ""Explainability""}]","[{""description"": ""Requires defining interventions on internal components, which needs expert knowledge of model architecture; results depend on correctness of causal assumptions and can be challenging to interpret conclusively.""}]","[{""type"": ""Paper"", ""title"": ""Investigating Gender Bias in LMs using Causal Mediation (Vig et al., 2020)"", ""url"": ""https://arxiv.org/abs/2004.12265""}]",5,4
43,Feature Attribution with Integrated Gradients in NLP,"Applies Integrated Gradients to attribute importance of input tokens in LLMs for specific predictions, often producing visualizations.",Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Identifying words influencing text sentiment classification or topic modeling."", ""goal"": ""Explainability""}]","[{""description"": ""Long text inputs mean integrating over many steps which is slow; attributions can be diffuse across many tokens, and choosing a neutral baseline (e.g., empty or padding text) is non-trivial.""}]","[{""type"": ""Documentation"", ""title"": ""Captum Integrated Gradients (Text Example)"", ""url"": ""https://captum.ai/tutorials/IMDB_TorchText_Interpret""}]",4,3
44,Concept Activation Vectors (CAVs),Represents human-understandable concepts as vectors in the model's latent space to analyze their influence on predictions.,Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Feature Analysis""}]","[{""category"": ""Feature Analysis"", ""subcategory"": ""Importance and Attribution""}]","[{""type"": ""Scope"", ""value"": ""Global""}, {""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Assessing how concepts like ""negativity"" affect language model outputs."", ""goal"": ""Explainability""}]","[{""description"": ""Relies on having clearly defined concept examples; concept directions might not exist clearly in the model's internal space, and one must choose which layer to examine, affecting results.""}]","[{""type"": ""Paper"", ""title"": ""TCAV: Concept Activation Vectors (Kim et al., 2018)"", ""url"": ""https://arxiv.org/abs/1711.11279""}]",4,3
45,In-Context Learning Analysis,"Examines how LLMs learn from examples provided in the input prompt, revealing capacity for few-shot learning.",Model-Specific,Explainability,"[{""goal"": ""Explainability"", ""category"": ""Example-Based Methods""}]","[{""category"": ""Example-Based Methods"", ""subcategory"": ""Prototype and Criticism Methods""}]","[{""type"": ""Scope"", ""value"": ""Local""}]","[{""description"": ""Analyzing the effect of examples on an LLM's ability to perform a new task like translation."", ""goal"": ""Explainability""}]","[{""description"": ""Observational analysis without guaranteed causal insight; any findings can be specific to the tested tasks or prompts, making general conclusions about model behavior difficult.""}]","[{""type"": ""Paper"", ""title"": ""Investigating Few-Shot Learning in GPT-3 (Zhao et al., 2021)"", ""url"": ""https://arxiv.org/abs/2108.13888""}]",4,2
46,Reweighing,Assigns weights to instances in the training data to ensure different groups are equally represented in all labels.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Pre-Processing Techniques""}]","[{""category"": ""Pre-Processing Techniques"", ""subcategory"": ""Data Transformation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Preprocessing and Feature Engineering""}]","[{""description"": ""Balancing gender representation in credit approval datasets before training a classifier."", ""goal"": ""Fairness""}]","[{""description"": ""Does not remove bias present in features; simply changing weights can lead to higher variance if some groups are underrepresented, and it assumes labels are unbiased which might not hold.""}]","[{""type"": ""Paper"", ""title"": ""Data Preprocessing Techniques for Classification Without Discrimination (Kamiran & Calders, 2012)"", ""url"": ""https://link.springer.com/article/10.1007/s10115-010-0352-x""}, {""type"": ""Library"", ""title"": ""AIF360 Reweighing Documentation"", ""url"": ""https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html""}]",2,1
47,Disparate Impact Remover,"Edits feature values to reduce dependence between features and protected attributes, aiming to mitigate disparate impact.",Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Pre-Processing Techniques""}]","[{""category"": ""Pre-Processing Techniques"", ""subcategory"": ""Data Transformation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Preprocessing and Feature Engineering""}]","[{""description"": ""Adjusting salary features to reduce gender bias in income prediction models."", ""goal"": ""Fairness""}]","[{""description"": ""Altering features could reduce model accuracy if important information is removed; addresses only measured attributes and might not eliminate bias through proxies.""}]","[{""type"": ""Paper"", ""title"": ""Certifying and Removing Disparate Impact (Feldman et al., 2015)"", ""url"": ""https://arxiv.org/abs/1412.3756""}, {""type"": ""Library"", ""title"": ""AIF360 DisparateImpactRemover Documentation"", ""url"": ""https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.DisparateImpactRemover.html""}]",3,2
48,Learning Fair Representations,Learns latent representations that encode data well but obfuscate information about protected attributes.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Fair Representation Learning""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Preprocessing and Feature Engineering""}]","[{""description"": ""Creating unbiased data representations for hiring algorithms."", ""goal"": ""Fairness""}]","[{""description"": ""Requires training a complex model (encoder) with adversarial or constrained objectives; balancing reconstruction and fairness is tricky and may lead to loss of useful information.""}]","[{""type"": ""Paper"", ""title"": ""Learning Fair Representations (Zemel et al., 2013)"", ""url"": ""https://proceedings.mlr.press/v28/zemel13.html""}]",4,4
49,Fairness GAN,Employs Generative Adversarial Networks to generate fair representations of data that obfuscate protected attributes.,Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Fair Representation Learning""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Preprocessing and Feature Engineering""}]","[{""description"": ""Creating unbiased datasets for training fair image recognition models."", ""goal"": ""Fairness""}]","[{""description"": ""GAN training is notoriously difficult to stabilize; ensuring fairness might come at the cost of utility, and it needs a large dataset to train the generator and discriminator effectively.""}]","[{""type"": ""Paper"", ""title"": ""FairGAN: Fairness-aware Generative Adversarial Networks (Xu et al., 2018)"", ""url"": ""https://arxiv.org/abs/1805.11202""}]",5,5
50,Optimised Pre-Processing,Modifies training data features and labels to induce fairness while preserving data utility.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Pre-Processing Techniques""}]","[{""category"": ""Pre-Processing Techniques"", ""subcategory"": ""Data Transformation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Preprocessing and Feature Engineering""}]","[{""description"": ""Adjusting criminal justice data to reduce racial bias before training models."", ""goal"": ""Fairness""}]","[{""description"": ""May involve solving complex optimization problems; could distort data relationships in ways that degrade model performance if not carefully calibrated to preserve utility.""}]","[{""type"": ""Paper"", ""title"": ""Optimized Preprocessing for Discrimination Prevention (Calmon et al., 2017)"", ""url"": ""https://arxiv.org/abs/1704.03354""}]",4,4
51,Relabelling,"Changes labels of certain instances in training data to reduce bias, often based on fairness constraints.",Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Pre-Processing Techniques""}]","[{""category"": ""Pre-Processing Techniques"", ""subcategory"": ""Data Transformation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Preprocessing and Feature Engineering""}]","[{""description"": ""Modifying labels in loan default datasets to mitigate historical biases."", ""goal"": ""Fairness""}]","[{""description"": ""Altering labels risks introducing new biases or reducing prediction accuracy; deciding which instances to relabel can require a fairness criterion and ground truth fairness assessment.""}]","[{""type"": ""Paper"", ""title"": ""Classification with No Discrimination via Massaging (Kamiran et al., 2012)"", ""url"": ""https://ieeexplore.ieee.org/document/6233097""}]",3,2
52,Preferential Sampling,Re-samples data with preference for certain groups to achieve fair representation in training datasets.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Pre-Processing Techniques""}]","[{""category"": ""Pre-Processing Techniques"", ""subcategory"": ""Data Transformation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Preprocessing and Feature Engineering""}]","[{""description"": ""Oversampling minority groups in medical data to train unbiased models."", ""goal"": ""Fairness""}]","[{""description"": ""Over-sampling can cause overfitting to minority examples; under-sampling can drop important data from majority group, and it doesn't adjust the model if it inherently treats groups differently.""}]","[{""type"": ""Paper"", ""title"": ""Preferential Sampling for Fair Classification (Kamal et al., 2010)"", ""url"": ""https://link.springer.com/chapter/10.1007/978-3-642-15880-3_3""}]",2,2
53,Fairness Through Unawareness,"Ensures the model does not use protected attributes in decisions; however, indirect bias may persist.",Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Pre-Processing Techniques""}]","[{""category"": ""Pre-Processing Techniques"", ""subcategory"": ""Data Transformation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Preprocessing and Feature Engineering""}]","[{""description"": ""Removing gender as a feature in employee promotion predictions."", ""goal"": ""Fairness""}]","[{""description"": ""Ignoring protected attributes doesn't guarantee fairness; proxies for the protected attribute in other features can still lead to biased outcomes.""}]","[{""type"": ""Principle"", ""title"": ""Fairness Through Unawareness Principle (Dwork et al., 2012)"", ""url"": ""https://arxiv.org/abs/1104.3913""}]",1,1
54,Adversarial Debiasing,Trains a model to make accurate predictions while reducing the ability of an adversary to predict protected attributes from outputs.,Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Adversarial Debiasing""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Developing fair classification models for loan approvals by reducing gender predictability."", ""goal"": ""Fairness""}]","[{""description"": ""Training adversaries can be unstable; achieving a balance between accuracy and fairness is tricky, and the approach may reduce predictive performance on legitimate signals correlated with protected attributes.""}]","[{""type"": ""Paper"", ""title"": ""Mitigating Unwanted Biases with Adversarial Learning (Zhang et al., 2018)"", ""url"": ""https://arxiv.org/abs/1801.07593""}]",4,4
55,Adversarial Debiasing for Text,Applies adversarial debiasing techniques specifically to textual data to mitigate biases in language models.,Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Adversarial Debiasing""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Reducing gender bias in sentiment analysis models by adversarial training on text data."", ""goal"": ""Fairness""}]","[{""description"": ""Text data can carry subtle biases in language; adversarial removal of bias might strip out important linguistic context, and the technique inherits all challenges of adversarial training.""}]","[{""type"": ""Paper"", ""title"": ""Gender Bias in Text: Debiasing Word Embeddings with Adversaries (Zhao et al., 2018)"", ""url"": ""https://arxiv.org/abs/1807.11714""}]",4,4
56,Fair Adversarial Networks,Extends adversarial debiasing by incorporating fairness into deep learning via adversarial training.,Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Adversarial Debiasing""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Reducing bias in facial recognition systems with adversarial networks."", ""goal"": ""Fairness""}]","[{""description"": ""Extending adversarial debiasing to deep networks can be very complex to implement; requires careful tuning of loss trade-offs, and interpretations of fairness improvement can be opaque.""}]","[{""type"": ""Paper"", ""title"": ""Fairness in Deep Learning via Adversarial Networks"", ""url"": ""https://arxiv.org/abs/1707.00075""}]",5,4
57,Prejudice Remover Regulariser,Incorporates a fairness penalty into the learning objective to penalize models that encode biases with respect to protected attributes.,Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Fairness-Constrained Optimisation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Training logistic regression models with fairness constraints for university admissions."", ""goal"": ""Fairness""}]","[{""description"": ""Requires setting a hyperparameter for fairness penalty; too high can severely hurt accuracy, too low has little effect. Only applicable to models that can incorporate such a regularizer (e.g., logistic regression).""}]","[{""type"": ""Paper"", ""title"": ""Fairness-Aware Classifier with Prejudice Remover Regularizer (Kamishima et al., 2012)"", ""url"": ""https://arxiv.org/abs/1205.2999""}]",3,2
58,Meta Fair Classifier,Modifies any classifier to optimize for fairness metrics using a meta-learning algorithm.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Fairness-Constrained Optimisation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Applying fairness optimization to models in employee evaluation systems."", ""goal"": ""Fairness""}]","[{""description"": ""Meta-learning approach can be complex to implement and require extensive hyperparameter tuning; may result in longer training times and complexity that makes the method less accessible.""}]","[{""type"": ""Paper"", ""title"": ""Mitigating Bias in Classification via Meta-Algorithm (Celis et al., 2019)"", ""url"": ""https://arxiv.org/abs/1905.03850""}]",5,4
59,Exponentiated Gradient Reduction,"Formulates fairness as a constrained optimization problem, using exponentiated gradient methods to find optimal classifiers.",Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Fairness-Constrained Optimisation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Training fair classifiers for employment screening processes."", ""goal"": ""Fairness""}]","[{""description"": ""Involves iterative retraining of a classifier with adjusted weights; might require a convex base learner for theoretical guarantees, and can be sensitive to convergence criteria.""}]","[{""type"": ""Paper"", ""title"": ""A Reductions Approach to Fair Classification (Agarwal et al., 2018)"", ""url"": ""https://arxiv.org/abs/1803.02453""}]",4,3
60,Fair Transfer Learning,Adapts models trained on one domain to another while preserving fairness constraints across domains.,Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Fair Representation Learning""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Transferring fairness-aware models from one region's data to another in healthcare analytics."", ""goal"": ""Fairness""}]","[{""description"": ""Fairness achieved in one domain might not directly translate to another if data distributions differ; approach can be complicated to design and may need careful tuning to preserve fairness across domains.""}]","[{""type"": ""Paper"", ""title"": ""Fair Transfer Learning with Missing Demographics (Madras et al., 2018)"", ""url"": ""https://arxiv.org/abs/1809.10050""}]",5,4
61,Adaptive Sensitive Reweighting,Dynamically adjusts weights during training based on model performance across different groups.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Fairness-Constrained Optimisation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Balancing performance in speech recognition across accents and dialects."", ""goal"": ""Fairness""}]","[{""description"": ""Requires monitoring model performance across groups in training, which can introduce instability; if not tuned properly, could oscillate or focus too much on one group at a time.""}]","[{""type"": ""Paper"", ""title"": ""Adaptive Reweighting for Fair Classification (Biswas et al., 2020)"", ""url"": ""https://arxiv.org/abs/2010.00078""}]",4,3
62,Multi-Accuracy Boosting,Improves accuracy uniformly across groups by correcting errors where the model performs poorly for certain groups.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Fairness-Constrained Optimisation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Enhancing model performance for underrepresented groups in disease prediction."", ""goal"": ""Fairness""}]","[{""description"": ""Targets error patterns in subgroups which requires identifying those groups or error regions; could increase complexity of the model and may overfit if very granular corrections are made.""}]","[{""type"": ""Paper"", ""title"": ""Multiaccuracy: Black-Box Post-Processing for Fairness in Classification (Hebert-Johnson et al., 2018)"", ""url"": ""https://papers.nips.cc/paper/2018/hash/9a49a25d845a3cdfd8b3d0cce985d545-Abstract.html""}]",5,4
63,Equalised Odds Post-Processing,Adjusts output probabilities to equalise true positive and false positive rates across groups.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Post-Processing Techniques""}]","[{""category"": ""Post-Processing Techniques"", ""subcategory"": ""Outcome Adjustment""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Testing & Validation""}]","[{""description"": ""Ensuring fairness in recidivism risk assessments used in judicial decisions."", ""goal"": ""Fairness""}]","[{""description"": ""Adjusting outputs can reduce model confidence or require randomization in decisions; may sacrifice individual consistency (similar cases get different outcomes to satisfy group rates).""}]","[{""type"": ""Paper"", ""title"": ""Equality of Opportunity in Supervised Learning (Hardt et al., 2016)"", ""url"": ""https://arxiv.org/abs/1610.02413""}]",3,2
64,Threshold Optimiser,Adjusts decision thresholds for different groups to satisfy fairness constraints post-training.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Post-Processing Techniques""}]","[{""category"": ""Post-Processing Techniques"", ""subcategory"": ""Outcome Adjustment""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Testing & Validation""}]","[{""description"": ""Ensuring equal acceptance rates in college admissions across demographics."", ""goal"": ""Fairness""}]","[{""description"": ""Requires a held-out set to determine thresholds per group; if distributions shift, thresholds may need recalibration. Also, using different thresholds per group can raise legal or ethical concerns in deployment.""}]","[{""type"": ""Library"", ""title"": ""Fairlearn ThresholdOptimizer Documentation"", ""url"": ""https://fairlearn.org/v0.7.0/api_reference/fairlearn.postprocessing.html#fairlearn.postprocessing.ThresholdOptimizer""}]",3,1
65,Reject Option Classification,"Changes decisions where the model is least certain, favoring the disadvantaged group within this uncertain region.",Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Post-Processing Techniques""}]","[{""category"": ""Post-Processing Techniques"", ""subcategory"": ""Outcome Adjustment""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Testing & Validation""}]","[{""description"": ""Mitigating bias in hiring decisions by adjusting uncertain predictions."", ""goal"": ""Fairness""}]","[{""description"": ""Only applicable when model uncertainty can be estimated; choosing the 'reject' region and how to reassign decisions can be subjective and might reject too many instances if tuned conservatively.""}]","[{""type"": ""Paper"", ""title"": ""Reject Option Classification for Fairness (Kamiran et al., 2012)"", ""url"": ""https://ieeexplore.ieee.org/document/6233097""}]",3,2
66,Calibration with Equality of Opportunity,Adjusts probabilities to achieve equal true positive rates across groups while maintaining calibration within each group.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Post-Processing Techniques""}]","[{""category"": ""Post-Processing Techniques"", ""subcategory"": ""Calibration Methods""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Testing & Validation""}]","[{""description"": ""Balancing opportunity in credit scoring across different ethnic groups."", ""goal"": ""Fairness""}]","[{""description"": ""Achieving calibration within each group while equalizing true positive rates can be at odds with overall calibration; it may involve solving for probabilities in a way that lowers overall model calibration or accuracy.""}]","[{""type"": ""Paper"", ""title"": ""Equalized Odds and Calibration (Pleiss et al., 2017)"", ""url"": ""https://arxiv.org/abs/1707.00010""}]",4,3
67,Statistical Parity Difference,Measures the difference in positive outcome rates between protected and unprotected groups.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Fairness Metrics and Evaluation""}]","[{""category"": ""Fairness Metrics and Evaluation"", ""subcategory"": ""Group Fairness Metrics""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Data Analysis; Model Testing & Validation""}]","[{""description"": ""Evaluating fairness in hiring models by comparing selection rates across genders."", ""goal"": ""Fairness""}]","[{""description"": ""Ignores true labels or qualification differences; can be satisfied by trivial strategies (like random decisions) and does not ensure individual fairness.""}]","[{""type"": ""Explanation"", ""title"": ""Fairness Definitions Explained (Verma & Rubin, 2018)"", ""url"": ""https://arxiv.org/abs/1802.04896""}]",1,1
68,Disparate Impact,"Assesses whether decisions disproportionately affect members of a protected group, typically requiring a ratio between rates.",Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Fairness Metrics and Evaluation""}]","[{""category"": ""Fairness Metrics and Evaluation"", ""subcategory"": ""Group Fairness Metrics""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Data Analysis; Model Testing & Validation""}]","[{""description"": ""Checking for bias in loan approvals where minority groups are less likely to be approved."", ""goal"": ""Fairness""}]","[{""description"": ""Typically uses the 80% rule which is an arbitrary threshold; focuses only on selection rates and not on accuracy or errors, so a model could satisfy disparate impact yet still be unfair in errors.""}]","[{""type"": ""Law/Policy"", ""title"": ""Uniform Guidelines on Employee Selection (80% rule)"", ""url"": ""https://www.gpo.gov/fdsys/pkg/CFR-2016-title29-vol4/xml/CFR-2016-title29-vol4-part1607.xml""}]",1,1
69,Demographic Parity,"Evaluates if the outcome is independent of the protected attributes, aiming for equal outcome rates across groups.",Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Fairness Metrics and Evaluation""}]","[{""category"": ""Fairness Metrics and Evaluation"", ""subcategory"": ""Group Fairness Metrics""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Data Analysis; Model Testing & Validation""}]","[{""description"": ""Ensuring job advertisements are shown equally across genders."", ""goal"": ""Fairness""}]","[{""description"": ""If groups truly differ in label distribution, enforcing parity can harm utility; like statistical parity, it can be achieved in ways that ignore actual qualification (e.g., giving positive decisions randomly to meet rates).""}]","[{""type"": ""Article"", ""title"": ""Understanding Demographic Parity in ML"", ""url"": ""https://fairmlbook.org/pdf/demographic_parity.pdf""}]",2,1
70,Equal Opportunity Difference,"Computes the difference in true positive rates between groups, assessing fairness in terms of equal opportunity.",Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Fairness Metrics and Evaluation""}]","[{""category"": ""Fairness Metrics and Evaluation"", ""subcategory"": ""Group Fairness Metrics""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Data Analysis; Model Testing & Validation""}]","[{""description"": ""Assessing fairness in medical diagnosis models across age groups."", ""goal"": ""Fairness""}]","[{""description"": ""Addresses only true positive rates, ignoring false positive disparities; requires accurate ground truth labels for the positive class, and improving TPR for one group might increase FPR for that group.""}]","[{""type"": ""Paper"", ""title"": ""Equality of Opportunity (Hardt et al., 2016)"", ""url"": ""https://arxiv.org/abs/1610.02413""}]",2,1
71,Average Odds Difference,Calculates the average difference in false positive and true positive rates between groups.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Fairness Metrics and Evaluation""}]","[{""category"": ""Fairness Metrics and Evaluation"", ""subcategory"": ""Group Fairness Metrics""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Data Analysis; Model Testing & Validation""}]","[{""description"": ""Measuring bias in criminal risk assessment tools."", ""goal"": ""Fairness""}]","[{""description"": ""Combines multiple error rates which may obscure specific issues (a model could have one high and one low disparity and still average out); still needs ground truth labels and a balanced trade-off with accuracy.""}]","[{""type"": ""Paper"", ""title"": ""Fairness Metrics in ML (Chouldechova, 2017)"", ""url"": ""https://arxiv.org/abs/1703.00056""}]",2,1
72,Individual Fairness Metric (Consistency),"Evaluates whether similar individuals receive similar predictions, assessing fairness at an individual level.",Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Fairness Metrics and Evaluation""}]","[{""category"": ""Fairness Metrics and Evaluation"", ""subcategory"": ""Individual Fairness Metrics""}]","[{""type"": ""Fairness Approach"", ""value"": ""Individual Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Data Analysis; Model Testing & Validation""}]","[{""description"": ""Ensuring similar credit applicants receive similar loan decisions."", ""goal"": ""Fairness""}]","[{""description"": ""Requires a domain-specific similarity metric between individuals; hard to define and validate, and ensuring consistency can conflict with achieving good group-level metrics.""}]","[{""type"": ""Paper"", ""title"": ""Fairness Through Awareness (Dwork et al., 2012)"", ""url"": ""https://arxiv.org/abs/1104.3913""}]",3,3
73,Algorithmic Fairness using K-NN,Uses K-nearest neighbors to assess individual fairness by comparing predictions among similar instances.,Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Fairness Metrics and Evaluation""}]","[{""category"": ""Fairness Metrics and Evaluation"", ""subcategory"": ""Individual Fairness Metrics""}]","[{""type"": ""Fairness Approach"", ""value"": ""Individual Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Data Analysis; Model Testing & Validation""}]","[{""description"": ""Evaluating fairness in personalized recommendation systems."", ""goal"": ""Fairness""}]","[{""description"": ""Relies on the quality of the distance metric; high-dimensional data can make nearest neighbor comparisons noisy (curse of dimensionality), and it doesn't directly fix the model, just evaluates it.""}]","[{""type"": ""Paper"", ""title"": ""k-NN Consistency as Fairness Metric"", ""url"": ""https://openreview.net/pdf?id=SJxUjjR9tX""}]",3,3
74,Counterfactual Fairness (Causal Modeling),Ensures predictions remain the same in a counterfactual world where protected attributes are altered.,Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Causal Fairness Methods""}]","[{""category"": ""Causal Fairness Methods"", ""subcategory"": ""Counterfactual Fairness""}]","[{""type"": ""Fairness Approach"", ""value"": ""Individual Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Assessing fairness in loan approvals by simulating changes in applicant's race."", ""goal"": ""Fairness""}]","[{""description"": ""Requires a causal model of the data (including how protected attributes influence other features) which might be difficult to obtain; results depend on the correctness of this causal model.""}]","[{""type"": ""Paper"", ""title"": ""Counterfactual Fairness (Kusner et al., 2017)"", ""url"": ""https://arxiv.org/abs/1703.06856""}]",4,3
75,Path-Specific Counterfactual Fairness,"Considers specific causal pathways, allowing fairness interventions on certain paths.",Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Causal Fairness Methods""}]","[{""category"": ""Causal Fairness Methods"", ""subcategory"": ""Counterfactual Fairness""}]","[{""type"": ""Fairness Approach"", ""value"": ""Individual Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Modeling fair decisions in advertising without altering legitimate causal effects."", ""goal"": ""Fairness""}]","[{""description"": ""Requires identifying which causal pathways are 'allowable' and which are not—a subjective decision; analyzing specific paths adds complexity to the causal model and the fairness criterion.""}]","[{""type"": ""Paper"", ""title"": ""Path-Specific Counterfactual Fairness (Chiappa & Gillam, 2018)"", ""url"": ""https://arxiv.org/abs/1802.08139""}]",5,4
76,Causal Fairness Assessment with Do-Calculus,Utilizes causal inference techniques to assess and mitigate bias by computing interventional distributions.,Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Causal Fairness Methods""}]","[{""category"": ""Causal Fairness Methods"", ""subcategory"": ""Causal Inference""}]","[{""type"": ""Fairness Approach"", ""value"": ""Causal Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Data Analysis; Model Testing & Validation""}]","[{""description"": ""Understanding bias in hiring decisions through causal relationships."", ""goal"": ""Fairness""}]","[{""description"": ""Strongly dependent on having a correct causal graph; using do-calculus in practice can be computationally intense and challenging outside of relatively simple models or well-specified causal relationships.""}]","[{""type"": ""Paper"", ""title"": ""Causal Fairness Analysis (Nabi & Shpitser, 2018)"", ""url"": ""https://arxiv.org/abs/1707.00010""}]",5,4
77,Diversity Constraints in Recommendations,Incorporates diversity and fairness constraints in recommendation systems for varied and fair content exposure.,Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Fairness-Constrained Optimisation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training; System Design and Implementation""}]","[{""description"": ""Ensuring fair representation of genres in music recommendation platforms."", ""goal"": ""Fairness""}]","[{""description"": ""May reduce accuracy or relevance of recommendations if forced diversity conflicts with user preferences; implementing constraints can complicate the recommendation algorithm and objective function.""}]","[{""type"": ""Paper"", ""title"": ""Controlling Fairness and Diversity in Recommenders (Patro et al., 2020)"", ""url"": ""https://arxiv.org/abs/2005.11736""}]",4,3
78,Bayesian Fairness Regularization,"Applies Bayesian methods to include fairness as a prior, allowing probabilistic interpretation of fairness constraints.",Model-Specific,Fairness,"[{""goal"": ""Fairness"", ""category"": ""In-Processing Techniques""}]","[{""category"": ""In-Processing Techniques"", ""subcategory"": ""Fairness-Constrained Optimisation""}]","[{""type"": ""Fairness Approach"", ""value"": ""Group Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Selection & Training""}]","[{""description"": ""Applying fairness regularization in Bayesian models for credit risk assessment."", ""goal"": ""Fairness""}]","[{""description"": ""Selecting appropriate prior distributions for fairness is non-trivial; Bayesian methods can be computationally intensive (e.g., requiring sampling) and outcomes can be sensitive to prior assumptions.""}]","[{""type"": ""Paper"", ""title"": ""Bayesian Fairness: Posterior Regularization for Fair ML (Crawford et al., 2018)"", ""url"": ""https://arxiv.org/abs/1805.12564""}]",5,4
79,SHAP Values for Fairness,"Uses SHAP (SHapley Additive exPlanations) to attribute model predictions to input features, helping to identify bias contributions.",Model-Agnostic,Fairness,"[{""goal"": ""Fairness"", ""category"": ""Interpretability and Explainability""}]","[{""category"": ""Interpretability and Explainability"", ""subcategory"": ""Feature Attribution Methods""}]","[{""type"": ""Fairness Approach"", ""value"": ""Individual Fairness""}, {""type"": ""Project Lifecycle Stage"", ""value"": ""Model Testing & Validation; Model Documentation""}]","[{""description"": ""Explaining biased predictions in loan approvals by examining feature contributions."", ""goal"": ""Fairness""}]","[{""description"": ""Identifies feature contributions to bias but doesn't automatically mitigate it; interpretation requires understanding SHAP outputs, and correlated features can distribute bias attribution, complicating conclusions.""}]","[{""type"": ""Paper"", ""title"": ""Explaining Bias: SHAP values for Fairness Analysis"", ""url"": ""https://arxiv.org/abs/2011.03045""}]",3,3
