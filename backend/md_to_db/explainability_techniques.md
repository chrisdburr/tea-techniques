|**Technique Name**|**Description**|**Category**|**Sub-Category**|**Scope**|**Model Dependency**|**Example Use Case**|
|---|---|---|---|---|---|---|
|**SHAP (SHapley Additive exPlanations)**|Assigns importance values to each feature by computing their contribution to individual predictions, based on Shapley values from cooperative game theory.|Feature Analysis|Importance and Attribution|Global/Local|Model-Agnostic|Explaining individual predictions in complex models like neural networks or ensemble models.|
|**Permutation Importance**|Evaluates feature importance by measuring the decrease in model accuracy when a feature's values are randomly shuffled.|Feature Analysis|Importance and Attribution|Global|Model-Agnostic|Assessing feature importance in models where coefficients are not available, such as tree-based models.|
|**Mean Decrease Impurity (MDI)**|Calculates feature importance in tree-based models by measuring how much each feature decreases impurity across all trees.|Feature Analysis|Importance and Attribution|Global|Model-Specific|Determining important features in Random Forest classification tasks.|
|**Gini Importance**|Measures the total reduction of Gini impurity brought by a feature across all nodes and trees in decision trees and Random Forests.|Feature Analysis|Importance and Attribution|Global|Model-Specific|Selecting important features when building tree-based classification models.|
|**Coefficient Magnitudes (in Linear Models)**|Uses the absolute values of coefficients in linear models to represent feature importance, indicating the strength and direction of relationships.|Feature Analysis|Importance and Attribution|Global|Model-Specific|Interpreting which features influence housing price predictions in linear regression.|
|**Integrated Gradients**|Attributes feature importance by integrating gradients of the model's output with respect to inputs along a path from a baseline to the actual input.|Feature Analysis|Importance and Attribution|Local|Model-Specific|Understanding pixel contributions in image classification with deep neural networks.|
|**DeepLIFT**|Tracks changes in the output relative to a reference input, decomposing contributions from individual neurons to the final prediction in deep learning models.|Feature Analysis|Importance and Attribution|Local|Model-Specific|Explaining why a neural network classifies an image as a specific object by tracing neuron activations.|
|**Layer-wise Relevance Propagation (LRP)**|Explains predictions by backpropagating relevance scores from the output layer to input features, distributing the prediction score layer by layer.|Feature Analysis|Importance and Attribution|Local|Model-Specific|Visualising important regions in medical images for disease diagnosis using deep learning models.|
|**Variable Importance in Random Forests (MDA, MDG)**|Calculates feature importance by measuring the Mean Decrease Accuracy or Mean Decrease Gini when a feature is excluded from Random Forest models.|Feature Analysis|Importance and Attribution|Global|Model-Specific|Identifying key predictors in a Random Forest model for credit scoring.|
|**Contextual Decomposition**|Interprets neural networks by decomposing activations to explain predictions based on contributions of individual features or groups.|Feature Analysis|Importance and Attribution|Local|Model-Specific|Explaining sentiment predictions in text by attributing scores to words or phrases.|
|**Taylor Decomposition**|Decomposes predictions into contributions from individual features using a Taylor series expansion of the model's prediction function.|Feature Analysis|Importance and Attribution|Local|Model-Specific|Attributing feature contributions in complex models for specific predictions.|
|**Sobol Indices**|Quantifies the contribution of individual variables and their interactions to the output variance in sensitivity analysis.|Feature Analysis|Interaction Analysis|Global|Model-Agnostic|Understanding parameter impacts in environmental modeling outputs.|
|**Feature Interaction Detection (H-statistic)**|Measures feature interaction by comparing joint contributions to the model with the sum of individual contributions.|Feature Analysis|Interaction Analysis|Global|Model-Agnostic|Identifying significant interactions in healthcare predictive models.|
|**LIME (Local Interpretable Model-Agnostic Explanations)**|Generates local surrogate models that approximate complex model behaviour around a specific instance using interpretable models like linear models.|Model Approximation|Local Surrogates|Local|Model-Agnostic|Explaining why a customer was denied a loan by approximating the model's decision locally.|
|**Ridge Regression Surrogates**|Uses Ridge Regression as a surrogate to approximate global behaviour of complex models, balancing simplicity and interpretability with regularisation.|Model Approximation|Global Surrogates|Global|Model-Agnostic|Summarising complex model behaviour for regulatory reporting in finance.|
|**Partial Dependence Plots (PDP)**|Visualises the relationship between one or two features and the predicted outcome, averaging out effects of other features.|Visualisation Techniques|Feature Visualisation|Global|Model-Agnostic|Understanding how changes in age affect predicted disease risk in medical models.|
|**Accumulated Local Effects (ALE) Plots**|Similar to PDPs but account for feature interactions, providing accurate insights when features are correlated.|Visualisation Techniques|Feature Visualisation|Global|Model-Agnostic|Exploring the effect of house size on price predictions in real estate models with correlated features.|
|**Individual Conditional Expectation (ICE) Plots**|Shows how a feature affects predictions for individual instances, highlighting heterogeneous effects across data points.|Visualisation Techniques|Feature Visualisation|Local|Model-Agnostic|Visualising how customers' predicted spending changes with income in consumer behaviour models.|
|**Saliency Maps**|Highlights important pixels in input images that most influence the output prediction in computer vision models.|Visualisation Techniques|Model Behaviour Visualisation|Local|Model-Specific|Identifying regions contributing to tumor diagnosis in medical images.|
|**Grad-CAM (Gradient-weighted Class Activation Mapping)**|Uses gradients to produce heatmaps highlighting image regions that contribute most to the model's output.|Visualisation Techniques|Model Behaviour Visualisation|Local|Model-Specific|Visualising parts of an image leading to a 'dog' classification in image recognition models.|
|**Occlusion Sensitivity**|Measures prediction changes by systematically occluding parts of the input to identify important regions or features.|Visualisation Techniques|Model Behaviour Visualisation|Local|Model-Specific|Understanding which words affect sentiment prediction by masking them in NLP models.|
|**Attention Mechanisms in Neural Networks**|Visualises attention weights in models like transformers, highlighting important input parts for predictions.|Visualisation Techniques|Model Behaviour Visualisation|Local|Model-Specific|Analysing which words a transformer model focuses on during machine translation tasks.|
|**Factor Analysis**|Reduces dimensionality by identifying latent factors explaining observed variability, aiding in data interpretation.|Model Simplification|Dimensionality Reduction|Global|Model-Agnostic|Discovering underlying factors in psychological survey data for social science research.|
|**Principal Component Analysis (PCA)**|Reduces dimensionality by projecting data onto directions of maximum variance, simplifying data while retaining important information.|Visualisation Techniques|Dimensionality Reduction Visualisation|Global|Model-Agnostic|Visualising high-dimensional gene expression data in bioinformatics.|
|**t-SNE**|A non-linear technique that visualises high-dimensional data in 2D or 3D, preserving local relationships between points.|Visualisation Techniques|Dimensionality Reduction Visualisation|Global|Model-Agnostic|Visualising clusters in high-dimensional word embeddings.|
|**UMAP**|A non-linear technique similar to t-SNE but better at preserving global data structure.|Visualisation Techniques|Dimensionality Reduction Visualisation|Global|Model-Agnostic|Visualising patterns in user behaviour data for marketing analysis.|
|**Prototype and Criticism Models**|Identifies representative (prototypes) and non-representative (criticisms) examples to summarise and highlight model behaviour.|Example-Based Methods|Prototype and Criticism Methods|Global|Model-Agnostic|Selecting representative customer profiles for targeted marketing.|
|**Influence Functions**|Measures the impact of training examples on model predictions to identify influential data points.|Example-Based Methods|Prototype and Criticism Methods|Global|Model-Agnostic|Debugging model predictions by identifying influential training data points.|
|**Contrastive Explanation Method (CEM)**|Generates explanations by identifying minimal input changes that result in different outcomes, offering counterfactual reasoning.|Example-Based Methods|Counterfactual Explanations|Local|Model-Agnostic|Explaining loan rejections by showing what changes would lead to approval.|
|**Bayesian Networks (e.g., bnlearn)**|Probabilistic graphical models representing variables and their conditional dependencies for causal reasoning.|Causal Methods|Causal Analysis|Global|Model-Specific|Modeling causal relationships in gene regulatory networks.|
|**ANCHOR**|Provides high-precision if-then rules for specific predictions, explaining which features are responsible for the decision.|Rule Extraction|Decision Rules|Local|Model-Agnostic|Generating rules to explain individual predictions in text classification.|
|**RuleFit**|Combines decision rules with linear models to provide interpretable models capturing non-linear patterns.|Rule Extraction|Decision Rules|Global|Model-Agnostic|Building interpretable models for predicting customer churn with rule-based explanations.|
|**Monte Carlo Dropout**|Uses dropout at inference time in deep learning models to estimate uncertainty by approximating Bayesian inference.|Uncertainty and Reliability|Confidence Estimation|Global|Model-Specific|Estimating prediction uncertainty in medical diagnosis models.|
|**ODIN (Out-of-DIstribution detector for Neural networks)**|Detects out-of-distribution samples in neural networks by applying temperature scaling and input perturbations.|Uncertainty and Reliability|Out-of-Distribution Detection|Global|Model-Specific|Identifying when an image classifier encounters novel inputs.|
|**Permutation Tests**|Estimates uncertainty by permuting data labels and calculating test statistics to create a null distribution in non-parametric methods.|Uncertainty and Reliability|Uncertainty Quantification|Global|Model-Agnostic|Assessing the significance of model predictions in hypothesis testing.|
|**Fairness Metrics (e.g., Equalized Odds, Demographic Parity)**|Evaluates models for fairness by measuring disparities in predictions across different demographic groups.|Fairness Explanations|Bias Detection and Mitigation|Global|Model-Agnostic|Ensuring a hiring model does not discriminate based on gender or ethnicity.|
|**Model Pruning**|Simplifies neural networks by removing less important weights or neurons, reducing complexity while retaining performance.|Model Simplification|Model Pruning|Global|Model-Specific|Reducing model size for deployment on mobile devices without significant loss in accuracy.|
|**Knowledge Distillation**|Trains a simpler 'student' model to replicate the behaviour of a complex 'teacher' model, resulting in a more interpretable model.|Model Simplification|Model Distillation|Global|Model-Agnostic|Simplifying a deep neural network for faster inference in real-time applications.|
|**Attention Visualisation in Transformers**|Visualises attention weights in transformer-based models to show how the model focuses on different input parts during prediction.|Visualisation Techniques|Model Behaviour Visualisation|Local|Model-Specific|Understanding which words a transformer model focuses on during machine translation tasks.|
|**Neuron Activation Analysis**|Analyses activation patterns of neurons in large language models (LLMs) to interpret their roles and the concepts they represent.|Feature Analysis|Importance and Attribution|Global/Local|Model-Specific|Identifying neurons responsible for syntax or semantics in language models.|
|**Prompt Sensitivity Analysis**|Studies how variations in input prompts affect LLM outputs to understand model behaviour and sensitivity.|Example-Based Methods|Prototype and Criticism Methods|Local|Model-Specific|Evaluating how different phrasings influence an LLM's answers in question-answering tasks.|
|**Causal Mediation Analysis in Language Models**|Investigates causal relationships within LLMs by assessing how interventions on specific components affect outputs.|Causal Methods|Causal Analysis|Global/Local|Model-Specific|Understanding how adjusting embeddings changes model responses in language generation tasks.|
|**Feature Attribution with Integrated Gradients in NLP**|Applies Integrated Gradients to attribute importance of input tokens in LLMs for specific predictions, often producing visualisations.|Feature Analysis|Importance and Attribution|Local|Model-Specific|Identifying words influencing text sentiment classification or topic modeling.|
|**Concept Activation Vectors (CAVs)**|Represents human-understandable concepts as vectors in the model's latent space to analyse their influence on predictions.|Feature Analysis|Importance and Attribution|Global/Local|Model-Specific|Assessing how concepts like "negativity" affect language model outputs.|
|**In-Context Learning Analysis**|Examines how LLMs learn from examples provided in the input prompt, revealing capacity for few-shot learning.|Example-Based Methods|Prototype and Criticism Methods|Local|Model-Specific|Analysing the effect of examples on an LLM's ability to perform a new task like translation.|
