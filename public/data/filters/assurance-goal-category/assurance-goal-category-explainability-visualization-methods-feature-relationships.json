{
  "tag": {
    "name": "assurance-goal-category/explainability/visualization-methods/feature-relationships",
    "slug": "assurance-goal-category-explainability-visualization-methods-feature-relationships",
    "count": 4,
    "category": "assurance-goal-category"
  },
  "techniques": [
    {
      "slug": "partial-dependence-plots",
      "name": "Partial Dependence Plots",
      "description": "Partial Dependence Plots show how changing one or two features affects a model's predictions on average. The technique works by varying the selected feature(s) across their full range whilst keeping all other features fixed at their original values, then averaging the predictions. This creates a clear visualisation of whether increasing or decreasing a feature tends to increase or decrease predictions, and reveals patterns like linear trends, plateaus, or threshold effects that help explain model behaviour.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/perturbation-based",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/visualization-methods/feature-relationships",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing how house prices change with property size in a real estate prediction model, revealing whether the relationship is linear or if there are diminishing returns for very large properties.",
          "goal": "Explainability"
        },
        {
          "description": "Examining how customer age affects predicted loan default probability in a credit scoring model, showing whether risk increases steadily with age or has specific age ranges with higher risk.",
          "goal": "Explainability"
        },
        {
          "description": "Visualising how temperature affects crop yield predictions in agricultural models, identifying optimal temperature ranges and potential threshold effects.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Assumes features are independent when averaging, which can be misleading when features are highly correlated."
        },
        {
          "description": "Shows only average effects across all instances, potentially hiding important variations in how different subgroups respond to feature changes."
        },
        {
          "description": "Cannot reveal instance-specific effects or interactions between the plotted feature and other features."
        },
        {
          "description": "May be computationally expensive for large datasets since it requires making predictions across the full range of feature values."
        }
      ],
      "resources": [
        {
          "title": "DanielKerrigan/PDPilot",
          "url": "https://github.com/DanielKerrigan/PDPilot",
          "source_type": "software_package"
        },
        {
          "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation",
          "url": "http://arxiv.org/pdf/1309.6392v2",
          "source_type": "technical_paper",
          "authors": [
            "Alex Goldstein",
            "Adam Kapelner",
            "Justin Bleich",
            "Emil Pitkin"
          ],
          "publication_date": "2013-09-25"
        },
        {
          "title": "SauceCat/PDPbox",
          "url": "https://github.com/SauceCat/PDPbox",
          "source_type": "software_package"
        },
        {
          "title": "iPDP: On Partial Dependence Plots in Dynamic Modeling Scenarios",
          "url": "http://arxiv.org/pdf/2306.07775v1",
          "source_type": "technical_paper",
          "authors": [
            "Maximilian Muschalik",
            "Fabian Fumagalli",
            "Rohit Jagtani",
            "Barbara Hammer",
            "Eyke Hüllermeier"
          ],
          "publication_date": "2023-06-13"
        },
        {
          "title": "How to Interpret Models: PDP and ICE | Towards Data Science",
          "url": "https://towardsdatascience.com/how-to-interpret-models-pdp-and-ice-eabed0062e2c/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "acronym": "PDP",
      "related_techniques": [
        "individual-conditional-expectation-plots"
      ]
    },
    {
      "slug": "individual-conditional-expectation-plots",
      "name": "Individual Conditional Expectation Plots",
      "description": "ICE plots display the predicted output for individual instances as a function of a feature, with all other features held fixed for each instance. Each line on an ICE plot represents one instance's prediction trajectory as the feature of interest changes, revealing whether different instances are affected differently by that feature.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/visualization-methods/feature-relationships",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/low",
        "explanatory-scope/global",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/visualization"
      ],
      "example_use_cases": [
        {
          "description": "Examining how house price predictions vary with property age for individual properties, revealing that whilst most houses follow a declining price trend with age, historic properties (built before 1900) show different patterns due to heritage value.",
          "goal": "Explainability"
        },
        {
          "description": "Analysing how individual patients' diabetes risk predictions change with BMI, showing that whilst most patients follow the expected increasing risk pattern, some patients with specific genetic markers show different response curves.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Plots can become cluttered and difficult to interpret when displaying many instances simultaneously."
        },
        {
          "description": "Does not provide automatic summarisation of overall effects, requiring manual visual inspection to identify patterns."
        },
        {
          "description": "Still assumes all other features remain fixed at their observed values, which may not reflect realistic scenarios."
        },
        {
          "description": "Cannot reveal interactions between the plotted feature and other features for individual instances."
        }
      ],
      "resources": [
        {
          "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation",
          "url": "http://arxiv.org/pdf/1309.6392v2",
          "source_type": "technical_paper",
          "authors": [
            "Alex Goldstein",
            "Adam Kapelner",
            "Justin Bleich",
            "Emil Pitkin"
          ],
          "publication_date": "2013-09-25"
        },
        {
          "title": "Bringing a Ruler Into the Black Box: Uncovering Feature Impact from Individual Conditional Expectation Plots",
          "url": "http://arxiv.org/pdf/2109.02724v1",
          "source_type": "technical_paper",
          "authors": [
            "Andrew Yeh",
            "Anhthy Ngo"
          ],
          "publication_date": "2021-09-06"
        },
        {
          "title": "Explainable AI(XAI) - A guide to 7 packages in Python to explain ...",
          "url": "https://towardsdatascience.com/explainable-ai-xai-a-guide-to-7-packages-in-python-to-explain-your-models-932967f0634b/",
          "source_type": "tutorial"
        },
        {
          "title": "Communicating Uncertainty in Machine Learning Explanations: A Visualization Analytics Approach for Predictive Process Monitoring",
          "url": "https://www.semanticscholar.org/paper/3d0090df2b73369b502559eb49fd6d1ae432b952",
          "source_type": "technical_paper",
          "authors": [
            "Nijat Mehdiyev",
            "Maxim Majlatow",
            "Peter Fettke"
          ]
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "acronym": "ICE",
      "related_techniques": [
        "partial-dependence-plots"
      ]
    },
    {
      "slug": "t-sne",
      "name": "t-SNE",
      "description": "t-SNE (t-Distributed Stochastic Neighbour Embedding) is a non-linear dimensionality reduction technique that creates 2D or 3D visualisations of high-dimensional data by preserving local neighbourhood relationships. The algorithm converts similarities between data points into joint probabilities in the high-dimensional space, then tries to minimise the divergence between these probabilities and those in the low-dimensional embedding. This approach excels at revealing cluster structures and local patterns, making it particularly effective for exploratory data analysis and understanding complex data relationships that linear methods like PCA might miss.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
        "assurance-goal-category/explainability/visualization-methods/feature-relationships",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/visualization"
      ],
      "example_use_cases": [
        {
          "description": "Analyzing genomic data with thousands of gene expression features to visualize how different cancer subtypes cluster together, revealing which tumors have similar molecular signatures and potentially similar treatment responses.",
          "goal": "Explainability"
        },
        {
          "description": "Exploring deep learning model embeddings to understand how a neural network represents different categories of images, showing whether the model groups similar objects (cars, animals, furniture) in meaningful clusters in its internal feature space.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Non-deterministic algorithm produces different results on each run, making it difficult to reproduce exact visualizations or compare results across studies."
        },
        {
          "description": "Prioritizes preserving local neighborhood structure at the expense of global relationships, potentially creating misleading impressions about overall data topology."
        },
        {
          "description": "Computationally expensive with O(n²) complexity, making it impractical for datasets with more than ~10,000 points without approximation methods."
        },
        {
          "description": "Sensitive to hyperparameter choices (perplexity, learning rate, iterations) that can dramatically affect clustering patterns and require domain expertise to tune appropriately."
        }
      ],
      "resources": [
        {
          "title": "pavlin-policar/openTSNE",
          "url": "https://github.com/pavlin-policar/openTSNE",
          "source_type": "software_package"
        },
        {
          "title": "openTSNE: Extensible, parallel implementations of t-SNE ...",
          "url": "https://opentsne.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "How t-SNE works — openTSNE 1.0.0 documentation",
          "url": "https://opentsne.readthedocs.io/en/stable/tsne_algorithm.html",
          "source_type": "documentation"
        },
        {
          "title": "t-SNE from Scratch (ft. NumPy) | Towards Data Science",
          "url": "https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "related_techniques": [
        "factor-analysis",
        "principal-component-analysis",
        "umap"
      ]
    },
    {
      "slug": "generalized-additive-models",
      "name": "Generalized Additive Models",
      "description": "An intrinsically interpretable modelling technique that extends linear models by allowing flexible, nonlinear relationships between individual features and the target whilst maintaining the additive structure that preserves transparency. Each feature's effect is modelled separately as a smooth function, visualised as a curve showing how the feature influences predictions across its range. GAMs achieve this through spline functions or other smoothing techniques that capture complex patterns in individual variables without interactions, making them particularly valuable for domains requiring both predictive accuracy and model interpretability.",
      "assurance_goals": [
        "Transparency",
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/linear-models/gam",
        "applicable-models/paradigm/parametric",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/explainability/surrogate-models/global-surrogates",
        "assurance-goal-category/explainability/visualization-methods/feature-relationships",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/labelled-data",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/visualisation",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/project-design",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Predicting hospital readmission risk with a GAM that provides transparent, auditable risk assessments by showing how readmission probability varies nonlinearly with patient age, blood pressure, and medication adherence, enabling clinicians to understand and trust the model's reasoning for regulatory compliance.",
          "goal": "Transparency"
        },
        {
          "description": "Building a credit scoring model that explains loan decisions to applicants by visualising how income, credit history, and debt-to-income ratio individually affect approval likelihood, providing clear feature attributions that satisfy fair lending requirements and regulatory explainability mandates.",
          "goal": "Explainability"
        },
        {
          "description": "Developing an environmental monitoring system that reliably predicts air quality using GAMs to model the smooth, nonlinear relationships between weather variables, ensuring stable predictions across seasonal variations whilst maintaining interpretable relationships that environmental scientists can validate.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Cannot capture complex interactions between features unless explicitly modelled, limiting their ability to represent relationships where variables influence each other."
        },
        {
          "description": "Setup requires domain expertise to decide which features need nonlinear treatment and appropriate smoothing parameters, making model specification more challenging than linear models."
        },
        {
          "description": "Fitting process is computationally more expensive than linear models, particularly for large datasets with many features requiring smoothing."
        },
        {
          "description": "Risk of overfitting individual feature relationships if smoothing parameters are not properly regularised, potentially reducing generalisation performance."
        },
        {
          "description": "Interpretation complexity increases with the number of nonlinear features, as understanding multiple smooth curves simultaneously becomes cognitively demanding."
        }
      ],
      "resources": [
        {
          "title": "Generalized Additive Models",
          "url": "https://hastie.su.domains/Papers/gam.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Trevor Hastie",
            "Robert Tibshirani"
          ],
          "publication_date": "1986-01-01"
        },
        {
          "title": "pyGAM: Generalized Additive Models in Python",
          "url": "https://github.com/dswah/pyGAM",
          "source_type": "software_package"
        },
        {
          "title": "mgcv: Mixed GAM Computation Vehicle with Automatic Smoothness Estimation",
          "url": "https://cran.r-project.org/web/packages/mgcv/index.html",
          "source_type": "software_package"
        },
        {
          "title": "A Tour of pyGAM — pyGAM documentation",
          "url": "https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "GAMs",
      "related_techniques": [
        "monotonicity-constraints",
        "intrinsically-interpretable-models"
      ]
    }
  ],
  "count": 4
}