{
  "tag": {
    "name": "applicable-models/requirements/architecture-specific",
    "slug": "applicable-models-requirements-architecture-specific",
    "count": 5,
    "category": "applicable-models"
  },
  "techniques": [
    {
      "slug": "contextual-decomposition",
      "name": "Contextual Decomposition",
      "description": "Contextual Decomposition explains LSTM and RNN predictions by decomposing the final hidden state into contributions from individual inputs and their interactions. Unlike simpler attribution methods, it separates the direct contribution of specific words or phrases from the contextual effects of surrounding words. This is particularly useful for understanding how sequential models process language, as it can identify whether a word's influence comes from its individual meaning or from its interaction with nearby words in the sequence.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/attribution-methods/model-specific",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic",
        "applicable-models/architecture/neural-networks/recurrent",
        "applicable-models/requirements/white-box",
        "applicable-models/requirements/architecture-specific"
      ],
      "example_use_cases": [
        {
          "description": "Analysing why an LSTM-based spam filter flagged an email by decomposing contributions from individual words ('free', 'urgent') versus their contextual interactions ('free trial' together).",
          "goal": "Explainability"
        },
        {
          "description": "Understanding how a medical text classifier diagnoses conditions from clinical notes by separating direct symptom mentions from contextual medical reasoning patterns.",
          "goal": "Explainability"
        },
        {
          "description": "Providing transparent explanations for automated content moderation decisions by showing which words and phrase interactions contributed to toxicity detection.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Primarily designed for LSTM and simple RNN architectures, not suitable for modern transformers or attention-based models."
        },
        {
          "description": "Not widely implemented in standard machine learning libraries, often requiring custom implementation."
        },
        {
          "description": "Computational overhead increases significantly with sequence length and model depth."
        },
        {
          "description": "May not scale well to very complex models or capture all types of feature interactions in deep networks."
        }
      ],
      "resources": [
        {
          "title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs",
          "url": "http://arxiv.org/pdf/1801.05453v2",
          "source_type": "technical_paper",
          "authors": [
            "W. James Murdoch",
            "Peter J. Liu",
            "Bin Yu"
          ],
          "publication_date": "2018-01-16"
        },
        {
          "title": "FredericGodin/ContextualDecomposition-NLP",
          "url": "https://github.com/FredericGodin/ContextualDecomposition-NLP",
          "source_type": "software_package"
        },
        {
          "title": "Interpreting patient-Specific risk prediction using contextual decomposition of BiLSTMs: Application to children with asthma",
          "url": "https://core.ac.uk/download/294758884.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Alsaad R.",
            "Boughorbel S.",
            "Janahi I.",
            "Malluhi Q."
          ],
          "publication_date": "2019-01-01"
        },
        {
          "title": "Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models",
          "url": "http://arxiv.org/pdf/1911.06194v2",
          "source_type": "technical_paper",
          "authors": [
            "Xisen Jin",
            "Zhongyu Wei",
            "Junyi Du",
            "Xiangyang Xue",
            "Xiang Ren"
          ],
          "publication_date": "2019-11-08"
        },
        {
          "title": "Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition",
          "url": "http://arxiv.org/pdf/2407.00886v3",
          "source_type": "technical_paper",
          "authors": [
            "Aliyah R. Hsu",
            "Georgia Zhou",
            "Yeshwanth Cherapanamjeri",
            "Yaxuan Huang",
            "Anobel Y. Odisho",
            "Peter R. Carroll",
            "Bin Yu"
          ],
          "publication_date": "2024-07-01"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 3,
      "related_techniques": [
        "taylor-decomposition",
        "influence-functions"
      ]
    },
    {
      "slug": "gradient-weighted-class-activation-mapping",
      "name": "Gradient-weighted Class Activation Mapping",
      "description": "Grad-CAM creates visual heatmaps showing which regions of an image a convolutional neural network focuses on when making a specific classification. Unlike pixel-level techniques, Grad-CAM produces coarser region-based explanations by using gradients from the predicted class to weight the CNN's final feature maps, then projecting these weighted activations back to create an overlay on the original image. This provides intuitive visual explanations of where the model is 'looking' for evidence of different classes.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/convolutional",
        "applicable-models/requirements/architecture-specific",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/explainability/visualization-methods/activation-maps",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Validating that a melanoma detection model focuses on the actual skin lesion rather than surrounding healthy skin, medical equipment, or artifacts when making cancer/benign classifications.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging an autonomous vehicle's traffic sign recognition system by visualising whether the model correctly focuses on the sign itself rather than background objects, shadows, or irrelevant visual elements.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a medical imaging system for racial bias by examining whether diagnostic predictions inappropriately focus on skin tone regions rather than actual pathological features, ensuring equitable healthcare AI deployment.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires access to the CNN's internal feature maps and gradients, limiting use to white-box scenarios."
        },
        {
          "description": "Resolution is constrained by the final convolutional layer's feature map size, producing coarser localisation than pixel-level methods."
        },
        {
          "description": "Only applicable to CNN architectures with clearly defined convolutional layers, not suitable for other neural network types."
        },
        {
          "description": "May highlight regions that correlate with the class but aren't causally important for the model's decision-making process."
        }
      ],
      "resources": [
        {
          "title": "jacobgil/pytorch-grad-cam",
          "url": "https://github.com/jacobgil/pytorch-grad-cam",
          "source_type": "software_package"
        },
        {
          "title": "Grad-CAM: Visualize class activation maps with Keras, TensorFlow ...",
          "url": "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/",
          "source_type": "tutorial"
        },
        {
          "title": "kazuto1011/grad-cam-pytorch",
          "url": "https://github.com/kazuto1011/grad-cam-pytorch",
          "source_type": "software_package"
        },
        {
          "title": "A Tutorial on Explainable Image Classification for Dementia Stages Using Convolutional Neural Network and Gradient-weighted Class Activation Mapping",
          "url": "https://www.semanticscholar.org/paper/8b1139cb06cfe5ba69e2fd05e1450b43df031a02",
          "source_type": "documentation",
          "authors": [
            "Kevin Kam Fung Yuen"
          ]
        },
        {
          "title": "A Guide to Grad-CAM in Deep Learning - Analytics Vidhya",
          "url": "https://www.analyticsvidhya.com/blog/2023/12/grad-cam-in-deep-learning/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "Grad-CAM",
      "related_techniques": [
        "saliency-maps",
        "occlusion-sensitivity"
      ]
    },
    {
      "slug": "classical-attention-analysis-in-neural-networks",
      "name": "Classical Attention Analysis in Neural Networks",
      "description": "Classical attention mechanisms in RNNs and CNNs create alignment matrices and temporal attention patterns that show how models focus on different input elements over time or space. This technique analyses these traditional attention patterns, particularly in encoder-decoder architectures and sequence-to-sequence models, where attention weights reveal which source elements influence each output step. Unlike transformer self-attention analysis, this focuses on understanding alignment patterns, temporal dependencies, and encoder-decoder attention dynamics in classical neural architectures.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/recurrent",
        "applicable-models/requirements/architecture-specific",
        "applicable-models/requirements/model-internals",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/visualization-methods/attention-patterns",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing encoder-decoder attention in a neural machine translation model to verify the alignment between source and target words, ensuring the model learns proper translation correspondences rather than positional biases.",
          "goal": "Explainability"
        },
        {
          "description": "Examining temporal attention patterns in an RNN-based image captioning model to understand how attention moves across different image regions as it generates each word of the caption description.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Attention weights are not always strongly correlated with feature importance for the final prediction."
        },
        {
          "description": "High attention does not necessarily imply causal influence - models can attend to irrelevant but correlated features."
        },
        {
          "description": "Only applicable to neural network architectures that explicitly use attention mechanisms."
        },
        {
          "description": "Interpretation can be misleading without understanding the specific attention mechanism implementation and training dynamics."
        }
      ],
      "resources": [
        {
          "title": "An Attentive Survey of Attention Models",
          "url": "https://www.semanticscholar.org/paper/a8427ce5aee6d62800c725588e89940ed4910e0d",
          "source_type": "documentation",
          "authors": [
            "S. Chaudhari",
            "Gungor Polatkan",
            "R. Ramanath",
            "Varun Mithal"
          ]
        },
        {
          "title": "Attention, please! A survey of neural attention models in deep learning",
          "url": "https://www.semanticscholar.org/paper/44930df2a3186edb58c4d6f6e5ed828c5d6a0089",
          "source_type": "documentation",
          "authors": [
            "Alana de Santana Correia",
            "E. Colombini"
          ]
        },
        {
          "title": "ecco - Explain, Analyze, and Visualize NLP Language Models",
          "url": "https://github.com/jalammar/ecco",
          "source_type": "software_package"
        },
        {
          "title": "Enhancing Sentiment Analysis of Twitter Data Using Recurrent Neural Networks with Attention Mechanism",
          "url": "https://www.semanticscholar.org/paper/c59e0158280a567114ae8ca64a932eefd127e0aa",
          "source_type": "technical_paper",
          "authors": [
            "S. Nithya",
            "X. A. Presskila",
            "B. Sakthivel",
            "R. Krishnan",
            "K. Narayanan",
            "S. Sundararajan"
          ]
        },
        {
          "title": "Can Neural Networks Develop Attention? Google Thinks they Can ...",
          "url": "https://www.kdnuggets.com/2019/11/neural-networks-develop-attention-google.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "attention-visualisation-in-transformers"
      ]
    },
    {
      "slug": "monte-carlo-dropout",
      "name": "Monte Carlo Dropout",
      "description": "Monte Carlo Dropout estimates prediction uncertainty by applying dropout (randomly setting neural network weights to zero) during inference rather than just training. It performs multiple forward passes through the network with different random dropout patterns and collects the resulting predictions to form a distribution. Low variance across predictions indicates epistemic certainty (the model is confident), while high variance suggests epistemic uncertainty (the model is unsure). This technique transforms any dropout-trained neural network into a Bayesian approximation for uncertainty quantification.",
      "assurance_goals": [
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/paradigm/probabilistic",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/architecture-specific",
        "applicable-models/requirements/model-internals",
        "applicable-models/requirements/probabilistic-output",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/prediction-confidence",
        "assurance-goal-category/explainability/property/consistency",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Quantifying diagnostic uncertainty in medical imaging models by running 50+ Monte Carlo forward passes to detect when a chest X-ray classification is highly uncertain, prompting radiologist review for borderline cases.",
          "goal": "Reliability"
        },
        {
          "description": "Estimating prediction confidence in autonomous vehicle perception systems, where high uncertainty in object detection (e.g., variance > 0.3 across MC samples) triggers more conservative driving behaviour or human handover.",
          "goal": "Reliability"
        },
        {
          "description": "Providing uncertainty estimates in financial fraud detection models, where high epistemic uncertainty (wide prediction variance) indicates the model lacks sufficient training data for similar transaction patterns, requiring manual review.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Only captures epistemic (model) uncertainty, not aleatoric (data) uncertainty, providing an incomplete picture of total prediction uncertainty."
        },
        {
          "description": "Computationally expensive as it requires multiple forward passes (typically 50-100) for each prediction, significantly increasing inference time."
        },
        {
          "description": "Results depend critically on dropout rate matching the training configuration, and poorly calibrated dropout can lead to misleading uncertainty estimates."
        },
        {
          "description": "Approximation quality varies with network architecture and dropout placement, with some configurations providing poor uncertainty calibration despite theoretical foundations."
        }
      ],
      "resources": [
        {
          "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
          "url": "http://arxiv.org/pdf/1506.02142v6",
          "source_type": "technical_paper",
          "authors": [
            "Yarin Gal",
            "Zoubin Ghahramani"
          ],
          "publication_date": "2016-06-06"
        },
        {
          "title": "mattiasegu/uncertainty_estimation_deep_learning",
          "url": "https://github.com/mattiasegu/uncertainty_estimation_deep_learning",
          "source_type": "software_package"
        },
        {
          "title": "uzh-rpg/deep_uncertainty_estimation",
          "url": "https://github.com/uzh-rpg/deep_uncertainty_estimation",
          "source_type": "software_package"
        },
        {
          "title": "How certain are tansformers in image classification: uncertainty analysis with Monte Carlo dropout",
          "url": "https://www.semanticscholar.org/paper/d7ff734c5b62a4a140fd560373d890e43d5b36cf",
          "source_type": "technical_paper",
          "authors": [
            "Md. Farhadul Islam",
            "Sarah Zabeen",
            "Md. Azharul Islam",
            "Fardin Bin Rahman",
            "Anushua Ahmed",
            "Dewan Ziaul Karim",
            "Annajiat Alim Rasel",
            "Meem Arafat Manab"
          ]
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "prediction-intervals",
        "quantile-regression",
        "conformal-prediction",
        "deep-ensembles",
        "bootstrapping",
        "jackknife-resampling"
      ]
    },
    {
      "slug": "attention-visualisation-in-transformers",
      "name": "Attention Visualisation in Transformers",
      "description": "Attention Visualisation in Transformers analyses the multi-head self-attention mechanisms that enable transformers to process sequences by attending to different positions simultaneously. The technique visualises attention weights as heatmaps showing how strongly each token attends to every other token across different heads and layers. By examining these attention patterns, practitioners can understand how models like BERT, GPT, and T5 build contextual representations, identify which tokens influence predictions most strongly, and detect potential biases in how the model processes different types of input. This provides insights into positional encoding effects, head specialisation patterns, and the evolution of attention from local to global context across layers.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/transformer",
        "applicable-models/requirements/architecture-specific",
        "applicable-models/requirements/model-internals",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/feature-analysis",
        "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/visualization-methods/attention-patterns",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "data-type/text",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/global",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Examining attention patterns in a medical language model processing clinical notes to verify it focuses on relevant symptoms and conditions rather than irrelevant demographic identifiers, revealing that certain attention heads specialise in medical terminology whilst others track syntactic relationships between diagnoses and treatments.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a sentiment analysis model for customer reviews by visualising how attention weights differ when processing reviews from different demographic groups, discovering that the model pays disproportionate attention to certain cultural expressions or colloquialisms that could lead to biased sentiment predictions.",
          "goal": "Fairness"
        },
        {
          "description": "Creating visual explanations for regulatory compliance in a financial document classification system, showing which specific words and phrases in loan applications or contracts triggered particular risk assessments, enabling auditors to verify that decisions are based on legitimate financial factors rather than discriminatory language patterns.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "High attention weights do not necessarily indicate causal importance for predictions, as models may attend strongly to tokens that serve structural rather than semantic purposes."
        },
        {
          "description": "The sheer number of attention heads and layers in modern transformers creates visualisation overload, making it difficult to identify meaningful patterns without systematic analysis tools."
        },
        {
          "description": "Attention patterns can be misleading when models use residual connections and layer normalisation, as the final representation incorporates information beyond what attention weights suggest."
        },
        {
          "description": "Different transformer architectures (encoder-only, decoder-only, encoder-decoder) exhibit fundamentally different attention patterns, limiting the generalisability of insights across model types."
        },
        {
          "description": "The technique cannot explain the reasoning process within feed-forward layers or how attention patterns translate into specific predictions, providing only a partial view of model behaviour."
        }
      ],
      "resources": [
        {
          "title": "jessevig/bertviz",
          "url": "https://github.com/jessevig/bertviz",
          "source_type": "software_package",
          "description": "Interactive tool for visualising attention patterns in transformer language models including BERT, GPT-2, and T5"
        },
        {
          "title": "Attention is All You Need",
          "url": "https://arxiv.org/abs/1706.03762",
          "source_type": "technical_paper",
          "description": "Foundational paper introducing the transformer architecture and self-attention mechanism"
        },
        {
          "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
          "url": "https://arxiv.org/abs/1905.09418",
          "source_type": "technical_paper",
          "description": "Research showing how different attention heads specialise in distinct linguistic phenomena"
        },
        {
          "title": "What Does BERT Look At? An Analysis of BERT's Attention",
          "url": "https://arxiv.org/abs/1906.04341",
          "source_type": "technical_paper",
          "description": "Comprehensive analysis of attention patterns in BERT revealing syntactic and semantic specialisation"
        },
        {
          "title": "Transformer Explainability Beyond Attention Visualization",
          "url": "https://arxiv.org/abs/2012.09838",
          "source_type": "technical_paper",
          "description": "Methods for attribution beyond raw attention weights including relevancy propagation and gradient-based approaches"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "integrated-gradients",
        "layer-wise-relevance-propagation",
        "saliency-maps",
        "gradient-weighted-class-activation-mapping",
        "classical-attention-analysis-in-neural-networks",
        "contrastive-explanation-method"
      ]
    }
  ],
  "count": 5
}