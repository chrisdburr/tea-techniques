{
  "tag": {
    "name": "assurance-goal-category/explainability/surrogate-models/global-surrogates",
    "slug": "assurance-goal-category-explainability-surrogate-models-global-surrogates",
    "count": 4,
    "category": "assurance-goal-category"
  },
  "techniques": [
    {
      "slug": "ridge-regression-surrogates",
      "name": "Ridge Regression Surrogates",
      "description": "This technique approximates a complex model by training a ridge regression (a linear model with L2 regularization) on the original model's predictions. The ridge regression serves as a global surrogate that balances fidelity and interpretability, capturing the main linear relationships that the complex model learned while ignoring noise due to regularization.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/surrogate-models/global-surrogates",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/decision-boundaries",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/structured-output",
        "expertise-needed/ml-engineering",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic",
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box"
      ],
      "example_use_cases": [
        {
          "description": "Approximating a complex ensemble model used for credit scoring with a ridge regression surrogate to identify the most influential features (income, credit history, debt-to-income ratio) and their linear relationships for regulatory compliance reporting.",
          "goal": "Explainability"
        },
        {
          "description": "Creating a ridge regression surrogate of a neural network used for medical diagnosis to understand which patient symptoms and biomarkers have the strongest linear predictive relationships with disease outcomes.",
          "goal": "Explainability"
        },
        {
          "description": "Creating an interpretable approximation of a complex insurance pricing model for regulatory compliance, enabling stakeholders to understand and validate the decision-making process through transparent linear relationships.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Linear approximation may miss important non-linear relationships and interactions captured by the original complex model."
        },
        {
          "description": "Requires a representative dataset to train the surrogate model, which may not be available or may be expensive to generate."
        },
        {
          "description": "Ridge regularisation may oversimplify the model by shrinking coefficients, potentially hiding important but less dominant features."
        },
        {
          "description": "Surrogate fidelity depends on how well linear relationships approximate the original model's behaviour across the entire input space."
        }
      ],
      "resources": [
        {
          "title": "scikit-learn Ridge Regression Documentation",
          "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html",
          "source_type": "documentation"
        },
        {
          "title": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable",
          "url": "https://christophm.github.io/interpretable-ml-book/global.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "rulefit",
        "model-distillation"
      ]
    },
    {
      "slug": "rulefit",
      "name": "RuleFit",
      "description": "RuleFit creates interpretable surrogate models that can explain complex black-box models or serve as interpretable alternatives. It works by learning a sparse linear model that combines automatically extracted decision rules with original features. The technique first builds tree ensembles to generate candidate rules, then uses LASSO regression to select the most important rules and features. The resulting model provides global explanations through human-readable rules (e.g., 'IF age > 50 AND income < 30k THEN ...') combined with linear feature weights, making complex model behavior transparent and auditable.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/training-data",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/decision-boundaries",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/explainability/surrogate-models/global-surrogates",
        "assurance-goal-category/explainability/surrogate-models/rule-extraction",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Building customer churn prediction models with rules like 'IF contract_length < 12_months AND support_calls > 5 THEN churn_risk = high', allowing marketing teams to understand and act on the key drivers of customer attrition.",
          "goal": "Explainability"
        },
        {
          "description": "Creating credit scoring models that combine traditional linear factors (income, age) with interpretable rules (IF recent_missed_payments = 0 AND account_age > 2_years THEN creditworthy), providing transparent lending decisions.",
          "goal": "Explainability"
        },
        {
          "description": "Developing regulatory-compliant medical diagnosis models where treatment recommendations combine clinical measurements with clear decision rules (IF blood_pressure > 140 AND diabetes = true THEN high_risk), enabling audit trails for healthcare decisions.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Can generate large numbers of rules even with regularisation, potentially overwhelming users and reducing practical interpretability."
        },
        {
          "description": "Performance may be inferior to complex ensemble methods when rule complexity is constrained for interpretability."
        },
        {
          "description": "Rule extraction quality depends heavily on the underlying tree ensemble, which may miss important feature interactions if not properly configured."
        },
        {
          "description": "Requires careful hyperparameter tuning to balance between model complexity and interpretability, with no universal optimal setting."
        }
      ],
      "resources": [
        {
          "title": "christophM/rulefit",
          "url": "https://github.com/christophM/rulefit",
          "source_type": "software_package"
        },
        {
          "title": "Tree Ensembles with Rule Structured Horseshoe Regularization",
          "url": "http://arxiv.org/pdf/1702.05008v2",
          "source_type": "technical_paper",
          "authors": [
            "Malte Nalenz",
            "Mattias Villani"
          ],
          "publication_date": "2017-02-16"
        },
        {
          "title": "Safe RuleFit: Learning Optimal Sparse Rule Model by Meta Safe Screening",
          "url": "http://arxiv.org/pdf/1810.01683v2",
          "source_type": "technical_paper",
          "authors": [
            "Hiroki Kato",
            "Hiroyuki Hanada",
            "Ichiro Takeuchi"
          ],
          "publication_date": "2018-10-03"
        },
        {
          "title": "csinva/imodels",
          "url": "https://github.com/csinva/imodels",
          "source_type": "software_package"
        },
        {
          "title": "Getting More From Regression Models with RuleFit | Towards Data ...",
          "url": "https://towardsdatascience.com/getting-more-from-regression-models-with-rulefit-2e6be8d77432/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "ridge-regression-surrogates",
        "model-distillation"
      ]
    },
    {
      "slug": "model-distillation",
      "name": "Model Distillation",
      "description": "Model distillation transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) by training the student to mimic the teacher's behaviour. The student learns from the teacher's soft predictions and intermediate representations rather than just hard labels, capturing nuanced decision boundaries and uncertainty. This produces models that are faster, require less memory, and are often more interpretable whilst maintaining much of the original performance. Beyond compression, distillation can improve model reliability by regularising training and enable deployment in resource-constrained environments.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/paradigm/parametric",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/training-data",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/decision-boundaries",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/model-simplification/knowledge-transfer",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/surrogate-models/global-surrogates",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/access-to-training-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/deployment",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Compressing a large medical diagnosis model into a smaller student model that can run on edge devices in resource-limited clinics, making the decision process more transparent for healthcare professionals whilst maintaining diagnostic accuracy for critical patient care.",
          "goal": "Explainability"
        },
        {
          "description": "Creating a compressed fraud detection model from a complex ensemble teacher that maintains detection performance whilst being more robust to adversarial attacks and data drift, ensuring consistent protection of financial transactions across varying conditions.",
          "goal": "Reliability"
        },
        {
          "description": "Distilling a large autonomous vehicle perception model into a smaller student model that can run with guaranteed inference times and lower computational requirements, ensuring predictable safety-critical decision-making under real-time constraints.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Student models typically achieve 90-95% of teacher performance, creating a trade-off between model efficiency and predictive accuracy that may be unacceptable for high-stakes applications requiring maximum precision."
        },
        {
          "description": "Distillation process can be computationally expensive, requiring extensive teacher model inference during training and careful hyperparameter tuning to balance knowledge transfer with student model capacity."
        },
        {
          "description": "Knowledge transfer quality depends heavily on teacher-student architecture compatibility and the chosen distillation objectives, with mismatched designs potentially leading to ineffective learning or mode collapse."
        },
        {
          "description": "Student models may inherit teacher model biases and vulnerabilities whilst potentially introducing new failure modes, requiring separate validation for fairness, robustness, and safety properties."
        },
        {
          "description": "Compressed models may lack the teacher's capability to handle edge cases or out-of-distribution inputs, potentially creating safety risks when deployed in environments different from the training distribution."
        }
      ],
      "resources": [
        {
          "title": "airaria/TextBrewer",
          "url": "https://github.com/airaria/TextBrewer",
          "source_type": "software_package",
          "description": "PyTorch-based knowledge distillation toolkit for natural language processing with support for transformer models, flexible distillation strategies, and multi-teacher approaches."
        },
        {
          "title": "Main features — TextBrewer 0.2.1.post1 documentation",
          "url": "https://textbrewer.readthedocs.io/",
          "source_type": "documentation",
          "description": "Comprehensive documentation for TextBrewer including tutorials, API reference, configuration guides, and experimental results for knowledge distillation in NLP tasks."
        },
        {
          "title": "A Generic Approach for Reproducible Model Distillation",
          "url": "http://arxiv.org/abs/2211.12631",
          "source_type": "technical_paper",
          "authors": [
            "Hooker, Giles",
            "Xu, Peiru",
            "Zhou, Yunzhe"
          ],
          "publication_date": "2023-04-27",
          "description": "Research paper presenting a framework for reproducible knowledge distillation with standardised evaluation protocols and benchmarking across different model architectures and distillation techniques."
        },
        {
          "title": "dkozlov/awesome-knowledge-distillation",
          "url": "https://github.com/dkozlov/awesome-knowledge-distillation",
          "source_type": "software_package",
          "description": "Curated collection of knowledge distillation resources including academic papers, implementation code across multiple frameworks (PyTorch, TensorFlow, Keras), and educational videos."
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "related_techniques": [
        "ridge-regression-surrogates",
        "rulefit",
        "model-pruning"
      ]
    },
    {
      "slug": "generalized-additive-models",
      "name": "Generalized Additive Models",
      "description": "An intrinsically interpretable modelling technique that extends linear models by allowing flexible, nonlinear relationships between individual features and the target whilst maintaining the additive structure that preserves transparency. Each feature's effect is modelled separately as a smooth function, visualised as a curve showing how the feature influences predictions across its range. GAMs achieve this through spline functions or other smoothing techniques that capture complex patterns in individual variables without interactions, making them particularly valuable for domains requiring both predictive accuracy and model interpretability.",
      "assurance_goals": [
        "Transparency",
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/linear-models/gam",
        "applicable-models/paradigm/parametric",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/explainability/surrogate-models/global-surrogates",
        "assurance-goal-category/explainability/visualization-methods/feature-relationships",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/labelled-data",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/visualisation",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/project-design",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Predicting hospital readmission risk with a GAM that provides transparent, auditable risk assessments by showing how readmission probability varies nonlinearly with patient age, blood pressure, and medication adherence, enabling clinicians to understand and trust the model's reasoning for regulatory compliance.",
          "goal": "Transparency"
        },
        {
          "description": "Building a credit scoring model that explains loan decisions to applicants by visualising how income, credit history, and debt-to-income ratio individually affect approval likelihood, providing clear feature attributions that satisfy fair lending requirements and regulatory explainability mandates.",
          "goal": "Explainability"
        },
        {
          "description": "Developing an environmental monitoring system that reliably predicts air quality using GAMs to model the smooth, nonlinear relationships between weather variables, ensuring stable predictions across seasonal variations whilst maintaining interpretable relationships that environmental scientists can validate.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Cannot capture complex interactions between features unless explicitly modelled, limiting their ability to represent relationships where variables influence each other."
        },
        {
          "description": "Setup requires domain expertise to decide which features need nonlinear treatment and appropriate smoothing parameters, making model specification more challenging than linear models."
        },
        {
          "description": "Fitting process is computationally more expensive than linear models, particularly for large datasets with many features requiring smoothing."
        },
        {
          "description": "Risk of overfitting individual feature relationships if smoothing parameters are not properly regularised, potentially reducing generalisation performance."
        },
        {
          "description": "Interpretation complexity increases with the number of nonlinear features, as understanding multiple smooth curves simultaneously becomes cognitively demanding."
        }
      ],
      "resources": [
        {
          "title": "Generalized Additive Models",
          "url": "https://hastie.su.domains/Papers/gam.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Trevor Hastie",
            "Robert Tibshirani"
          ],
          "publication_date": "1986-01-01"
        },
        {
          "title": "pyGAM: Generalized Additive Models in Python",
          "url": "https://github.com/dswah/pyGAM",
          "source_type": "software_package"
        },
        {
          "title": "mgcv: Mixed GAM Computation Vehicle with Automatic Smoothness Estimation",
          "url": "https://cran.r-project.org/web/packages/mgcv/index.html",
          "source_type": "software_package"
        },
        {
          "title": "A Tour of pyGAM — pyGAM documentation",
          "url": "https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "GAMs",
      "related_techniques": [
        "monotonicity-constraints",
        "intrinsically-interpretable-models"
      ]
    }
  ],
  "count": 4
}