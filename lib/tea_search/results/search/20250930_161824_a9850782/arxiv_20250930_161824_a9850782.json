{
  "search_id": "20250930_161824_a9850782",
  "provider": "arxiv",
  "technique_slug": "local-interpretable-model-agnostic-explanations",
  "timestamp": "2025-09-30T16:18:42.310729",
  "count": 20,
  "results": [
    {
      "title": "BMVC 2019: Workshop on Interpretable and Explainable Machine Vision",
      "url": "http://arxiv.org/pdf/1909.07245v1",
      "source": "arxiv",
      "abstract": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable\nMachine Vision, Cardiff, UK, September 12, 2019.",
      "authors": [
        "Alun Preece"
      ],
      "date": "2019-09-16",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/1909.07245v1",
        "categories": [
          "cs.CV",
          "cs.AI",
          "cs.LG"
        ],
        "comment": null
      }
    },
    {
      "title": "Optimal Local Explainer Aggregation for Interpretable Prediction",
      "url": "http://arxiv.org/pdf/2003.09466v2",
      "source": "arxiv",
      "abstract": "A key challenge for decision makers when incorporating black box machine\nlearned models into practice is being able to understand the predictions\nprovided by these models. One proposed set of methods is training surrogate\nexplainer models which approximate the more complex model. Explainer methods\nare generally classified as either local or global, depending on what portion\nof the data space they are purported to explain. The improved coverage of\nglobal explainers usually comes at the expense of",
      "authors": [
        "Qiaomei Li",
        "Rachel Cummings",
        "Yonatan Mintz"
      ],
      "date": "2020-03-20",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2003.09466v2",
        "categories": [
          "cs.LG",
          "cs.CY",
          "stat.ML"
        ],
        "comment": null
      }
    },
    {
      "title": "Towards a Research Community in Interpretable Reinforcement Learning: the InterpPol Workshop",
      "url": "http://arxiv.org/pdf/2404.10906v1",
      "source": "arxiv",
      "abstract": "Embracing the pursuit of intrinsically explainable reinforcement learning\nraises crucial questions: what distinguishes explainability from\ninterpretability? Should explainable and interpretable agents be developed\noutside of domains where transparency is imperative? What advantages do\ninterpretable policies offer over neural networks? How can we rigorously define\nand measure interpretability in policies, without user studies? What\nreinforcement learning paradigms,are the most suited to develop i",
      "authors": [
        "Hector Kohler",
        "Quentin Delfosse",
        "Paul Festor",
        "Philippe Preux"
      ],
      "date": "2024-04-16",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2404.10906v1",
        "categories": [
          "cs.AI",
          "cs.HC",
          "cs.LG",
          "cs.SC"
        ],
        "comment": null
      }
    },
    {
      "title": "Dual Explanations via Subgraph Matching for Malware Detection",
      "url": "http://arxiv.org/pdf/2504.20904v1",
      "source": "arxiv",
      "abstract": "Interpretable malware detection is crucial for understanding harmful\nbehaviors and building trust in automated security systems. Traditional\nexplainable methods for Graph Neural Networks (GNNs) often highlight important\nregions within a graph but fail to associate them with known benign or\nmalicious behavioral patterns. This limitation reduces their utility in\nsecurity contexts, where alignment with verified prototypes is essential. In\nthis work, we introduce a novel dual prototype-driven explai",
      "authors": [
        "Hossein Shokouhinejad",
        "Roozbeh Razavi-Far",
        "Griffin Higgins",
        "Ali A. Ghorbani"
      ],
      "date": "2025-04-29",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2504.20904v1",
        "categories": [
          "cs.CR",
          "cs.LG"
        ],
        "comment": null
      }
    },
    {
      "title": "Efficient Explanations from Empirical Explainers",
      "url": "http://arxiv.org/pdf/2103.15429v2",
      "source": "arxiv",
      "abstract": "Amid a discussion about Green AI in which we see explainability neglected, we\nexplore the possibility to efficiently approximate computationally expensive\nexplainers. To this end, we propose feature attribution modelling with\nEmpirical Explainers. Empirical Explainers learn from data to predict the\nattribution maps of expensive explainers. We train and test Empirical\nExplainers in the language domain and find that they model their expensive\ncounterparts surprisingly well, at a fraction of the co",
      "authors": [
        "Robert Schwarzenberg",
        "Nils Feldhus",
        "Sebastian M\u00f6ller"
      ],
      "date": "2021-03-29",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2103.15429v2",
        "categories": [
          "cs.LG",
          "cs.CL"
        ],
        "comment": "Accepted to the EMNLP 2021 Workshop on Analyzing and Interpreting\n  Neural Networks for NLP (BlackboxNLP)"
      }
    },
    {
      "title": "RelEx: A Model-Agnostic Relational Model Explainer",
      "url": "http://arxiv.org/pdf/2006.00305v1",
      "source": "arxiv",
      "abstract": "In recent years, considerable progress has been made on improving the\ninterpretability of machine learning models. This is essential, as complex deep\nlearning models with millions of parameters produce state of the art results,\nbut it can be nearly impossible to explain their predictions. While various\nexplainability techniques have achieved impressive results, nearly all of them\nassume each data instance to be independent and identically distributed (iid).\nThis excludes relational models, such ",
      "authors": [
        "Yue Zhang",
        "David Defazio",
        "Arti Ramesh"
      ],
      "date": "2020-05-30",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2006.00305v1",
        "categories": [
          "cs.LG",
          "stat.ML"
        ],
        "comment": null
      }
    },
    {
      "title": "Unsupervised Learning of Neural Networks to Explain Neural Networks (extended abstract)",
      "url": "http://arxiv.org/pdf/1901.07538v1",
      "source": "arxiv",
      "abstract": "This paper presents an unsupervised method to learn a neural network, namely\nan explainer, to interpret a pre-trained convolutional neural network (CNN),\ni.e., the explainer uses interpretable visual concepts to explain features in\nmiddle conv-layers of a CNN. Given feature maps of a conv-layer of the CNN, the\nexplainer performs like an auto-encoder, which decomposes the feature maps into\nobject-part features. The object-part features are learned to reconstruct CNN\nfeatures without much loss of ",
      "authors": [
        "Quanshi Zhang",
        "Yu Yang",
        "Ying Nian Wu"
      ],
      "date": "2019-01-21",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/1901.07538v1",
        "categories": [
          "cs.LG",
          "cs.AI",
          "stat.ML"
        ],
        "comment": "In AAAI-19 Workshop on Network Interpretability for Deep Learning.\n  arXiv admin note: substantial text overlap with arXiv:1805.07468"
      }
    },
    {
      "title": "Interpretation miniatures",
      "url": "http://arxiv.org/pdf/1703.08341v2",
      "source": "arxiv",
      "abstract": "Most physicists do not have patience for reading long and obscure\ninterpretation arguments and disputes. Hence, to attract attention of a wider\nphysics community, in this paper various old and new aspects of quantum\ninterpretations are explained in a concise and simple (almost trivial) form.\nAbout the \"Copenhagen\" interpretation, we note that there are several different\nversions of it and explain how to make sense of \"local non-reality\"\ninterpretation. About the many-world interpretation, we exp",
      "authors": [
        "H. Nikolic"
      ],
      "date": "2017-03-24",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/1703.08341v2",
        "categories": [
          "quant-ph",
          "gr-qc",
          "physics.hist-ph"
        ],
        "comment": "12 pages, revised, new references, accepted for publication in Int.\n  J. Quantum Inf"
      }
    },
    {
      "title": "Unsupervised Learning of Neural Networks to Explain Neural Networks",
      "url": "http://arxiv.org/pdf/1805.07468v1",
      "source": "arxiv",
      "abstract": "This paper presents an unsupervised method to learn a neural network, namely\nan explainer, to interpret a pre-trained convolutional neural network (CNN),\ni.e., explaining knowledge representations hidden in middle conv-layers of the\nCNN. Given feature maps of a certain conv-layer of the CNN, the explainer\nperforms like an auto-encoder, which first disentangles the feature maps into\nobject-part features and then inverts object-part features back to features of\nhigher conv-layers of the CNN. More ",
      "authors": [
        "Quanshi Zhang",
        "Yu Yang",
        "Yuchen Liu",
        "Ying Nian Wu",
        "Song-Chun Zhu"
      ],
      "date": "2018-05-18",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/1805.07468v1",
        "categories": [
          "cs.CV",
          "cs.LG"
        ],
        "comment": null
      }
    },
    {
      "title": "Model-agnostic vs. Model-intrinsic Interpretability for Explainable Product Search",
      "url": "http://arxiv.org/pdf/2108.05317v2",
      "source": "arxiv",
      "abstract": "Product retrieval systems have served as the main entry for customers to\ndiscover and purchase products online. With increasing concerns on the\ntransparency and accountability of AI systems, studies on explainable\ninformation retrieval has received more and more attention in the research\ncommunity. Interestingly, in the domain of e-commerce, despite the extensive\nstudies on explainable product recommendation, the studies of explainable\nproduct search is still in an early stage. In this paper, we",
      "authors": [
        "Qingyao Ai",
        "Lakshmi Narayanan Ramasamy"
      ],
      "date": "2021-08-11",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2108.05317v2",
        "categories": [
          "cs.IR"
        ],
        "comment": null
      }
    },
    {
      "title": "Explainability in Human-Agent Systems",
      "url": "http://arxiv.org/pdf/1904.08123v1",
      "source": "arxiv",
      "abstract": "This paper presents a taxonomy of explainability in Human-Agent Systems. We\nconsider fundamental questions about the Why, Who, What, When and How of\nexplainability. First, we define explainability, and its relationship to the\nrelated terms of interpretability, transparency, explicitness, and\nfaithfulness. These definitions allow us to answer why explainability is needed\nin the system, whom it is geared to and what explanations can be generated to\nmeet this need. We then consider when the user sh",
      "authors": [
        "Avi Rosenfeld",
        "Ariella Richardson"
      ],
      "date": "2019-04-17",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/1904.08123v1",
        "categories": [
          "cs.AI"
        ],
        "comment": null
      }
    },
    {
      "title": "A general-purpose method for applying Explainable AI for Anomaly Detection",
      "url": "http://arxiv.org/pdf/2207.11564v1",
      "source": "arxiv",
      "abstract": "The need for explainable AI (XAI) is well established but relatively little\nhas been published outside of the supervised learning paradigm. This paper\nfocuses on a principled approach to applying explainability and\ninterpretability to the task of unsupervised anomaly detection. We argue that\nexplainability is principally an algorithmic task and interpretability is\nprincipally a cognitive task, and draw on insights from the cognitive sciences\nto propose a general-purpose method for practical diag",
      "authors": [
        "John Sipple",
        "Abdou Youssef"
      ],
      "date": "2022-07-23",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2207.11564v1",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "comment": "26th International Symposium on Intelligent Systems (ISMIS 2022)"
      }
    },
    {
      "title": "Explainability in Graph Neural Networks: A Taxonomic Survey",
      "url": "http://arxiv.org/pdf/2012.15445v3",
      "source": "arxiv",
      "abstract": "Deep learning methods are achieving ever-increasing performance on many\nartificial intelligence tasks. A major limitation of deep models is that they\nare not amenable to interpretability. This limitation can be circumvented by\ndeveloping post hoc techniques to explain the predictions, giving rise to the\narea of explainability. Recently, explainability of deep models on images and\ntexts has achieved significant progress. In the area of graph data, graph\nneural networks (GNNs) and their explainabi",
      "authors": [
        "Hao Yuan",
        "Haiyang Yu",
        "Shurui Gui",
        "Shuiwang Ji"
      ],
      "date": "2020-12-31",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2012.15445v3",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "comment": null
      }
    },
    {
      "title": "Evaluating Search System Explainability with Psychometrics and Crowdsourcing",
      "url": "http://arxiv.org/pdf/2210.09430v3",
      "source": "arxiv",
      "abstract": "As information retrieval (IR) systems, such as search engines and\nconversational agents, become ubiquitous in various domains, the need for\ntransparent and explainable systems grows to ensure accountability, fairness,\nand unbiased results. Despite recent advances in explainable AI and IR\ntechniques, there is no consensus on the definition of explainability. Existing\napproaches often treat it as a singular notion, disregarding the\nmultidimensional definition postulated in the literature. In this ",
      "authors": [
        "Catherine Chen",
        "Carsten Eickhoff"
      ],
      "date": "2022-10-17",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2210.09430v3",
        "categories": [
          "cs.IR"
        ],
        "comment": "11 pages, 4 figures, accepted at SIGIR 2024 as full paper"
      }
    },
    {
      "title": "Experimental Insights Towards Explainable and Interpretable Pedestrian Crossing Prediction",
      "url": "http://arxiv.org/pdf/2312.02872v1",
      "source": "arxiv",
      "abstract": "In the context of autonomous driving, pedestrian crossing prediction is a key\ncomponent for improving road safety. Presently, the focus of these predictions\nextends beyond achieving trustworthy results; it is shifting towards the\nexplainability and interpretability of these predictions. This research\nintroduces a novel neuro-symbolic approach that combines deep learning and\nfuzzy logic for an explainable and interpretable pedestrian crossing\nprediction. We have developed an explainable predictor",
      "authors": [
        "Angie Nataly Melo",
        "Carlota Salinas",
        "Miguel Angel Sotelo"
      ],
      "date": "2023-12-05",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2312.02872v1",
        "categories": [
          "cs.LG",
          "cs.AI",
          "cs.NE",
          "cs.SY",
          "eess.SY"
        ],
        "comment": null
      }
    },
    {
      "title": "Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat",
      "url": "http://arxiv.org/pdf/2307.05350v2",
      "source": "arxiv",
      "abstract": "ML model design either starts with an interpretable model or a Blackbox and\nexplains it post hoc. Blackbox models are flexible but difficult to explain,\nwhile interpretable models are inherently explainable. Yet, interpretable\nmodels require extensive ML knowledge and tend to be less flexible and\nunderperforming than their Blackbox variants. This paper aims to blur the\ndistinction between a post hoc explanation of a Blackbox and constructing\ninterpretable models. Beginning with a Blackbox, we it",
      "authors": [
        "Shantanu Ghosh",
        "Ke Yu",
        "Forough Arabshahi",
        "Kayhan Batmanghelich"
      ],
      "date": "2023-07-07",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2307.05350v2",
        "categories": [
          "cs.LG",
          "cs.CV",
          "cs.CY"
        ],
        "comment": "appeared as v5 of arXiv:2302.10289 which was replaced in error, which\n  drifted into a different work, accepted in ICML 2023"
      }
    },
    {
      "title": "Local Interpretable Model Agnostic Shap Explanations for machine learning models",
      "url": "http://arxiv.org/pdf/2210.04533v1",
      "source": "arxiv",
      "abstract": "With the advancement of technology for artificial intelligence (AI) based\nsolutions and analytics compute engines, machine learning (ML) models are\ngetting more complex day by day. Most of these models are generally used as a\nblack box without user interpretability. Such complex ML models make it more\ndifficult for people to understand or trust their predictions. There are\nvariety of frameworks using explainable AI (XAI) methods to demonstrate\nexplainability and interpretability of ML models to ",
      "authors": [
        "P. Sai Ram Aditya",
        "Mayukha Pal"
      ],
      "date": "2022-10-10",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2210.04533v1",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "comment": "Under review"
      }
    },
    {
      "title": "Collapse challenge for interpretations of quantum mechanics",
      "url": "http://arxiv.org/pdf/quant-ph/0505172v1",
      "source": "arxiv",
      "abstract": "The collapse challenge for interpretations of quantum mechanics is to build\nfrom first principles and your preferred interpretation a complete,\nobserver-free quantum model of the described experiment (involving a photon and\ntwo screens), together with a formal analysis that completely explains the\nexperimental result. The challenge is explained in detail, and discussed in the\nlight of the Copenhagen interpretation and the decoherence setting.",
      "authors": [
        "Arnold Neumaier"
      ],
      "date": "2005-05-23",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/quant-ph/0505172v1",
        "categories": [
          "quant-ph"
        ],
        "comment": "6 pages"
      }
    },
    {
      "title": "Explainable AI for survival analysis: a median-SHAP approach",
      "url": "http://arxiv.org/pdf/2402.00072v1",
      "source": "arxiv",
      "abstract": "With the adoption of machine learning into routine clinical practice comes\nthe need for Explainable AI methods tailored to medical applications. Shapley\nvalues have sparked wide interest for locally explaining models. Here, we\ndemonstrate their interpretation strongly depends on both the summary statistic\nand the estimator for it, which in turn define what we identify as an 'anchor\npoint'. We show that the convention of using a mean anchor point may generate\nmisleading interpretations for surviv",
      "authors": [
        "Lucile Ter-Minassian",
        "Sahra Ghalebikesabi",
        "Karla Diaz-Ordaz",
        "Chris Holmes"
      ],
      "date": "2024-01-30",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2402.00072v1",
        "categories": [
          "cs.LG",
          "stat.ME",
          "stat.ML"
        ],
        "comment": "Accepted to the Interpretable Machine Learning for Healthcare (IMLH)\n  workshop of the ICML 2022 Conference"
      }
    },
    {
      "title": "Human-interpretable model explainability on high-dimensional data",
      "url": "http://arxiv.org/pdf/2010.07384v2",
      "source": "arxiv",
      "abstract": "The importance of explainability in machine learning continues to grow, as\nboth neural-network architectures and the data they model become increasingly\ncomplex. Unique challenges arise when a model's input features become high\ndimensional: on one hand, principled model-agnostic approaches to\nexplainability become too computationally expensive; on the other, more\nefficient explainability algorithms lack natural interpretations for general\nusers. In this work, we introduce a framework for human-i",
      "authors": [
        "Damien de Mijolla",
        "Christopher Frye",
        "Markus Kunesch",
        "John Mansir",
        "Ilya Feige"
      ],
      "date": "2020-10-14",
      "resource_type": "paper",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2010.07384v2",
        "categories": [
          "cs.LG",
          "cs.AI",
          "stat.ML"
        ],
        "comment": "8 pages, 6 figures, 1 appendix"
      }
    }
  ]
}