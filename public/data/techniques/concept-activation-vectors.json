{
  "slug": "concept-activation-vectors",
  "name": "Concept Activation Vectors",
  "description": "Concept Activation Vectors (CAVs), also known as Testing with Concept Activation Vectors (TCAV), identify mathematical directions in neural network representation space that correspond to human-understandable concepts such as 'stripes', 'young', or 'medical equipment'. The technique works by finding linear directions that separate activations of concept examples from non-concept examples, then measuring how much these concept directions influence the model's predictions. This provides quantitative answers to questions like 'How much does the concept of youth affect this model's hiring decisions?' enabling systematic bias detection and model understanding.",
  "assurance_goals": [
    "Explainability",
    "Fairness",
    "Transparency"
  ],
  "tags": [
    "applicable-models/neural-network",
    "applicable-models/transformer",
    "applicable-models/cnn",
    "assurance-goal-category/explainability",
    "assurance-goal-category/fairness",
    "assurance-goal-category/transparency",
    "data-requirements/access-to-model-internals",
    "data-type/any",
    "evidence-type/quantitative-metric",
    "evidence-type/visualization",
    "explanatory-scope/local",
    "explanatory-scope/global",
    "expertise-needed/ml-engineering",
    "expertise-needed/domain-knowledge",
    "lifecycle-stage/model-development",
    "lifecycle-stage/system-deployment-and-use/auditing",
    "technique-type/algorithmic",
    "assurance-goal-category/explainability/representation-analysis/concept-identification",
    "assurance-goal-category/explainability/explains/internal-mechanisms",
    "assurance-goal-category/explainability/explains/causal-pathways",
    "assurance-goal-category/explainability/property/comprehensibility",
    "assurance-goal-category/explainability/property/causality"
  ],
  "example_use_cases": [
    {
      "description": "Auditing a medical imaging model to verify it focuses on diagnostic features (like 'tumour characteristics') rather than irrelevant concepts (like 'scanner type' or 'patient positioning') when classifying chest X-rays, ensuring clinical decisions rely on medically relevant information.",
      "goal": "Explainability"
    },
    {
      "description": "Testing whether a hiring algorithm's resume screening decisions are influenced by concepts related to protected characteristics such as 'gender-associated names', 'prestigious universities', or 'employment gaps', enabling systematic bias detection and compliance verification.",
      "goal": "Fairness"
    },
    {
      "description": "Providing regulatory-compliant explanations for financial lending decisions by quantifying how concepts like 'debt-to-income ratio', 'employment stability', and 'credit history length' influence loan approval models, with precise sensitivity scores for audit documentation.",
      "goal": "Transparency"
    }
  ],
  "limitations": [
    {
      "description": "Requires clearly defined concept examples and non-concept examples, which can be challenging to obtain for abstract or subjective concepts."
    },
    {
      "description": "Assumes that meaningful concept directions exist as linear separable directions in the model's internal representation space, which may not hold for all concepts."
    },
    {
      "description": "Results depend heavily on which network layer is examined, as different layers capture different levels of abstraction and concept representation."
    },
    {
      "description": "Computational cost grows significantly with model size and number of concepts tested, though recent advances like FastCAV address this limitation."
    },
    {
      "description": "Interpretation requires domain expertise to define meaningful concepts and understand the significance of sensitivity scores in practical contexts."
    }
  ],
  "resources": [
    {
      "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks",
      "url": "http://arxiv.org/pdf/2505.17883v1",
      "source_type": "technical_paper",
      "authors": [
        "Laines Schmalwasser",
        "Niklas Penzel",
        "Joachim Denzler",
        "Julia Niebling"
      ],
      "publication_date": "2025-05-23"
    },
    {
      "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
      "url": "http://arxiv.org/pdf/2311.15303v1",
      "source_type": "technical_paper",
      "authors": [
        "Avani Gupta",
        "Saurabh Saini",
        "P J Narayanan"
      ],
      "publication_date": "2023-11-26"
    },
    {
      "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations",
      "url": "http://arxiv.org/pdf/2503.05522v1",
      "source_type": "technical_paper",
      "authors": [
        "Eren Erogullari",
        "Sebastian Lapuschkin",
        "Wojciech Samek",
        "Frederik Pahde"
      ],
      "publication_date": "2025-03-07"
    },
    {
      "title": "Concept Gradient: Concept-based Interpretation Without Linear Assumption",
      "url": "http://arxiv.org/pdf/2208.14966v2",
      "source_type": "technical_paper",
      "authors": [
        "Andrew Bai",
        "Chih-Kuan Yeh",
        "Pradeep Ravikumar",
        "Neil Y. C. Lin",
        "Cho-Jui Hsieh"
      ],
      "publication_date": "2022-08-31"
    },
    {
      "title": "SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation",
      "url": "http://arxiv.org/pdf/2310.07698v1",
      "source_type": "technical_paper",
      "authors": [
        "Bo Pan",
        "Zhenke Liu",
        "Yifei Zhang",
        "Liang Zhao"
      ],
      "publication_date": "2023-10-11"
    }
  ],
  "complexity_rating": 4,
  "computational_cost_rating": 3,
  "acronym": "CAVs",
  "related_techniques": [
    "prototype-and-criticism-models",
    "neuron-activation-analysis"
  ]
}