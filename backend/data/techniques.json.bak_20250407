[
  {
    "id": 1,
    "name": "SHapley Additive exPlanations (SHAP)",
    "description": "SHAP explains the impact of each feature on a prediction based on Shapley values from cooperative game theory, explicitly assuming feature independence, thus quantifying contributions to predictions in a globally consistent and locally accurate manner.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "categories": [
      {
        "name": "feature-analysis",
        "assurance_goal": "Explainability"
      }
    ],
    "subcategories": [
      {
        "name": "importance-and-attribution",
        "category": "feature-analysis",
        "assurance_goal": "Explainability"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      },
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Explaining why a complex model (like a neural network) made a specific prediction by showing how each feature influenced that prediction.",
        "goal": "Explainability"
      },
      {
        "description": "Identifying which factors contributed most to a loan approval decision in a financial model using SHAP values.",
        "goal": "Explainability"
      }
    ],
    "resources": [
      {
        "title": "shap",
        "url": "https://github.com/shap/shap",
        "source_type": "software_package",
        "authors": [
          "shap"
        ],
        "publication_date": "2016-11-22"
      },
      {
        "title": "Welcome to the SHAP documentation — SHAP latest documentation",
        "url": "https://shap.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "Introduction to SHapley Additive exPlanations (SHAP) — XAI Tutorials",
        "url": "https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/shap.html",
        "source_type": "tutorial"
      },
      {
        "title": "Explanation of Machine Learning Models Using Shapley Additive Explanation and Application for Real Data in Hospital",
        "url": "http://arxiv.org/pdf/2112.11071v2",
        "source_type": "technical_paper",
        "authors": [
          "Yasunobu Nohara",
          "Koutarou Matsumoto",
          "Hidehisa Soejima",
          "Naoki Nakashima"
        ],
        "publication_date": "2021-12-21"
      },
      {
        "title": "Additive-feature-attribution methods: a review on explainable artificial intelligence for fluid dynamics and heat transfer",
        "url": "http://arxiv.org/pdf/2409.11992v1",
        "source_type": "review_paper",
        "authors": [
          "Andrés Cremades",
          "Sergio Hoyas",
          "Ricardo Vinuesa"
        ],
        "publication_date": "2024-09-18"
      }
    ],
    "applicable_models": [],
    "limitations": [
      {
        "description": "Assumes feature independence, so results can be less reliable when features are highly correlated."
      },
      {
        "description": "Computationally intensive for large models or datasets (calculating many Shapley values is slow)."
      }
    ]
  },
  {
    "id": 2,
    "name": "Permutation Importance",
    "description": "Permutation importance measures a feature's importance by shuffling its values and observing the impact on model performance. If the model's accuracy drops significantly when a feature is permuted, that feature is considered important.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "categories": [
      {
        "name": "feature-analysis",
        "assurance_goal": "Explainability"
      }
    ],
    "subcategories": [
      {
        "name": "importance-and-attribution",
        "category": "feature-analysis",
        "assurance_goal": "Explainability"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Assessing which features are most important in a trained Random Forest by measuring performance drop when each feature's values are shuffled.",
        "goal": "Explainability"
      },
      {
        "description": "Checking that a model is not relying on an irrelevant field by confirming that shuffling that field doesn't change the model's accuracy.",
        "goal": "Explainability"
      }
    ],
    "resources": [
      {
        "title": "Permutation Importance — ELI5 0.11.0 documentation",
        "url": "https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html",
        "source_type": "documentation"
      },
      {
        "title": "classification - XGBoost Feature Importance, Permutation ...",
        "url": "https://datascience.stackexchange.com/questions/65608/xgboost-feature-importance-permutation-importance-and-model-evaluation-criteri",
        "source_type": "tutorial"
      },
      {
        "title": "Asymptotic Unbiasedness of the Permutation Importance Measure in Random Forest Models",
        "url": "http://arxiv.org/pdf/1912.03306v1",
        "source_type": "technical_paper",
        "authors": [
          "Burim Ramosaj",
          "Markus Pauly"
        ],
        "publication_date": "2019-12-05"
      },
      {
        "title": "lofo-importance",
        "url": "https://github.com/aerdem4/lofo-importance",
        "source_type": "software_package",
        "authors": [
          "aerdem4"
        ],
        "publication_date": "2019-01-14"
      }
    ],
    "applicable_models": [],
    "limitations": [
      {
        "description": "Can be misleading when features are correlated, and computationally expensive on high-dimensional datasets due to repetitive model evaluations."
      }
    ]
  },
  {
    "id": 3,
    "name": "Mean Decrease Impurity",
    "description": "Mean Decrease Impurity evaluates feature importance by calculating the reduction in impurity across all splits in tree-based models, typically biased towards numerical features with many distinct values or categories.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Determining important features in Random Forest classification tasks.",
        "goal": "Explainability"
      }
    ],
    "resources": [
      {
        "type": "Transparency",
        "title": "Feature Importance Measures in Random Forests (Scikit-learn)",
        "url": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"
      },
      {
        "title": "optuna.importance.MeanDecreaseImpurityImportanceEvaluator ...",
        "url": "https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.MeanDecreaseImpurityImportanceEvaluator.html",
        "source_type": "documentation"
      },
      {
        "title": "A Debiased MDI Feature Importance Measure for Random Forests",
        "url": "http://arxiv.org/pdf/1906.10845v2",
        "source_type": "technical_paper",
        "authors": [
          "Xiao Li",
          "Yu Wang",
          "Sumanta Basu",
          "Karl Kumbier",
          "Bin Yu"
        ],
        "publication_date": "2019-06-26"
      },
      {
        "title": "Variable Importance in Random Forests | Towards Data Science",
        "url": "https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0/",
        "source_type": "tutorial"
      }
    ],
    "applicable_models": [
      "Decision Trees",
      "Random Forests",
      "Gradient Boosting Models"
    ],
    "limitations": [
      "[{\"description\": \"Biased towards features with more splits or categories; only applicable to tree-based models and can overestimate importance of high-cardinality features.\"}]"
    ]
  },
  {
    "id": 4,
    "name": "Gini Importance",
    "description": "Gini importance sums the total reduction in Gini impurity across all splits, essentially identical to Mean Decrease Impurity (MDI), sharing its biases and limitations.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Selecting important features when building tree-based classification models.",
        "goal": "Explainability"
      }
    ],
    "resources": [
      {
        "type": "Transparency",
        "title": "Feature Importance Measures in Random Forests (Scikit-learn)",
        "url": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"
      },
      {
        "title": "Unbiased variable importance for random forests",
        "url": "http://arxiv.org/pdf/2003.02106v2",
        "source_type": "technical_paper",
        "authors": [
          "Markus Loecher"
        ],
        "publication_date": "2020-03-04"
      },
      {
        "title": "Best Practice to Calculate and Interpret Model Feature Importance ...",
        "url": "https://towardsdatascience.com/best-practice-to-calculate-and-interpret-model-feature-importance-14f0e11ee660/",
        "source_type": "tutorial"
      },
      {
        "title": "random-forest-importances",
        "url": "https://github.com/parrt/random-forest-importances",
        "source_type": "software_package",
        "authors": [
          "parrt"
        ],
        "publication_date": "2018-03-22"
      },
      {
        "title": "lightgbm.plot_importance — LightGBM 4.6.0.99 documentation",
        "url": "https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.plot_importance.html",
        "source_type": "documentation"
      }
    ],
    "applicable_models": [
      "Decision Trees",
      "Random Forests"
    ],
    "limitations": [
      "[{\"description\": \"Tends to inflate importance for features with many values; like MDI, it is specific to tree models and not reliable for comparing across different model types.\"}]"
    ]
  },
  {
    "id": 5,
    "name": "Coefficient Magnitudes (in Linear Models)",
    "description": "Examines the absolute values of coefficients in linear models to judge feature influence. Features with larger absolute coefficients have a stronger impact on the prediction, and the sign of a coefficient shows if it pushes the outcome up or down.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Interpreting which features influence housing price predictions in linear regression.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Linear Regression",
      "Logistic Regression",
      "Ridge Regression",
      "Lasso Regression"
    ],
    "limitations": [
      "[{\"description\": \"Only valid for linear relationships; can be affected by feature scaling and multicollinearity, and does not capture non-linear importance.\"}]"
    ]
  },
  {
    "id": 6,
    "name": "Integrated Gradients",
    "description": "Integrated Gradients attributes each feature's contribution by accumulating the gradients of the model's output with respect to that feature as the input moves from a baseline value to its actual value. This results in a score for each feature that reflects how much it influenced the prediction compared to a reference baseline.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Understanding pixel contributions in image classification with deep neural networks.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "General Neural Networks",
      "CNNs",
      "RNNs",
      "Transformers"
    ],
    "limitations": [
      "[{\"description\": \"Requires a meaningful baseline input; results can be sensitive to the choice of baseline and model must be differentiable, limiting use with non-differentiable components.\"}]"
    ]
  },
  {
    "id": 7,
    "name": "DeepLIFT",
    "description": "DeepLIFT assigns credit (or blame) to each input feature by comparing a neuron's activation to a reference (baseline) and tracking the difference backward through the network. It attributes the change in the output to changes in each input feature relative to what the output would be at the reference input.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Explaining why a neural network classifies an image as a specific object by tracing neuron activations.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "General Neural Networks",
      "CNNs",
      "RNNs"
    ],
    "limitations": [
      "[{\"description\": \"Needs careful configuration for each network architecture; may produce inconsistent scores if multiple reference points are possible, and not all model types supported.\"}]"
    ]
  },
  {
    "id": 8,
    "name": "Layer-wise Relevance Propagation (LRP)",
    "description": "LRP explains a model's prediction by propagating the result backwards through the network and assigning a relevance score to each input feature. Starting from the predicted output, it distributes the 'relevance' of that output to neurons in each preceding layer, all the way back to the input features, indicating how each feature contributed.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Visualising important regions in medical images for disease diagnosis using deep learning models.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "General Neural Networks",
      "CNNs",
      "RNNs"
    ],
    "limitations": [
      "[{\"description\": \"Requires tailored rules for each layer type; results can sometimes be hard to interpret (negative relevances), and implementation is complex for new architectures.\"}]"
    ]
  },
  {
    "id": 9,
    "name": "Variable Importance in Random Forests (MDA MDG)",
    "description": "Random Forest algorithms can provide two types of feature importance: Mean Decrease Accuracy (MDA) measures how model accuracy decreases when a feature is permuted, while Mean Decrease Gini (MDG) measures how each feature contributes to node purity in the forest. Together, they provide different perspectives on which features drive the model's decisions.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Identifying key predictors in a Random Forest model for credit scoring.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Random Forests"
    ],
    "limitations": [
      "[{\"description\": \"Permutation importance (MDA) shares limitations with correlated features; Gini importance (MDG) biases towards continuous or high-cardinality features.\"}]"
    ]
  },
  {
    "id": 10,
    "name": "Contextual Decomposition",
    "description": "Contextual Decomposition interprets neural network predictions by breaking down the model's output into parts attributed to specific input features or groups of features. It isolates the contribution of a particular input (or a combination of inputs) to the final prediction, taking into account the context provided by the other inputs.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Explaining sentiment predictions in text by attributing scores to words or phrases.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "LSTMs",
      "RNNs"
    ],
    "limitations": [
      "[{\"description\": \"Primarily designed for LSTMs; not widely implemented in standard libraries, requiring custom code, and may not scale well to very deep or different model types.\"}]"
    ]
  },
  {
    "id": 11,
    "name": "Taylor Decomposition",
    "description": "Taylor Decomposition explains a prediction by using a Taylor series expansion of the model's prediction function around a given input. It breaks the prediction into components attributed to each feature (and combinations of features) by interpreting the series terms, giving an approximate contribution of each feature to the output.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Attributing feature contributions in complex models for specific predictions.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "General Neural Networks"
    ],
    "limitations": [
      "[{\"description\": \"The approach is mathematically complex and can be computationally intensive; approximations may introduce error and it's mainly theoretical with limited tooling.\"}]"
    ]
  },
  {
    "id": 12,
    "name": "Sobol Indices",
    "description": "Sobol Indices are global sensitivity analysis measures that quantify how much each input variable (and combinations of variables) contribute to the output's variance. They provide values for individual features and feature interactions, indicating the proportion of output variation each is responsible for in the model.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/interaction-analysis",
    "complexity_rating": 4,
    "computational_cost_rating": 5,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Understanding parameter impacts in environmental modeling outputs.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires a large number of model evaluations for accurate estimation; assumes independent input distributions and may be difficult to apply to high-dimensional inputs.\"}]"
    ]
  },
  {
    "id": 13,
    "name": "Feature Interaction Detection (H-statistic)",
    "description": "The H-statistic measures the strength of interaction between two features in a model by comparing their combined effect on the prediction to the sum of their individual effects. A higher H-statistic for a feature pair means the model's prediction cannot be explained by additive effects of the two features alone, indicating a significant interaction.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/interaction-analysis",
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Identifying significant interactions in healthcare predictive models.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Only captures pairwise interactions (or one feature vs all); relies on partial dependence which can be misleading if features are correlated or have complex interactions.\"}]"
    ]
  },
  {
    "id": 14,
    "name": "Local Interpretable Model-Agnostic Explanations (LIME)",
    "description": "LIME explains an individual prediction by training a simple surrogate model around that specific data point. It perturbs the input data point to create synthetic data, gets the complex model's predictions for these new points, and then fits an interpretable model (like a small linear model) on this local dataset. The surrogate's parameters (or rules) then highlight which features of the original data point influenced the prediction the most in that locality.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#model-approximation/local-surrogates",
    "complexity_rating": 2,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Explaining why a customer was denied a loan by approximating the model's decision locally.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Explanations can vary with repeated runs due to randomness; the linear surrogate may not be faithful if the model behavior is highly non-linear in that locality.\"}]"
    ]
  },
  {
    "id": 15,
    "name": "Ridge Regression Surrogates",
    "description": "This technique approximates a complex model by training a ridge regression (a linear model with L2 regularization) on the original model's predictions. The ridge regression serves as a global surrogate that balances fidelity and interpretability, capturing the main linear relationships that the complex model learned while ignoring noise due to regularization.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#model-approximation/global-surrogates",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Summarizing complex model behavior for regulatory reporting in finance.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"The surrogate only approximates the original model, potentially losing important non-linear behavior; requires a representative dataset to train the surrogate model.\"}]"
    ]
  },
  {
    "id": 16,
    "name": "Partial Dependence Plots (PDP)",
    "description": "Partial Dependence Plots illustrate how the predicted outcome changes as one (or two) features vary, averaging out the effects of all other features. They show the marginal effect of selected features on the model prediction by plotting the average prediction as a function of those features.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/feature-visualization",
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Understanding how changes in age affect predicted disease risk in medical models.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Assumes features are independent of others (due to averaging); can be misleading when features are correlated, and only shows average effects, not instance-specific.\"}]"
    ]
  },
  {
    "id": 17,
    "name": "Accumulated Local Effects Plots (ALE)",
    "description": "ALE plots show how features influence predictions, like PDPs, but they do so by calculating local changes in the prediction as the feature moves through its range. They accumulate these local effects and account for the presence of other features, making ALE plots more robust than PDPs when features are correlated.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/feature-visualization",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Exploring the effect of house size on price predictions in real estate models with correlated features.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"More complex to compute than PDP; still provides only average effects and can be harder to interpret for higher-order interactions beyond pairs.\"}]"
    ]
  },
  {
    "id": 18,
    "name": "Individual Conditional Expectation Plots (ICE)",
    "description": "ICE plots display the predicted output for individual instances as a function of a feature, with all other features held fixed for each instance. Each line on an ICE plot represents one instance's prediction trajectory as the feature of interest changes, revealing whether different instances are affected differently by that feature.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/feature-visualization",
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Visualizing how customers' predicted spending changes with income in consumer behavior models.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Plots can become cluttered with many instances; does not inherently summarize the overall effect without visual inspection, and still assumes fixed other features.\"}]"
    ]
  },
  {
    "id": 19,
    "name": "Saliency Maps",
    "description": "Saliency maps highlight parts of an input (such as pixels in an image) that strongly influence the model's prediction. Typically computed via the gradient of the output with respect to the input, they produce a heatmap where brighter regions indicate greater influence on the model's decision.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/model-behavior-visualization",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Identifying regions contributing to tumor diagnosis in medical images.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "CNNs",
      "Vision Models"
    ],
    "limitations": [
      "[{\"description\": \"Often noisy and sensitive to small perturbations; highlights may not correspond to human-understandable features, and they only indicate local gradient, not causal importance.\"}]"
    ]
  },
  {
    "id": 20,
    "name": "Gradient-weighted Class Activation Mapping (Grad-CAM)",
    "description": "Grad-CAM produces a coarse localization map highlighting image regions important for a classification. It works by using the gradients of a target class flowing into the last convolutional layer of a CNN to weight the feature maps, and then projects these weights back to the image space as a heatmap of influential regions.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/model-behavior-visualization",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Visualizing parts of an image leading to a 'dog' classification in image recognition models.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "CNNs",
      "Vision Models"
    ],
    "limitations": [
      "[{\"description\": \"Requires access to internal feature maps; resolution is limited to coarse feature map size, and it is specific to CNN-based vision models with recognizable layers.\"}]"
    ]
  },
  {
    "id": 21,
    "name": "Occlusion Sensitivity",
    "description": "Occlusion sensitivity tests which parts of the input are important by occluding (masking or removing) them and seeing how the model's prediction changes. For example, portions of an image can be covered up in a sliding window fashion; if the model's confidence drops significantly when a certain region is occluded, that region was important for the prediction.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/model-behavior-visualization",
    "complexity_rating": 2,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Understanding which words affect sentiment prediction by masking them in NLP models.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "CNNs",
      "Vision Models",
      "Text Models"
    ],
    "limitations": [
      "[{\"description\": \"Computationally expensive if many parts need to be occluded; choice of occlusion size can bias results, and it may not capture interactions if multiple parts jointly matter.\"}]"
    ]
  },
  {
    "id": 22,
    "name": "Attention Mechanisms in Neural Networks",
    "description": "Attention mechanisms allow models, especially in sequence tasks like NLP, to weight different parts of the input when making predictions. Visualizing or examining the learned attention weights can provide insight into which parts of the input the model found most relevant for a given output. By looking at these attention heatmaps, we can interpret which parts of the input were most influential for the model's decision.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/model-behavior-visualization",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Analyzing which words a transformer model focuses on during machine translation tasks.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Transformers",
      "Attention-based Models",
      "LLMs"
    ],
    "limitations": [
      "[{\"description\": \"Attention weights are not always strongly correlated with importance; focusing solely on attention can be misleading ('attention is not explanation' debate) and only applies to models with attention layers.\"}]"
    ]
  },
  {
    "id": 23,
    "name": "Factor Analysis",
    "description": "Factor analysis is a technique to discover latent variables (factors) that collectively explain the patterns observed in many features. It models each observed feature as a linear combination of a few underlying factors, thereby reducing dimensionality and potentially revealing the hidden structure or themes in the data.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#model-simplification/dimensionality-reduction",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Discovering underlying factors in psychological survey data for social science research.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Assumes linear relationships and normality; results (factors) can be abstract and not directly interpretable, and requires deciding on number of factors and rotation method.\"}]"
    ]
  },
  {
    "id": 24,
    "name": "Principal Component Analysis (PCA)",
    "description": "PCA is a dimensionality reduction method that transforms the original features into a new set of orthogonal components (principal components) ordered by how much variance they explain in the data. By keeping only the top components, PCA provides a simpler representation of the data that captures the most important variance, which can aid visualization or reduce complexity.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/dimensionality-reduction-visualization",
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Visualizing high-dimensional gene expression data in bioinformatics.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Principal components are linear combinations that may not correspond to clear real-world concepts; only captures linear variance and can be affected by scaling of features.\"}]"
    ]
  },
  {
    "id": 25,
    "name": "t-SNE",
    "description": "t-SNE (t-distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction technique that visualizes high-dimensional data in 2D or 3D. It preserves local similarities, meaning points that are close in the original space remain close in the reduced space, making it effective for showing clusters and patterns that might not be visible with linear methods.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/dimensionality-reduction-visualization",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Visualizing clusters in high-dimensional word embeddings.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Results can vary between runs; may distort global structure in favor of local clustering and requires tuning (perplexity, iterations) to avoid misleading patterns.\"}]"
    ]
  },
  {
    "id": 26,
    "name": "UMAP",
    "description": "UMAP is a non-linear dimensionality reduction technique, similar to t-SNE, aimed at visualizing high-dimensional data in 2D or 3D. It tends to preserve both local structure and some global structure of the data better than t-SNE, often producing more interpretable overall layouts of clusters.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/dimensionality-reduction-visualization",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Visualizing patterns in user behavior data for marketing analysis.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Choice of parameters (neighbors, min distance) affects outcome; like t-SNE, it can sometimes be difficult to interpret distances in the reduced space in terms of original features.\"}]"
    ]
  },
  {
    "id": 27,
    "name": "Prototype and Criticism Models",
    "description": "This approach identifies representative examples (prototypes) of the dataset and outlier or hard-to-represent examples (criticisms). Prototypes capture the typical patterns the model or data exhibits, while criticisms highlight where those patterns fail to cover, giving insight into the diversity and exceptions in the data or model behavior.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#example-based-methods/prototype-and-criticism-methods",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Selecting representative customer profiles for targeted marketing.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Determining prototypes/criticisms can be computationally complex for large datasets; results depend on the metric chosen and might not capture all important aspects of data variability.\"}]"
    ]
  },
  {
    "id": 28,
    "name": "Influence Functions",
    "description": "Influence functions estimate how a model's predictions would change if a particular training point were removed. By using a second-order Taylor approximation, they identify which training examples have the most influence (positive or negative) on a given prediction. This helps identify training data points that are most responsible for a particular prediction or for a model's behavior, which can be useful for debugging or understanding model decisions.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#example-based-methods/prototype-and-criticism-methods",
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Debugging model predictions by identifying influential training data points.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires access to and differentiation through the training process; can be intractable for large models due to needing Hessian computations, and may not be accurate if model is highly non-convex.\"}]"
    ]
  },
  {
    "id": 29,
    "name": "Contrastive Explanation Method (CEM)",
    "description": "CEM explains model decisions by producing contrastive examples: it finds the minimal changes to an input that would switch the model's prediction. It outputs pertinent negatives (what could be removed from the input to change the prediction) and pertinent positives (what minimal additional features would be needed to reach the same decision) as a way to highlight what is essential in the input for that prediction.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#example-based-methods/counterfactual-explanations",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Explaining loan rejections by showing what changes would lead to approval.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Solving for pertinent positives/negatives requires iterative optimization per instance; results can be sensitive to parameter settings and might yield unrealistic contrastive inputs if constraints are not tight.\"}]"
    ]
  },
  {
    "id": 31,
    "name": "ANCHOR",
    "description": "ANCHOR generates precision if-then rules as explanations for individual predictions. It finds a minimal set of conditions (on input features) that 'anchor' the prediction, meaning that if those conditions are met, the model will almost always give the same prediction. These anchor rules are designed to be easily understood and highly predictive for that specific instance.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#rule-extraction/decision-rules",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Generating rules to explain individual predictions in text classification.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Only provides explanations for individual predictions (local rules); may struggle with continuous features (usually requires discretization) and might not find an anchor if conditions are too strict.\"}]"
    ]
  },
  {
    "id": 32,
    "name": "RuleFit",
    "description": "RuleFit is a method that creates an interpretable model by combining linear terms with decision rules. It first extracts potential rules from ensemble trees, then builds a sparse linear model where those rules (binary conditions) and original features are used as predictors, with regularization to keep the model simple. The final model is a linear combination of a small set of rules and original features, balancing interpretability with predictive power.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#rule-extraction/decision-rules",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Building interpretable models for predicting customer churn with rule-based explanations.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"The resulting model can still have many rules, complicating interpretability; performance may lag behind black-box models if too few rules are allowed for simplicity.\"}]"
    ]
  },
  {
    "id": 33,
    "name": "Monte Carlo Dropout",
    "description": "Monte Carlo Dropout uses dropout at prediction time to estimate model uncertainty. By running multiple forward passes with random dropout activated and observing the variation in outputs, it provides a distribution of predictions. A consistent prediction across runs indicates high confidence, while widely varying predictions indicate uncertainty.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#uncertainty-and-reliability/confidence-estimation",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Estimating prediction uncertainty in medical diagnosis models.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Neural Networks with Dropout"
    ],
    "limitations": [
      "[{\"description\": \"Only captures model uncertainty, not data uncertainty; requires multiple forward passes and results depend on dropout rate, which must be same as training to be meaningful.\"}]"
    ]
  },
  {
    "id": 34,
    "name": "Out-of-DIstribution detector for Neural networks (ODIN)",
    "description": "ODIN is a technique to detect when an input to a neural network is something very different from what it was trained on (out-of-distribution). It works by scaling the model's outputs (using a temperature parameter) and adding a small perturbation to inputs to better separate the confidence scores for normal vs out-of-distribution examples. By examining the adjusted confidence score, ODIN can flag inputs that are likely far from the training distribution.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#uncertainty-and-reliability/outlier-detection",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Detecting unusual inputs that a classifier wasn't trained on, like identifying when an image recognition model is shown a completely new object class.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Neural Networks"
    ],
    "limitations": [
      "[{\"description\": \"Requires tuning (temperature and perturbation magnitude) which may vary across different out-of-distribution types; may still struggle with adversarial examples or inputs very similar to training data.\"}]"
    ]
  },
  {
    "id": 35,
    "name": "Permutation Tests",
    "description": "Permutation tests assess the significance of an observed result (like model accuracy or feature importance) by comparing it to what would happen purely by chance. This is done by randomly shuffling labels or data many times and calculating the result each time, building a distribution of outcomes under the null hypothesis (no real relationship). If the actual result is far out in the tail of this distribution, it is deemed statistically significant.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#uncertainty-and-reliability/statistical-testing",
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Validating that a feature is truly important by comparing its importance score to what would be expected from random noise.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Computationally expensive, especially for large datasets or models with long inference times; requires many permutations to get reliable p-values for strict significance thresholds.\"}]"
    ]
  },
  {
    "id": 37,
    "name": "Demographic Parity (Disparate Impact Assessment)",
    "description": "Demographic parity requires that prediction rates are the same across different demographic groups—for example, loan approval rates should be equal across racial categories. This metric helps identify algorithmic bias that could perpetuate or amplify social inequalities. By comparing outcome distributions across groups, it provides a straightforward way to detect potentially discriminatory patterns in model predictions.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-assessment/group-fairness-metrics",
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Checking if a job candidate screening algorithm selects candidates from different gender groups at equal rates.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Can force ignoring relevant features that legitimately correlate with protected attributes; may reduce accuracy and can conflict with individual fairness notions.\"}]"
    ]
  },
  {
    "id": 38,
    "name": "Fairness-Aware Data Preprocessing",
    "description": "Fairness-aware preprocessing transforms training data to remove or reduce bias before model training. Techniques include reweighting examples, transforming features to obscure protected attributes, or generating synthetic data with more balanced characteristics. By addressing bias at the data level, these methods can improve fairness across multiple downstream models without requiring model-specific modifications.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-mitigation/data-preprocessing",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Rebalancing a historical hiring dataset to counter past discrimination before training a resume screening model.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"May reduce model accuracy; difficult to identify and remove all sources of bias, and some approaches can introduce new biases or distort important relationships.\"}]"
    ]
  },
  {
    "id": 39,
    "name": "Adversarial Debiasing",
    "description": "Adversarial debiasing trains a model to maximize prediction accuracy while simultaneously preventing an adversarial component from being able to predict sensitive attributes from the model's representations. By learning representations that are predictive of the target but not of protected characteristics, the model's decisions become less influenced by demographic factors, reducing discriminatory outcomes.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-mitigation/inprocessing",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Training a resume screening model that makes hiring recommendations while ensuring representations don't enable prediction of applicants' gender or ethnicity.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Neural Networks"
    ],
    "limitations": [
      "[{\"description\": \"More complex to implement than standard models; may require careful tuning to balance task performance and fairness objectives, and effectiveness depends on adversary quality.\"}]"
    ]
  },
  {
    "id": 40,
    "name": "Counterfactual Fairness",
    "description": "Counterfactual fairness ensures that a model's prediction for an individual would remain the same in a counterfactual world where their sensitive attributes were different. It uses causal modeling to distinguish between legitimate and discriminatory influences of protected characteristics. This approach addresses fairness at an individual level, ensuring people aren't treated differently based solely on demographic factors that should be irrelevant to the decision.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-assessment/individual-fairness-metrics",
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Ensuring a credit scoring model gives the same result for a person regardless of what their race would have been, while still accounting for legitimate factors affected by systemic inequalities.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires specifying causal relationships, which can be subjective; may be mathematically impossible to satisfy along with other fairness definitions, and implementation is complex.\"}]"
    ]
  },
  {
    "id": 41,
    "name": "Fairness Constraints and Regularization",
    "description": "This approach incorporates fairness objectives directly into the model training process by adding constraints or regularization terms. The model learns to balance predictive performance with fairness criteria, such as minimizing disparate treatment across groups. By adjusting how heavily fairness is weighted against accuracy, developers can control the trade-off to meet application-specific needs.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-mitigation/inprocessing",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Training a college admissions prediction model with a fairness constraint that penalizes the model if it gives systematically lower scores to certain demographic groups.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires careful tuning of fairness-accuracy trade-offs; constraints may lead to reduced performance, and optimization can be more complex than standard training.\"}]"
    ]
  },
  {
    "id": 42,
    "name": "Multi-calibration and Multi-accuracy",
    "description": "Multi-calibration ensures that a model's predictions are well-calibrated not just overall, but across all identifiable subgroups and intersections of groups. Similarly, multi-accuracy requires a model to be accurate across all subgroups. These approaches address the problem of models that perform well on average but fail for minority groups, ensuring reliable predictions for everyone regardless of their demographic characteristics.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-mitigation/postprocessing",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Ensuring a medical risk prediction model has accurate confidence scores across all combinations of age, sex, and ethnicity groups, not just for the majority population.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"May require substantial data for reliable calibration across many subgroups; computational cost increases with number of groups considered, and improvement procedure can be complex.\"}]"
    ]
  },
  {
    "id": 43,
    "name": "Individual Fairness Metrics",
    "description": "Individual fairness metrics ensure that similar individuals receive similar outcomes, regardless of group membership. They define a distance function in the feature space and require that individuals who are close according to this distance receive similar predictions. This approach focuses on treating each person fairly rather than just achieving statistical parity between groups, addressing fairness at a more granular level.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-assessment/individual-fairness-metrics",
    "complexity_rating": 4,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Ensuring that job candidates with similar qualifications receive similar scores from a resume screening algorithm, regardless of demographic factors.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires defining an appropriate similarity metric, which can be subjective; can be difficult to satisfy alongside group fairness metrics, and evaluation requires more computation.\"}]"
    ]
  },
  {
    "id": 44,
    "name": "Sensitivity Analysis for Fairness",
    "description": "Sensitivity analysis for fairness examines how model outputs change when sensitive attributes or their correlates are varied. By systematically altering certain inputs and observing prediction changes, it reveals if and how much protected characteristics influence decisions. This approach helps identify potential discrimination even in models that don't explicitly use sensitive attributes but might rely on correlated proxies.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-assessment/causal-analysis",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Testing whether a lending model's decisions change significantly when only the applicant's zip code (which may correlate with race) is altered, while keeping all other factors constant.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"May require domain expertise to identify relevant proxies for sensitive attributes; can be computationally intensive for complex models, and choosing comparison points involves subjective judgment.\"}]"
    ]
  },
  {
    "id": 45,
    "name": "Synthetic Data Generation",
    "description": "Synthetic data generation creates artificial datasets that maintain the statistical properties and patterns of real data without containing actual information from real individuals. By using techniques like differential privacy or generative models, teams can develop, test, and share ML models without exposing sensitive information. This approach balances data utility with privacy protection, enabling collaboration while minimizing re-identification risks.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Privacy"
    ],
    "category_tags": "#data-protection/synthetic-data",
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Creating realistic but synthetic electronic health records for developing and testing medical diagnosis algorithms without exposing real patient data.",
        "goal": "Privacy"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"May not capture all nuances of real data, potentially reducing model performance; generating high-quality synthetic data can be challenging, and some approaches may still leak information.\"}]"
    ]
  },
  {
    "id": 46,
    "name": "Federated Learning",
    "description": "Federated learning trains models across multiple devices or servers without exchanging the underlying data, only sharing model updates. Data remains on the local devices, preserving privacy while still enabling collaborative learning from distributed datasets. By keeping sensitive information local and only sharing model parameters or gradients, organizations can build effective models while respecting privacy boundaries and data sovereignty requirements.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Privacy"
    ],
    "category_tags": "#privacypreserving-training/distributed-learning",
    "complexity_rating": 4,
    "computational_cost_rating": 5,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Developing a smartphone keyboard prediction model by learning from users' typing patterns without their text ever leaving their devices.",
        "goal": "Privacy"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Communication overhead can be significant; heterogeneous client data may lead to training instability, and the approach still has potential privacy vulnerabilities to inference attacks.\"}]"
    ]
  },
  {
    "id": 47,
    "name": "Differential Privacy",
    "description": "Differential privacy adds carefully calibrated noise to data or model outputs to mathematically guarantee that individual data points cannot be identified. It provides a formal privacy framework with a tunable privacy budget (epsilon) that controls the privacy-utility trade-off. By quantifying and limiting information leakage, differential privacy enables robust privacy protection while still extracting valuable insights from sensitive data.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Privacy"
    ],
    "category_tags": "#privacypreserving-training/training-with-privacy-guarantees",
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Adding privacy protection to a census data analysis model to ensure individual households cannot be identified while still obtaining accurate population statistics.",
        "goal": "Privacy"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Adds noise that reduces model accuracy; setting the privacy budget requires careful consideration, and strong privacy guarantees may significantly impact utility.\"}]"
    ]
  },
  {
    "id": 48,
    "name": "Homomorphic Encryption",
    "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Privacy"
    ],
    "category_tags": "#privacypreserving-inference/encrypted-computation",
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Enabling a cloud-based medical diagnosis service to process encrypted patient data and return encrypted results without ever seeing the actual medical information.",
        "goal": "Privacy"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Extremely computationally expensive, often orders of magnitude slower than unencrypted computation; limited operations supported efficiently, and implementation requires cryptographic expertise.\"}]"
    ]
  },
  {
    "id": 49,
    "name": "Prediction Intervals",
    "description": "Prediction intervals provide a range within which a future observation is likely to fall with a specified probability. Unlike single-point predictions, they quantify the uncertainty in a model's predictions, giving upper and lower bounds that account for both the inherent noise in the data and the model's uncertainty. This helps users understand how precise and reliable a prediction really is.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#uncertainty-and-reliability/confidence-estimation",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Providing realistic ranges for sales forecasts to help business planning under uncertainty.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires assumptions about error distribution; can be overconfident if model is miscalibrated, data distribution shifts, or assumptions are violated.\"}]"
    ]
  },
  {
    "id": 50,
    "name": "Quantile Regression",
    "description": "Quantile regression estimates different percentiles of the prediction's conditional distribution rather than just the mean. By modeling multiple quantiles (e.g., 10th, 50th, and 90th percentiles), it provides insights into the full range of possible outcomes and how features differently affect various parts of the outcome distribution. This helps understand prediction uncertainty and how the relationships might vary across different segments of the data.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#uncertainty-and-reliability/conditional-prediction",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Modeling how housing prices vary across different market segments, by estimating how factors affect low-end, median, and luxury properties differently.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Regression Models",
      "Quantile Regression Forests"
    ],
    "limitations": [
      "[{\"description\": \"Can be computationally intensive to fit multiple quantiles; may yield crossing quantiles without constraints, creating logically inconsistent prediction intervals.\"}]"
    ]
  },
  {
    "id": 51,
    "name": "Conformal Prediction",
    "description": "Conformal prediction creates prediction sets that contain the true outcome with a guaranteed coverage probability. Unlike traditional confidence intervals, conformal prediction makes minimal assumptions and works with any model type. It provides rigorous uncertainty quantification by using past prediction errors on similar examples to calibrate the size of prediction intervals, ensuring they have the specified coverage rate (e.g., 95% of intervals contain the true value).",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#uncertainty-and-reliability/distributionfree-prediction",
    "complexity_rating": 4,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Creating prediction intervals for medical test results that have a provable 95% coverage rate, regardless of the underlying model complexity.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Intervals can be unnecessarily wide if nonconformity scores vary greatly across the data; requires a held-out calibration set which reduces data available for training.\"}]"
    ]
  },
  {
    "id": 52,
    "name": "Empirical Calibration",
    "description": "Empirical calibration adjusts a model's predicted probabilities to match observed frequencies. For example, if events predicted with 80% confidence only occur 60% of the time, calibration would correct this overconfidence. Common techniques include Platt scaling and isotonic regression, which learn transformations that map the model's raw scores to well-calibrated probabilities, improving the reliability of confidence measures for downstream decisions.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#uncertainty-and-reliability/probability-calibration",
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Adjusting a credit default prediction model's probabilities to ensure that loan applicants with a predicted 30% default risk actually default 30% of the time, improving decision-making.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires held-out calibration data; calibration can degrade over time if data distribution shifts, and might sacrifice discrimination power for calibration in some cases.\"}]"
    ]
  },
  {
    "id": 53,
    "name": "Temperature Scaling",
    "description": "Temperature scaling is a simple but effective calibration method for neural networks that divides the logits (pre-softmax outputs) by a single scalar parameter called temperature. This parameter is optimized on a validation set to minimize calibration error. Higher temperatures smooth out confidence, making the model less overconfident. It preserves the model's accuracy while improving its calibration, ensuring probability estimates better reflect true likelihoods.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#uncertainty-and-reliability/probability-calibration",
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Adjusting a deep learning image classifier's confidence scores to be realistic, ensuring that when it's 90% confident, it's right 90% of the time.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Neural Networks",
      "Classification Models"
    ],
    "limitations": [
      "[{\"description\": \"Only addresses calibration at the dataset level; doesn't fix miscalibration that varies across subgroups or feature values, and does not improve the rank ordering of predictions.\"}]"
    ]
  },
  {
    "id": 54,
    "name": "Deep Ensembles",
    "description": "Deep ensembles combine predictions from multiple neural networks trained independently with different random initializations. By capturing model uncertainty through the diversity of the ensemble's predictions, they provide more reliable uncertainty estimates and better out-of-distribution detection than single models. The disagreement between ensemble members naturally indicates prediction uncertainty, improving both accuracy and calibration.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#uncertainty-and-reliability/ensemble-methods",
    "complexity_rating": 3,
    "computational_cost_rating": 5,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Improving self-driving car safety by using multiple neural networks to detect obstacles, where disagreement between models signals uncertainty and triggers extra caution.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Neural Networks"
    ],
    "limitations": [
      "[{\"description\": \"Computationally expensive to train and deploy multiple complete models; may still provide overconfident predictions for inputs far from the training distribution.\"}]"
    ]
  },
  {
    "id": 55,
    "name": "Bootstrapping",
    "description": "Bootstrapping estimates uncertainty by resampling data with replacement many times, training a model on each sample, and analyzing the variation in predictions. This approach provides confidence intervals and stability measures without making strong statistical assumptions. By showing how predictions change with different data samples, it reveals how sensitive the model is to the specific training examples it sees.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#uncertainty-and-reliability/resampling-methods",
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Estimating uncertainty in financial risk models by resampling historical data to understand how predictions might vary under different historical scenarios.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Computationally expensive to train multiple models; does not account for uncertainty in model structure or for systematically missing data patterns.\"}]"
    ]
  },
  {
    "id": 56,
    "name": "Jackknife Resampling",
    "description": "Jackknife resampling assesses model stability by systematically leaving out one (or a few) data points at a time and retraining the model. This approach reveals how individual points influence results and provides an estimate of the standard error. By examining the distribution of predictions across these leave-one-out models, it identifies unusually influential points and characterizes prediction uncertainty.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#uncertainty-and-reliability/resampling-methods",
    "complexity_rating": 3,
    "computational_cost_rating": 5,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Evaluating how removing individual countries from a global climate model affects predictions, to identify which regions have outsized influence on the results.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Extremely computationally intensive for large datasets, as it requires training n models for n data points; may underestimate uncertainty compared to other methods.\"}]"
    ]
  },
  {
    "id": 57,
    "name": "Cross-validation",
    "description": "Cross-validation evaluates model performance and stability by partitioning data into multiple subsets. The model is trained and tested repeatedly on different train-test splits, revealing how performance varies across different subsamples of data. This provides a realistic assessment of how well the model will generalize to new data and helps quantify prediction uncertainty, making it a fundamental technique for reliable model evaluation.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#performance-assessment/validation-methods",
    "complexity_rating": 2,
    "computational_cost_rating": 3,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Using 10-fold cross-validation to estimate a healthcare prediction model's true accuracy and assess if performance is consistent across different patient subgroups.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Time-consuming for large datasets or complex models; standard cross-validation can give optimistic estimates if there are dependencies in the data (e.g., time series).\"}]"
    ]
  },
  {
    "id": 58,
    "name": "Statistical Hypothesis Testing",
    "description": "Statistical hypothesis testing systematically evaluates whether observed effects or differences could have occurred by chance. By calculating test statistics and p-values, it determines if results are statistically significant given a null hypothesis. This approach brings rigor to model evaluation and comparison, helping to distinguish real patterns from random fluctuations when assessing model performance or feature importance.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#performance-assessment/significance-testing",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Determining if one machine learning model significantly outperforms another, or if the observed performance difference could be due to chance.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Vulnerable to p-hacking and multiple testing problems; significance doesn't always imply practical importance, and test validity depends on distribution assumptions.\"}]"
    ]
  },
  {
    "id": 59,
    "name": "Area Under Precision-Recall Curve (AUPRC)",
    "description": "AUPRC measures model performance by plotting precision against recall at various classification thresholds and calculating the area under the resulting curve. Unlike accuracy or AUC-ROC, it's particularly valuable for imbalanced datasets where the minority class is of primary interest. By focusing on the trade-off between precision and recall, it provides a more informative assessment for use cases where false positives and false negatives have different costs.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#performance-assessment/metric-selection",
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Evaluating fraud detection models where genuine transactions far outnumber fraudulent ones, to ensure fraud is caught without overwhelming reviewers with false alarms.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"More sensitive to class distribution than ROC curves; comparing models across datasets with different class balances can be misleading.\"}]"
    ]
  },
  {
    "id": 60,
    "name": "Precision Metrics for High-Risk Domains",
    "description": "In high-risk domains, specialized metrics focus on worst-case performance and extreme error cases rather than averages. These include metrics like maximum error, 99th percentile error, and failure rate above critical thresholds. By explicitly measuring performance in the most challenging situations, these metrics ensure models meet safety requirements and help identify potentially catastrophic failure modes.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Reliability"
    ],
    "category_tags": "#performance-assessment/edge-case-evaluation",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Evaluating an autonomous vehicle's detection system by measuring its worst-case performance in challenging visibility conditions, rather than average-case performance.",
        "goal": "Reliability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"May require more test cases and data to accurately estimate tail performance; optimizing for extreme cases can sometimes harm average performance.\"}]"
    ]
  },
  {
    "id": 61,
    "name": "Internal Review Boards",
    "description": "Internal review boards evaluate ML projects for ethical and safety concerns before development or deployment. Comprised of cross-functional experts (technical, legal, ethics, domain specialists), they review use cases, potential harms, mitigation strategies, and monitoring plans. By formalizing ethical review processes, they help organizations identify risks early and ensure responsible AI development practices.",
    "model_dependency": "Organizational",
    "assurance_goals": [
      "Safety"
    ],
    "category_tags": "#governance-mechanisms/review-processes",
    "complexity_rating": 4,
    "computational_cost_rating": 1,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Reviewing a proposed criminal risk assessment tool to evaluate potential discriminatory impacts and privacy implications before development begins.",
        "goal": "Safety"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Can slow development timelines; effectiveness depends on board composition and authority, and organizations may face pressure to approve revenue-generating projects.\"}]"
    ]
  },
  {
    "id": 62,
    "name": "Red Teaming",
    "description": "Red teaming involves dedicated adversarial testing of ML systems by specialists who try to find flaws, vulnerabilities, harmful outputs, or ways to circumvent safety measures. Drawing on security practices, red teams probe systems from multiple angles, including prompt engineering, adversarial examples, and edge case testing. This approach reveals non-obvious risks before deployment and helps build more robust, safer AI systems.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Safety"
    ],
    "category_tags": "#testing-methods/adversarial-testing",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Having a specialized team attempt to make a content moderation AI generate harmful outputs by using creative prompting techniques.",
        "goal": "Safety"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires specialized expertise and significant resources; can only find issues that testers think to look for, and systems may remain vulnerable to novel attack types.\"}]"
    ]
  },
  {
    "id": 63,
    "name": "Anomaly Detection",
    "description": "Anomaly detection identifies unusual behaviors, inputs, or outputs that deviate significantly from normal patterns. Applied to ML systems, it can flag unexpected model predictions, suspicious inputs, or drift in behavior. By continuously monitoring for anomalies in production, organizations can catch potential issues early, investigate causes, and prevent harm from system misuse or malfunction.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Safety"
    ],
    "category_tags": "#monitoring-methods/runtime-monitoring",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Detecting unusual patterns of API calls to a machine translation service that might indicate attempts to extract harmful outputs or attack the model.",
        "goal": "Safety"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Setting appropriate thresholds is challenging; may generate false alarms in legitimate edge cases, and novel anomalies might not match patterns the system is trained to detect.\"}]"
    ]
  },
  {
    "id": 64,
    "name": "Human-in-the-Loop Safeguards",
    "description": "Human-in-the-loop safeguards integrate human oversight into model operations, requiring human review of high-risk or uncertain predictions before actions are taken. This approach combines model efficiency with human judgment for sensitive decisions. By designating appropriate intervention points and routing challenging cases to human experts, organizations can better manage the risks of automated decisions while still benefiting from ML capabilities.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Safety"
    ],
    "category_tags": "#operational-procedures/human-oversight",
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Requiring human moderator approval before taking down content that an AI system has flagged for potential policy violations.",
        "goal": "Safety"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Scales poorly with high request volumes; introduces latency into the decision process, and humans may experience fatigue or defer too easily to automation.\"}]"
    ]
  },
  {
    "id": 65,
    "name": "Confidence Thresholding",
    "description": "Confidence thresholding routes predictions to different handling paths based on the model's confidence level. High-confidence predictions proceed automatically, while low-confidence cases receive extra scrutiny, human review, or fallback handling. By establishing appropriate thresholds for different risk levels, organizations can balance automation benefits with prudent safeguards, ensuring greater oversight where uncertainty is high.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Safety"
    ],
    "category_tags": "#operational-procedures/filtering-pipeline",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Only allowing a medical diagnosis system to make recommendations automatically when its confidence exceeds 95%, routing less certain cases to human physicians.",
        "goal": "Safety"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Many models have poorly calibrated confidence scores; threshold selection can be challenging, and some risky predictions may still have high confidence.\"}]"
    ]
  },
  {
    "id": 66,
    "name": "Runtime Monitoring and Circuit Breakers",
    "description": "Runtime monitoring tracks key system metrics and prediction patterns in production, with automated circuit breakers that can throttle, disable, or revert ML systems when anomalies exceed thresholds. This approach provides real-time protection against unexpected behavior. By continuously monitoring inputs, outputs, and system health, organizations can quickly detect and respond to potential issues before they cause significant harm.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Safety"
    ],
    "category_tags": "#monitoring-methods/runtime-monitoring",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Automatically disabling an automated trading system if it starts making a volume of trades that exceeds historical patterns by a large margin.",
        "goal": "Safety"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Setting appropriate thresholds requires careful calibration; false alarms can disrupt service unnecessarily, and some harmful behaviors may still fall within monitored metrics ranges.\"}]"
    ]
  },
  {
    "id": 67,
    "name": "Model Cards",
    "description": "Model cards are standardized documentation templates that provide essential information about ML models, including their intended uses, performance metrics across different conditions and demographic groups, training data characteristics, and known limitations. By creating transparency about a model's behavior and appropriate contexts, they help prevent misuse and enable users to make informed decisions about when and how to apply the model.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#documentation-standards/model-governance",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Creating comprehensive documentation for a facial recognition API that clearly describes performance differences across skin tones and lighting conditions.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Creating thorough model cards requires significant effort; information may become outdated as models are updated, and some organizations may provide incomplete information.\"}]"
    ]
  },
  {
    "id": 68,
    "name": "Datasheets for Datasets",
    "description": "Datasheets for datasets document a dataset's creation, composition, intended uses, and maintenance. They include information about collection methods, preprocessing steps, recommended uses, potential biases, and legal/ethical considerations. By providing this context, datasheets improve transparency, help users make informed decisions about dataset suitability, and encourage dataset creators to reflect on responsible practices throughout the data lifecycle.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#documentation-standards/model-governance",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Creating comprehensive documentation for a public facial image dataset, detailing consent procedures, demographic representation, and appropriate use guidelines.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires time and effort to create and maintain; information may become outdated if the dataset evolves, and there's no enforced standard for completeness.\"}]"
    ]
  },
  {
    "id": 69,
    "name": "System Documentation Templates",
    "description": "System documentation templates provide standardized frameworks for describing ML systems beyond just models and datasets. They capture information about infrastructure, deployment environments, monitoring systems, failure modes, and maintenance procedures. By ensuring comprehensive documentation of the entire ML pipeline, they support better governance, reproducibility, and safety, serving both technical and non-technical stakeholders.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#documentation-standards/model-governance",
    "complexity_rating": 3,
    "computational_cost_rating": 1,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Documenting an automated trading system's components, monitoring protocols, and emergency shutdown procedures to support operational safety and regulatory compliance.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Maintaining comprehensive documentation can be time-consuming; different stakeholders may require different levels of detail, and templates may not fit all types of ML systems equally well.\"}]"
    ]
  },
  {
    "id": 70,
    "name": "ML System Lineage",
    "description": "ML system lineage tracks the complete history of models, datasets, and experiments through a system's lifecycle. It records which datasets were used to train each model version, what hyperparameters were set, who approved changes, and when models were deployed. By maintaining this audit trail, organizations can reproduce past results, track sources of performance issues, and demonstrate regulatory compliance when needed.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#documentation-standards/versioning-and-provenance",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Maintaining complete records of model versions and datasets for a medical diagnosis system to support regulatory audits and trace the origin of any prediction issues.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Implementing comprehensive lineage tracking requires dedicated infrastructure; can generate large volumes of metadata, and requires discipline from all team members to maintain.\"}]"
    ]
  },
  {
    "id": 71,
    "name": "Automated Documentation Generation",
    "description": "Automated documentation generation extracts information directly from code, models, and data pipelines to create and maintain up-to-date documentation. It can capture model architectures, data schemas, feature importance, performance metrics, and lineage information without manual writing. By reducing the burden of documentation maintenance, it helps teams keep comprehensive records that remain accurate as systems evolve.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#documentation-standards/documentation-automation",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Automatically generating model cards each time a new model version is trained, with updated performance metrics and data statistics.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"May miss context and insights that human documentation would include; quality depends on instrumentation comprehensiveness, and unstructured information is harder to capture.\"}]"
    ]
  },
  {
    "id": 72,
    "name": "Model Distillation",
    "description": "Model distillation compresses a large, complex model (the teacher) into a smaller, simpler model (the student) that approximates the original's behavior. The student model learns from the teacher's outputs rather than the raw data. This technique makes models more interpretable, deployable, and efficient while preserving most of the original performance. It helps balance the benefits of complex models with the practical requirements of responsible deployment.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#model-simplification/knowledge-distillation",
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Compressing a large computer vision model with billions of parameters into a smaller model that can run on mobile devices while being easier to inspect.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Neural Networks"
    ],
    "limitations": [
      "[{\"description\": \"Student models typically lose some performance compared to teacher models; the distillation process may still produce black-box models if not combined with interpretable architectures.\"}]"
    ]
  },
  {
    "id": 73,
    "name": "Model Extraction",
    "description": "Model extraction creates a simpler, interpretable model (like a decision tree) that approximates a complex black-box model's behavior. Unlike distillation which transfers knowledge during training, extraction works with already-trained models by analyzing their inputs and outputs. This technique helps understand what patterns a black-box model has learned and provides explanations that stakeholders can understand without technical expertise.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#model-simplification/posthoc-approximation",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Converting a complex neural network credit score model into a set of human-readable rules to explain to regulators and applicants how decisions are made.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Extracted models may not faithfully represent the original if it's highly complex; accuracy and fidelity trade off against simplicity, and training data for extraction may not be diverse enough.\"}]"
    ]
  },
  {
    "id": 74,
    "name": "Feature Importance Ranking",
    "description": "Feature importance ranking identifies and orders the most influential features in a model's predictions, typically using techniques like permutation importance, SHAP values, or built-in model metrics. By quantifying each feature's impact, it provides transparency into what factors drive the model's decisions. This helps stakeholders understand the model's focus and can reveal whether it relies on reasonable factors or potentially problematic ones.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#feature-analysis/importance-ranking",
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Revealing that customer purchase history and product views are the top features in an e-commerce recommendation system, rather than demographic information.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Different importance metrics can yield conflicting rankings; importance doesn't reveal feature interactions, and global rankings may not explain individual predictions.\"}]"
    ]
  },
  {
    "id": 75,
    "name": "Monotonicity Constraints",
    "description": "Monotonicity constraints ensure that a model's predictions always increase (or decrease) as a specific feature increases, enforcing a consistent directional relationship. For instance, income can only positively affect loan approval chances. This technique makes models more intuitive and transparent, as users can understand how changing a particular input will affect the output, without unexpected reversals or non-linear behaviors.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#interpretable-design/constraint-based-models",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [],
    "example_use_cases": [
      {
        "description": "Ensuring a credit scoring model always treats higher income as a positive factor (or at least never as a negative factor) for creditworthiness assessment.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"May reduce model accuracy if true relationships are non-monotonic; requires domain knowledge to identify which features should be monotonic, and increases training complexity.\"}]"
    ]
  },
  {
    "id": 76,
    "name": "Decision Trees and Rule Lists",
    "description": "Decision trees and rule lists create models that make predictions through a series of interpretable if-then rules arranged in a flowchart-like structure. Each decision point is based on a clear condition about a feature, and the path to any prediction can be traced and explained in natural language. These inherently transparent models allow stakeholders to understand exactly how inputs lead to outputs.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#interpretable-design/inherently-interpretable-models",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Building a loan approval system using a decision tree with clear rules based on income, credit history, and debt ratio that can be explained to applicants.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Decision Trees",
      "Rule-based Models"
    ],
    "limitations": [
      "[{\"description\": \"Often less accurate than complex models for difficult problems; deep trees can still be hard to interpret, and training can be unstable with small data changes.\"}]"
    ]
  },
  {
    "id": 77,
    "name": "Linear/Logistic Models with Few Features",
    "description": "Linear and logistic regression models with a small number of carefully selected features provide transparent predictions through simple, weighted combinations of inputs. Each coefficient represents a feature's impact, and the entire model can be expressed as a single equation. By prioritizing simplicity and selecting the most important features, these models balance interpretability with adequate performance for many applications.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#interpretable-design/inherently-interpretable-models",
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Using a simple logistic regression with 5-10 key features for a medical screening test, where clinicians need to understand and explain the factors behind each risk assessment.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Linear Regression",
      "Logistic Regression"
    ],
    "limitations": [
      "[{\"description\": \"May miss complex nonlinear relationships or interactions; predictive performance often lower than more complex models for difficult problems.\"}]"
    ]
  },
  {
    "id": 78,
    "name": "Generalized Additive Models (GAMs)",
    "description": "GAMs extend linear models by allowing flexible, nonlinear relationships between individual features and the target while maintaining the additive structure that keeps them interpretable. Each feature's effect is modeled separately as a smooth function, visualized as a curve showing how the feature influences predictions across its range. This approach balances the transparency of linear models with the ability to capture more complex patterns.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#interpretable-design/inherently-interpretable-models",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Predicting hospital readmission risk with a GAM that shows how the risk varies nonlinearly with patient age while still keeping the model transparent enough for clinical use.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [
      "GAMs"
    ],
    "limitations": [
      "[{\"description\": \"Cannot capture complex interactions between features unless explicitly modeled; setup requires deciding which features need nonlinear treatment, and fitting process is more complex than linear models.\"}]"
    ]
  },
  {
    "id": 79,
    "name": "Naive Bayes and Probabilistic Models",
    "description": "Naive Bayes and related probabilistic models make predictions based on Bayes' theorem and conditional probabilities, with an assumption that features are conditionally independent. Their transparency comes from the straightforward probabilistic reasoning, where each feature's contribution to the prediction is clearly defined as a likelihood. By explicitly modeling the probability of outcomes given evidence, these models provide intuitive explanations for how inputs affect predictions.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Transparency"
    ],
    "category_tags": "#interpretable-design/inherently-interpretable-models",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Explanatory Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Using a Naive Bayes classifier for email spam detection that can explain its decisions by showing which words increased the probability of the spam classification.",
        "goal": "Transparency"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Naive Bayes",
      "Probabilistic Classifiers"
    ],
    "limitations": [
      "[{\"description\": \"Independence assumption often violated in real data, leading to miscalibrated probabilities; performance typically lower than more complex models for tasks with intricate patterns.\"}]"
    ]
  },
  {
    "id": 80,
    "name": "Variable Importance in Random Forests (MDA, MDG)",
    "description": "Calculates feature importance by measuring the Mean Decrease Accuracy or Mean Decrease Gini when a feature is excluded from Random Forest models.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Identifying key predictors in a Random Forest model for credit scoring.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Random Forests"
    ],
    "limitations": [
      "[{\"description\": \"Permutation importance (MDA) shares limitations with correlated features; Gini importance (MDG) biases towards continuous or high-cardinality features.\"}]"
    ]
  },
  {
    "id": 81,
    "name": "Bayesian Networks",
    "description": "Probabilistic graphical models representing variables and their conditional dependencies for causal reasoning.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#example-based-methods/causal-analysis",
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Modeling causal relationships in gene regulatory networks.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Bayesian Networks",
      "Probabilistic Graphical Models"
    ],
    "limitations": [
      "[{\"description\": \"Learning the network structure from data is NP-hard for many variables; causal conclusions require correct model specification and may be invalid if important variables are omitted.\"}]"
    ]
  },
  {
    "id": 82,
    "name": "Fairness Metrics (e.g., Equalized Odds, Demographic Parity)",
    "description": "Evaluates models for fairness by measuring disparities in predictions across different demographic groups.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#fairness-explanations/bias-detection-and-mitigation",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Ensuring a hiring model does not discriminate based on gender or ethnicity.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Each metric addresses a specific notion of fairness and may conflict with others; they require true outcome labels for evaluation and do not directly tell how to fix bias.\"}]"
    ]
  },
  {
    "id": 83,
    "name": "Model Pruning",
    "description": "Simplifies neural networks by removing less important weights or neurons, reducing complexity while retaining performance.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#model-simplification/model-pruning",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Reducing model size for deployment on mobile devices without significant loss in accuracy.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Neural Networks"
    ],
    "limitations": [
      "[{\"description\": \"Over-pruning can significantly reduce accuracy; finding the right pruning threshold is trial-and-error, and pruned models may still be complex to interpret if remaining structure is not simple.\"}]"
    ]
  },
  {
    "id": 84,
    "name": "Knowledge Distillation",
    "description": "Trains a simpler 'student' model to replicate the behavior of a complex 'teacher' model, resulting in a more interpretable model.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#model-simplification/model-distillation",
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Scope",
        "value": "Global"
      }
    ],
    "example_use_cases": [
      {
        "description": "Simplifying a deep neural network for faster inference in real-time applications.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Student model performance depends on teacher quality and training technique; distilled model might still be a black box (though simpler) and requires additional training data or time.\"}]"
    ]
  },
  {
    "id": 85,
    "name": "Attention Visualisation in Transformers",
    "description": "Visualizes attention weights in transformer-based models to show how the model focuses on different input parts during prediction.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#visualization-techniques/model-behavior-visualization",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Understanding which words a transformer model focuses on during machine translation tasks.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Transformers",
      "LLMs"
    ],
    "limitations": [
      "[{\"description\": \"Similar to other attention interpretations: not always clear if high attention means importance; only applicable to transformer-based models and doesn't explain the model's reasoning beyond attention weights.\"}]"
    ]
  },
  {
    "id": 86,
    "name": "Neuron Activation Analysis",
    "description": "Analyzes activation patterns of neurons in large language models (LLMs) to interpret their roles and the concepts they represent.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Scope",
        "value": "Global"
      },
      {
        "type": "Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Identifying neurons responsible for syntax or semantics in language models.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "LLMs",
      "Neural Networks"
    ],
    "limitations": [
      "[{\"description\": \"Interpreting individual neurons requires analyzing large numbers of activations; insights are often qualitative, and important behavior may be distributed across many neurons rather than single ones.\"}]"
    ]
  },
  {
    "id": 87,
    "name": "Prompt Sensitivity Analysis",
    "description": "Studies how variations in input prompts affect LLM outputs to understand model behavior and sensitivity.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#example-based-methods/prototype-and-criticism-methods",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Evaluating how different phrasings influence an LLM's answers in question-answering tasks.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "LLMs"
    ],
    "limitations": [
      "[{\"description\": \"Only surfaces sensitivity to tested prompt variations; may not cover all aspects of model behavior, and systematic prompt generation can be time-consuming or incomplete.\"}]"
    ]
  },
  {
    "id": 88,
    "name": "Causal Mediation Analysis in Language Models",
    "description": "Investigates causal relationships within LLMs by assessing how interventions on specific components affect outputs.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#example-based-methods/causal-analysis",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Scope",
        "value": "Global"
      },
      {
        "type": "Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Understanding how adjusting embeddings changes model responses in language generation tasks.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "LLMs"
    ],
    "limitations": [
      "[{\"description\": \"Requires defining interventions on internal components, which needs expert knowledge of model architecture; results depend on correctness of causal assumptions and can be challenging to interpret conclusively.\"}]"
    ]
  },
  {
    "id": 89,
    "name": "Feature Attribution with Integrated Gradients in NLP",
    "description": "Applies Integrated Gradients to attribute importance of input tokens in LLMs for specific predictions, often producing visualizations.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Identifying words influencing text sentiment classification or topic modeling.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "LLMs",
      "NLP Models"
    ],
    "limitations": [
      "[{\"description\": \"Long text inputs mean integrating over many steps which is slow; attributions can be diffuse across many tokens, and choosing a neutral baseline (e.g., empty or padding text) is non-trivial.\"}]"
    ]
  },
  {
    "id": 90,
    "name": "Concept Activation Vectors (CAVs)",
    "description": "Represents human-understandable concepts as vectors in the model's latent space to analyze their influence on predictions.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#feature-analysis/importance-and-attribution",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Scope",
        "value": "Global"
      },
      {
        "type": "Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Assessing how concepts like 'negativity' affect language model outputs.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Relies on having clearly defined concept examples; concept directions might not exist clearly in the model's internal space, and one must choose which layer to examine, affecting results.\"}]"
    ]
  },
  {
    "id": 91,
    "name": "In-Context Learning Analysis",
    "description": "Examines how LLMs learn from examples provided in the input prompt, revealing capacity for few-shot learning.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Explainability"
    ],
    "category_tags": "#example-based-methods/prototype-and-criticism-methods",
    "complexity_rating": 4,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Scope",
        "value": "Local"
      }
    ],
    "example_use_cases": [
      {
        "description": "Analyzing the effect of examples on an LLM's ability to perform a new task like translation.",
        "goal": "Explainability"
      }
    ],
    "resources": [],
    "applicable_models": [
      "LLMs"
    ],
    "limitations": [
      "[{\"description\": \"Observational analysis without guaranteed causal insight; any findings can be specific to the tested tasks or prompts, making general conclusions about model behavior difficult.\"}]"
    ]
  },
  {
    "id": 92,
    "name": "Reweighing",
    "description": "Assigns weights to instances in the training data to ensure different groups are equally represented in all labels.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#preprocessing-techniques/data-transformation",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Balancing gender representation in credit approval datasets before training a classifier.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Does not remove bias present in features; simply changing weights can lead to higher variance if some groups are underrepresented, and it assumes labels are unbiased which might not hold.\"}]"
    ]
  },
  {
    "id": 93,
    "name": "Disparate Impact Remover",
    "description": "Edits feature values to reduce dependence between features and protected attributes, aiming to mitigate disparate impact.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#preprocessing-techniques/data-transformation",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Adjusting salary features to reduce gender bias in income prediction models.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Altering features could reduce model accuracy if important information is removed; addresses only measured attributes and might not eliminate bias through proxies.\"}]"
    ]
  },
  {
    "id": 94,
    "name": "Learning Fair Representations",
    "description": "Learns latent representations that encode data well but obfuscate information about protected attributes.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/fair-representation-learning",
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Creating unbiased data representations for hiring algorithms.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires training a complex model (encoder) with adversarial or constrained objectives; balancing reconstruction and fairness is tricky and may lead to loss of useful information.\"}]"
    ]
  },
  {
    "id": 95,
    "name": "Fairness GAN",
    "description": "Employs Generative Adversarial Networks to generate fair representations of data that obfuscate protected attributes.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/fair-representation-learning",
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Creating unbiased datasets for training fair image recognition models.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "GANs"
    ],
    "limitations": [
      "[{\"description\": \"GAN training is notoriously difficult to stabilize; ensuring fairness might come at the cost of utility, and it needs a large dataset to train the generator and discriminator effectively.\"}]"
    ]
  },
  {
    "id": 96,
    "name": "Optimised Pre-Processing",
    "description": "Modifies training data features and labels to induce fairness while preserving data utility.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#preprocessing-techniques/data-transformation",
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Adjusting criminal justice data to reduce racial bias before training models.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"May involve solving complex optimization problems; could distort data relationships in ways that degrade model performance if not carefully calibrated to preserve utility.\"}]"
    ]
  },
  {
    "id": 97,
    "name": "Relabelling",
    "description": "Changes labels of certain instances in training data to reduce bias, often based on fairness constraints.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#preprocessing-techniques/data-transformation",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Modifying labels in loan default datasets to mitigate historical biases.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Altering labels risks introducing new biases or reducing prediction accuracy; deciding which instances to relabel can require a fairness criterion and ground truth fairness assessment.\"}]"
    ]
  },
  {
    "id": 98,
    "name": "Preferential Sampling",
    "description": "Re-samples data with preference for certain groups to achieve fair representation in training datasets.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#preprocessing-techniques/data-transformation",
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Oversampling minority groups in medical data to train unbiased models.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Over-sampling can cause overfitting to minority examples; under-sampling can drop important data from majority group, and it doesn't adjust the model if it inherently treats groups differently.\"}]"
    ]
  },
  {
    "id": 99,
    "name": "Fairness Through Unawareness",
    "description": "Ensures the model does not use protected attributes in decisions; however, indirect bias may persist.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#preprocessing-techniques/data-transformation",
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Removing gender as a feature in employee promotion predictions.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Ignoring protected attributes doesn't guarantee fairness; proxies for the protected attribute in other features can still lead to biased outcomes.\"}]"
    ]
  },
  {
    "id": 100,
    "name": "Adversarial Debiasing for Text",
    "description": "Applies adversarial debiasing techniques specifically to textual data to mitigate biases in language models.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/adversarial-debiasing",
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Reducing gender bias in sentiment analysis models by adversarial training on text data.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "NLP Models",
      "LLMs"
    ],
    "limitations": [
      "[{\"description\": \"Text data can carry subtle biases in language; adversarial removal of bias might strip out important linguistic context, and the technique inherits all challenges of adversarial training.\"}]"
    ]
  },
  {
    "id": 101,
    "name": "Fair Adversarial Networks",
    "description": "Extends adversarial debiasing by incorporating fairness into deep learning via adversarial training.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/adversarial-debiasing",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Reducing bias in facial recognition systems with adversarial networks.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Neural Networks"
    ],
    "limitations": [
      "[{\"description\": \"Extending adversarial debiasing to deep networks can be very complex to implement; requires careful tuning of loss trade-offs, and interpretations of fairness improvement can be opaque.\"}]"
    ]
  },
  {
    "id": 102,
    "name": "Prejudice Remover Regulariser",
    "description": "Incorporates a fairness penalty into the learning objective to penalize models that encode biases with respect to protected attributes.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/fairnessconstrained-optimisation",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Training logistic regression models with fairness constraints for university admissions.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Logistic Regression",
      "Other Regularizable Models"
    ],
    "limitations": [
      "[{\"description\": \"Requires setting a hyperparameter for fairness penalty; too high can severely hurt accuracy, too low has little effect. Only applicable to models that can incorporate such a regularizer (e.g., logistic regression).\"}]"
    ]
  },
  {
    "id": 103,
    "name": "Meta Fair Classifier",
    "description": "Modifies any classifier to optimize for fairness metrics using a meta-learning algorithm.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/fairnessconstrained-optimisation",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Applying fairness optimization to models in employee evaluation systems.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Meta-learning approach can be complex to implement and require extensive hyperparameter tuning; may result in longer training times and complexity that makes the method less accessible.\"}]"
    ]
  },
  {
    "id": 104,
    "name": "Exponentiated Gradient Reduction",
    "description": "Formulates fairness as a constrained optimization problem, using exponentiated gradient methods to find optimal classifiers.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/fairnessconstrained-optimisation",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Training fair classifiers for employment screening processes.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Involves iterative retraining of a classifier with adjusted weights; might require a convex base learner for theoretical guarantees, and can be sensitive to convergence criteria.\"}]"
    ]
  },
  {
    "id": 105,
    "name": "Fair Transfer Learning",
    "description": "Adapts models trained on one domain to another while preserving fairness constraints across domains.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/fair-representation-learning",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Transferring fairness-aware models from one region's data to another in healthcare analytics.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Transfer Learning Models",
      "Fine-tuned Models"
    ],
    "limitations": [
      "[{\"description\": \"Fairness achieved in one domain might not directly translate to another if data distributions differ; approach can be complicated to design and may need careful tuning to preserve fairness across domains.\"}]"
    ]
  },
  {
    "id": 106,
    "name": "Adaptive Sensitive Reweighting",
    "description": "Dynamically adjusts weights during training based on model performance across different groups.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/fairnessconstrained-optimisation",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Balancing performance in speech recognition across accents and dialects.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires monitoring model performance across groups in training, which can introduce instability; if not tuned properly, could oscillate or focus too much on one group at a time.\"}]"
    ]
  },
  {
    "id": 107,
    "name": "Multi-Accuracy Boosting",
    "description": "Improves accuracy uniformly across groups by correcting errors where the model performs poorly for certain groups.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/fairnessconstrained-optimisation",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Enhancing model performance for underrepresented groups in disease prediction.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Targets error patterns in subgroups which requires identifying those groups or error regions; could increase complexity of the model and may overfit if very granular corrections are made.\"}]"
    ]
  },
  {
    "id": 108,
    "name": "Equalised Odds Post-Processing",
    "description": "Adjusts output probabilities to equalise true positive and false positive rates across groups.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#postprocessing-techniques/outcome-adjustment",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Ensuring fairness in recidivism risk assessments used in judicial decisions.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Adjusting outputs can reduce model confidence or require randomization in decisions; may sacrifice individual consistency (similar cases get different outcomes to satisfy group rates).\"}]"
    ]
  },
  {
    "id": 109,
    "name": "Threshold Optimiser",
    "description": "Adjusts decision thresholds for different groups to satisfy fairness constraints post-training.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#postprocessing-techniques/outcome-adjustment",
    "complexity_rating": 3,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Ensuring equal acceptance rates in college admissions across demographics.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires a held-out set to determine thresholds per group; if distributions shift, thresholds may need recalibration. Also, using different thresholds per group can raise legal or ethical concerns in deployment.\"}]"
    ]
  },
  {
    "id": 110,
    "name": "Reject Option Classification",
    "description": "Changes decisions where the model is least certain, favoring the disadvantaged group within this uncertain region.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#postprocessing-techniques/outcome-adjustment",
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Mitigating bias in hiring decisions by adjusting uncertain predictions.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Only applicable when model uncertainty can be estimated; choosing the 'reject' region and how to reassign decisions can be subjective and might reject too many instances if tuned conservatively.\"}]"
    ]
  },
  {
    "id": 111,
    "name": "Calibration with Equality of Opportunity",
    "description": "Adjusts probabilities to achieve equal true positive rates across groups while maintaining calibration within each group.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#postprocessing-techniques/calibration-methods",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Balancing opportunity in credit scoring across different ethnic groups.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Achieving calibration within each group while equalizing true positive rates can be at odds with overall calibration; it may involve solving for probabilities in a way that lowers overall model calibration or accuracy.\"}]"
    ]
  },
  {
    "id": 112,
    "name": "Statistical Parity Difference",
    "description": "Measures the difference in positive outcome rates between protected and unprotected groups.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-metrics-and-evaluation/group-fairness-metrics",
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Evaluating fairness in hiring models by comparing selection rates across genders.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Ignores true labels or qualification differences; can be satisfied by trivial strategies (like random decisions) and does not ensure individual fairness.\"}]"
    ]
  },
  {
    "id": 113,
    "name": "Disparate Impact",
    "description": "Assesses whether decisions disproportionately affect members of a protected group, typically requiring a ratio between rates.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-metrics-and-evaluation/group-fairness-metrics",
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Checking for bias in loan approvals where minority groups are less likely to be approved.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Typically uses the 80% rule which is an arbitrary threshold; focuses only on selection rates and not on accuracy or errors, so a model could satisfy disparate impact yet still be unfair in errors.\"}]"
    ]
  },
  {
    "id": 114,
    "name": "Demographic Parity",
    "description": "Evaluates if the outcome is independent of the protected attributes, aiming for equal outcome rates across groups.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-metrics-and-evaluation/group-fairness-metrics",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Ensuring job advertisements are shown equally across genders.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"If groups truly differ in label distribution, enforcing parity can harm utility; like statistical parity, it can be achieved in ways that ignore actual qualification (e.g., giving positive decisions randomly to meet rates).\"}]"
    ]
  },
  {
    "id": 115,
    "name": "Equal Opportunity Difference",
    "description": "Computes the difference in true positive rates between groups, assessing fairness in terms of equal opportunity.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-metrics-and-evaluation/group-fairness-metrics",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Assessing fairness in medical diagnosis models across age groups.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Addresses only true positive rates, ignoring false positive disparities; requires accurate ground truth labels for the positive class, and improving TPR for one group might increase FPR for that group.\"}]"
    ]
  },
  {
    "id": 116,
    "name": "Average Odds Difference",
    "description": "Calculates the average difference in false positive and true positive rates between groups.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-metrics-and-evaluation/group-fairness-metrics",
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Measuring bias in criminal risk assessment tools.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Combines multiple error rates which may obscure specific issues (a model could have one high and one low disparity and still average out); still needs ground truth labels and a balanced trade-off with accuracy.\"}]"
    ]
  },
  {
    "id": 117,
    "name": "Individual Fairness Metric (Consistency)",
    "description": "Evaluates whether similar individuals receive similar predictions, assessing fairness at an individual level.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-metrics-and-evaluation/individual-fairness-metrics",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Individual Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Ensuring similar credit applicants receive similar loan decisions.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Requires a domain-specific similarity metric between individuals; hard to define and validate, and ensuring consistency can conflict with achieving good group-level metrics.\"}]"
    ]
  },
  {
    "id": 118,
    "name": "Algorithmic Fairness using K-NN",
    "description": "Uses K-nearest neighbors to assess individual fairness by comparing predictions among similar instances.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#fairness-metrics-and-evaluation/individual-fairness-metrics",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Individual Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Evaluating fairness in personalized recommendation systems.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Relies on the quality of the distance metric; high-dimensional data can make nearest neighbor comparisons noisy (curse of dimensionality), and it doesn't directly fix the model, just evaluates it.\"}]"
    ]
  },
  {
    "id": 119,
    "name": "Counterfactual Fairness (Causal Modeling)",
    "description": "Ensures predictions remain the same in a counterfactual world where protected attributes are altered.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#causal-fairness-methods/counterfactual-fairness",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Individual Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Assessing fairness in loan approvals by simulating changes in applicant's race.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Structural Causal Models",
      "Causal Inference Models"
    ],
    "limitations": [
      "[{\"description\": \"Requires a causal model of the data (including how protected attributes influence other features) which might be difficult to obtain; results depend on the correctness of this causal model.\"}]"
    ]
  },
  {
    "id": 120,
    "name": "Path-Specific Counterfactual Fairness",
    "description": "Considers specific causal pathways, allowing fairness interventions on certain paths.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#causal-fairness-methods/counterfactual-fairness",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Individual Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Modeling fair decisions in advertising without altering legitimate causal effects.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Structural Causal Models"
    ],
    "limitations": [
      "[{\"description\": \"Requires identifying which causal pathways are 'allowable' and which are not—a subjective decision; analyzing specific paths adds complexity to the causal model and the fairness criterion.\"}]"
    ]
  },
  {
    "id": 121,
    "name": "Causal Fairness Assessment with Do-Calculus",
    "description": "Utilizes causal inference techniques to assess and mitigate bias by computing interventional distributions.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#causal-fairness-methods/causal-inference",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Causal Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Understanding bias in hiring decisions through causal relationships.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Causal Inference Models"
    ],
    "limitations": [
      "[{\"description\": \"Strongly dependent on having a correct causal graph; using do-calculus in practice can be computationally intense and challenging outside of relatively simple models or well-specified causal relationships.\"}]"
    ]
  },
  {
    "id": 122,
    "name": "Diversity Constraints in Recommendations",
    "description": "Incorporates diversity and fairness constraints in recommendation systems for varied and fair content exposure.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/fairnessconstrained-optimisation",
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Ensuring fair representation of genres in music recommendation platforms.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Recommendation Systems",
      "Collaborative Filtering",
      "Content-Based Recommenders"
    ],
    "limitations": [
      "[{\"description\": \"May reduce accuracy or relevance of recommendations if forced diversity conflicts with user preferences; implementing constraints can complicate the recommendation algorithm and objective function.\"}]"
    ]
  },
  {
    "id": 123,
    "name": "Bayesian Fairness Regularization",
    "description": "Applies Bayesian methods to include fairness as a prior, allowing probabilistic interpretation of fairness constraints.",
    "model_dependency": "Model-Specific",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#inprocessing-techniques/fairnessconstrained-optimisation",
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Group Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Applying fairness regularization in Bayesian models for credit risk assessment.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [
      "Bayesian Models"
    ],
    "limitations": [
      "[{\"description\": \"Selecting appropriate prior distributions for fairness is non-trivial; Bayesian methods can be computationally intensive (e.g., requiring sampling) and outcomes can be sensitive to prior assumptions.\"}]"
    ]
  },
  {
    "id": 124,
    "name": "SHAP Values for Fairness",
    "description": "Uses SHAP (SHapley Additive exPlanations) to attribute model predictions to input features, helping to identify bias contributions.",
    "model_dependency": "Model-Agnostic",
    "assurance_goals": [
      "Fairness"
    ],
    "category_tags": "#interpretability-and-explainability/feature-attribution-methods",
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "attributes": [
      {
        "type": "Fairness Approach",
        "value": "Individual Fairness"
      }
    ],
    "example_use_cases": [
      {
        "description": "Explaining biased predictions in loan approvals by examining feature contributions.",
        "goal": "Fairness"
      }
    ],
    "resources": [],
    "applicable_models": [],
    "limitations": [
      "[{\"description\": \"Identifies feature contributions to bias but doesn't automatically mitigate it; interpretation requires understanding SHAP outputs, and correlated features can distribute bias attribution, complicating conclusions.\"}]"
    ]
  }
]