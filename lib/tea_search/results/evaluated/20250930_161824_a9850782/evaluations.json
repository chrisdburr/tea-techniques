{
  "search_id": "20250930_161824_a9850782",
  "technique_name": "Local Interpretable Model-Agnostic Explanations",
  "technique_slug": "local-interpretable-model-agnostic-explanations",
  "timestamp": "2025-09-30T16:30:00.000000",
  "total_evaluated": 51,
  "included_count": 29,
  "rejected_count": 22,
  "results": [
    {
      "title": "thomasp85/lime",
      "url": "https://github.com/thomasp85/lime",
      "source": "github",
      "resource_type": "tool",
      "decision": "INCLUDE",
      "rationale": "Official R implementation of LIME, directly implements the technique",
      "abstract": "Local Interpretable Model-Agnostic Explanations (R port of original Python package)",
      "authors": [
        "thomasp85"
      ],
      "date": "2017-03-17",
      "metadata": {
        "stars": 489,
        "language": "R",
        "topics": [
          "caret",
          "model-checking",
          "model-evaluation",
          "modeling",
          "r"
        ],
        "license": "NOASSERTION"
      }
    },
    {
      "title": "Local Interpretable Model-Agnostic Explanations (lime) — lime 0.1 ...",
      "url": "https://lime-ml.readthedocs.io/",
      "source": "google",
      "resource_type": "documentation",
      "decision": "INCLUDE",
      "rationale": "Official documentation for the LIME Python package",
      "abstract": "In this page, you can find the Python API reference for the lime package (local interpretable model-agnostic explanations). For tutorials and more information, ...",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "lime-ml.readthedocs.io",
        "mime_type": "",
        "search_type": "tutorial",
        "strategic_query": true
      }
    },
    {
      "title": "Introduction to SHapley Additive exPlanations (SHAP) — XAI Tutorials",
      "url": "https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/shap.html",
      "source": "google",
      "resource_type": "tutorial",
      "decision": "INCLUDE",
      "rationale": "Tutorial site that includes LIME explanations as referenced in abstract, part of broader XAI tutorial collection",
      "abstract": "Lulu.com. 2022. previous. Introduction to Local Interpretable Model-Agnostic Explanations (LIME).",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "xai-tutorials.readthedocs.io",
        "mime_type": "",
        "search_type": "tutorial",
        "strategic_query": true
      }
    },
    {
      "title": "Tutorials for eXplainable Artificial Intelligence (XAI) methods — XAI ...",
      "url": "https://xai-tutorials.readthedocs.io/",
      "source": "google",
      "resource_type": "tutorial",
      "decision": "INCLUDE",
      "rationale": "Contains comprehensive tutorials on LIME and other XAI methods with practical exercises",
      "abstract": "Each tutorial comes in a Jupyter Notebook with practical exercises. In addition ... Introduction to Local Interpretable Model-Agnostic Explanations (LIME) ...",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "xai-tutorials.readthedocs.io",
        "mime_type": "",
        "search_type": "tutorial",
        "strategic_query": true
      }
    },
    {
      "title": "Interpretable and Explainable NER with LIME | Towards Data Science",
      "url": "https://towardsdatascience.com/interpretable-and-explainable-ner-with-lime-d643512c524/",
      "source": "google",
      "resource_type": "tutorial",
      "decision": "INCLUDE",
      "rationale": "Practical tutorial demonstrating LIME application for NER tasks",
      "abstract": "Jan 14, 2022 ... ... (Local Interpretable Model-Agnostic Explanations). You can learn more from the original paper. LIME is model agnostic, meaning it can be ...",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "towardsdatascience.com",
        "mime_type": "",
        "search_type": "tutorial",
        "strategic_query": true
      }
    },
    {
      "title": "Introduction to Local Interpretable Model-Agnostic Explanations ...",
      "url": "https://www.kdnuggets.com/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html",
      "source": "google",
      "resource_type": "tutorial",
      "decision": "INCLUDE",
      "rationale": "Dedicated tutorial introducing LIME technique and its applications",
      "abstract": "Aug 25, 2016 ... Introduction to Local Interpretable Model-Agnostic Explanations (LIME) ... user, but personal information that would be stored about you ...",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "www.kdnuggets.com",
        "mime_type": "",
        "search_type": "tutorial",
        "strategic_query": true
      }
    },
    {
      "title": "Decoding the Black Box: An Important Introduction to Interpretable ...",
      "url": "https://www.analyticsvidhya.com/blog/2019/08/decoding-black-box-step-by-step-guide-interpretable-machine-learning-models-python/",
      "source": "google",
      "resource_type": "tutorial",
      "decision": "INCLUDE",
      "rationale": "Comprehensive tutorial covering LIME as a key interpretable ML technique",
      "abstract": "Nov 12, 2024 ... Learn more about decision trees in this superb tutorial. An ... local interpretable model agnostic explanations. LIME is based on ...",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "www.analyticsvidhya.com",
        "mime_type": "",
        "search_type": "tutorial",
        "strategic_query": true
      }
    },
    {
      "title": "Introduction to Machine Learning Model Interpretation | Towards ...",
      "url": "https://towardsdatascience.com/introduction-to-machine-learning-model-interpretation-55036186eeab/",
      "source": "google",
      "resource_type": "tutorial",
      "decision": "INCLUDE",
      "rationale": "Tutorial covering LIME as a primary model interpretation technique",
      "abstract": "May 12, 2019 ... LIME (LOCAL INTERPRETABLE MODEL-AGNOSTIC EXPLANATIONS). Local surrogate models are interpretable models that are used to explain individual ...",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "towardsdatascience.com",
        "mime_type": "",
        "search_type": "tutorial",
        "strategic_query": true
      }
    },
    {
      "title": "Enhanced LIME — ADS 2.6.3 documentation",
      "url": "https://accelerated-data-science.readthedocs.io/en/v2.6.3/user_guide/model_explainability/lime.html",
      "source": "google",
      "resource_type": "documentation",
      "decision": "INCLUDE",
      "rationale": "Documentation for enhanced LIME implementation in Oracle's ADS library",
      "abstract": "ADS provides an enhanced version of Local Interpretable Model-Agnostic Explanations (LIME), which improves on the explanation quality, performance, and ...",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "accelerated-data-science.readthedocs.io",
        "mime_type": "",
        "search_type": "documentation",
        "strategic_query": true
      }
    },
    {
      "title": "Model Explainability — ADS 2.5.9 documentation",
      "url": "https://accelerated-data-science.readthedocs.io/en/v2.5.9/user_guide/model_explainability/mlx.html",
      "source": "google",
      "resource_type": "documentation",
      "decision": "INCLUDE",
      "rationale": "Documentation covering LIME implementation for model explainability",
      "abstract": "Enhanced Local Interpretable Model-Agnostic Explanations · Overview · Description · Interpretation · Example · References · WhatIf Explainer · Description ...",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "accelerated-data-science.readthedocs.io",
        "mime_type": "",
        "search_type": "documentation",
        "strategic_query": true
      }
    },
    {
      "title": "Local Explainers — ADS 2.6.1 documentation",
      "url": "https://accelerated-data-science.readthedocs.io/en/v2.6.1/user_guide/model_explainability/local.html",
      "source": "google",
      "resource_type": "documentation",
      "decision": "INCLUDE",
      "rationale": "Documentation focusing on local explainability methods including LIME",
      "abstract": "Enhanced Local Interpretable Model-Agnostic Explanations · WhatIf Explainer · Model Serialization · Secrets · Text Extraction. Classes: Class Documentation.",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "accelerated-data-science.readthedocs.io",
        "mime_type": "",
        "search_type": "documentation",
        "strategic_query": true
      }
    },
    {
      "title": "Welcome to the visuaLIME documentation! — visuaLIME 0.1.0 ...",
      "url": "https://visualime.readthedocs.io/",
      "source": "google",
      "resource_type": "documentation",
      "decision": "INCLUDE",
      "rationale": "Documentation for visuaLIME, a specialized implementation of LIME focused on visualization",
      "abstract": "Welcome to the visuaLIME documentation! . VisuaLIME is an implementation of LIME (Local Interpretable Model-Agnostic Explanations) [1] focused on producing ...",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "visualime.readthedocs.io",
        "mime_type": "",
        "search_type": "documentation",
        "strategic_query": true
      }
    },
    {
      "title": "Supported Models — interpret-community 0.26.0 documentation",
      "url": "https://interpret-community.readthedocs.io/en/v0.26.0/explainers.html",
      "source": "google",
      "resource_type": "documentation",
      "decision": "INCLUDE",
      "rationale": "Documentation covering LIME explainer in Microsoft's interpret-community library",
      "abstract": "Supported Explainers¶ ; LIME Explainer. Local Interpretable Model-agnostic Explanations (LIME) is a local linear approximation of the model's behavior. The ...",
      "authors": [],
      "date": "",
      "metadata": {
        "display_link": "interpret-community.readthedocs.io",
        "mime_type": "",
        "search_type": "documentation",
        "strategic_query": true
      }
    },
    {
      "title": "Deterministic Local Interpretable Model-Agnostic Explanations for Stable Explainability",
      "url": "https://www.semanticscholar.org/paper/ee5775e16b00c2a150119c0cb6a4de8c29b30cd3",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Highly cited paper (227 citations) addressing LIME's instability problem, proposes deterministic variant",
      "abstract": "Local Interpretable Model-Agnostic Explanations (LIME) is a popular technique used to increase the interpretability and explainability of black box Machine Learning (ML) algorithms. LIME typically creates an explanation for a single prediction by any ML model by learning a simpler interpretable model (e.g., linear classifier) around the prediction through generating simulated data around the instance by random perturbation, and obtaining feature importance through applying some form of feature s",
      "authors": [
        "Muhammad Rehman Zafar",
        "N. Khan"
      ],
      "date": "2021",
      "metadata": {
        "paper_id": "ee5775e16b00c2a150119c0cb6a4de8c29b30cd3",
        "citation_count": 227,
        "venue": "Machine Learning and Knowledge Extraction"
      }
    },
    {
      "title": "Enhancing Visualization and Explainability of Computer Vision Models with Local Interpretable Model-Agnostic Explanations (LIME)",
      "url": "https://www.semanticscholar.org/paper/6a806b8c2bcf5ed4d026b10c4cb24f2796c0385f",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Proposes SubLIME technique to enhance LIME stability for computer vision applications",
      "abstract": "It is important that humans understand why machine learning models behave the way they do, especially in the field of computer vision. Having methods for visualizing which regions of an image are responsible for classifying or detecting objects can be a very useful resource. One popular algorithm for doing so is Local Interpretable Model-agnostic Explanations (LIME). We introduce Sub-model Stabilized and Sub grid Superimposed LIME (SubLIME), a technique for enhancing the stability of LIME-based ",
      "authors": [
        "Nicholas Hamilton",
        "Adam J. Webb",
        "Matt Wilder",
        "Ben Hendrickson",
        "Matthew Blanck"
      ],
      "date": "2022",
      "metadata": {
        "paper_id": "6a806b8c2bcf5ed4d026b10c4cb24f2796c0385f",
        "citation_count": 7,
        "venue": "IEEE Symposium Series on Computational Intelligence"
      }
    },
    {
      "title": "A Multiobjective Genetic Algorithm to Evolving Local Interpretable Model-Agnostic Explanations for Deep Neural Networks in Image Classification",
      "url": "https://www.semanticscholar.org/paper/8877d3da0e5ebfa84026f735bdaca78e399bd45a",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Novel evolutionary approach to improving LIME for deep CNNs in image classification",
      "abstract": "Deep convolutional neural networks have become a dominant solution for numerous image classification tasks. However, a main criticism is the poor explainability due to the black-box characteristic, which hurdles the extensive usage of deep convolutional neural networks. To address this issue, this article proposes a new evolutionary multiobjective-based method, which aims to explain the behaviors of deep convolutional neural networks by evolving local explanations on specific images. To the best",
      "authors": [
        "Bin Wang",
        "Wenbin Pei",
        "Bing Xue",
        "Mengjie Zhang"
      ],
      "date": "2024",
      "metadata": {
        "paper_id": "8877d3da0e5ebfa84026f735bdaca78e399bd45a",
        "citation_count": 7,
        "venue": "IEEE Transactions on Evolutionary Computation"
      }
    },
    {
      "title": "Improving Local Interpretable Model-agnostic Explanations Stability",
      "url": "https://www.semanticscholar.org/paper/ede2435a2c92caaa6a3a0e2c67b02b52be6e41ac",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Directly addresses LIME's instability issue with ST-LIME variant for tabular data",
      "abstract": ": Local Interpretable Model-Agnostic Explanations (LIME) is a widely used explainable artificial intelligence (XAI) technique for tabular data. LIME explains how classifiers or regressors make decisions. However, randomization during synthetic data production makes LIME unstable, leading to distrust in its explanations. The objective of this paper is to present STable LIME (ST-LIME). ST-LIME is an extension of LIME that addresses its instability by using fixed noise to generate consistent synthe",
      "authors": [
        "Asmaa Elgezawy",
        "Hatem Abdul-kader",
        "Asmaa H. Elsaid"
      ],
      "date": "2024",
      "metadata": {
        "paper_id": "ede2435a2c92caaa6a3a0e2c67b02b52be6e41ac",
        "citation_count": 0,
        "venue": "International Journal of Intelligent Engineering and Systems"
      }
    },
    {
      "title": "Explainable machine learning techniques based on attention gate recurrent unit and local interpretable model‐agnostic explanations for multivariate wind speed forecasting",
      "url": "https://www.semanticscholar.org/paper/bb68facee9c98c92a381613af4d8bc83ad35c468",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Applies LIME to wind speed forecasting with attention-based ML, demonstrating practical application",
      "abstract": "Wind power has emerged as a successful component within power systems. The ability to reliably and accurately forecast wind speed is of great importance in maintaining the security and stability of the power grid. However, the significance of explaining prediction models has often been overlooked by researchers. To address this gap, this study introduces a novel approach to wind speed forecasting that incorporates a significant decomposition method, attention‐based machine learning, and local ex",
      "authors": [
        "Lu Peng",
        "Sheng-Xiang Lv",
        "Lin Wang"
      ],
      "date": "2024",
      "metadata": {
        "paper_id": "bb68facee9c98c92a381613af4d8bc83ad35c468",
        "citation_count": 21,
        "venue": "Journal of Forecasting"
      }
    },
    {
      "title": "Stable local interpretable model-agnostic explanations based on a variational autoencoder",
      "url": "https://www.semanticscholar.org/paper/9b786b6f019e2f361ae2b5ee4d57bf8abb84b359",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Proposes VAE-based approach to improve LIME stability, directly relevant improvement",
      "abstract": "",
      "authors": [
        "Xu Xiang",
        "Hong Yu",
        "Ye Wang",
        "Guoyin Wang"
      ],
      "date": "2023",
      "metadata": {
        "paper_id": "9b786b6f019e2f361ae2b5ee4d57bf8abb84b359",
        "citation_count": 14,
        "venue": "Applied intelligence (Boston)"
      }
    },
    {
      "title": "DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems",
      "url": "https://www.semanticscholar.org/paper/e5c703aba8af983c36fedf08c32a6978eadd91b9",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Highly cited (163 citations) deterministic variant of LIME for medical diagnosis applications",
      "abstract": "Local Interpretable Model-Agnostic Explanations (LIME) is a popular technique used to increase the interpretability and explainability of black box Machine Learning (ML) algorithms. LIME typically generates an explanation for a single prediction by any ML model by learning a simpler interpretable model (e.g. linear classifier) around the prediction through generating simulated data around the instance by random perturbation, and obtaining feature importance through applying some form of feature ",
      "authors": [
        "Muhammad Rehman Zafar",
        "N. Khan"
      ],
      "date": "2019",
      "metadata": {
        "paper_id": "e5c703aba8af983c36fedf08c32a6978eadd91b9",
        "citation_count": 163,
        "venue": "arXiv.org"
      }
    },
    {
      "title": "Interpretable ensemble deep learning model for early detection of Alzheimer's disease using local interpretable model‐agnostic explanations",
      "url": "https://www.semanticscholar.org/paper/df6077a8273cbe66018c7e5c84c2c8e14e015bf1",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Medical application of LIME for Alzheimer's detection, demonstrates practical clinical use",
      "abstract": "",
      "authors": [
        "Atefeh Aghaei",
        "M. Moghaddam",
        "H. Malek"
      ],
      "date": "2022",
      "metadata": {
        "paper_id": "df6077a8273cbe66018c7e5c84c2c8e14e015bf1",
        "citation_count": 17,
        "venue": "International journal of imaging systems and technology (Print)"
      }
    },
    {
      "title": "Explaining Deep Convolutional Neural Networks for Image Classification by Evolving Local Interpretable Model-agnostic Explanations",
      "url": "https://www.semanticscholar.org/paper/57a66ec816b30319c6564d18920412c683b2322f",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Genetic algorithm approach to evolve better LIME explanations for deep CNNs",
      "abstract": "Deep convolutional neural networks have proven their effectiveness, and have been acknowledged as the most dominant method for image classification. However, a severe drawback of deep convolutional neural networks is poor explainability. Unfortunately, in many real-world applications, users need to understand the rationale behind the predictions of deep convolutional neural networks when determining whether they should trust the predictions or not. To resolve this issue, a novel genetic algorith",
      "authors": [
        "Bin Wang",
        "Wenbin Pei",
        "Bing Xue",
        "Mengjie Zhang"
      ],
      "date": "2022",
      "metadata": {
        "paper_id": "57a66ec816b30319c6564d18920412c683b2322f",
        "citation_count": 6,
        "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence"
      }
    },
    {
      "title": "Early knee osteoarthritis classification using distributed explainable convolutional neural network with local interpretable model-agnostic explanations",
      "url": "https://www.semanticscholar.org/paper/727badaa3ca949e835e83345a7502a77daaaee1f",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Medical imaging application using LIME (LExNN model) for knee osteoarthritis classification",
      "abstract": "Knee Osteoarthritis (KOA) is a type of Knee Arthritis (KA) that causes pain, swelling, and other discomforts to the knee joints, which is quite complicated to classify using previous methods due to its various limitations such as computational cost, over-fitting issues, less reliability and so on. In this research, the classification using a distributed explainable convolutional neural network with local interpretable model-agnostic explanations (LExNN) model is proposed for knee Osteoarthritis.",
      "authors": [
        "M. Ganesh Kumar",
        "Lakshmi Narayana Gumma",
        "Saikiran Neelam",
        "Narikamalli Yaswanth",
        "Jammisetty Yedukondalu"
      ],
      "date": "2024",
      "metadata": {
        "paper_id": "727badaa3ca949e835e83345a7502a77daaaee1f",
        "citation_count": 1,
        "venue": "Engineering Research Express"
      }
    },
    {
      "title": "Skin Lesion Classification: A Deep Learning Approach with Local Interpretable Model-Agnostic Explanations (LIME) for Explainable Artificial Intelligence (XAI)",
      "url": "https://www.semanticscholar.org/paper/fea971b5a61bd668397302b21cbce9f67d6381da",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Medical application of LIME for skin cancer classification with VGG16",
      "abstract": "The classification of skin cancer is crucial as the chance of survival increases significantly with timely and accurate treatment. Convolution Neural Networks (CNNs) have proven effective in classifying skin cancer. However, CNN models are often regarded as \"black boxes\", due to the lack of transparency in the decision-making. Therefore, explainable artificial intelligence (XAI) has emerged as a tool for understanding AI decisions. This study employed a CNN model, VGG16, to classify five skin le",
      "authors": [
        "Sin Yi Hong",
        "Lih Poh Lin"
      ],
      "date": "2024",
      "metadata": {
        "paper_id": "fea971b5a61bd668397302b21cbce9f67d6381da",
        "citation_count": 1,
        "venue": "JOIV: International Journal on Informatics Visualization"
      }
    },
    {
      "title": "Local Interpretable Model-Agnostic Explanations for Neural Ranking Models",
      "url": "https://www.semanticscholar.org/paper/8aab0e3782e0f42ed272e7ade296fa5c2c554727",
      "source": "semantic_scholar",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Adaptation of LIME specifically for neural ranking models in information retrieval",
      "abstract": "Neural Ranking Models have shown state-of-the-art performance in Learning-To-Rank (LTR) tasks. However, they are considered black-box models. Understanding the logic behind the predictions of such black-box models is paramount for their adaptability in the real-world and high-stake decision-making domains. Local explanation techniques can help us understand the importance of features in the dataset relative to the predicted output of these black-box models. This study investigates new adaptation",
      "authors": [
        "Amir Hossein Akhavan Rahnama",
        "Laura Galera Alfaro",
        "Zhendong Wang",
        "Maria Movin"
      ],
      "date": "2024",
      "metadata": {
        "paper_id": "8aab0e3782e0f42ed272e7ade296fa5c2c554727",
        "citation_count": 0,
        "venue": "Scandinavian Conference on AI"
      }
    },
    {
      "title": "Optimal Local Explainer Aggregation for Interpretable Prediction",
      "url": "http://arxiv.org/pdf/2003.09466v2",
      "source": "arxiv",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Addresses aggregation of local explainers like LIME for improved coverage and interpretability",
      "abstract": "A key challenge for decision makers when incorporating black box machine\nlearned models into practice is being able to understand the predictions\nprovided by these models. One proposed set of methods is training surrogate\nexplainer models which approximate the more complex model. Explainer methods\nare generally classified as either local or global, depending on what portion\nof the data space they are purported to explain. The improved coverage of\nglobal explainers usually comes at the expense of",
      "authors": [
        "Qiaomei Li",
        "Rachel Cummings",
        "Yonatan Mintz"
      ],
      "date": "2020-03-20",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2003.09466v2",
        "categories": [
          "cs.LG",
          "cs.CY",
          "stat.ML"
        ],
        "comment": null
      }
    },
    {
      "title": "Local Interpretable Model Agnostic Shap Explanations for machine learning models",
      "url": "http://arxiv.org/pdf/2210.04533v1",
      "source": "arxiv",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Compares LIME with SHAP explanations, discusses XAI frameworks for ML model interpretability",
      "abstract": "With the advancement of technology for artificial intelligence (AI) based\nsolutions and analytics compute engines, machine learning (ML) models are\ngetting more complex day by day. Most of these models are generally used as a\nblack box without user interpretability. Such complex ML models make it more\ndifficult for people to understand or trust their predictions. There are\nvariety of frameworks using explainable AI (XAI) methods to demonstrate\nexplainability and interpretability of ML models to ",
      "authors": [
        "P. Sai Ram Aditya",
        "Mayukha Pal"
      ],
      "date": "2022-10-10",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2210.04533v1",
        "categories": [
          "cs.LG",
          "cs.AI"
        ],
        "comment": "Under review"
      }
    },
    {
      "title": "Human-interpretable model explainability on high-dimensional data",
      "url": "http://arxiv.org/pdf/2010.07384v2",
      "source": "arxiv",
      "resource_type": "paper",
      "decision": "INCLUDE",
      "rationale": "Framework for model-agnostic explainability on high-dimensional data, addresses LIME computational challenges",
      "abstract": "The importance of explainability in machine learning continues to grow, as\nboth neural-network architectures and the data they model become increasingly\ncomplex. Unique challenges arise when a model's input features become high\ndimensional: on one hand, principled model-agnostic approaches to\nexplainability become too computationally expensive; on the other, more\nefficient explainability algorithms lack natural interpretations for general\nusers. In this work, we introduce a framework for human-i",
      "authors": [
        "Damien de Mijolla",
        "Christopher Frye",
        "Markus Kunesch",
        "John Mansir",
        "Ilya Feige"
      ],
      "date": "2020-10-14",
      "metadata": {
        "arxiv_id": "http://arxiv.org/abs/2010.07384v2",
        "categories": [
          "cs.LG",
          "cs.AI",
          "stat.ML"
        ],
        "comment": "8 pages, 6 figures, 1 appendix"
      }
    }
  ]
}