{
  "tag": {
    "name": "explanatory-scope/global",
    "slug": "explanatory-scope-global",
    "count": 25,
    "category": "explanatory-scope"
  },
  "techniques": [
    {
      "slug": "shapley-additive-explanations",
      "name": "SHapley Additive exPlanations",
      "description": "SHAP explains model predictions by quantifying how much each input feature contributes to the outcome. It assigns an importance score to every feature, indicating whether it pushes the prediction towards or away from the average. The method systematically evaluates how predictions change as features are included or excluded, drawing on game theory concepts to ensure a fair distribution of contributions.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/perturbation-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/explainability/property/consistency",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing a customer churn prediction model to understand why a specific high-value customer was flagged as likely to leave, revealing that recent support ticket interactions and declining purchase frequency were the main drivers.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a loan approval model by comparing SHAP values for applicants from different demographic groups, ensuring that protected characteristics like race or gender do not have an undue influence on credit decisions.",
          "goal": "Fairness"
        },
        {
          "description": "Validating a medical diagnosis model by confirming that its predictions are based on relevant clinical features (e.g., blood pressure, cholesterol levels) rather than spurious correlations (e.g., patient ID or appointment time), thereby improving model reliability.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Assumes feature independence, which can produce misleading explanations when features are highly correlated, as the model may attribute importance to features that are merely proxies for others."
        },
        {
          "description": "Computationally expensive for models with many features or large datasets, as the number of required predictions grows exponentially with the number of features."
        },
        {
          "description": "The choice of background dataset for generating explanations can significantly influence the results, requiring careful selection to ensure a representative baseline."
        },
        {
          "description": "Global explanations derived from averaging local SHAP values may obscure important heterogeneous effects where features impact subgroups of the population differently."
        }
      ],
      "resources": [
        {
          "title": "shap/shap",
          "url": "https://github.com/shap/shap",
          "source_type": "software_package"
        },
        {
          "title": "Introduction to SHapley Additive exPlanations (SHAP) — XAI Tutorials",
          "url": "https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/shap.html",
          "source_type": "tutorial"
        },
        {
          "title": "An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models",
          "url": "http://arxiv.org/pdf/2204.11351v3",
          "source_type": "technical_paper",
          "authors": [
            "Han Yuan",
            "Mingxuan Liu",
            "Lican Kang",
            "Chenkui Miao",
            "Ying Wu"
          ],
          "publication_date": "2022-04-24"
        },
        {
          "title": "SHAP: Shapley Additive Explanations | Towards Data Science",
          "url": "https://towardsdatascience.com/shap-shapley-additive-explanations-5a2a271ed9c3/",
          "source_type": "tutorial"
        },
        {
          "title": "MAIF/shapash",
          "url": "https://github.com/MAIF/shapash",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "acronym": "SHAP",
      "related_techniques": [
        "integrated-gradients",
        "deeplift",
        "layer-wise-relevance-propagation",
        "local-interpretable-model-agnostic-explanations",
        "contrastive-explanation-method",
        "anchor"
      ]
    },
    {
      "slug": "permutation-importance",
      "name": "Permutation Importance",
      "description": "Permutation Importance quantifies a feature's contribution to a model's performance by randomly shuffling its values and measuring the resulting drop in predictive accuracy. If shuffling a feature significantly degrades the model's performance, that feature is considered important. This model-agnostic technique helps identify which inputs are genuinely driving predictions, rather than just being correlated with the outcome.",
      "assurance_goals": [
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/perturbation-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/consistency",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Assessing which patient characteristics (e.g., age, blood pressure, cholesterol) are most critical for a medical diagnosis model by observing the performance drop when each characteristic's values are randomly shuffled, ensuring the model relies on clinically relevant factors.",
          "goal": "Explainability"
        },
        {
          "description": "Validating the robustness of a fraud detection model by permuting features like transaction amount or location, and confirming that the model's ability to detect fraud significantly decreases only for truly important features, thereby improving confidence in its reliability.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Can be misleading when features are highly correlated, as shuffling one feature might indirectly affect others, leading to an overestimation of its importance."
        },
        {
          "description": "Computationally expensive for large datasets or complex models, as it requires re-evaluating the model many times for each feature."
        },
        {
          "description": "Does not account for interactions between features; it measures the marginal importance of a feature, assuming other features remain unchanged."
        },
        {
          "description": "The choice of metric for evaluating performance drop (e.g., accuracy, F1-score) can influence the perceived importance of features."
        }
      ],
      "resources": [
        {
          "title": "Asymptotic Unbiasedness of the Permutation Importance Measure in Random Forest Models",
          "url": "http://arxiv.org/pdf/1912.03306v1",
          "source_type": "technical_paper",
          "authors": [
            "Burim Ramosaj",
            "Markus Pauly"
          ],
          "publication_date": "2019-12-05"
        },
        {
          "title": "eli5.permutation_importance — ELI5 0.15.0 documentation",
          "url": "https://eli5.readthedocs.io/en/latest/autodocs/permutation_importance.html",
          "source_type": "documentation"
        },
        {
          "title": "Permutation Importance — PermutationImportance 1.2.1.5 ...",
          "url": "https://permutationimportance.readthedocs.io/en/latest/permutation.html",
          "source_type": "documentation"
        },
        {
          "title": "parrt/random-forest-importances",
          "url": "https://github.com/parrt/random-forest-importances",
          "source_type": "software_package"
        },
        {
          "title": "Statistically Valid Variable Importance Assessment through Conditional Permutations",
          "url": "http://arxiv.org/pdf/2309.07593v2",
          "source_type": "technical_paper",
          "authors": [
            "Ahmad Chamma",
            "Denis A. Engemann",
            "Bertrand Thirion"
          ],
          "publication_date": "2023-09-14"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 3,
      "related_techniques": [
        "mean-decrease-impurity",
        "coefficient-magnitudes-in-linear-models",
        "sobol-indices"
      ]
    },
    {
      "slug": "mean-decrease-impurity",
      "name": "Mean Decrease Impurity",
      "description": "Mean Decrease Impurity (MDI) quantifies a feature's importance in tree-based models (e.g., Random Forests, Gradient Boosting Machines) by measuring the total reduction in impurity (e.g., Gini impurity, entropy) across all splits where the feature is used. Features that lead to larger, more consistent reductions in impurity are considered more important, indicating their effectiveness in creating homogeneous child nodes and improving predictive accuracy.",
      "assurance_goals": [
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/tree-based",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/model-specific",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Determining the most influential genetic markers in a decision tree model predicting disease susceptibility, by identifying which markers consistently lead to the purest splits between healthy and diseased patient groups.",
          "goal": "Explainability"
        },
        {
          "description": "Assessing the key factors driving customer purchasing decisions in an e-commerce random forest model, revealing which product attributes or customer demographics are most effective in segmenting buyers.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "MDI is inherently biased towards features with more unique values or those that allow for more splits, potentially overestimating their true importance."
        },
        {
          "description": "It is only applicable to tree-based models and cannot be directly used with other model architectures."
        },
        {
          "description": "The importance scores can be unstable, varying significantly with small changes in the training data or model parameters."
        },
        {
          "description": "MDI does not account for feature interactions, meaning it might not accurately reflect the importance of features that are only relevant when combined with others."
        }
      ],
      "resources": [
        {
          "title": "Trees, forests, and impurity-based variable importance",
          "url": "http://arxiv.org/pdf/2001.04295v3",
          "source_type": "technical_paper",
          "authors": [
            "Erwan Scornet"
          ],
          "publication_date": "2020-01-13"
        },
        {
          "title": "A Debiased MDI Feature Importance Measure for Random Forests",
          "url": "http://arxiv.org/pdf/1906.10845v2",
          "source_type": "technical_paper",
          "authors": [
            "Xiao Li",
            "Yu Wang",
            "Sumanta Basu",
            "Karl Kumbier",
            "Bin Yu"
          ],
          "publication_date": "2019-06-26"
        },
        {
          "title": "Variable Importance in Random Forests | Towards Data Science",
          "url": "https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0/",
          "source_type": "tutorial"
        },
        {
          "title": "Interpreting Deep Forest through Feature Contribution and MDI Feature Importance",
          "url": "http://arxiv.org/pdf/2305.00805v1",
          "source_type": "technical_paper",
          "authors": [
            "Yi-Xiao He",
            "Shen-Huan Lyu",
            "Yuan Jiang"
          ],
          "publication_date": "2023-05-01"
        },
        {
          "title": "optuna.importance.MeanDecreaseImpurityImportanceEvaluator ...",
          "url": "https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.MeanDecreaseImpurityImportanceEvaluator.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "permutation-importance",
        "coefficient-magnitudes-in-linear-models",
        "sobol-indices"
      ]
    },
    {
      "slug": "coefficient-magnitudes-in-linear-models",
      "name": "Coefficient Magnitudes (in Linear Models)",
      "description": "Coefficient Magnitudes assess feature influence in linear models by examining the absolute values of their coefficients. Features with larger absolute coefficients are considered to have a stronger impact on the prediction, while the sign of the coefficient indicates the direction of that influence (positive or negative). This technique provides a straightforward and transparent way to understand the direct linear relationship between each input feature and the model's output.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/linear-model",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/model-specific",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "expertise-needed/low",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/metric"
      ],
      "example_use_cases": [
        {
          "description": "Interpreting which features influence housing price predictions in linear regression, such as identifying that 'number of bedrooms' has a larger positive impact than 'distance to city centre' based on coefficient magnitudes.",
          "goal": "Explainability"
        },
        {
          "description": "Explaining the factors contributing to customer lifetime value (CLV) in a linear model, showing how 'average monthly spend' has a strong positive coefficient, making the model transparent for business stakeholders.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Only valid for linear relationships; it cannot capture complex non-linear patterns or interactions between features."
        },
        {
          "description": "Highly sensitive to feature scaling; features with larger numerical ranges can appear more important even if their true impact is smaller."
        },
        {
          "description": "Can be misleading in the presence of multicollinearity, where correlated features may split importance or have unstable coefficients."
        },
        {
          "description": "Does not imply causation; a strong correlation (large coefficient) does not necessarily mean a causal relationship."
        }
      ],
      "resources": [],
      "complexity_rating": 1,
      "computational_cost_rating": 1,
      "related_techniques": [
        "permutation-importance",
        "mean-decrease-impurity",
        "sobol-indices"
      ]
    },
    {
      "slug": "sobol-indices",
      "name": "Sobol Indices",
      "description": "Sobol Indices quantify how much each input feature contributes to the total variance in a model's predictions through global sensitivity analysis. The technique calculates first-order indices (individual feature contributions) and total-order indices (including all interaction effects involving that feature). By systematically sampling the input space and decomposing output variance, Sobol Indices reveal which features drive model uncertainty and which interactions between features are most important for predictions.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/causal-analysis/interaction-effects",
        "assurance-goal-category/explainability/attribution-methods/perturbation-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/causal-pathways",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing a climate prediction model to determine which atmospheric parameters (temperature, humidity, pressure) contribute most to rainfall forecast uncertainty, helping meteorologists understand which measurements need the highest precision.",
          "goal": "Explainability"
        },
        {
          "description": "Evaluating a financial risk model to identify which economic indicators (interest rates, inflation, GDP growth) drive the most variability in portfolio value predictions, enabling better risk management strategies.",
          "goal": "Explainability"
        },
        {
          "description": "Analysing a credit scoring model to quantify how much prediction variance stems from zip code (a potential proxy for race), helping identify features that may cause disparate impact across demographic groups.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive, requiring thousands of model evaluations to achieve stable variance estimates, making it impractical for very slow models."
        },
        {
          "description": "Assumes input features are independently distributed, which can lead to misleading results when features are correlated in real data."
        },
        {
          "description": "Curse of dimensionality makes the technique increasingly difficult and expensive to apply as the number of input features grows beyond 10-20."
        },
        {
          "description": "Requires defining appropriate probability distributions for input features, which may not accurately reflect real-world feature distributions."
        }
      ],
      "resources": [
        {
          "title": "Sobol Tensor Trains for Global Sensitivity Analysis",
          "url": "http://arxiv.org/pdf/1712.00233v1",
          "source_type": "technical_paper",
          "authors": [
            "Rafael Ballester-Ripoll",
            "Enrique G. Paredes",
            "Renato Pajarola"
          ],
          "publication_date": "2017-12-01"
        },
        {
          "title": "Sobol indices — UQpy v4.2.0 documentation",
          "url": "https://uqpyproject.readthedocs.io/en/latest/sensitivity/sobol.html",
          "source_type": "documentation"
        },
        {
          "title": "Sobol Indices to Measure Feature Importance | Towards Data Science",
          "url": "https://towardsdatascience.com/sobol-indices-to-measure-feature-importance-54cedc3281bc/",
          "source_type": "tutorial"
        },
        {
          "title": "Basics — SALib's documentation",
          "url": "https://salib.readthedocs.io/en/latest/user_guide/basics.html",
          "source_type": "documentation"
        },
        {
          "title": "UQpy (Uncertainty Quantification with python)",
          "url": "https://github.com/SURGroup/UQpy",
          "source_type": "software_package"
        },
        {
          "title": "SALib/SALib",
          "url": "https://github.com/SALib/SALib",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 5,
      "related_techniques": [
        "permutation-importance",
        "mean-decrease-impurity",
        "coefficient-magnitudes-in-linear-models"
      ]
    },
    {
      "slug": "ridge-regression-surrogates",
      "name": "Ridge Regression Surrogates",
      "description": "This technique approximates a complex model by training a ridge regression (a linear model with L2 regularization) on the original model's predictions. The ridge regression serves as a global surrogate that balances fidelity and interpretability, capturing the main linear relationships that the complex model learned while ignoring noise due to regularization.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/surrogate-models/global-surrogates",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/decision-boundaries",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/structured-output",
        "expertise-needed/ml-engineering",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Approximating a complex ensemble model used for credit scoring with a ridge regression surrogate to identify the most influential features (income, credit history, debt-to-income ratio) and their linear relationships for regulatory compliance reporting.",
          "goal": "Explainability"
        },
        {
          "description": "Creating a ridge regression surrogate of a neural network used for medical diagnosis to understand which patient symptoms and biomarkers have the strongest linear predictive relationships with disease outcomes.",
          "goal": "Explainability"
        },
        {
          "description": "Creating an interpretable approximation of a complex insurance pricing model for regulatory compliance, enabling stakeholders to understand and validate the decision-making process through transparent linear relationships.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Linear approximation may miss important non-linear relationships and interactions captured by the original complex model."
        },
        {
          "description": "Requires a representative dataset to train the surrogate model, which may not be available or may be expensive to generate."
        },
        {
          "description": "Ridge regularisation may oversimplify the model by shrinking coefficients, potentially hiding important but less dominant features."
        },
        {
          "description": "Surrogate fidelity depends on how well linear relationships approximate the original model's behaviour across the entire input space."
        }
      ],
      "resources": [
        {
          "title": "scikit-learn Ridge Regression Documentation",
          "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html",
          "source_type": "documentation"
        },
        {
          "title": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable",
          "url": "https://christophm.github.io/interpretable-ml-book/global.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "rulefit",
        "model-distillation"
      ]
    },
    {
      "slug": "partial-dependence-plots",
      "name": "Partial Dependence Plots",
      "description": "Partial Dependence Plots show how changing one or two features affects a model's predictions on average. The technique works by varying the selected feature(s) across their full range whilst keeping all other features fixed at their original values, then averaging the predictions. This creates a clear visualisation of whether increasing or decreasing a feature tends to increase or decrease predictions, and reveals patterns like linear trends, plateaus, or threshold effects that help explain model behaviour.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/visualization-methods/feature-relationships",
        "assurance-goal-category/explainability/attribution-methods/perturbation-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/fidelity",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing how house prices change with property size in a real estate prediction model, revealing whether the relationship is linear or if there are diminishing returns for very large properties.",
          "goal": "Explainability"
        },
        {
          "description": "Examining how customer age affects predicted loan default probability in a credit scoring model, showing whether risk increases steadily with age or has specific age ranges with higher risk.",
          "goal": "Explainability"
        },
        {
          "description": "Visualising how temperature affects crop yield predictions in agricultural models, identifying optimal temperature ranges and potential threshold effects.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Assumes features are independent when averaging, which can be misleading when features are highly correlated."
        },
        {
          "description": "Shows only average effects across all instances, potentially hiding important variations in how different subgroups respond to feature changes."
        },
        {
          "description": "Cannot reveal instance-specific effects or interactions between the plotted feature and other features."
        },
        {
          "description": "May be computationally expensive for large datasets since it requires making predictions across the full range of feature values."
        }
      ],
      "resources": [
        {
          "title": "DanielKerrigan/PDPilot",
          "url": "https://github.com/DanielKerrigan/PDPilot",
          "source_type": "software_package"
        },
        {
          "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation",
          "url": "http://arxiv.org/pdf/1309.6392v2",
          "source_type": "technical_paper",
          "authors": [
            "Alex Goldstein",
            "Adam Kapelner",
            "Justin Bleich",
            "Emil Pitkin"
          ],
          "publication_date": "2013-09-25"
        },
        {
          "title": "SauceCat/PDPbox",
          "url": "https://github.com/SauceCat/PDPbox",
          "source_type": "software_package"
        },
        {
          "title": "iPDP: On Partial Dependence Plots in Dynamic Modeling Scenarios",
          "url": "http://arxiv.org/pdf/2306.07775v1",
          "source_type": "technical_paper",
          "authors": [
            "Maximilian Muschalik",
            "Fabian Fumagalli",
            "Rohit Jagtani",
            "Barbara Hammer",
            "Eyke Hüllermeier"
          ],
          "publication_date": "2023-06-13"
        },
        {
          "title": "How to Interpret Models: PDP and ICE | Towards Data Science",
          "url": "https://towardsdatascience.com/how-to-interpret-models-pdp-and-ice-eabed0062e2c/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "acronym": "PDP",
      "related_techniques": [
        "individual-conditional-expectation-plots"
      ]
    },
    {
      "slug": "individual-conditional-expectation-plots",
      "name": "Individual Conditional Expectation Plots",
      "description": "ICE plots display the predicted output for individual instances as a function of a feature, with all other features held fixed for each instance. Each line on an ICE plot represents one instance's prediction trajectory as the feature of interest changes, revealing whether different instances are affected differently by that feature.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/visualization-methods/feature-relationships",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/property/fidelity",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/low",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/visualization"
      ],
      "example_use_cases": [
        {
          "description": "Examining how house price predictions vary with property age for individual properties, revealing that whilst most houses follow a declining price trend with age, historic properties (built before 1900) show different patterns due to heritage value.",
          "goal": "Explainability"
        },
        {
          "description": "Analysing how individual patients' diabetes risk predictions change with BMI, showing that whilst most patients follow the expected increasing risk pattern, some patients with specific genetic markers show different response curves.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Plots can become cluttered and difficult to interpret when displaying many instances simultaneously."
        },
        {
          "description": "Does not provide automatic summarisation of overall effects, requiring manual visual inspection to identify patterns."
        },
        {
          "description": "Still assumes all other features remain fixed at their observed values, which may not reflect realistic scenarios."
        },
        {
          "description": "Cannot reveal interactions between the plotted feature and other features for individual instances."
        }
      ],
      "resources": [
        {
          "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation",
          "url": "http://arxiv.org/pdf/1309.6392v2",
          "source_type": "technical_paper",
          "authors": [
            "Alex Goldstein",
            "Adam Kapelner",
            "Justin Bleich",
            "Emil Pitkin"
          ],
          "publication_date": "2013-09-25"
        },
        {
          "title": "Bringing a Ruler Into the Black Box: Uncovering Feature Impact from Individual Conditional Expectation Plots",
          "url": "http://arxiv.org/pdf/2109.02724v1",
          "source_type": "technical_paper",
          "authors": [
            "Andrew Yeh",
            "Anhthy Ngo"
          ],
          "publication_date": "2021-09-06"
        },
        {
          "title": "Explainable AI(XAI) - A guide to 7 packages in Python to explain ...",
          "url": "https://towardsdatascience.com/explainable-ai-xai-a-guide-to-7-packages-in-python-to-explain-your-models-932967f0634b/",
          "source_type": "tutorial"
        },
        {
          "title": "Communicating Uncertainty in Machine Learning Explanations: A Visualization Analytics Approach for Predictive Process Monitoring",
          "url": "https://www.semanticscholar.org/paper/3d0090df2b73369b502559eb49fd6d1ae432b952",
          "source_type": "technical_paper",
          "authors": [
            "Nijat Mehdiyev",
            "Maxim Majlatow",
            "Peter Fettke"
          ]
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "acronym": "ICE",
      "related_techniques": [
        "partial-dependence-plots"
      ]
    },
    {
      "slug": "factor-analysis",
      "name": "Factor Analysis",
      "description": "Factor analysis is a statistical technique that identifies latent variables (hidden factors) underlying observed correlations in data. It works by analysing how variables relate to each other, finding a smaller number of unobserved factors that explain patterns among multiple observed variables. Unlike PCA which maximises total variance, factor analysis focuses on shared variance (communalities - the variance variables have in common) whilst separating out unique variance and measurement error. After extracting factors, rotation methods like varimax (which creates uncorrelated factors) or oblimin (allowing correlated factors) help make factors more interpretable by aligning them with distinct groups of variables.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/structured-output",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing customer satisfaction surveys to identify key drivers (e.g., 'service quality', 'product value', 'convenience') from dozens of individual questions, helping businesses focus improvement efforts.",
          "goal": "Explainability"
        },
        {
          "description": "Reducing dimensionality of financial indicators to identify underlying economic factors (e.g., 'growth', 'inflation', 'credit risk') for more interpretable risk models.",
          "goal": "Explainability"
        },
        {
          "description": "Creating transparent feature groups for regulatory reporting by showing how multiple correlated features can be summarised into interpretable factors with clear business meaning.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Assumes linear relationships between variables and multivariate normality of data."
        },
        {
          "description": "Results can be abstract and require domain expertise to interpret meaningfully."
        },
        {
          "description": "Sensitive to the choice of number of factors and rotation method, which can significantly affect interpretability."
        },
        {
          "description": "Requires sufficiently large sample sizes relative to the number of variables for stable results."
        }
      ],
      "resources": [
        {
          "title": "Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey",
          "url": "http://arxiv.org/pdf/2101.00734v2",
          "source_type": "documentation",
          "authors": [
            "Benyamin Ghojogh",
            "Ali Ghodsi",
            "Fakhri Karray",
            "Mark Crowley"
          ],
          "publication_date": "2021-01-04"
        },
        {
          "title": "Factor Analysis in R Course | DataCamp",
          "url": "https://www.datacamp.com/courses/factor-analysis-in-r",
          "source_type": "tutorial"
        },
        {
          "title": "EducationalTestingService/factor_analyzer",
          "url": "https://github.com/EducationalTestingService/factor_analyzer",
          "source_type": "software_package"
        },
        {
          "title": "Confirmatory Factor Analysis Fundamentals | Towards Data Science",
          "url": "https://towardsdatascience.com/confirmatory-factor-analysis-theory-aac11af008a6/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "principal-component-analysis",
        "t-sne",
        "umap"
      ]
    },
    {
      "slug": "principal-component-analysis",
      "name": "Principal Component Analysis",
      "description": "Principal Component Analysis transforms high-dimensional data into a lower-dimensional representation by finding the directions (principal components) that capture the maximum variance in the data. Each component is a linear combination of original features, with the first component explaining the most variance, the second component the most remaining variance orthogonal to the first, and so on. This technique reveals underlying patterns in data structure, enables visualization of complex datasets, and helps identify which combinations of features drive the most variation in the data.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/property/comprehensibility",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/visualization",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing customer behavior data with dozens of variables (purchase frequency, spending patterns, demographics) to identify the 2-3 main dimensions that explain customer segmentation, revealing whether customers cluster by spending level, product preferences, or shopping frequency.",
          "goal": "Explainability"
        },
        {
          "description": "Reducing dimensionality of image data for facial recognition systems by finding the principal components that capture the most variation in face shapes and expressions, helping understand which facial features contribute most to distinguishing between individuals.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Principal components are abstract linear combinations of original features that often lack clear real-world interpretation or meaning."
        },
        {
          "description": "Only captures linear relationships between features, missing non-linear patterns and complex interactions in the data."
        },
        {
          "description": "Results are highly sensitive to feature scaling - features with larger numerical ranges can dominate the principal components."
        },
        {
          "description": "Information loss is inherent when reducing dimensions, and choosing the optimal number of components requires balancing simplicity with retained variance."
        }
      ],
      "resources": [
        {
          "title": "erdogant/pca",
          "url": "https://github.com/erdogant/pca",
          "source_type": "software_package"
        },
        {
          "title": "How to Calculate Principal Component Analysis (PCA) from Scratch ...",
          "url": "https://www.machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/",
          "source_type": "tutorial"
        },
        {
          "title": "A One-Stop Shop for Principal Component Analysis | Towards Data ...",
          "url": "https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c/",
          "source_type": "tutorial"
        },
        {
          "title": "Principal Component Analysis (PCA) with Scikit-Learn - KDnuggets",
          "url": "https://www.kdnuggets.com/2023/05/principal-component-analysis-pca-scikitlearn.html",
          "source_type": "tutorial"
        },
        {
          "title": "willtownes/glmpca-py",
          "url": "https://github.com/willtownes/glmpca-py",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "acronym": "PCA",
      "related_techniques": [
        "factor-analysis",
        "t-sne",
        "umap"
      ]
    },
    {
      "slug": "t-sne",
      "name": "t-SNE",
      "description": "t-SNE (t-Distributed Stochastic Neighbour Embedding) is a non-linear dimensionality reduction technique that creates 2D or 3D visualisations of high-dimensional data by preserving local neighbourhood relationships. The algorithm converts similarities between data points into joint probabilities in the high-dimensional space, then tries to minimise the divergence between these probabilities and those in the low-dimensional embedding. This approach excels at revealing cluster structures and local patterns, making it particularly effective for exploratory data analysis and understanding complex data relationships that linear methods like PCA might miss.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
        "assurance-goal-category/explainability/visualization-methods/feature-relationships",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/comprehensibility",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/visualization"
      ],
      "example_use_cases": [
        {
          "description": "Analyzing genomic data with thousands of gene expression features to visualize how different cancer subtypes cluster together, revealing which tumors have similar molecular signatures and potentially similar treatment responses.",
          "goal": "Explainability"
        },
        {
          "description": "Exploring deep learning model embeddings to understand how a neural network represents different categories of images, showing whether the model groups similar objects (cars, animals, furniture) in meaningful clusters in its internal feature space.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Non-deterministic algorithm produces different results on each run, making it difficult to reproduce exact visualizations or compare results across studies."
        },
        {
          "description": "Prioritizes preserving local neighborhood structure at the expense of global relationships, potentially creating misleading impressions about overall data topology."
        },
        {
          "description": "Computationally expensive with O(n²) complexity, making it impractical for datasets with more than ~10,000 points without approximation methods."
        },
        {
          "description": "Sensitive to hyperparameter choices (perplexity, learning rate, iterations) that can dramatically affect clustering patterns and require domain expertise to tune appropriately."
        }
      ],
      "resources": [
        {
          "title": "pavlin-policar/openTSNE",
          "url": "https://github.com/pavlin-policar/openTSNE",
          "source_type": "software_package"
        },
        {
          "title": "openTSNE: Extensible, parallel implementations of t-SNE ...",
          "url": "https://opentsne.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "How t-SNE works — openTSNE 1.0.0 documentation",
          "url": "https://opentsne.readthedocs.io/en/stable/tsne_algorithm.html",
          "source_type": "documentation"
        },
        {
          "title": "t-SNE from Scratch (ft. NumPy) | Towards Data Science",
          "url": "https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "related_techniques": [
        "factor-analysis",
        "principal-component-analysis",
        "umap"
      ]
    },
    {
      "slug": "umap",
      "name": "UMAP",
      "description": "UMAP (Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique that creates 2D or 3D visualisations of high-dimensional data by constructing a mathematical model of the data's underlying manifold structure. Unlike t-SNE, UMAP preserves both local neighbourhood relationships and global topology more effectively, using techniques from topological data analysis and Riemannian geometry. This approach often produces more interpretable cluster layouts while maintaining meaningful distances between clusters, making it particularly valuable for exploratory data analysis and understanding complex dataset structures.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/property/fidelity",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/visualization"
      ],
      "example_use_cases": [
        {
          "description": "Analysing single-cell RNA sequencing data to visualise how different cell types cluster based on gene expression patterns, revealing developmental trajectories and identifying previously unknown cell subtypes in tissue samples.",
          "goal": "Explainability"
        },
        {
          "description": "Exploring customer segmentation by reducing hundreds of behavioural and demographic features to 2D space, showing how different customer groups relate to each other and identifying transition zones where customers might move between segments.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Hyperparameter choices (n_neighbors, min_dist, metric) significantly influence the embedding structure and can lead to very different interpretations of the same data."
        },
        {
          "description": "While preserving global structure better than t-SNE, distances in the reduced space still don't directly correspond to distances in the original feature space."
        },
        {
          "description": "Performance can be sensitive to the choice of distance metric, which may not be obvious for complex or mixed data types."
        },
        {
          "description": "Like other manifold learning techniques, it assumes the data lies on a lower-dimensional manifold, which may not hold for all datasets."
        }
      ],
      "resources": [
        {
          "title": "lmcinnes/umap",
          "url": "https://github.com/lmcinnes/umap",
          "source_type": "software_package"
        },
        {
          "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
          "url": "http://arxiv.org/pdf/1802.03426v3",
          "source_type": "technical_paper",
          "authors": [
            "Leland McInnes",
            "John Healy",
            "James Melville"
          ],
          "publication_date": "2018-02-09"
        },
        {
          "title": "Uniform Manifold Approximation and Projection (UMAP) and its Variants: Tutorial and Survey",
          "url": "http://arxiv.org/pdf/2109.02508v1",
          "source_type": "documentation",
          "authors": [
            "Benyamin Ghojogh",
            "Ali Ghodsi",
            "Fakhri Karray",
            "Mark Crowley"
          ],
          "publication_date": "2021-08-25"
        },
        {
          "title": "How UMAP Works — umap 0.5.8 documentation",
          "url": "https://umap-learn.readthedocs.io/en/latest/how_umap_works.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "factor-analysis",
        "principal-component-analysis",
        "t-sne"
      ]
    },
    {
      "slug": "prototype-and-criticism-models",
      "name": "Prototype and Criticism Models",
      "description": "Prototype and Criticism Models provide data understanding by identifying two complementary sets of examples: prototypes represent the most typical instances that best summarise common patterns in the data, whilst criticisms are outliers or edge cases that are poorly represented by the prototypes. For example, in a dataset of customer transactions, prototypes might be the most representative buying patterns (frequent small purchases, occasional large purchases), whilst criticisms could be unusual behaviors (bulk buyers, one-time high-value customers). This dual approach reveals both what is normal and what is exceptional, helping understand data coverage and model blind spots.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/instance-based/prototypes",
        "assurance-goal-category/explainability/explains/decision-boundaries",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/structured-output",
        "expertise-needed/ml-engineering",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing medical imaging datasets to identify prototype scans that represent typical healthy tissue patterns and criticism examples showing rare disease presentations, helping radiologists understand what the model considers 'normal' versus cases requiring special attention.",
          "goal": "Explainability"
        },
        {
          "description": "Evaluating credit scoring models by finding prototype borrowers who represent typical low-risk profiles and criticism cases showing unusual but legitimate financial patterns that the model might misclassify, ensuring fair treatment of edge cases.",
          "goal": "Explainability"
        },
        {
          "description": "Evaluating representation bias in hiring datasets by examining whether prototypes systematically exclude certain demographic groups and criticisms disproportionately represent minorities, revealing data collection inequities.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Selection of prototypes and criticisms is highly dependent on the choice of distance metric or similarity measure, which may not capture all meaningful relationships in the data."
        },
        {
          "description": "Computational complexity can become prohibitive for very large datasets, as the method often requires pairwise comparisons or optimisation over the entire dataset."
        },
        {
          "description": "The number of prototypes and criticisms to select is typically a hyperparameter that requires domain expertise to set appropriately."
        },
        {
          "description": "Results may not generalise well if the training data distribution differs significantly from the deployment data distribution."
        }
      ],
      "resources": [
        {
          "title": "Examples are not Enough, Learn to Criticize! Criticism for Interpretability",
          "url": "https://proceedings.neurips.cc/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Been Kim",
            "Rajiv Khanna",
            "Oluwasanmi O. Koyejo"
          ],
          "publication_date": "2016-12-05"
        },
        {
          "title": "SeldonIO/alibi",
          "url": "https://github.com/SeldonIO/alibi",
          "source_type": "software_package"
        },
        {
          "title": "Prototype Selection for Interpretable Classification",
          "url": "http://arxiv.org/pdf/1202.5933v1",
          "source_type": "technical_paper",
          "authors": [
            "Oscar Reyes",
            "Carlos Morell",
            "Sebastian Ventura"
          ],
          "publication_date": "2012-02-27"
        },
        {
          "title": "Alibi Explain Documentation",
          "url": "https://docs.seldon.io/projects/alibi/en/stable/",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "neuron-activation-analysis",
        "concept-activation-vectors"
      ]
    },
    {
      "slug": "rulefit",
      "name": "RuleFit",
      "description": "RuleFit is a method that creates an interpretable model by combining linear terms with decision rules. It first extracts potential rules from ensemble trees, then builds a sparse linear model where those rules (binary conditions) and original features are used as predictors, with regularization to keep the model simple. The final model is a linear combination of a small set of rules and original features, balancing interpretability with predictive power.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/surrogate-models/rule-extraction",
        "assurance-goal-category/explainability/surrogate-models/global-surrogates",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/decision-boundaries",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Building customer churn prediction models with rules like 'IF contract_length < 12_months AND support_calls > 5 THEN churn_risk = high', allowing marketing teams to understand and act on the key drivers of customer attrition.",
          "goal": "Explainability"
        },
        {
          "description": "Creating credit scoring models that combine traditional linear factors (income, age) with interpretable rules (IF recent_missed_payments = 0 AND account_age > 2_years THEN creditworthy), providing transparent lending decisions.",
          "goal": "Explainability"
        },
        {
          "description": "Developing regulatory-compliant medical diagnosis models where treatment recommendations combine clinical measurements with clear decision rules (IF blood_pressure > 140 AND diabetes = true THEN high_risk), enabling audit trails for healthcare decisions.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Can generate large numbers of rules even with regularisation, potentially overwhelming users and reducing practical interpretability."
        },
        {
          "description": "Performance may be inferior to complex ensemble methods when rule complexity is constrained for interpretability."
        },
        {
          "description": "Rule extraction quality depends heavily on the underlying tree ensemble, which may miss important feature interactions if not properly configured."
        },
        {
          "description": "Requires careful hyperparameter tuning to balance between model complexity and interpretability, with no universal optimal setting."
        }
      ],
      "resources": [
        {
          "title": "christophM/rulefit",
          "url": "https://github.com/christophM/rulefit",
          "source_type": "software_package"
        },
        {
          "title": "Tree Ensembles with Rule Structured Horseshoe Regularization",
          "url": "http://arxiv.org/pdf/1702.05008v2",
          "source_type": "technical_paper",
          "authors": [
            "Malte Nalenz",
            "Mattias Villani"
          ],
          "publication_date": "2017-02-16"
        },
        {
          "title": "Safe RuleFit: Learning Optimal Sparse Rule Model by Meta Safe Screening",
          "url": "http://arxiv.org/pdf/1810.01683v2",
          "source_type": "technical_paper",
          "authors": [
            "Hiroki Kato",
            "Hiroyuki Hanada",
            "Ichiro Takeuchi"
          ],
          "publication_date": "2018-10-03"
        },
        {
          "title": "csinva/imodels",
          "url": "https://github.com/csinva/imodels",
          "source_type": "software_package"
        },
        {
          "title": "Getting More From Regression Models with RuleFit | Towards Data ...",
          "url": "https://towardsdatascience.com/getting-more-from-regression-models-with-rulefit-2e6be8d77432/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "ridge-regression-surrogates",
        "model-distillation"
      ]
    },
    {
      "slug": "monte-carlo-dropout",
      "name": "Monte Carlo Dropout",
      "description": "Monte Carlo Dropout estimates prediction uncertainty by applying dropout (randomly setting neural network weights to zero) during inference rather than just training. It performs multiple forward passes through the network with different random dropout patterns and collects the resulting predictions to form a distribution. Low variance across predictions indicates epistemic certainty (the model is confident), while high variance suggests epistemic uncertainty (the model is unsure). This technique transforms any dropout-trained neural network into a Bayesian approximation for uncertainty quantification.",
      "assurance_goals": [
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty",
        "assurance-goal-category/explainability/explains/prediction-confidence",
        "assurance-goal-category/explainability/property/consistency",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Quantifying diagnostic uncertainty in medical imaging models by running 50+ Monte Carlo forward passes to detect when a chest X-ray classification is highly uncertain, prompting radiologist review for borderline cases.",
          "goal": "Reliability"
        },
        {
          "description": "Estimating prediction confidence in autonomous vehicle perception systems, where high uncertainty in object detection (e.g., variance > 0.3 across MC samples) triggers more conservative driving behaviour or human handover.",
          "goal": "Reliability"
        },
        {
          "description": "Providing uncertainty estimates in financial fraud detection models, where high epistemic uncertainty (wide prediction variance) indicates the model lacks sufficient training data for similar transaction patterns, requiring manual review.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Only captures epistemic (model) uncertainty, not aleatoric (data) uncertainty, providing an incomplete picture of total prediction uncertainty."
        },
        {
          "description": "Computationally expensive as it requires multiple forward passes (typically 50-100) for each prediction, significantly increasing inference time."
        },
        {
          "description": "Results depend critically on dropout rate matching the training configuration, and poorly calibrated dropout can lead to misleading uncertainty estimates."
        },
        {
          "description": "Approximation quality varies with network architecture and dropout placement, with some configurations providing poor uncertainty calibration despite theoretical foundations."
        }
      ],
      "resources": [
        {
          "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
          "url": "http://arxiv.org/pdf/1506.02142v6",
          "source_type": "technical_paper",
          "authors": [
            "Yarin Gal",
            "Zoubin Ghahramani"
          ],
          "publication_date": "2016-06-06"
        },
        {
          "title": "mattiasegu/uncertainty_estimation_deep_learning",
          "url": "https://github.com/mattiasegu/uncertainty_estimation_deep_learning",
          "source_type": "software_package"
        },
        {
          "title": "uzh-rpg/deep_uncertainty_estimation",
          "url": "https://github.com/uzh-rpg/deep_uncertainty_estimation",
          "source_type": "software_package"
        },
        {
          "title": "How certain are tansformers in image classification: uncertainty analysis with Monte Carlo dropout",
          "url": "https://www.semanticscholar.org/paper/d7ff734c5b62a4a140fd560373d890e43d5b36cf",
          "source_type": "technical_paper",
          "authors": [
            "Md. Farhadul Islam",
            "Sarah Zabeen",
            "Md. Azharul Islam",
            "Fardin Bin Rahman",
            "Anushua Ahmed",
            "Dewan Ziaul Karim",
            "Annajiat Alim Rasel",
            "Meem Arafat Manab"
          ]
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "prediction-intervals",
        "quantile-regression",
        "conformal-prediction",
        "deep-ensembles",
        "bootstrapping",
        "jackknife-resampling"
      ]
    },
    {
      "slug": "out-of-distribution-detector-for-neural-networks",
      "name": "Out-of-DIstribution detector for Neural networks",
      "description": "ODIN (Out-of-Distribution Detector for Neural Networks) identifies when a neural network encounters inputs significantly different from its training distribution. It enhances detection by applying temperature scaling to soften the model's output distribution and adding small, carefully calibrated perturbations to the input that push in-distribution samples towards higher confidence predictions. By measuring the maximum softmax probability after these adjustments, ODIN can effectively distinguish between in-distribution and out-of-distribution inputs, flagging potentially unreliable predictions before they cause downstream errors.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty",
        "assurance-goal-category/explainability/attribution-methods/perturbation-based",
        "assurance-goal-category/explainability/explains/prediction-confidence",
        "assurance-goal-category/explainability/explains/decision-boundaries",
        "assurance-goal-category/explainability/property/consistency",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Detecting anomalous medical images in diagnostic systems, where ODIN flags X-rays or scans containing rare pathologies or imaging artefacts not present in training data, preventing misdiagnosis and prompting specialist review.",
          "goal": "Reliability"
        },
        {
          "description": "Protecting autonomous vehicle perception systems by identifying novel road scenarios (e.g., unusual weather conditions, rare obstacle types) that fall outside the training distribution, triggering fallback safety mechanisms.",
          "goal": "Safety"
        },
        {
          "description": "Monitoring production ML systems for data drift by detecting when incoming customer behaviour patterns deviate significantly from training data, helping explain why model performance may degrade over time.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Requires careful tuning of temperature scaling and perturbation magnitude parameters, which may need adjustment for different types of out-of-distribution data."
        },
        {
          "description": "Performance degrades when out-of-distribution samples are very similar to training data, making near-distribution detection challenging."
        },
        {
          "description": "Vulnerable to adversarial examples specifically crafted to evade detection by mimicking in-distribution characteristics."
        },
        {
          "description": "Computational overhead from input preprocessing and perturbation generation can impact real-time inference applications."
        }
      ],
      "resources": [
        {
          "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
          "url": "http://arxiv.org/pdf/1706.02690v5",
          "source_type": "technical_paper",
          "authors": [
            "Shiyu Liang",
            "Yixuan Li",
            "R. Srikant"
          ],
          "publication_date": "2017-06-08"
        },
        {
          "title": "facebookresearch/odin",
          "url": "https://github.com/facebookresearch/odin",
          "source_type": "software_package"
        },
        {
          "title": "Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data",
          "url": "http://arxiv.org/pdf/2002.11297v2",
          "source_type": "technical_paper",
          "authors": [
            "Yen-Chang Hsu",
            "Yilin Shen",
            "Hongxia Jin",
            "Zsolt Kira"
          ],
          "publication_date": "2020-02-26"
        },
        {
          "title": "Detection of out-of-distribution samples using binary neuron activation patterns",
          "url": "http://arxiv.org/abs/2212.14268",
          "source_type": "technical_paper",
          "authors": [
            "Chachuła, Krystian",
            "Olber, Bartlomiej",
            "Popowicz, Adam",
            "Radlak, Krystian",
            "Szczepankiewicz, Michal"
          ],
          "publication_date": "2023-03-24"
        },
        {
          "title": "Out-of-Distribution Detection with ODIN - A Tutorial",
          "url": "https://medium.com/@abhaypatil2000/out-of-distribution-detection-using-odin-f1a3e9e6b3b8",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "acronym": "ODIN",
      "related_techniques": [
        "anomaly-detection"
      ]
    },
    {
      "slug": "permutation-tests",
      "name": "Permutation Tests",
      "description": "Permutation tests assess the statistical significance of observed results (such as model accuracy, feature importance, or group differences) by comparing them to what would occur purely by chance. The technique randomly shuffles labels or data thousands of times, recalculating the metric of interest each time to build an empirical null distribution. If the actual observed result falls in the extreme tail of this distribution (typically beyond the 95th or 99th percentile), it provides strong evidence that the relationship is genuine rather than due to random chance, without requiring parametric assumptions about data distributions.",
      "assurance_goals": [
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/perturbation-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/consistency",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Validating feature importance in medical diagnosis models by permuting each feature 10,000 times to ensure that identified risk factors (e.g., blood pressure, cholesterol) have statistically significant predictive power beyond random chance.",
          "goal": "Reliability"
        },
        {
          "description": "Testing whether observed differences in loan approval rates between demographic groups are statistically significant by permuting group labels and calculating the approval rate difference distribution under the null hypothesis of no discrimination.",
          "goal": "Explainability"
        },
        {
          "description": "Verifying that a model's claimed 95% accuracy on test data is genuinely better than random guessing by permuting labels 5,000 times and confirming the actual accuracy falls beyond the 99th percentile of the null distribution.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive as it requires thousands of model evaluations or metric calculations, scaling poorly with dataset size and model complexity."
        },
        {
          "description": "Requires many permutations (typically 5,000-10,000) to achieve reliable p-values for strict significance thresholds like p < 0.01."
        },
        {
          "description": "Assumes exchangeability of observations under the null hypothesis, which may be violated in time series or hierarchical data structures."
        },
        {
          "description": "Cannot be easily parallelised for some metrics that require global model retraining, limiting scalability for complex machine learning pipelines."
        }
      ],
      "resources": [
        {
          "title": "Permutation Tests for Classification",
          "url": "https://core.ac.uk/download/4383831.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Golland, Polina",
            "Mukherjee, Sayan",
            "Panchenko, Dmitry"
          ],
          "publication_date": "2003-01-01"
        },
        {
          "title": "How to use Permutation Tests | Towards Data Science",
          "url": "https://towardsdatascience.com/how-to-use-permutation-tests-bacc79f45749/",
          "source_type": "tutorial"
        },
        {
          "title": "Permutation test in R | Towards Data Science",
          "url": "https://towardsdatascience.com/permutation-test-in-r-77d551a9f891/",
          "source_type": "tutorial"
        },
        {
          "title": "The Exchangeability Assumption for Permutation Tests of Multiple Regression Models: Implications for Statistics and Data Science Educators",
          "url": "http://arxiv.org/pdf/2406.07756v2",
          "source_type": "documentation",
          "authors": [
            "Johanna Hardin",
            "Lauren Quesada",
            "Julie Ye",
            "Nicholas J. Horton"
          ],
          "publication_date": "2024-06-11"
        },
        {
          "title": "scikit-learn permutation_importance",
          "url": "https://scikit-learn.org/stable/modules/permutation_importance.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "related_techniques": [
        "cross-validation",
        "area-under-precision-recall-curve"
      ]
    },
    {
      "slug": "empirical-calibration",
      "name": "Empirical Calibration",
      "description": "Empirical calibration adjusts a model's predicted probabilities to match observed frequencies. For example, if events predicted with 80% confidence only occur 60% of the time, calibration would correct this overconfidence. Common techniques include Platt scaling and isotonic regression, which learn transformations that map the model's raw scores to well-calibrated probabilities, improving the reliability of confidence measures for downstream decisions.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/calibration-set",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting a credit default prediction model's probabilities to ensure that loan applicants with a predicted 30% default risk actually default 30% of the time, improving decision-making.",
          "goal": "Reliability"
        },
        {
          "description": "Calibrating a medical diagnosis model's confidence scores so that stakeholders can meaningfully interpret probability outputs, enabling doctors to make informed decisions about treatment urgency based on reliable confidence estimates.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring that a hiring algorithm's confidence scores are equally well-calibrated across different demographic groups, preventing systematically overconfident predictions for certain populations that could lead to biased decision-making.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires a separate held-out calibration dataset, which reduces the amount of data available for model training."
        },
        {
          "description": "Calibration performance can degrade over time if the underlying data distribution shifts, requiring periodic recalibration."
        },
        {
          "description": "May sacrifice some discriminative power in favour of calibration, potentially reducing the model's ability to distinguish between classes."
        },
        {
          "description": "Calibration methods assume that the calibration set is representative of future data, which may not hold in dynamic environments."
        }
      ],
      "resources": [
        {
          "title": "google/empirical_calibration",
          "url": "https://github.com/google/empirical_calibration",
          "source_type": "software_package"
        },
        {
          "title": "A Python Library For Empirical Calibration",
          "url": "http://arxiv.org/pdf/1906.11920v2",
          "source_type": "technical_paper",
          "authors": [
            "Xiaojing Wang",
            "Jingang Miao",
            "Yunting Sun"
          ],
          "publication_date": "2019-07-25"
        },
        {
          "title": "Assessing the effectiveness of empirical calibration under different bias scenarios",
          "url": "http://arxiv.org/pdf/2111.04233v2",
          "source_type": "technical_paper",
          "authors": [
            "Hon Hwang",
            "Juan C Quiroz",
            "Blanca Gallego"
          ],
          "publication_date": "2021-11-08"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "related_techniques": [
        "temperature-scaling"
      ]
    },
    {
      "slug": "temperature-scaling",
      "name": "Temperature Scaling",
      "description": "Temperature scaling adjusts a model's confidence by applying a single parameter (temperature) to its predictions. When a model is too confident in its wrong answers, temperature scaling can fix this by making the predictions more realistic. It works by dividing the model's outputs by the temperature value before converting them to probabilities. Higher temperatures make the model less confident, whilst lower temperatures increase confidence. The technique maintains the model's accuracy whilst ensuring that when it says it's 90% confident, it's actually right about 90% of the time.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-requirements/calibration-set",
        "data-requirements/validation-set",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "explanatory-scope/global",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting a deep learning image classifier's confidence scores to be realistic, ensuring that when it's 90% confident, it's right 90% of the time.",
          "goal": "Reliability"
        },
        {
          "description": "Making medical diagnosis model predictions more trustworthy by providing realistic confidence scores that doctors can interpret and use to make informed decisions about patient care.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring fair treatment across patient demographics by calibrating confidence scores equally across different groups, preventing systematic over-confidence in predictions for certain populations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Only addresses calibration at the overall dataset level, not subgroup-specific miscalibration issues."
        },
        {
          "description": "Does not improve the rank ordering or accuracy of predictions, only adjusts confidence levels."
        },
        {
          "description": "Assumes that calibration errors are consistent across different types of inputs and feature values."
        },
        {
          "description": "Requires a separate validation set for temperature parameter optimisation, which may not be available in small datasets."
        }
      ],
      "resources": [
        {
          "title": "gpleiss/temperature_scaling",
          "url": "https://github.com/gpleiss/temperature_scaling",
          "source_type": "software_package"
        },
        {
          "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
          "url": "http://arxiv.org/pdf/2502.20604v1",
          "source_type": "technical_paper",
          "authors": [
            "Hao Xuan",
            "Bokai Yang",
            "Xingyu Li"
          ],
          "publication_date": "2025-02-28"
        },
        {
          "title": "Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration",
          "url": "http://arxiv.org/pdf/2209.11604v2",
          "source_type": "technical_paper",
          "authors": [
            "Yung-Chen Tang",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
          ],
          "publication_date": "2024-07-24"
        },
        {
          "title": "On Calibration of Modern Neural Networks | arXiv",
          "url": "https://arxiv.org/abs/1706.04599",
          "source_type": "technical_paper",
          "authors": [
            "Chuan Guo",
            "Geoff Pleiss",
            "Yu Sun",
            "Kilian Q. Weinberger"
          ],
          "publication_date": "2017-06-14"
        },
        {
          "title": "On the Limitations of Temperature Scaling for Distributions with Overlaps",
          "url": "http://arxiv.org/pdf/2306.00740v3",
          "source_type": "technical_paper",
          "authors": [
            "Muthu Chidambaram",
            "Rong Ge"
          ],
          "publication_date": "2023-06-01"
        }
      ],
      "complexity_rating": 1,
      "computational_cost_rating": 1,
      "related_techniques": [
        "empirical-calibration"
      ]
    },
    {
      "slug": "bootstrapping",
      "name": "Bootstrapping",
      "description": "Bootstrapping estimates uncertainty by repeatedly resampling the original dataset with replacement to create many new training sets, training a model on each sample, and analysing the variation in predictions. This approach provides confidence intervals and stability measures without making strong statistical assumptions. By showing how predictions change with different random samples of the data, it reveals how sensitive the model is to the specific training examples and provides robust uncertainty estimates.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-training-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "explanatory-scope/global",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Estimating uncertainty in financial risk models by resampling historical data to understand how predictions might vary under different historical scenarios.",
          "goal": "Reliability"
        },
        {
          "description": "Providing confidence intervals for medical diagnosis predictions to help doctors understand the reliability of AI recommendations and make more informed treatment decisions.",
          "goal": "Transparency"
        },
        {
          "description": "Assessing whether prediction uncertainty is consistent across different demographic groups in hiring algorithms, identifying if the model is systematically more uncertain for certain populations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive as it requires training multiple models on resampled datasets."
        },
        {
          "description": "Does not account for uncertainty in model structure or architecture choices."
        },
        {
          "description": "Cannot detect systematically missing data patterns or biases present in the original dataset."
        },
        {
          "description": "Assumes that the original dataset is representative of the population of interest."
        }
      ],
      "resources": [
        {
          "title": "Deterministic bootstrapping for a class of bootstrap methods",
          "url": "http://arxiv.org/pdf/1903.10816v2",
          "source_type": "technical_paper",
          "authors": [
            "Thomas Pitschel"
          ],
          "publication_date": "2019-03-26"
        },
        {
          "title": "A Gentle Introduction to the Bootstrap Method ...",
          "url": "https://www.machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/",
          "source_type": "tutorial"
        },
        {
          "title": "scipy.stats.bootstrap",
          "url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html",
          "source_type": "software_package"
        },
        {
          "title": "Bootstrapping and bagging — modAL documentation",
          "url": "https://modal-python.readthedocs.io/en/latest/content/examples/bootstrapping_and_bagging.html",
          "source_type": "tutorial"
        },
        {
          "title": "Machine Learning: What is Bootstrapping? - KDnuggets",
          "url": "https://www.kdnuggets.com/2023/03/bootstrapping.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "related_techniques": [
        "monte-carlo-dropout",
        "prediction-intervals",
        "quantile-regression",
        "conformal-prediction",
        "deep-ensembles",
        "jackknife-resampling"
      ]
    },
    {
      "slug": "intrinsically-interpretable-models",
      "name": "Intrinsically Interpretable Models",
      "description": "Intrinsically interpretable models are machine learning algorithms that are transparent by design, allowing users to understand their decision-making process without requiring additional explanation techniques. This category includes decision trees and rule lists (which use if-then logic), linear and logistic regression models (which use weighted feature combinations), and other simple algorithms where the model structure itself provides interpretability. These models prioritise transparency over complexity, making them ideal when stakeholder understanding and regulatory compliance are paramount.",
      "assurance_goals": [
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/tree-based",
        "applicable-models/linear",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/structured-output",
        "evidence-type/quantitative-metric",
        "expertise-needed/low",
        "explanatory-scope/global",
        "lifecycle-stage/project-design",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Developing a medical diagnosis support system using a decision tree with clear if-then rules based on symptoms and test results, allowing healthcare professionals to trace the reasoning path and explain diagnoses to patients whilst ensuring clinical transparency and accountability.",
          "goal": "Transparency"
        },
        {
          "description": "Creating a fraud detection model using logistic regression with carefully selected features (transaction amount, location, time patterns) where each coefficient's contribution can be understood and validated, ensuring reliable performance that financial institutions can audit and regulatory bodies can approve.",
          "goal": "Reliability"
        },
        {
          "description": "Implementing a hiring decision support tool using rule lists that explicitly state qualification criteria and scoring logic, providing transparent candidate evaluation that can be explained to applicants and reviewed for fairness whilst meeting legal requirements for employment decision documentation.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Generally achieve lower predictive accuracy than complex models (neural networks, ensembles) for difficult problems involving high-dimensional data, non-linear relationships, or complex feature interactions."
        },
        {
          "description": "Linear models cannot capture non-linear relationships or feature interactions without manual feature engineering, limiting their applicability to inherently non-linear domains like image recognition or natural language processing."
        },
        {
          "description": "Decision trees can become unstable with small changes in training data, potentially leading to completely different tree structures and predictions, affecting model reliability in dynamic environments."
        },
        {
          "description": "Deep decision trees may lose interpretability despite being inherently transparent, as human cognitive limits make it difficult to follow complex branching logic with many levels and conditions."
        },
        {
          "description": "Feature selection becomes critical for maintaining interpretability, requiring domain expertise to identify the most relevant variables whilst potentially missing important but subtle predictive signals."
        }
      ],
      "resources": [
        {
          "title": "scikit-learn Decision Trees",
          "url": "https://scikit-learn.org/stable/modules/tree.html",
          "source_type": "documentation",
          "description": "Comprehensive documentation for decision tree implementation in scikit-learn, including classification and regression trees with interpretability guidelines and visualisation tools."
        },
        {
          "title": "scikit-learn Linear Models",
          "url": "https://scikit-learn.org/stable/modules/linear_model.html",
          "source_type": "documentation",
          "description": "Complete guide to linear and logistic regression models in scikit-learn, covering implementation, feature selection, and coefficient interpretation for transparent modeling."
        },
        {
          "title": "Interpretable Machine Learning",
          "url": "https://christophm.github.io/interpretable-ml-book/",
          "source_type": "tutorial",
          "description": "Open-source book providing comprehensive coverage of interpretable machine learning models including decision trees, linear models, and rule-based systems with practical examples."
        },
        {
          "title": "R package 'rpart' for Recursive Partitioning",
          "url": "https://cran.r-project.org/web/packages/rpart/index.html",
          "source_type": "software_package",
          "description": "R implementation of recursive partitioning for classification, regression and survival trees with extensive documentation and plotting capabilities for interpretable tree models."
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "monotonicity-constraints",
        "generalized-additive-models"
      ]
    },
    {
      "slug": "generalized-additive-models",
      "name": "Generalized Additive Models",
      "description": "An intrinsically interpretable modelling technique that extends linear models by allowing flexible, nonlinear relationships between individual features and the target whilst maintaining the additive structure that preserves transparency. Each feature's effect is modelled separately as a smooth function, visualised as a curve showing how the feature influences predictions across its range. GAMs achieve this through spline functions or other smoothing techniques that capture complex patterns in individual variables without interactions, making them particularly valuable for domains requiring both predictive accuracy and model interpretability.",
      "assurance_goals": [
        "Transparency",
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/gam",
        "applicable-models/linear-model",
        "assurance-goal-category/transparency",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/surrogate-models/global-surrogates",
        "assurance-goal-category/explainability/visualization-methods/feature-relationships",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/reliability",
        "data-requirements/labelled-data",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/visualisation",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/project-design",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Predicting hospital readmission risk with a GAM that provides transparent, auditable risk assessments by showing how readmission probability varies nonlinearly with patient age, blood pressure, and medication adherence, enabling clinicians to understand and trust the model's reasoning for regulatory compliance.",
          "goal": "Transparency"
        },
        {
          "description": "Building a credit scoring model that explains loan decisions to applicants by visualising how income, credit history, and debt-to-income ratio individually affect approval likelihood, providing clear feature attributions that satisfy fair lending requirements and regulatory explainability mandates.",
          "goal": "Explainability"
        },
        {
          "description": "Developing an environmental monitoring system that reliably predicts air quality using GAMs to model the smooth, nonlinear relationships between weather variables, ensuring stable predictions across seasonal variations whilst maintaining interpretable relationships that environmental scientists can validate.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Cannot capture complex interactions between features unless explicitly modelled, limiting their ability to represent relationships where variables influence each other."
        },
        {
          "description": "Setup requires domain expertise to decide which features need nonlinear treatment and appropriate smoothing parameters, making model specification more challenging than linear models."
        },
        {
          "description": "Fitting process is computationally more expensive than linear models, particularly for large datasets with many features requiring smoothing."
        },
        {
          "description": "Risk of overfitting individual feature relationships if smoothing parameters are not properly regularised, potentially reducing generalisation performance."
        },
        {
          "description": "Interpretation complexity increases with the number of nonlinear features, as understanding multiple smooth curves simultaneously becomes cognitively demanding."
        }
      ],
      "resources": [
        {
          "title": "Generalized Additive Models",
          "url": "https://hastie.su.domains/Papers/gam.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Trevor Hastie",
            "Robert Tibshirani"
          ],
          "publication_date": "1986-01-01"
        },
        {
          "title": "pyGAM: Generalized Additive Models in Python",
          "url": "https://github.com/dswah/pyGAM",
          "source_type": "software_package"
        },
        {
          "title": "mgcv: Mixed GAM Computation Vehicle with Automatic Smoothness Estimation",
          "url": "https://cran.r-project.org/web/packages/mgcv/index.html",
          "source_type": "software_package"
        },
        {
          "title": "A Tour of pyGAM — pyGAM documentation",
          "url": "https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "GAMs",
      "related_techniques": [
        "monotonicity-constraints",
        "intrinsically-interpretable-models"
      ]
    },
    {
      "slug": "neuron-activation-analysis",
      "name": "Neuron Activation Analysis",
      "description": "Neuron activation analysis examines the firing patterns of individual neurons in neural networks by probing them with diverse inputs and analysing their activation responses. This technique helps understand what concepts, features, or patterns different neurons have learned to recognise, providing insights into the model's internal representations. For large language models, this can reveal neurons specialised for linguistic concepts, semantic categories, or even potentially harmful patterns, enabling targeted interventions and deeper model understanding.",
      "assurance_goals": [
        "Explainability",
        "Safety",
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/llm",
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/safety",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "evidence-type/visualisation",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "lifecycle-stage/monitoring",
        "technique-type/algorithmic",
        "assurance-goal-category/explainability/representation-analysis/concept-identification",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/property/comprehensibility"
      ],
      "example_use_cases": [
        {
          "description": "Analysing GPT-based models to identify specific neurons that activate on toxic or harmful content, enabling targeted interventions to reduce model toxicity whilst preserving general language capabilities for safer AI deployment.",
          "goal": "Safety"
        },
        {
          "description": "Examining activation patterns in multilingual language models to detect neurons that exhibit systematic biases when processing text from different linguistic communities, revealing implicit representation inequalities that could affect downstream applications.",
          "goal": "Fairness"
        },
        {
          "description": "Investigating individual neurons in medical language models to understand which clinical concepts and medical knowledge representations drive diagnostic suggestions, enabling healthcare professionals to validate the model's medical reasoning pathways.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Many neurons exhibit polysemantic behaviour, representing multiple unrelated concepts simultaneously, making it difficult to assign clear interpretable meanings to individual neural units."
        },
        {
          "description": "Important model behaviours are often distributed across many neurons rather than localised in single units, requiring analysis of neural circuits and interactions that can be exponentially complex."
        },
        {
          "description": "Computational costs scale dramatically with modern large language models containing billions of parameters, making comprehensive neuron-by-neuron analysis prohibitively expensive for complete model understanding."
        },
        {
          "description": "Neuron activation patterns are highly context-dependent, with the same neuron potentially serving different roles based on surrounding input context, complicating consistent interpretation across diverse scenarios."
        },
        {
          "description": "Interpretation of activation patterns often relies on subjective human analysis without rigorous validation methods, potentially leading to confirmation bias or misattribution of neural functions."
        }
      ],
      "resources": [
        {
          "title": "jalammar/ecco",
          "url": "https://github.com/jalammar/ecco",
          "source_type": "software_package"
        },
        {
          "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models",
          "url": "http://arxiv.org/pdf/2504.21053v1",
          "source_type": "technical_paper",
          "authors": [
            "Yi Zhou",
            "Wenpeng Xing",
            "Dezhang Kong",
            "Changting Lin",
            "Meng Han"
          ],
          "publication_date": "2025-04-29"
        },
        {
          "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron Activation Analysis",
          "url": "http://arxiv.org/pdf/2404.13567v1",
          "source_type": "technical_paper",
          "authors": [
            "Abhilekha Dalal",
            "Rushrukh Rayan",
            "Adrita Barua",
            "Eugene Y. Vasserman",
            "Md Kamruzzaman Sarker",
            "Pascal Hitzler"
          ],
          "publication_date": "2024-04-21"
        },
        {
          "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron\n  Activation Analysis",
          "url": "http://arxiv.org/abs/2404.13567",
          "source_type": "technical_paper",
          "authors": [
            "Barua, Adrita",
            "Dalal, Abhilekha",
            "Hitzler, Pascal",
            "Rayan, Rushrukh",
            "Sarker, Md Kamruzzaman",
            "Vasserman, Eugene Y."
          ],
          "publication_date": "2024-04-21"
        },
        {
          "title": "Ecco",
          "url": "https://ecco.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "Tracing the Thoughts in Language Models",
          "url": "https://www.anthropic.com/news/tracing-thoughts-language-model",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "prototype-and-criticism-models",
        "concept-activation-vectors"
      ]
    },
    {
      "slug": "concept-activation-vectors",
      "name": "Concept Activation Vectors",
      "description": "Concept Activation Vectors (CAVs), also known as Testing with Concept Activation Vectors (TCAV), identify mathematical directions in neural network representation space that correspond to human-understandable concepts such as 'stripes', 'young', or 'medical equipment'. The technique works by finding linear directions that separate activations of concept examples from non-concept examples, then measuring how much these concept directions influence the model's predictions. This provides quantitative answers to questions like 'How much does the concept of youth affect this model's hiring decisions?' enabling systematic bias detection and model understanding.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/transformer",
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/visualization",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-knowledge",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use/auditing",
        "technique-type/algorithmic",
        "assurance-goal-category/explainability/representation-analysis/concept-identification",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/causal-pathways",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/causality"
      ],
      "example_use_cases": [
        {
          "description": "Auditing a medical imaging model to verify it focuses on diagnostic features (like 'tumour characteristics') rather than irrelevant concepts (like 'scanner type' or 'patient positioning') when classifying chest X-rays, ensuring clinical decisions rely on medically relevant information.",
          "goal": "Explainability"
        },
        {
          "description": "Testing whether a hiring algorithm's resume screening decisions are influenced by concepts related to protected characteristics such as 'gender-associated names', 'prestigious universities', or 'employment gaps', enabling systematic bias detection and compliance verification.",
          "goal": "Fairness"
        },
        {
          "description": "Providing regulatory-compliant explanations for financial lending decisions by quantifying how concepts like 'debt-to-income ratio', 'employment stability', and 'credit history length' influence loan approval models, with precise sensitivity scores for audit documentation.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires clearly defined concept examples and non-concept examples, which can be challenging to obtain for abstract or subjective concepts."
        },
        {
          "description": "Assumes that meaningful concept directions exist as linear separable directions in the model's internal representation space, which may not hold for all concepts."
        },
        {
          "description": "Results depend heavily on which network layer is examined, as different layers capture different levels of abstraction and concept representation."
        },
        {
          "description": "Computational cost grows significantly with model size and number of concepts tested, though recent advances like FastCAV address this limitation."
        },
        {
          "description": "Interpretation requires domain expertise to define meaningful concepts and understand the significance of sensitivity scores in practical contexts."
        }
      ],
      "resources": [
        {
          "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks",
          "url": "http://arxiv.org/pdf/2505.17883v1",
          "source_type": "technical_paper",
          "authors": [
            "Laines Schmalwasser",
            "Niklas Penzel",
            "Joachim Denzler",
            "Julia Niebling"
          ],
          "publication_date": "2025-05-23"
        },
        {
          "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
          "url": "http://arxiv.org/pdf/2311.15303v1",
          "source_type": "technical_paper",
          "authors": [
            "Avani Gupta",
            "Saurabh Saini",
            "P J Narayanan"
          ],
          "publication_date": "2023-11-26"
        },
        {
          "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations",
          "url": "http://arxiv.org/pdf/2503.05522v1",
          "source_type": "technical_paper",
          "authors": [
            "Eren Erogullari",
            "Sebastian Lapuschkin",
            "Wojciech Samek",
            "Frederik Pahde"
          ],
          "publication_date": "2025-03-07"
        },
        {
          "title": "Concept Gradient: Concept-based Interpretation Without Linear Assumption",
          "url": "http://arxiv.org/pdf/2208.14966v2",
          "source_type": "technical_paper",
          "authors": [
            "Andrew Bai",
            "Chih-Kuan Yeh",
            "Pradeep Ravikumar",
            "Neil Y. C. Lin",
            "Cho-Jui Hsieh"
          ],
          "publication_date": "2022-08-31"
        },
        {
          "title": "SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation",
          "url": "http://arxiv.org/pdf/2310.07698v1",
          "source_type": "technical_paper",
          "authors": [
            "Bo Pan",
            "Zhenke Liu",
            "Yifei Zhang",
            "Liang Zhao"
          ],
          "publication_date": "2023-10-11"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "acronym": "CAVs",
      "related_techniques": [
        "prototype-and-criticism-models",
        "neuron-activation-analysis"
      ]
    },
    {
      "slug": "attention-visualisation-in-transformers",
      "name": "Attention Visualisation in Transformers",
      "description": "Attention Visualisation in Transformers analyses the multi-head self-attention mechanisms that enable transformers to process sequences by attending to different positions simultaneously. The technique visualises attention weights as heatmaps showing how strongly each token attends to every other token across different heads and layers. By examining these attention patterns, practitioners can understand how models like BERT, GPT, and T5 build contextual representations, identify which tokens influence predictions most strongly, and detect potential biases in how the model processes different types of input. This provides insights into positional encoding effects, head specialisation patterns, and the evolution of attention from local to global context across layers.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/feature-analysis",
        "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "technique-type/algorithmic",
        "assurance-goal-category/explainability/visualization-methods/attention-patterns",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/efficiency"
      ],
      "example_use_cases": [
        {
          "description": "Examining attention patterns in a medical language model processing clinical notes to verify it focuses on relevant symptoms and conditions rather than irrelevant demographic identifiers, revealing that certain attention heads specialise in medical terminology whilst others track syntactic relationships between diagnoses and treatments.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a sentiment analysis model for customer reviews by visualising how attention weights differ when processing reviews from different demographic groups, discovering that the model pays disproportionate attention to certain cultural expressions or colloquialisms that could lead to biased sentiment predictions.",
          "goal": "Fairness"
        },
        {
          "description": "Creating visual explanations for regulatory compliance in a financial document classification system, showing which specific words and phrases in loan applications or contracts triggered particular risk assessments, enabling auditors to verify that decisions are based on legitimate financial factors rather than discriminatory language patterns.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "High attention weights do not necessarily indicate causal importance for predictions, as models may attend strongly to tokens that serve structural rather than semantic purposes."
        },
        {
          "description": "The sheer number of attention heads and layers in modern transformers creates visualisation overload, making it difficult to identify meaningful patterns without systematic analysis tools."
        },
        {
          "description": "Attention patterns can be misleading when models use residual connections and layer normalisation, as the final representation incorporates information beyond what attention weights suggest."
        },
        {
          "description": "Different transformer architectures (encoder-only, decoder-only, encoder-decoder) exhibit fundamentally different attention patterns, limiting the generalisability of insights across model types."
        },
        {
          "description": "The technique cannot explain the reasoning process within feed-forward layers or how attention patterns translate into specific predictions, providing only a partial view of model behaviour."
        }
      ],
      "resources": [
        {
          "title": "jessevig/bertviz",
          "url": "https://github.com/jessevig/bertviz",
          "source_type": "software_package",
          "description": "Interactive tool for visualising attention patterns in transformer language models including BERT, GPT-2, and T5"
        },
        {
          "title": "Attention is All You Need",
          "url": "https://arxiv.org/abs/1706.03762",
          "source_type": "technical_paper",
          "description": "Foundational paper introducing the transformer architecture and self-attention mechanism"
        },
        {
          "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
          "url": "https://arxiv.org/abs/1905.09418",
          "source_type": "technical_paper",
          "description": "Research showing how different attention heads specialise in distinct linguistic phenomena"
        },
        {
          "title": "What Does BERT Look At? An Analysis of BERT's Attention",
          "url": "https://arxiv.org/abs/1906.04341",
          "source_type": "technical_paper",
          "description": "Comprehensive analysis of attention patterns in BERT revealing syntactic and semantic specialisation"
        },
        {
          "title": "Transformer Explainability Beyond Attention Visualization",
          "url": "https://arxiv.org/abs/2012.09838",
          "source_type": "technical_paper",
          "description": "Methods for attribution beyond raw attention weights including relevancy propagation and gradient-based approaches"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "integrated-gradients",
        "layer-wise-relevance-propagation",
        "saliency-maps",
        "gradient-weighted-class-activation-mapping",
        "classical-attention-analysis-in-neural-networks",
        "contrastive-explanation-method"
      ]
    }
  ],
  "count": 25
}