{
  "slug": "conformal-prediction",
  "name": "Conformal Prediction",
  "acronym": null,
  "description": "Conformal prediction provides mathematically guaranteed uncertainty quantification by creating prediction sets that contain the true outcome with a specified probability (e.g., exactly 95% coverage). The technique works by measuring how 'strange' or 'nonconforming' new predictions are compared to calibration data - if a prediction seems unusual, it gets wider intervals. For example, in medical diagnosis, instead of saying 'likely cancer', it might say 'possible diagnoses: {cancer, benign tumour} with 95% confidence'. This distribution-free method works with any underlying model (neural networks, random forests, etc.) and requires no assumptions about data distribution, making it a robust framework for reliable uncertainty estimates in high-stakes applications.",
  "complexity_rating": 4,
  "computational_cost_rating": 2,
  "assurance_goals": [
    {
      "id": 3,
      "name": "Reliability"
    },
    {
      "id": 6,
      "name": "Transparency"
    },
    {
      "id": 2,
      "name": "Fairness"
    }
  ],
  "tags": [
    {
      "name": "applicable-models/agnostic",
      "id": 1
    },
    {
      "name": "assurance-goal-category/reliability",
      "id": 27
    },
    {
      "name": "assurance-goal-category/reliability/uncertainty-quantification",
      "id": 28
    },
    {
      "name": "assurance-goal-category/transparency",
      "id": 31
    },
    {
      "name": "assurance-goal-category/fairness",
      "id": 20
    },
    {
      "name": "data-requirements/calibration-set",
      "id": 36
    },
    {
      "name": "data-type/any",
      "id": 46
    },
    {
      "name": "evidence-type/quantitative-metric",
      "id": 59
    },
    {
      "name": "evidence-type/prediction-interval",
      "id": 56
    },
    {
      "name": "expertise-needed/ml-engineering",
      "id": 73
    },
    {
      "name": "expertise-needed/statistics",
      "id": 79
    },
    {
      "name": "lifecycle-stage/model-development",
      "id": 92
    },
    {
      "name": "lifecycle-stage/system-deployment-and-use",
      "id": 103
    },
    {
      "name": "technique-type/algorithmic",
      "id": 107
    }
  ],
  "related_techniques": [
    "monte-carlo-dropout",
    "prediction-intervals",
    "quantile-regression",
    "deep-ensembles",
    "bootstrapping",
    "jackknife-resampling"
  ],
  "related_technique_slugs": [
    "monte-carlo-dropout",
    "prediction-intervals",
    "quantile-regression",
    "deep-ensembles",
    "bootstrapping",
    "jackknife-resampling"
  ],
  "resources": [
    {
      "title": "A tutorial on conformal prediction",
      "url": "http://arxiv.org/pdf/0706.3188v1",
      "description": "Foundational tutorial introducing conformal prediction theory and applications by the method's creators",
      "authors": ["Glenn Shafer", "Vladimir Vovk"],
      "publication_date": "2007-06-21",
      "source_type": "introductory_paper",
      "resource_type": "documentation"
    },
    {
      "title": "valeman/awesome-conformal-prediction",
      "url": "https://github.com/valeman/awesome-conformal-prediction",
      "description": "Curated collection of conformal prediction resources including videos, tutorials, books, papers, and open-source libraries",
      "authors": "",
      "publication_date": null,
      "source_type": "software_package",
      "resource_type": "documentation"
    },
    {
      "title": "scikit-learn-contrib/MAPIE",
      "url": "https://github.com/scikit-learn-contrib/MAPIE",
      "description": "Python library for uncertainty quantification using conformal prediction across regression, classification, and time series tasks",
      "authors": "",
      "publication_date": null,
      "source_type": "software_package",
      "resource_type": "documentation"
    },
    {
      "title": "Tutorial for classification â€” MAPIE 0.8.6 documentation",
      "url": "https://mapie.readthedocs.io/en/v0.8.6/examples_classification/4-tutorials/plot_main-tutorial-classification.html",
      "description": "Practical tutorial demonstrating conformal prediction for classification tasks with guaranteed coverage",
      "authors": "",
      "publication_date": null,
      "source_type": "tutorial",
      "resource_type": "documentation"
    },
    {
      "title": "Conformal Prediction: a Unified Review of Theory and New Challenges",
      "url": "http://arxiv.org/pdf/2005.07972v2",
      "description": "Comprehensive review of conformal prediction theory, recent advances, and emerging challenges in the field",
      "authors": ["Matteo Fontana", "Gianluca Zeni", "Simone Vantini"],
      "publication_date": "2020-05-16",
      "source_type": "review_paper",
      "resource_type": "documentation"
    }
  ],
  "example_use_cases": [
    {
      "description": "Creating prediction sets for drug discovery that guarantee 95% coverage, such as 'this compound will likely have activity against {target A, target B, target C}', ensuring reliable decision-making in costly experimental validation.",
      "goal": "Reliability"
    },
    {
      "description": "Providing transparent multi-class predictions in judicial risk assessment by showing all plausible risk categories with guaranteed coverage, enabling judges to see the full range of possibilities rather than just a single point estimate.",
      "goal": "Transparency"
    },
    {
      "description": "Ensuring fair uncertainty quantification across demographic groups in college admissions by verifying that prediction set sizes (number of possible outcomes) are consistent across protected groups, preventing discriminatory overconfidence for certain populations.",
      "goal": "Fairness"
    }
  ],
  "limitations": [
    {
      "description": "Prediction sets can be unnecessarily wide when nonconformity scores vary greatly across the feature space, leading to conservative intervals that reduce practical utility."
    },
    {
      "description": "Requires a held-out calibration set separate from training data, reducing the amount of data available for model training, which can impact performance on small datasets."
    },
    {
      "description": "Guarantees only hold under the exchangeability assumption - if test data distribution differs significantly from calibration data, coverage guarantees may be violated."
    },
    {
      "description": "For multi-class problems, prediction sets may include many classes when the model is uncertain, making decisions difficult when sets contain opposing outcomes."
    },
    {
      "description": "Computational cost increases with the number of calibration samples, and efficient implementation requires careful design for large-scale or real-time applications."
    }
  ]
}
