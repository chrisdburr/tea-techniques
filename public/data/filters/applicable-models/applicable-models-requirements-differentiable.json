{
  "tag": {
    "name": "applicable-models/requirements/differentiable",
    "slug": "applicable-models-requirements-differentiable",
    "count": 9,
    "category": "applicable-models"
  },
  "techniques": [
    {
      "slug": "integrated-gradients",
      "name": "Integrated Gradients",
      "description": "Integrated Gradients is an attribution technique that explains a model's prediction by quantifying the contribution of each input feature. It works by accumulating gradients along a straight path from a user-defined baseline input (e.g., a black image or an all-zero vector) to the actual input. This path integral ensures that the attributions satisfy fundamental axioms like completeness (attributions sum up to the difference between the prediction and the baseline prediction) and sensitivity (non-zero attributions for features that change the prediction). The output is a set of importance scores, often visualised as heatmaps, indicating which parts of the input were most influential for the model's decision.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/explainability/property/consistency",
        "data-requirements/labelled-data",
        "data-requirements/reference-dataset",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic",
        "applicable-models/architecture/neural-networks",
        "applicable-models/paradigm/parametric",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/differentiable"
      ],
      "example_use_cases": [
        {
          "description": "Analysing a medical image classification model to understand which specific pixels or regions in an X-ray image contribute most to a diagnosis of pneumonia, ensuring the model focuses on relevant pathological features rather than artifacts.",
          "goal": "Explainability"
        },
        {
          "description": "Explaining the sentiment prediction of a natural language processing model by highlighting which words or phrases in a review most strongly influenced its classification as positive or negative, revealing the model's interpretative focus.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Requires a carefully chosen and meaningful baseline input; an inappropriate baseline can lead to misleading or uninformative attributions."
        },
        {
          "description": "The model must be differentiable, which limits its direct application to models with non-differentiable components or discrete inputs without workarounds."
        },
        {
          "description": "Computationally more expensive than simple gradient-based methods, as it requires multiple gradient calculations along the integration path."
        },
        {
          "description": "While satisfying completeness, the attributions can sometimes be visually noisy or difficult for humans to interpret intuitively, especially for complex inputs."
        }
      ],
      "resources": [
        {
          "title": "ankurtaly/Integrated-Gradients",
          "url": "https://github.com/ankurtaly/Integrated-Gradients",
          "source_type": "software_package"
        },
        {
          "title": "pytorch/captum",
          "url": "https://github.com/pytorch/captum",
          "source_type": "software_package"
        },
        {
          "title": "Maximum Entropy Baseline for Integrated Gradients",
          "url": "http://arxiv.org/pdf/2204.05948v1",
          "source_type": "technical_paper",
          "authors": [
            "Hanxiao Tan"
          ],
          "publication_date": "2022-04-12"
        },
        {
          "title": "Integrated Gradients from Scratch | Towards Data Science",
          "url": "https://towardsdatascience.com/integrated-gradients-from-scratch-b46311e4ab4/",
          "source_type": "tutorial"
        },
        {
          "title": "Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution",
          "url": "http://arxiv.org/pdf/2004.10484v2",
          "source_type": "technical_paper",
          "authors": [
            "Gary S. W. Goh",
            "Sebastian Lapuschkin",
            "Leander Weber",
            "Wojciech Samek",
            "Alexander Binder"
          ],
          "publication_date": "2020-04-22"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "shapley-additive-explanations",
        "deeplift",
        "layer-wise-relevance-propagation",
        "local-interpretable-model-agnostic-explanations",
        "contrastive-explanation-method",
        "anchor"
      ]
    },
    {
      "slug": "deeplift",
      "name": "DeepLIFT",
      "description": "DeepLIFT (Deep Learning Important FeaTures) explains neural network predictions by decomposing the difference between the actual output and a reference output back to individual input features. It compares each neuron's activation to a reference activation (typically from a baseline input like all zeros or the dataset mean) and propagates these differences backwards through the network using chain rule modifications. Unlike gradient-based methods, DeepLIFT satisfies the sensitivity property (zero input gets zero attribution) and provides more stable attributions by using discrete differences rather than gradients.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/explainability/property/consistency",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-requirements/reference-dataset",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic",
        "applicable-models/architecture/neural-networks",
        "applicable-models/requirements/white-box",
        "applicable-models/requirements/differentiable"
      ],
      "example_use_cases": [
        {
          "description": "Identifying which genomic sequences contribute to a neural network's prediction of protein binding sites, helping biologists understand regulatory mechanisms by comparing to neutral DNA baselines.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging a deep learning image classifier that misclassifies medical scans by attributing the decision to specific image regions, revealing if the model focuses on irrelevant artifacts rather than pathological features.",
          "goal": "Explainability"
        },
        {
          "description": "Providing transparent explanations for automated loan approval decisions by showing which financial features (relative to typical applicant profiles) most influenced the neural network's recommendation.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires careful selection of reference baseline, as different choices can lead to substantially different attribution scores."
        },
        {
          "description": "Implementation complexity varies significantly across different neural network architectures and layer types."
        },
        {
          "description": "May produce unintuitive results when the chosen reference is not representative of the decision boundary."
        },
        {
          "description": "Limited to feedforward networks and specific layer types, not suitable for all modern architectures like transformers."
        }
      ],
      "resources": [
        {
          "title": "Learning Important Features Through Propagating Activation Differences",
          "url": "http://arxiv.org/pdf/1704.02685v2",
          "source_type": "technical_paper",
          "authors": [
            "Avanti Shrikumar",
            "Peyton Greenside",
            "Anshul Kundaje"
          ],
          "publication_date": "2017-04-10"
        },
        {
          "title": "pytorch/captum",
          "url": "https://github.com/pytorch/captum",
          "source_type": "software_package"
        },
        {
          "title": "Tutorial A3: DeepLIFT/SHAP — tangermeme v0.1.0 documentation",
          "url": "https://tangermeme.readthedocs.io/en/latest/tutorials/Tutorial_A3_Deep_LIFT_SHAP.html",
          "source_type": "tutorial"
        },
        {
          "title": "DeepLIFT Documentation - Captum",
          "url": "https://captum.ai/api/deep_lift.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "shapley-additive-explanations",
        "integrated-gradients",
        "layer-wise-relevance-propagation",
        "local-interpretable-model-agnostic-explanations",
        "contrastive-explanation-method",
        "anchor"
      ]
    },
    {
      "slug": "layer-wise-relevance-propagation",
      "name": "Layer-wise Relevance Propagation",
      "description": "Layer-wise Relevance Propagation (LRP) explains neural network predictions by working backwards through the network to show how much each input feature contributed to the final decision. It follows a simple conservation rule: the total contribution scores always add up to the original prediction. Starting from the output, LRP distributes 'relevance' backwards through each layer using different rules depending on the layer type. This creates a detailed breakdown showing which input features helped or hindered the prediction, making it easier to understand why the network made a particular decision.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/model-specific",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/explainability/property/consistency",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic",
        "applicable-models/architecture/neural-networks",
        "applicable-models/paradigm/parametric",
        "applicable-models/requirements/white-box",
        "applicable-models/requirements/differentiable"
      ],
      "example_use_cases": [
        {
          "description": "Identifying which pixels in chest X-rays contribute to pneumonia detection, helping radiologists verify AI diagnoses by highlighting anatomical regions the model considers relevant.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging a neural network's misclassification of handwritten digits by tracing relevance through layers to identify which input pixels caused the error and which network layers amplified it.",
          "goal": "Explainability"
        },
        {
          "description": "Providing transparent explanations for automated credit scoring decisions by showing which financial features received positive or negative relevance scores, enabling clear regulatory reporting.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires different propagation rules for each layer type, making implementation complex for new architectures."
        },
        {
          "description": "Can produce negative relevance scores which may be difficult to interpret intuitively."
        },
        {
          "description": "Rule selection (LRP-ε, LRP-γ, etc.) significantly affects results and requires domain expertise."
        },
        {
          "description": "Limited to feedforward networks and may not work well with modern architectures like transformers without substantial modifications."
        }
      ],
      "resources": [
        {
          "title": "rachtibat/LRP-eXplains-Transformers",
          "url": "https://github.com/rachtibat/LRP-eXplains-Transformers",
          "source_type": "software_package"
        },
        {
          "title": "sebastian-lapuschkin/lrp_toolbox",
          "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
          "source_type": "software_package"
        },
        {
          "title": "Getting started — zennit documentation",
          "url": "https://zennit.readthedocs.io/en/latest/getting-started.html",
          "source_type": "documentation"
        },
        {
          "title": "Layer-wise Relevance Propagation eXplains Transformers (LXT) documentation",
          "url": "https://lxt.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation",
          "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140",
          "source_type": "technical_paper",
          "authors": [
            "Sebastian Bach",
            "Alexander Binder",
            "Grégoire Montavon",
            "Frederick Klauschen",
            "Klaus-Robert Müller",
            "Wojciech Samek"
          ],
          "publication_date": "2015-07-10"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 3,
      "acronym": "LRP",
      "related_techniques": [
        "shapley-additive-explanations",
        "integrated-gradients",
        "deeplift",
        "local-interpretable-model-agnostic-explanations",
        "contrastive-explanation-method",
        "anchor"
      ]
    },
    {
      "slug": "taylor-decomposition",
      "name": "Taylor Decomposition",
      "description": "Taylor Decomposition is a mathematical technique that explains neural network predictions by computing first-order and higher-order derivatives of the network's output with respect to input features. It decomposes the prediction into relevance scores that indicate how much each input feature and feature interaction contributes to the final decision. The method uses Layer-wise Relevance Propagation (LRP) principles to trace prediction contributions backwards through the network layers, providing precise mathematical attributions for each input element.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/explainability/property/fidelity",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic",
        "applicable-models/architecture/neural-networks",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/differentiable",
        "applicable-models/requirements/white-box"
      ],
      "example_use_cases": [
        {
          "description": "Analyzing which pixels in an image contribute most to a convolutional neural network's classification decision, showing both positive and negative relevance scores for different regions of the input image.",
          "goal": "Explainability"
        },
        {
          "description": "Understanding how different word embeddings in a sentiment analysis model contribute to the final sentiment score, revealing which terms drive positive vs negative predictions.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Mathematically complex requiring deep understanding of calculus and neural network architectures."
        },
        {
          "description": "Computationally intensive as it requires computing gradients and higher-order derivatives through the entire network."
        },
        {
          "description": "Approximations used in practice may introduce errors that affect attribution accuracy."
        },
        {
          "description": "Limited tooling availability compared to other explainability methods, with most implementations being research-focused rather than production-ready."
        }
      ],
      "resources": [
        {
          "title": "Explaining NonLinear Classification Decisions with Deep Taylor Decomposition",
          "url": "http://arxiv.org/pdf/1512.02479v1",
          "source_type": "technical_paper",
          "authors": [
            "Grégoire Montavon",
            "Sebastian Bach",
            "Alexander Binder",
            "Wojciech Samek",
            "Klaus-Robert Müller"
          ],
          "publication_date": "2015-12-08"
        },
        {
          "title": "A Rigorous Study Of The Deep Taylor Decomposition",
          "url": "http://arxiv.org/pdf/2211.08425v1",
          "source_type": "technical_paper",
          "authors": [
            "Leon Sixt",
            "Tim Landgraf"
          ],
          "publication_date": "2022-11-14"
        },
        {
          "title": "sebastian-lapuschkin/lrp_toolbox",
          "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "contextual-decomposition",
        "influence-functions"
      ]
    },
    {
      "slug": "saliency-maps",
      "name": "Saliency Maps",
      "description": "Saliency maps are visual explanations for image classification models that highlight which pixels in an image most strongly influence the model's prediction. Computed by calculating gradients of the model's output with respect to input pixels, saliency maps produce heatmaps where brighter regions indicate pixels that, when changed, would most significantly affect the prediction. This technique helps users understand which parts of an image the model is 'looking at' when making decisions.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/requirements/differentiable",
        "applicable-models/requirements/gradient-access",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/visualization-methods/activation-maps",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing X-ray images in a pneumonia detection model to verify that the algorithm focuses on lung regions showing inflammatory patterns rather than irrelevant areas like medical equipment or patient positioning markers.",
          "goal": "Explainability"
        },
        {
          "description": "Examining skin lesion classification models to ensure the algorithm identifies diagnostic features (irregular borders, colour variation) rather than artifacts like rulers, hair, or skin markings that shouldn't influence medical decisions.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a dermatology AI system to verify it focuses on medical symptoms rather than skin colour when diagnosing conditions, ensuring equitable treatment across racial groups by revealing inappropriate attention to demographic features.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Saliency maps are often noisy and can change dramatically with small input perturbations, making them unstable."
        },
        {
          "description": "Highlighted regions may not correspond to semantically meaningful or human-understandable features."
        },
        {
          "description": "Only indicates local gradient information, not causal importance or actual decision-making logic."
        },
        {
          "description": "May highlight irrelevant pixels that happen to have high gradients due to model artifacts rather than meaningful patterns."
        }
      ],
      "resources": [
        {
          "title": "utkuozbulak/pytorch-cnn-visualizations",
          "url": "https://github.com/utkuozbulak/pytorch-cnn-visualizations",
          "source_type": "software_package"
        },
        {
          "title": "Concepts of Saliency and Explainability in AI",
          "url": "https://xaitk-saliency.readthedocs.io/en/latest/xaitk_explanation.html",
          "source_type": "documentation"
        },
        {
          "title": "Occlusion Saliency Example",
          "url": "https://xaitk-saliency.readthedocs.io/en/latest/examples/OcclusionSaliency.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "gradient-weighted-class-activation-mapping",
        "occlusion-sensitivity"
      ]
    },
    {
      "slug": "influence-functions",
      "name": "Influence Functions",
      "description": "Influence functions quantify how much each training example influenced a model's predictions by computing the change in prediction that would occur if that training example were removed and the model retrained. Using calculus and the implicit function theorem, they approximate this 'leave-one-out' effect without actually retraining the model by computing gradients and Hessian information. This mathematical approach reveals which specific training examples were most responsible for pushing the model toward or away from particular predictions, enabling practitioners to trace problematic outputs back to their root causes in the training data.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Privacy"
      ],
      "tags": [
        "applicable-models/architecture/linear-models",
        "applicable-models/architecture/neural-networks",
        "applicable-models/paradigm/parametric",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/differentiable",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/training-data",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/causal-pathways",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/instance-based/influence-analysis",
        "assurance-goal-category/explainability/property/causality",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/fairness",
        "assurance-goal-category/privacy",
        "data-requirements/access-to-training-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Investigating why a medical diagnosis model misclassified a patient by identifying which specific training cases most influenced the incorrect prediction, revealing potential mislabelled examples or problematic patterns in the training data.",
          "goal": "Explainability"
        },
        {
          "description": "Analysing a spam detection system that falsely flagged legitimate emails by tracing the prediction back to influential training examples, discovering that certain training emails contained misleading patterns that caused the model to overfit.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a loan approval model for discriminatory patterns by identifying which training examples most influenced rejections of minority applicants, revealing whether biased historical decisions are driving current unfair outcomes.",
          "goal": "Fairness"
        },
        {
          "description": "Assessing membership inference risks in a medical model by identifying whether certain patient records have disproportionate influence on predictions, indicating potential data leakage vulnerabilities.",
          "goal": "Privacy"
        }
      ],
      "limitations": [
        {
          "description": "Computationally intensive, requiring Hessian matrix computations that become intractable for very large models with millions of parameters."
        },
        {
          "description": "Requires access to the complete training dataset and training process, making it impossible to apply to pre-trained models without access to original training data."
        },
        {
          "description": "Accuracy degrades for highly non-convex models where the linear approximation underlying influence functions breaks down."
        },
        {
          "description": "Results can be sensitive to hyperparameter choices and may not generalise well across different model architectures or training procedures."
        }
      ],
      "resources": [
        {
          "title": "Understanding Black-box Predictions via Influence Functions",
          "url": "https://www.semanticscholar.org/paper/08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
          "source_type": "technical_paper",
          "authors": [
            "Pang Wei Koh",
            "Percy Liang"
          ]
        },
        {
          "title": "nimarb/pytorch_influence_functions",
          "url": "https://github.com/nimarb/pytorch_influence_functions",
          "source_type": "software_package"
        },
        {
          "title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions",
          "url": "https://www.semanticscholar.org/paper/f33f3dece9f34c1ec5417dccf9e0acf592d8e8cb",
          "source_type": "technical_paper",
          "authors": [
            "Sang Keun Choe",
            "Hwijeen Ahn",
            "Juhan Bae",
            "Kewen Zhao",
            "Minsoo Kang",
            "Youngseog Chung",
            "Adithya Pratapa",
            "W. Neiswanger",
            "Emma Strubell",
            "Teruko Mitamura",
            "Jeff G. Schneider",
            "Eduard H. Hovy",
            "Roger B. Grosse",
            "Eric P. Xing"
          ]
        },
        {
          "title": "Scaling Up Influence Functions",
          "url": "https://www.semanticscholar.org/paper/ef2a773c3c7848a6cc16b18164be5f8876a310af",
          "source_type": "technical_paper",
          "authors": [
            "Andrea Schioppa",
            "Polina Zablotskaia",
            "David Vilar",
            "Artem Sokolov"
          ]
        },
        {
          "title": "Welcome to torch-influence's API Reference! — torch-influence 0.1.0 ...",
          "url": "https://torch-influence.readthedocs.io/",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "contextual-decomposition",
        "taylor-decomposition"
      ]
    },
    {
      "slug": "contrastive-explanation-method",
      "name": "Contrastive Explanation Method",
      "description": "The Contrastive Explanation Method (CEM) explains model decisions by generating contrastive examples that reveal what makes a prediction distinctive. It identifies 'pertinent negatives' (minimal features that could be removed to change the prediction) and 'pertinent positives' (minimal features that must be present to maintain the prediction). This approach helps users understand not just what led to a decision, but what would need to change to achieve a different outcome, providing actionable insights for decision-making.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/paradigm/discriminative",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/differentiable",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/perturbation-based",
        "assurance-goal-category/explainability/explains/decision-boundaries",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/instance-based/counterfactual",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/counterfactual-validity",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Explaining loan application rejections by showing that removing recent late payments (pertinent negative) or adding £5,000 more annual income (pertinent positive) would change the decision to approval, giving applicants clear actionable guidance.",
          "goal": "Explainability"
        },
        {
          "description": "Analysing medical diagnosis models by identifying that removing a specific symptom combination would change a high-risk classification to low-risk, helping clinicians understand the critical diagnostic factors.",
          "goal": "Explainability"
        },
        {
          "description": "Providing transparent hiring decisions by showing job candidates exactly which qualifications (pertinent positives) they need to acquire or which application elements (pertinent negatives) might be hindering their success.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive as it requires solving an optimisation problem for each individual instance to find minimal perturbations."
        },
        {
          "description": "Results can be highly sensitive to hyperparameter settings, requiring careful tuning to produce meaningful explanations."
        },
        {
          "description": "May generate unrealistic or impossible contrastive examples if constraints are not properly specified, leading to impractical recommendations."
        },
        {
          "description": "Limited to scenarios where feature perturbations are meaningful and actionable, making it less suitable for immutable characteristics or highly constrained domains."
        }
      ],
      "resources": [
        {
          "title": "Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives",
          "url": "http://arxiv.org/pdf/1802.07623v2",
          "source_type": "technical_paper",
          "authors": [
            "Amit Dhurandhar",
            "Pin-Yu Chen",
            "Ronny Luss",
            "Chun-Chen Tu",
            "Paishun Ting",
            "Karthikeyan Shanmugam",
            "Payel Das"
          ],
          "publication_date": "2018-02-21"
        },
        {
          "title": "Interpretable Machine Learning",
          "url": "https://christophm.github.io/interpretable-ml-book/interpretability.html",
          "source_type": "documentation"
        },
        {
          "title": "Benchmarking and survey of explanation methods for black box models",
          "url": "https://core.ac.uk/download/599106733.pdf",
          "source_type": "documentation",
          "authors": [
            "Bodria Francesco",
            "Giannotti Fosca",
            "Guidotti R.",
            "Naretto Francesca",
            "Pedreschi Dino",
            "Rinzivillo S."
          ],
          "publication_date": "2023-01-01"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "acronym": "CEM",
      "related_techniques": [
        "shapley-additive-explanations",
        "integrated-gradients",
        "deeplift",
        "layer-wise-relevance-propagation",
        "local-interpretable-model-agnostic-explanations",
        "anchor"
      ]
    },
    {
      "slug": "adversarial-debiasing",
      "name": "Adversarial Debiasing",
      "description": "Adversarial debiasing reduces bias by training models using a competitive adversarial setup, similar to Generative Adversarial Networks (GANs). The technique involves two neural networks: a predictor that learns to make accurate predictions on the main task, and an adversary (bias detector) that attempts to predict protected attributes (such as race, gender, or age) from the predictor's internal representations. Through adversarial training, the predictor learns to produce representations that retain predictive power for the main task whilst being uninformative about protected characteristics, thereby reducing discriminatory bias.",
      "assurance_goals": [
        "Fairness"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/paradigm/discriminative",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/differentiable",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training a resume screening model for a technology company that evaluates candidates based on skills and experience whilst preventing the internal representations from encoding gender or ethnicity information, ensuring hiring decisions cannot be influenced by protected characteristics even indirectly through correlated features.",
          "goal": "Fairness"
        },
        {
          "description": "Developing a credit scoring model for loan approvals that accurately predicts default risk whilst ensuring the model's internal features cannot be used to infer applicants' race or age, thereby preventing discriminatory lending practices whilst maintaining predictive accuracy.",
          "goal": "Fairness"
        },
        {
          "description": "Creating a medical diagnosis model that makes accurate predictions about patient conditions whilst ensuring that the learned representations cannot reveal sensitive demographic information like gender or ethnicity, protecting patient privacy whilst maintaining clinical effectiveness.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Significantly more complex to implement than standard models, requiring expertise in adversarial training techniques and careful architecture design for both predictor and adversary networks."
        },
        {
          "description": "Requires careful hyperparameter tuning to balance the competing objectives of task performance and bias mitigation, as overly strong adversarial training can harm predictive accuracy."
        },
        {
          "description": "Effectiveness heavily depends on the quality and design of the adversary network - a weak adversary may fail to detect subtle biases, whilst an overly strong adversary may eliminate useful information."
        },
        {
          "description": "Training can be unstable and may suffer from convergence issues common to adversarial training, requiring careful learning rate scheduling and regularisation techniques."
        },
        {
          "description": "Provides no formal guarantees about bias elimination and may not prevent all forms of discrimination, particularly when protected attributes can be inferred from other available features."
        }
      ],
      "resources": [
        {
          "title": "AI Fairness 360 (AIF360)",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package",
          "description": "Comprehensive toolkit for bias detection and mitigation including adversarial debiasing implementations"
        },
        {
          "title": "On the Fairness ROAD: Robust Optimization for Adversarial Debiasing",
          "url": "https://www.semanticscholar.org/paper/0c887592d781538a1b5c2168eae541b563c0ba9a",
          "source_type": "technical_paper",
          "authors": [
            "Vincent Grari",
            "Thibault Laugel",
            "Tatsunori B. Hashimoto",
            "S. Lamprier",
            "Marcin Detyniecki"
          ]
        },
        {
          "title": "aif360.sklearn.inprocessing.AdversarialDebiasing — aif360 0.6.1 ...",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.inprocessing.AdversarialDebiasing.html",
          "source_type": "documentation"
        },
        {
          "title": "Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing",
          "url": "http://arxiv.org/pdf/2103.06179v1",
          "source_type": "technical_paper",
          "authors": [
            "Christian Reimers",
            "Paul Bodesheim",
            "Jakob Runge",
            "Joachim Denzler"
          ],
          "publication_date": "2021-03-10"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "homomorphic-encryption",
      "name": "Homomorphic Encryption",
      "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
      "assurance_goals": [
        "Privacy",
        "Safety",
        "Transparency",
        "Security"
      ],
      "tags": [
        "applicable-models/architecture/linear-models",
        "applicable-models/architecture/neural-networks/feedforward",
        "applicable-models/paradigm/discriminative",
        "applicable-models/paradigm/parametric",
        "applicable-models/requirements/differentiable",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/privacy",
        "assurance-goal-category/privacy/formal-guarantee",
        "assurance-goal-category/safety",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/privacy-guarantee",
        "evidence-type/quantitative-metric",
        "expertise-needed/cryptography",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Enabling a cloud-based medical diagnosis service to process encrypted patient data and return encrypted results without the cloud provider ever accessing actual medical information, ensuring complete patient privacy during outsourced computation.",
          "goal": "Privacy"
        },
        {
          "description": "Securing financial risk assessment computations by allowing banks to jointly analyse encrypted transaction patterns for fraud detection without exposing individual customer data, reducing systemic security risks.",
          "goal": "Safety"
        },
        {
          "description": "Enabling transparent audit of algorithmic decision-making by allowing regulators to verify model computations on encrypted data, providing accountability whilst protecting the proprietary nature of both the algorithm and the underlying data.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Extremely computationally expensive, often 100-1000x slower than unencrypted computation, making it impractical for real-time applications or large-scale data processing."
        },
        {
          "description": "Limited range of operations supported efficiently, with complex operations like divisions, comparisons, and non-polynomial functions being particularly challenging or impossible to implement."
        },
        {
          "description": "Implementation requires deep cryptographic expertise to avoid security vulnerabilities, choose appropriate parameters, and optimise performance for specific use cases."
        },
        {
          "description": "Memory and storage requirements are significantly higher than traditional computation, as encrypted data typically requires much more space than plaintext equivalents."
        },
        {
          "description": "Current fully homomorphic encryption schemes have practical limitations on computation depth before noise accumulation requires expensive bootstrapping operations to refresh ciphertexts."
        }
      ],
      "resources": [
        {
          "title": "zama-ai/concrete-ml",
          "url": "https://github.com/zama-ai/concrete-ml",
          "source_type": "software_package",
          "description": "Privacy-preserving machine learning library that enables data scientists to run ML models on encrypted data using FHE without cryptography expertise"
        },
        {
          "title": "Survey on Fully Homomorphic Encryption, Theory, and Applications",
          "url": "https://core.ac.uk/download/579858842.pdf",
          "source_type": "documentation",
          "authors": [
            "Chiara Marcolla",
            "Frank H.P. Fitzek",
            "Marc Manzano",
            "Najwa Aaraj",
            "Riccardo Bassoli",
            "Victor Sucasas"
          ],
          "publication_date": "2022-10-06",
          "description": "Comprehensive survey covering FHE theory, cryptographic schemes, and practical applications across different domains"
        },
        {
          "title": "Welcome to OpenFHE's documentation! — OpenFHE documentation",
          "url": "https://openfhe-development.readthedocs.io/",
          "source_type": "documentation",
          "description": "Documentation for open-source C++ library supporting multiple FHE schemes including BFV, BGV, CKKS, and Boolean circuits"
        },
        {
          "title": "Evaluation of Privacy-Preserving Support Vector Machine (SVM) Learning Using Homomorphic Encryption",
          "url": "https://core.ac.uk/download/656115203.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Ali, Hisham",
            "Buchanan, William J."
          ],
          "publication_date": "2025-01-01",
          "description": "Technical paper evaluating performance overhead of SVM learning with homomorphic encryption for privacy-preserving ML"
        },
        {
          "title": "microsoft/SEAL",
          "url": "https://github.com/microsoft/SEAL",
          "source_type": "software_package",
          "description": "Easy-to-use homomorphic encryption library enabling computations on encrypted integers and real numbers"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "synthetic-data-generation",
        "federated-learning",
        "differential-privacy"
      ]
    }
  ],
  "count": 9
}