{
  "tag": {
    "name": "applicable-models/tree-based",
    "slug": "applicable-models-tree-based",
    "count": 3,
    "category": "applicable-models"
  },
  "techniques": [
    {
      "slug": "mean-decrease-impurity",
      "name": "Mean Decrease Impurity",
      "description": "Mean Decrease Impurity (MDI) quantifies a feature's importance in tree-based models (e.g., Random Forests, Gradient Boosting Machines) by measuring the total reduction in impurity (e.g., Gini impurity, entropy) across all splits where the feature is used. Features that lead to larger, more consistent reductions in impurity are considered more important, indicating their effectiveness in creating homogeneous child nodes and improving predictive accuracy.",
      "assurance_goals": [
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/tree-based",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Determining the most influential genetic markers in a decision tree model predicting disease susceptibility, by identifying which markers consistently lead to the purest splits between healthy and diseased patient groups.",
          "goal": "Explainability"
        },
        {
          "description": "Assessing the key factors driving customer purchasing decisions in an e-commerce random forest model, revealing which product attributes or customer demographics are most effective in segmenting buyers.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "MDI is inherently biased towards features with more unique values or those that allow for more splits, potentially overestimating their true importance."
        },
        {
          "description": "It is only applicable to tree-based models and cannot be directly used with other model architectures."
        },
        {
          "description": "The importance scores can be unstable, varying significantly with small changes in the training data or model parameters."
        },
        {
          "description": "MDI does not account for feature interactions, meaning it might not accurately reflect the importance of features that are only relevant when combined with others."
        }
      ],
      "resources": [
        {
          "title": "Trees, forests, and impurity-based variable importance",
          "url": "http://arxiv.org/pdf/2001.04295v3",
          "source_type": "technical_paper",
          "authors": [
            "Erwan Scornet"
          ],
          "publication_date": "2020-01-13"
        },
        {
          "title": "A Debiased MDI Feature Importance Measure for Random Forests",
          "url": "http://arxiv.org/pdf/1906.10845v2",
          "source_type": "technical_paper",
          "authors": [
            "Xiao Li",
            "Yu Wang",
            "Sumanta Basu",
            "Karl Kumbier",
            "Bin Yu"
          ],
          "publication_date": "2019-06-26"
        },
        {
          "title": "Variable Importance in Random Forests | Towards Data Science",
          "url": "https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0/",
          "source_type": "tutorial"
        },
        {
          "title": "Interpreting Deep Forest through Feature Contribution and MDI Feature Importance",
          "url": "http://arxiv.org/pdf/2305.00805v1",
          "source_type": "technical_paper",
          "authors": [
            "Yi-Xiao He",
            "Shen-Huan Lyu",
            "Yuan Jiang"
          ],
          "publication_date": "2023-05-01"
        },
        {
          "title": "optuna.importance.MeanDecreaseImpurityImportanceEvaluator ...",
          "url": "https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.MeanDecreaseImpurityImportanceEvaluator.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "permutation-importance",
        "coefficient-magnitudes-in-linear-models",
        "sobol-indices"
      ]
    },
    {
      "slug": "monotonicity-constraints",
      "name": "Monotonicity Constraints",
      "description": "Monotonicity constraints enforce consistent directional relationships between input features and model predictions, ensuring that increasing a feature value either always increases, always decreases, or has no effect on the output. These constraints integrate domain knowledge into model training, preventing counterintuitive relationships that may arise from spurious correlations in data. By maintaining logical feature relationships (e.g., experience always positively influences salary), monotonicity constraints enhance model trustworthiness, interpretability, and alignment with business logic whilst often improving generalisation to new data.",
      "assurance_goals": [
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/tree-based",
        "applicable-models/gaussian-process",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/labelled-data",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "expertise-needed/domain-knowledge",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Enforcing that a mortgage approval model always treats higher income, longer employment history, and higher credit scores as positive factors, making the decision logic transparent and intuitive for loan officers and applicants whilst preventing counterintuitive relationships that could undermine trust in the system.",
          "goal": "Transparency"
        },
        {
          "description": "Constraining a healthcare cost prediction model so that age and number of chronic conditions always increase predicted costs, ensuring the model generalises reliably to new patient populations and maintains logical behaviour even when training data contains sampling biases or unusual correlations.",
          "goal": "Reliability"
        },
        {
          "description": "Implementing monotonic constraints in an insurance premium model where driving experience always reduces premiums and accident history always increases them, creating consistent pricing logic that regulatory authorities can easily validate and customers can understand and trust.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Can reduce model accuracy when real-world relationships are inherently non-monotonic, such as the inverted-U relationship between experience and performance, where constraints force oversimplified linear relationships."
        },
        {
          "description": "Requires substantial domain expertise to identify which features should have monotonic relationships, creating dependency on subject matter experts and potential for incorrect constraint specification."
        },
        {
          "description": "Increases computational complexity during training as optimisation algorithms must respect additional constraints, potentially leading to longer training times and convergence difficulties."
        },
        {
          "description": "May mask important non-linear patterns in data that could be crucial for understanding system behaviour, particularly in exploratory analysis where discovering unexpected relationships is valuable."
        },
        {
          "description": "Limited applicability to certain model types, with implementation varying significantly across algorithms (well-supported in tree-based models, more complex in neural networks), restricting technique flexibility."
        }
      ],
      "resources": [
        {
          "title": "Monotonic Constraints â€” xgboost 3.1.0-dev documentation",
          "url": "https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html",
          "source_type": "documentation",
          "description": "Comprehensive tutorial on implementing monotonic constraints in XGBoost, including parameter configuration, practical examples, and visual demonstrations of constraint effects on model predictions."
        },
        {
          "title": "NONPARAMETRIC KERNEL REGRESSION SUBJECT TO MONOTONICITY CONSTRAINTS",
          "url": "https://www.semanticscholar.org/paper/28e2be532d66694d3fe3486671f5c0217f58892d",
          "source_type": "technical_paper",
          "authors": [
            "P. Hall",
            "Li-Shan Huang"
          ],
          "description": "Foundational research paper on implementing monotonicity constraints in nonparametric kernel regression methods, providing theoretical background and algorithmic approaches for enforcing monotonic relationships."
        },
        {
          "title": "scikit-learn Isotonic Regression",
          "url": "https://scikit-learn.org/stable/modules/isotonic.html",
          "source_type": "documentation",
          "description": "Documentation for scikit-learn's isotonic regression implementation, providing alternative approach to monotonic relationships through non-parametric regression that preserves monotonic order."
        },
        {
          "title": "High-dimensional additive Gaussian processes under monotonicity constraints",
          "url": "https://www.semanticscholar.org/paper/4d4f1e2de3742735dcc47d2e51cc572a4415231e",
          "source_type": "technical_paper",
          "authors": [
            "AndrÃ©s F. LÃ³pez-Lopera",
            "F. Bachoc",
            "O. Roustant"
          ],
          "description": "Advanced research on extending monotonicity constraints to high-dimensional Gaussian process models, addressing scalability challenges and additive model structures for complex constraint applications."
        },
        {
          "title": "cagrell/gp_constr",
          "url": "https://github.com/cagrell/gp_constr",
          "source_type": "software_package",
          "description": "Python implementation of Gaussian process regression with linear operator constraints including boundedness and monotonicity, featuring RBF and MatÃ©rn kernels with practical examples."
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "intrinsically-interpretable-models",
        "generalized-additive-models"
      ]
    },
    {
      "slug": "intrinsically-interpretable-models",
      "name": "Intrinsically Interpretable Models",
      "description": "Intrinsically interpretable models are machine learning algorithms that are transparent by design, allowing users to understand their decision-making process without requiring additional explanation techniques. This category includes decision trees and rule lists (which use if-then logic), linear and logistic regression models (which use weighted feature combinations), and other simple algorithms where the model structure itself provides interpretability. These models prioritise transparency over complexity, making them ideal when stakeholder understanding and regulatory compliance are paramount.",
      "assurance_goals": [
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/tree-based",
        "applicable-models/linear",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/structured-output",
        "evidence-type/quantitative-metric",
        "expertise-needed/low",
        "explanatory-scope/global",
        "lifecycle-stage/project-design",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Developing a medical diagnosis support system using a decision tree with clear if-then rules based on symptoms and test results, allowing healthcare professionals to trace the reasoning path and explain diagnoses to patients whilst ensuring clinical transparency and accountability.",
          "goal": "Transparency"
        },
        {
          "description": "Creating a fraud detection model using logistic regression with carefully selected features (transaction amount, location, time patterns) where each coefficient's contribution can be understood and validated, ensuring reliable performance that financial institutions can audit and regulatory bodies can approve.",
          "goal": "Reliability"
        },
        {
          "description": "Implementing a hiring decision support tool using rule lists that explicitly state qualification criteria and scoring logic, providing transparent candidate evaluation that can be explained to applicants and reviewed for fairness whilst meeting legal requirements for employment decision documentation.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Generally achieve lower predictive accuracy than complex models (neural networks, ensembles) for difficult problems involving high-dimensional data, non-linear relationships, or complex feature interactions."
        },
        {
          "description": "Linear models cannot capture non-linear relationships or feature interactions without manual feature engineering, limiting their applicability to inherently non-linear domains like image recognition or natural language processing."
        },
        {
          "description": "Decision trees can become unstable with small changes in training data, potentially leading to completely different tree structures and predictions, affecting model reliability in dynamic environments."
        },
        {
          "description": "Deep decision trees may lose interpretability despite being inherently transparent, as human cognitive limits make it difficult to follow complex branching logic with many levels and conditions."
        },
        {
          "description": "Feature selection becomes critical for maintaining interpretability, requiring domain expertise to identify the most relevant variables whilst potentially missing important but subtle predictive signals."
        }
      ],
      "resources": [
        {
          "title": "scikit-learn Decision Trees",
          "url": "https://scikit-learn.org/stable/modules/tree.html",
          "source_type": "documentation",
          "description": "Comprehensive documentation for decision tree implementation in scikit-learn, including classification and regression trees with interpretability guidelines and visualisation tools."
        },
        {
          "title": "scikit-learn Linear Models",
          "url": "https://scikit-learn.org/stable/modules/linear_model.html",
          "source_type": "documentation",
          "description": "Complete guide to linear and logistic regression models in scikit-learn, covering implementation, feature selection, and coefficient interpretation for transparent modeling."
        },
        {
          "title": "Interpretable Machine Learning",
          "url": "https://christophm.github.io/interpretable-ml-book/",
          "source_type": "tutorial",
          "description": "Open-source book providing comprehensive coverage of interpretable machine learning models including decision trees, linear models, and rule-based systems with practical examples."
        },
        {
          "title": "R package 'rpart' for Recursive Partitioning",
          "url": "https://cran.r-project.org/web/packages/rpart/index.html",
          "source_type": "software_package",
          "description": "R implementation of recursive partitioning for classification, regression and survival trees with extensive documentation and plotting capabilities for interpretable tree models."
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "monotonicity-constraints",
        "generalized-additive-models"
      ]
    }
  ],
  "count": 3
}