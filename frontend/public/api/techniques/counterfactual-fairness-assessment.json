{
  "slug": "counterfactual-fairness-assessment",
  "name": "Counterfactual Fairness Assessment",
  "acronym": null,
  "description": "Counterfactual Fairness Assessment evaluates whether a model's predictions would remain unchanged if an individual's protected attributes (race, gender, age) were different, whilst keeping all other causally legitimate factors constant. The technique requires constructing a causal graph that maps relationships between variables, then using do-calculus or structural causal models to simulate counterfactual scenarios. For example, it asks: 'Would this loan application still be approved if the applicant were a different race, holding constant their actual qualifications and economic circumstances?' This individual-level fairness criterion helps identify when decisions depend improperly on protected characteristics.",
  "complexity_rating": 5,
  "computational_cost_rating": 3,
  "assurance_goals": ["Fairness"],
  "tags": [
    "applicable-models/agnostic",
    "assurance-goal-category/fairness",
    "data-requirements/sensitive-attributes",
    "data-type/any",
    "evidence-type/quantitative-metric",
    "expertise-needed/causal-inference",
    "expertise-needed/ml-engineering",
    "explanatory-scope/local",
    "fairness-approach/causal",
    "lifecycle-stage/model-development",
    "technique-type/algorithmic"
  ],
  "related_techniques": ["path-specific-counterfactual-fairness-assessment"],
  "resources": [
    {
      "title": "Counterfactual Fairness",
      "url": "https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf",
      "authors": [
        "Matt J. Kusner",
        "Joshua Loftus",
        "Chris Russell",
        "Ricardo Silva"
      ],
      "publication_date": "2017-12-04",
      "source_type": "technical_paper",
      "resource_type": "documentation"
    },
    {
      "title": "fairlearn/fairlearn",
      "url": "https://github.com/fairlearn/fairlearn",
      "authors": "",
      "publication_date": null,
      "source_type": "software_package",
      "resource_type": "documentation"
    },
    {
      "title": "Counterfactual Fairness in Text Classification through Robustness",
      "url": "http://arxiv.org/pdf/1809.10610v2",
      "authors": [
        "Sahaj Garg",
        "Vincent Perot",
        "Nicole Limtiaco",
        "Ankur Taly",
        "Ed H. Chi",
        "Alex Beutel"
      ],
      "publication_date": "2018-09-27",
      "source_type": "technical_paper",
      "resource_type": "documentation"
    }
  ],
  "example_use_cases": [
    {
      "description": "Evaluating a hiring algorithm by testing whether qualified candidates would receive the same evaluation scores if their gender were different, whilst controlling for actual skills, experience, and education, revealing whether gender bias affects recruitment decisions.",
      "goal": "Fairness"
    },
    {
      "description": "Assessing a criminal sentencing model by examining whether defendants with identical criminal histories and case circumstances would receive the same sentence recommendations regardless of their race, identifying potential discriminatory patterns in judicial AI systems.",
      "goal": "Fairness"
    }
  ],
  "limitations": [
    {
      "description": "Requires explicit specification of causal relationships between variables, which involves subjective assumptions about what constitutes legitimate versus illegitimate causal pathways."
    },
    {
      "description": "May be mathematically impossible to satisfy simultaneously with other fairness criteria (like statistical parity), forcing practitioners to choose between competing fairness definitions."
    },
    {
      "description": "Implementation complexity is high, requiring sophisticated causal inference techniques and structural causal models that are difficult to construct and validate."
    },
    {
      "description": "Depends heavily on the quality and completeness of the causal graph, which may be incorrect or missing important confounding variables."
    }
  ]
}
