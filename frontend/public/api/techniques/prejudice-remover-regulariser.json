{
  "slug": "prejudice-remover-regulariser",
  "name": "Prejudice Remover Regulariser",
  "acronym": null,
  "description": "An in-processing fairness technique that adds a fairness penalty to machine learning models to reduce bias against protected groups. The method works by minimising 'mutual information' - essentially reducing how much the model's predictions reveal about sensitive attributes like race or gender. By adding this penalty term to the learning objective (typically in logistic regression), the technique ensures predictions become less dependent on protected features. This addresses not only direct discrimination but also indirect bias through correlated features. Practitioners can adjust a tuning parameter to balance between maintaining accuracy and removing prejudice from the model.",
  "complexity_rating": 3,
  "computational_cost_rating": 2,
  "assurance_goals": [
    {
      "id": 2,
      "name": "Fairness"
    },
    {
      "id": 6,
      "name": "Transparency"
    },
    {
      "id": 3,
      "name": "Reliability"
    }
  ],
  "tags": [
    {
      "name": "applicable-models/logistic-regression",
      "id": 10
    },
    {
      "name": "applicable-models/probabilistic",
      "id": 12
    },
    {
      "name": "assurance-goal-category/fairness",
      "id": 20
    },
    {
      "name": "assurance-goal-category/fairness/group",
      "id": 22
    },
    {
      "name": "assurance-goal-category/transparency",
      "id": 31
    },
    {
      "name": "assurance-goal-category/reliability",
      "id": 27
    },
    {
      "name": "data-requirements/sensitive-attributes",
      "id": 43
    },
    {
      "name": "data-requirements/labelled-data",
      "id": 38
    },
    {
      "name": "data-type/tabular",
      "id": 48
    },
    {
      "name": "evidence-type/quantitative-metric",
      "id": 59
    },
    {
      "name": "evidence-type/fairness-metric",
      "id": 54
    },
    {
      "name": "expertise-needed/statistics",
      "id": 79
    },
    {
      "name": "expertise-needed/ml-engineering",
      "id": 73
    },
    {
      "name": "fairness-approach/group",
      "id": 83
    },
    {
      "name": "lifecycle-stage/model-development",
      "id": 92
    },
    {
      "name": "lifecycle-stage/model-development/training",
      "id": 95
    },
    {
      "name": "technique-type/algorithmic",
      "id": 107
    }
  ],
  "related_techniques": [
    "adversarial-debiasing",
    "fair-adversarial-networks",
    "meta-fair-classifier",
    "exponentiated-gradient-reduction",
    "fair-transfer-learning",
    "adaptive-sensitive-reweighting",
    "multi-accuracy-boosting"
  ],
  "related_technique_slugs": [
    "adversarial-debiasing",
    "fair-adversarial-networks",
    "meta-fair-classifier",
    "exponentiated-gradient-reduction",
    "fair-transfer-learning",
    "adaptive-sensitive-reweighting",
    "multi-accuracy-boosting"
  ],
  "resources": [
    {
      "title": "Fairness-Aware Classifier with Prejudice Remover Regularizer",
      "url": "https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3",
      "authors": [
        "Toshihiro Kamishima",
        "Shotaro Akaho",
        "Hideki Asoh",
        "Jun Sakuma"
      ],
      "publication_date": "2012-09-24",
      "source_type": "technical_paper",
      "resource_type": "documentation"
    },
    {
      "title": "Fairness-Aware Machine Learning and Data Mining",
      "url": "https://www.kamishima.net/faml/",
      "authors": "",
      "publication_date": null,
      "source_type": "documentation",
      "resource_type": "documentation"
    },
    {
      "title": "Fairness-aware Classifier (faclass)",
      "url": "https://www.kamishima.net/faclass/",
      "authors": "",
      "publication_date": null,
      "source_type": "software_package",
      "resource_type": "documentation"
    }
  ],
  "example_use_cases": [
    {
      "description": "Training credit scoring models with prejudice remover regularisation to ensure loan approval decisions are not influenced by gender or ethnicity, minimising mutual information between predictions and protected attributes whilst maintaining accurate risk assessment.",
      "goal": "Fairness"
    },
    {
      "description": "Developing transparent university admission models that provide clear evidence of bias mitigation by demonstrating reduced statistical dependence between acceptance decisions and protected characteristics, enabling regulatory compliance reporting.",
      "goal": "Transparency"
    },
    {
      "description": "Building reliable recruitment screening models that maintain consistent performance across demographic groups by regularising against indirect prejudice through correlated features like school names or postal codes that might proxy for protected attributes.",
      "goal": "Reliability"
    }
  ],
  "limitations": [
    {
      "description": "Requires careful tuning of the fairness penalty hyperparameter, where too high values severely degrade accuracy whilst too low values provide insufficient bias mitigation."
    },
    {
      "description": "Primarily applicable to probabilistic discriminative models like logistic regression, limiting its use with other model architectures such as deep neural networks or tree-based methods."
    },
    {
      "description": "Computational complexity increases with the calculation of mutual information between predictions and sensitive attributes, particularly for high-dimensional data."
    },
    {
      "description": "May not fully eliminate all forms of discrimination, particularly when complex interactions between multiple sensitive attributes create intersectional biases."
    },
    {
      "description": "Effectiveness depends on accurate identification and inclusion of all sensitive attributes, potentially missing hidden biases from unobserved protected characteristics."
    }
  ]
}
