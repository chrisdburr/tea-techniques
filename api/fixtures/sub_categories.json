[
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Importance and Attribution",
			"description": "Techniques that assign importance scores to input features by assessing their contribution to the model's outputs. These methods help identify which features are most influential in a model's predictions or outputs.",
			"category": 1
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Interaction Analysis",
			"description": "Techniques that examine how different features interact with each other within a model. These methods uncover interdependencies between features that might not be apparent when analysing them individually.",
			"category": 1
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Local Surrogates",
			"description": "Simple, interpretable models (e.g., decision trees, linear models) that approximate the behaviour of a complex model in a small neighborhood around a specific instance. They provide insight into why the model made a particular prediction for an individual case.",
			"category": 2
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Global Surrogates",
			"description": "Simpler models that aim to approximate (or emulate) the overall behaviour of a complex model across the entire dataset. These models provide a broader understanding of how the original model operates.",
			"category": 2
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Feature Visualisation",
			"description": "Techniques that create visual representations of how specific features impact a model's predictions, which may include plotting feature attributions or importance scores.",
			"category": 3
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Model Behaviour Visualisation",
			"description": "Techniques that visualise the internal workings of models, such as decision boundaries, attention mechanisms, or activation patterns. These methods help in understanding how the model processes input data and arrives at decisions.",
			"category": 3
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Dimensionality Reduction Visualisation",
			"description": "Techniques that reduce the dimensionality of data or model representations to make them more interpretable, helping to explore the structure of high-dimensional data and the internal workings of models.",
			"category": 3
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Prototype and Criticism Methods",
			"description": "Techniques that identify representative examples (prototypes) that summarise the model’s behaviour, as well as outliers or contrasting cases (criticisms) that highlight where the model might struggle.",
			"category": 4
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Counterfactual Explanations",
			"description": "Techniques that explain model outputs by generating hypothetical examples where the outcome would be different. Such explanations help identify which changes to an input would be necessary to alter the model’s decision.",
			"category": 4
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Causal Analysis",
			"description": "Techniques that explore potential cause-and-effect relationships within the data or model. They help in understanding how changes to input variables (or interventions) influence the model's output, providing deeper insights into the model’s reasoning beyond correlation.",
			"category": 4
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Decision Rules",
			"description": "Techniques that derive if-then rules from a model's decision-making process. These rules provide a clear, human-readable interpretation of how the model arrives at certain predictions.",
			"category": 5
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Decision Trees",
			"description": "Methods that represent model decisions in the form of tree structures, making the logic of the model’s decisions easy to follow. Decision trees are intrinsically interpretable models and can also be extracted from more complex models as surrogates.",
			"category": 5
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Confidence Estimation",
			"description": "Techniques that estimate a model’s confidence in its outputs, often by quantifying the likelihood of a prediction being correct.",
			"category": 6
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Out-of-Distribution Detection",
			"description": "Techniques that detect when input data differs significantly from the training data, which might lead to unreliable outputs. These methods help ensure that the model is aware of its own limitations when faced with novel situations.",
			"category": 6
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Uncertainty Quantification",
			"description": "Techniques that quantify the uncertainty in model outputs, often by incorporating probabilistic approaches.",
			"category": 6
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Bias Detection and Mitigation",
			"description": "Techniques that identify and correct biases in models to promote fairness across different demographic groups.",
			"category": 7
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Fairness Metrics Visualisation",
			"description": "Visual tools that display metrics related to fairness, helping stakeholders understand and address ethical concerns.",
			"category": 7
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Model Pruning",
			"description": "Techniques that remove unnecessary parameters from a model to reduce complexity and improve interpretability.",
			"category": 8
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Model Distillation",
			"description": "Methods where a simpler model is trained to replicate the behaviour of a complex model, resulting in a more interpretable version.",
			"category": 8
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Data Transformation",
			"description": "Techniques that adjust feature values or labels to mitigate bias, such as reweighting, resampling, or modifying features.",
			"category": 9
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Fair Representation Learning",
			"description": "Methods that learn new data representations less sensitive to protected attributes while retaining relevant information for the prediction task.",
			"category": 10
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Fairness-Constrained Optimisation",
			"description": "Techniques that add fairness constraints to the optimisation problem, ensuring the model satisfies certain fairness criteria during training.",
			"category": 10
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Adversarial Debiasing",
			"description": "Methods that use adversarial training to remove biases by having a discriminator predict protected attributes from the model’s outputs or internal representations, training the model to prevent this.",
			"category": 10
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Outcome Adjustment",
			"description": "Techniques that adjust decision thresholds or probabilities to achieve fairness objectives, such as equalising opportunity or predictive parity.",
			"category": 11
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Calibration Methods",
			"description": "Methods that adjust the model’s outputs to ensure predicted probabilities are well-calibrated across different groups.",
			"category": 11
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Group Fairness Metrics",
			"description": "Metrics that assess fairness across different demographic groups, such as demographic parity, equal opportunity, and equalised odds.",
			"category": 12
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Individual Fairness Metrics",
			"description": "Metrics that assess fairness at the individual level, ensuring similar individuals receive similar predictions.",
			"category": 12
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Counterfactual Fairness",
			"description": "Methods that ensure predictions are the same in a counterfactual world where the individual’s protected attributes are different.",
			"category": 13
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Causal Inference",
			"description": "Techniques that infer or model causal relationships and use them to adjust for biases in the data or model.",
			"category": 13
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Feature Attribution Methods",
			"description": "Techniques that attribute a model's predictions to its input features, helping to understand the influence of each feature on the predictions.",
			"category": 14
		}
	},
	{
		"model": "api.subcategory",
		"fields": {
			"name": "Dimensionality Reduction",
			"description": "Techniques that reduce the number of features or variables in the data to simplify models and improve interpretability.",
			"category": 8 
		}
	}
]