{
  "slug": "disparate-impact-remover",
  "name": "Disparate Impact Remover",
  "description": "Disparate Impact Remover is a preprocessing technique that transforms feature values in a dataset to reduce statistical dependence between features and protected attributes (like race or gender). The method modifies non-protected features through mathematical transformations that preserve the utility of the data whilst reducing correlations that could lead to discriminatory outcomes. This approach specifically targets the '80% rule' disparate impact threshold by adjusting feature distributions to ensure more equitable treatment across demographic groups in downstream model predictions.",
  "assurance_goals": [
    "Fairness",
    "Transparency",
    "Reliability"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/paradigm/supervised",
    "applicable-models/requirements/black-box",
    "applicable-models/requirements/training-data",
    "assurance-goal-category/fairness",
    "assurance-goal-category/reliability",
    "assurance-goal-category/transparency",
    "data-requirements/sensitive-attributes",
    "data-type/tabular",
    "evidence-type/quantitative-metric",
    "expertise-needed/statistics",
    "fairness-approach/group",
    "lifecycle-stage/model-development",
    "technique-type/algorithmic"
  ],
  "example_use_cases": [
    {
      "description": "Transforming features in a credit scoring dataset where variables like 'years of employment' and 'education level' are correlated with race, applying mathematical transformations to reduce these correlations whilst preserving the predictive value for creditworthiness assessment.",
      "goal": "Fairness"
    },
    {
      "description": "Preprocessing a recruitment dataset where features like 'previous job titles' and 'university attended' correlate with gender, modifying these features to ensure the '80% rule' is met whilst maintaining useful information for predicting job performance.",
      "goal": "Fairness"
    },
    {
      "description": "Preprocessing financial lending data to provide transparent bias metrics showing the quantified reduction in correlation between protected attributes and creditworthiness features, enabling institutions to demonstrate compliance with the 80% rule and explain their fairness interventions to regulators.",
      "goal": "Transparency"
    },
    {
      "description": "Ensuring consistent model performance across demographic groups in healthcare risk assessment by mathematically transforming features to reduce protected attribute correlations, improving reliability of predictions for minority populations who may have been systematically under-served.",
      "goal": "Reliability"
    }
  ],
  "limitations": [
    {
      "description": "Feature transformations may reduce model accuracy by removing or distorting important predictive information during the debiasing process."
    },
    {
      "description": "Only addresses measured protected attributes and cannot eliminate bias that operates through unmeasured proxy variables."
    },
    {
      "description": "Effectiveness depends on the specific transformation method chosen and may not generalise well to different datasets or domains."
    },
    {
      "description": "May create artificial feature distributions that don't reflect real-world data patterns, potentially causing issues in model deployment."
    }
  ],
  "resources": [
    {
      "title": "holistic-ai/holisticai",
      "url": "https://github.com/holistic-ai/holisticai",
      "source_type": "software_package",
      "description": "Comprehensive open-source toolkit for AI fairness with bias measurement, mitigation techniques, and visualisation tools"
    },
    {
      "title": "Disparate Impact Remover — holisticai documentation",
      "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/preprocessing/bc_disparate_impact_remover_disparate_impact_remover.html",
      "source_type": "tutorial",
      "description": "Comprehensive tutorial covering theoretical background, methodology, and practical implementation of disparate impact removal"
    },
    {
      "title": "Trusted-AI/AIF360",
      "url": "https://github.com/Trusted-AI/AIF360",
      "source_type": "software_package",
      "description": "IBM Research's extensible open-source library for detecting and mitigating algorithmic bias across multiple domains"
    },
    {
      "title": "aif360.algorithms.preprocessing.DisparateImpactRemover — aif360 ...",
      "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.DisparateImpactRemover.html",
      "source_type": "documentation",
      "description": "Technical API documentation for AIF360's DisparateImpactRemover class with parameters, methods, and usage examples"
    }
  ],
  "complexity_rating": 3,
  "computational_cost_rating": 2,
  "related_techniques": [
    "reweighing",
    "relabelling",
    "preferential-sampling"
  ]
}