[
  {
    "id": 1,
    "name": "SHapley Additive exPlanations (SHAP)",
    "description": "SHAP explains model predictions by quantifying how much each input feature contributes to the outcome. It assigns an importance score to every feature, indicating whether it pushes the prediction towards or away from the average. The method systematically evaluates how predictions change as features are included or excluded, drawing on game theory concepts to ensure a fair distribution of contributions.",
    "assurance_goals": ["Explainability", "Fairness", "Reliability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/feature-analysis",
      "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing a customer churn prediction model to understand why a specific high-value customer was flagged as likely to leave, revealing that recent support ticket interactions and declining purchase frequency were the main drivers.",
        "goal": "Explainability"
      },
      {
        "description": "Auditing a loan approval model by comparing SHAP values for applicants from different demographic groups, ensuring that protected characteristics like race or gender do not have an undue influence on credit decisions.",
        "goal": "Fairness"
      },
      {
        "description": "Validating a medical diagnosis model by confirming that its predictions are based on relevant clinical features (e.g., blood pressure, cholesterol levels) rather than spurious correlations (e.g., patient ID or appointment time), thereby improving model reliability.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Assumes feature independence, which can produce misleading explanations when features are highly correlated, as the model may attribute importance to features that are merely proxies for others."
      },
      {
        "description": "Computationally expensive for models with many features or large datasets, as the number of required predictions grows exponentially with the number of features."
      },
      {
        "description": "The choice of background dataset for generating explanations can significantly influence the results, requiring careful selection to ensure a representative baseline."
      },
      {
        "description": "Global explanations derived from averaging local SHAP values may obscure important heterogeneous effects where features impact subgroups of the population differently."
      }
    ],
    "resources": [
      {
        "title": "shap/shap",
        "url": "https://github.com/shap/shap",
        "source_type": "software_package"
      },
      {
        "title": "Introduction to SHapley Additive exPlanations (SHAP) — XAI Tutorials",
        "url": "https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/shap.html",
        "source_type": "tutorial"
      },
      {
        "title": "An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models",
        "url": "http://arxiv.org/pdf/2204.11351v3",
        "source_type": "technical_paper",
        "authors": [
          "Han Yuan",
          "Mingxuan Liu",
          "Lican Kang",
          "Chenkui Miao",
          "Ying Wu"
        ],
        "publication_date": "2022-04-24"
      },
      {
        "title": "SHAP: Shapley Additive Explanations | Towards Data Science",
        "url": "https://towardsdatascience.com/shap-shapley-additive-explanations-5a2a271ed9c3/",
        "source_type": "tutorial"
      },
      {
        "title": "MAIF/shapash",
        "url": "https://github.com/MAIF/shapash",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [5, 6, 7, 12, 27, 28]
  },
  {
    "id": 2,
    "name": "Permutation Importance",
    "description": "Permutation Importance quantifies a feature's contribution to a model's performance by randomly shuffling its values and measuring the resulting drop in predictive accuracy. If shuffling a feature significantly degrades the model's performance, that feature is considered important. This model-agnostic technique helps identify which inputs are genuinely driving predictions, rather than just being correlated with the outcome.",
    "assurance_goals": ["Explainability", "Reliability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/feature-analysis",
      "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Assessing which patient characteristics (e.g., age, blood pressure, cholesterol) are most critical for a medical diagnosis model by observing the performance drop when each characteristic's values are randomly shuffled, ensuring the model relies on clinically relevant factors.",
        "goal": "Explainability"
      },
      {
        "description": "Validating the robustness of a fraud detection model by permuting features like transaction amount or location, and confirming that the model's ability to detect fraud significantly decreases only for truly important features, thereby improving confidence in its reliability.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Can be misleading when features are highly correlated, as shuffling one feature might indirectly affect others, leading to an overestimation of its importance."
      },
      {
        "description": "Computationally expensive for large datasets or complex models, as it requires re-evaluating the model many times for each feature."
      },
      {
        "description": "Does not account for interactions between features; it measures the marginal importance of a feature, assuming other features remain unchanged."
      },
      {
        "description": "The choice of metric for evaluating performance drop (e.g., accuracy, F1-score) can influence the perceived importance of features."
      }
    ],
    "resources": [
      {
        "title": "Asymptotic Unbiasedness of the Permutation Importance Measure in Random Forest Models",
        "url": "http://arxiv.org/pdf/1912.03306v1",
        "source_type": "technical_paper",
        "authors": ["Burim Ramosaj", "Markus Pauly"],
        "publication_date": "2019-12-05"
      },
      {
        "title": "eli5.permutation_importance — ELI5 0.15.0 documentation",
        "url": "https://eli5.readthedocs.io/en/latest/autodocs/permutation_importance.html",
        "source_type": "documentation"
      },
      {
        "title": "Permutation Importance — PermutationImportance 1.2.1.5 ...",
        "url": "https://permutationimportance.readthedocs.io/en/latest/permutation.html",
        "source_type": "documentation"
      },
      {
        "title": "parrt/random-forest-importances",
        "url": "https://github.com/parrt/random-forest-importances",
        "source_type": "software_package"
      },
      {
        "title": "Statistically Valid Variable Importance Assessment through Conditional Permutations",
        "url": "http://arxiv.org/pdf/2309.07593v2",
        "source_type": "technical_paper",
        "authors": ["Ahmad Chamma", "Denis A. Engemann", "Bertrand Thirion"],
        "publication_date": "2023-09-14"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 3,
    "related_techniques": [3, 4, 10]
  },
  {
    "id": 3,
    "name": "Mean Decrease Impurity",
    "description": "Mean Decrease Impurity (MDI) quantifies a feature's importance in tree-based models (e.g., Random Forests, Gradient Boosting Machines) by measuring the total reduction in impurity (e.g., Gini impurity, entropy) across all splits where the feature is used. Features that lead to larger, more consistent reductions in impurity are considered more important, indicating their effectiveness in creating homogeneous child nodes and improving predictive accuracy.",
    "assurance_goals": ["Explainability", "Reliability"],
    "tags": [
      "applicable-models/tree-based",
      "assurance-goal-category/explainability",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Determining the most influential genetic markers in a decision tree model predicting disease susceptibility, by identifying which markers consistently lead to the purest splits between healthy and diseased patient groups.",
        "goal": "Explainability"
      },
      {
        "description": "Assessing the key factors driving customer purchasing decisions in an e-commerce random forest model, revealing which product attributes or customer demographics are most effective in segmenting buyers.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "MDI is inherently biased towards features with more unique values or those that allow for more splits, potentially overestimating their true importance."
      },
      {
        "description": "It is only applicable to tree-based models and cannot be directly used with other model architectures."
      },
      {
        "description": "The importance scores can be unstable, varying significantly with small changes in the training data or model parameters."
      },
      {
        "description": "MDI does not account for feature interactions, meaning it might not accurately reflect the importance of features that are only relevant when combined with others."
      }
    ],
    "resources": [
      {
        "title": "Trees, forests, and impurity-based variable importance",
        "url": "http://arxiv.org/pdf/2001.04295v3",
        "source_type": "technical_paper",
        "authors": ["Erwan Scornet"],
        "publication_date": "2020-01-13"
      },
      {
        "title": "A Debiased MDI Feature Importance Measure for Random Forests",
        "url": "http://arxiv.org/pdf/1906.10845v2",
        "source_type": "technical_paper",
        "authors": [
          "Xiao Li",
          "Yu Wang",
          "Sumanta Basu",
          "Karl Kumbier",
          "Bin Yu"
        ],
        "publication_date": "2019-06-26"
      },
      {
        "title": "Variable Importance in Random Forests | Towards Data Science",
        "url": "https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0/",
        "source_type": "tutorial"
      },
      {
        "title": "Interpreting Deep Forest through Feature Contribution and MDI Feature Importance",
        "url": "http://arxiv.org/pdf/2305.00805v1",
        "source_type": "technical_paper",
        "authors": ["Yi-Xiao He", "Shen-Huan Lyu", "Yuan Jiang"],
        "publication_date": "2023-05-01"
      },
      {
        "title": "optuna.importance.MeanDecreaseImpurityImportanceEvaluator ...",
        "url": "https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.MeanDecreaseImpurityImportanceEvaluator.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [2, 4, 10]
  },
  {
    "id": 4,
    "name": "Coefficient Magnitudes (in Linear Models)",
    "description": "Coefficient Magnitudes assess feature influence in linear models by examining the absolute values of their coefficients. Features with larger absolute coefficients are considered to have a stronger impact on the prediction, while the sign of the coefficient indicates the direction of that influence (positive or negative). This technique provides a straightforward and transparent way to understand the direct linear relationship between each input feature and the model's output.",
    "assurance_goals": ["Explainability", "Transparency"],
    "tags": [
      "applicable-models/linear-model",
      "assurance-goal-category/explainability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Interpreting which features influence housing price predictions in linear regression, such as identifying that 'number of bedrooms' has a larger positive impact than 'distance to city centre' based on coefficient magnitudes.",
        "goal": "Explainability"
      },
      {
        "description": "Explaining the factors contributing to customer lifetime value (CLV) in a linear model, showing how 'average monthly spend' has a strong positive coefficient, making the model transparent for business stakeholders.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Only valid for linear relationships; it cannot capture complex non-linear patterns or interactions between features."
      },
      {
        "description": "Highly sensitive to feature scaling; features with larger numerical ranges can appear more important even if their true impact is smaller."
      },
      {
        "description": "Can be misleading in the presence of multicollinearity, where correlated features may split importance or have unstable coefficients."
      },
      {
        "description": "Does not imply causation; a strong correlation (large coefficient) does not necessarily mean a causal relationship."
      }
    ],
    "resources": [],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [2, 3, 10]
  },
  {
    "id": 5,
    "name": "Integrated Gradients",
    "description": "Integrated Gradients explains predictions by calculating how much each input feature contributes to the final output. It works by gradually changing each feature from a neutral starting point (baseline) to its actual value, measuring the model's sensitivity to these changes step by step. This produces importance scores showing which features most influenced the prediction.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "data-requirements/labelled-data",
      "data-requirements/reference-dataset",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Understanding pixel contributions in image classification with deep neural networks.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires a meaningful baseline input; results can be sensitive to the choice of baseline and model must be differentiable, limiting use with non-differentiable components."
      }
    ],
    "resources": [
      {
        "title": "ankurtaly/Integrated-Gradients",
        "url": "https://github.com/ankurtaly/Integrated-Gradients",
        "source_type": "software_package"
      },
      {
        "title": "pytorch/captum",
        "url": "https://github.com/pytorch/captum",
        "source_type": "software_package"
      },
      {
        "title": "Maximum Entropy Baseline for Integrated Gradients",
        "url": "http://arxiv.org/pdf/2204.05948v1",
        "source_type": "technical_paper",
        "authors": ["Hanxiao Tan"],
        "publication_date": "2022-04-12"
      },
      {
        "title": "Integrated Gradients from Scratch | Towards Data Science",
        "url": "https://towardsdatascience.com/integrated-gradients-from-scratch-b46311e4ab4/",
        "source_type": "tutorial"
      },
      {
        "title": "Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution",
        "url": "http://arxiv.org/pdf/2004.10484v2",
        "source_type": "technical_paper",
        "authors": [
          "Gary S. W. Goh",
          "Sebastian Lapuschkin",
          "Leander Weber",
          "Wojciech Samek",
          "Alexander Binder"
        ],
        "publication_date": "2020-04-22"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [1, 6, 7, 12, 27, 28]
  },
  {
    "id": 6,
    "name": "DeepLIFT",
    "description": "DeepLIFT assigns credit (or blame) to each input feature by comparing a neuron's activation to a reference (baseline) and tracking the difference backward through the network. It attributes the change in the output to changes in each input feature relative to what the output would be at the reference input.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-requirements/access-to-training-data",
      "data-requirements/labelled-data",
      "data-requirements/reference-dataset",
      "data-type/graph",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining why a neural network classifies an image as a specific object by tracing neuron activations.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Needs careful configuration for each network architecture; may produce inconsistent scores if multiple reference points are possible, and not all model types supported."
      }
    ],
    "resources": [
      {
        "title": "Learning Important Features Through Propagating Activation Differences",
        "url": "http://arxiv.org/pdf/1704.02685v2",
        "source_type": "technical_paper",
        "authors": ["Avanti Shrikumar", "Peyton Greenside", "Anshul Kundaje"],
        "publication_date": "2017-04-10"
      },
      {
        "title": "pytorch/captum",
        "url": "https://github.com/pytorch/captum",
        "source_type": "software_package"
      },
      {
        "title": "Tutorial A3: DeepLIFT/SHAP \u2014 tangermeme v0.1.0 documentation",
        "url": "https://tangermeme.readthedocs.io/en/latest/tutorials/Tutorial_A3_Deep_LIFT_SHAP.html",
        "source_type": "tutorial"
      },
      {
        "title": "SHAP, Deep LIFT and so on through Captum Integration - PyTorch ...",
        "url": "https://pytorch-tabular.readthedocs.io/en/latest/tutorials/14-Explainability/",
        "source_type": "tutorial"
      },
      {
        "title": "iNNvestigate Documentation",
        "url": "https://innvestigate.readthedocs.io/_/downloads/en/latest/pdf/",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [1, 5, 7, 12, 27, 28]
  },
  {
    "id": 7,
    "name": "Layer-wise Relevance Propagation (LRP)",
    "description": "LRP explains a model's prediction by propagating the result backwards through the network and assigning a relevance score to each input feature. Starting from the predicted output, it distributes the 'relevance' of that output to neurons in each preceding layer, all the way back to the input features, indicating how each feature contributed.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-requirements/labelled-data",
      "data-type/graph",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Visualising important regions in medical images for disease diagnosis using deep learning models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires tailored rules for each layer type; results can sometimes be hard to interpret (negative relevances), and implementation is complex for new architectures."
      }
    ],
    "resources": [
      {
        "title": "rachtibat/LRP-eXplains-Transformers",
        "url": "https://github.com/rachtibat/LRP-eXplains-Transformers",
        "source_type": "software_package"
      },
      {
        "title": "sebastian-lapuschkin/lrp_toolbox",
        "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
        "source_type": "software_package"
      },
      {
        "title": "Getting started \u2014 zennit documentation",
        "url": "https://zennit.readthedocs.io/en/latest/getting-started.html",
        "source_type": "documentation"
      },
      {
        "title": "2.0 documentation",
        "url": "https://lxt.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "Writing Custom Canonizers \u2014 zennit documentation",
        "url": "https://zennit.readthedocs.io/en/latest/how-to/write-custom-canonizers.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [1, 5, 6, 12, 27, 28]
  },
  {
    "id": 8,
    "name": "Contextual Decomposition",
    "description": "Contextual Decomposition interprets neural network predictions by breaking down the model's output into parts attributed to specific input features or groups of features. It isolates the contribution of a particular input (or a combination of inputs) to the final prediction, taking into account the context provided by the other inputs.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "data-requirements/labelled-data",
      "data-type/graph",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining sentiment predictions in text by attributing scores to words or phrases.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Primarily designed for LSTMs; not widely implemented in standard libraries, requiring custom code, and may not scale well to very deep or different model types."
      }
    ],
    "resources": [
      {
        "title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs",
        "url": "http://arxiv.org/pdf/1801.05453v2",
        "source_type": "technical_paper",
        "authors": ["W. James Murdoch", "Peter J. Liu", "Bin Yu"],
        "publication_date": "2018-01-16"
      },
      {
        "title": "FredericGodin/ContextualDecomposition-NLP",
        "url": "https://github.com/FredericGodin/ContextualDecomposition-NLP",
        "source_type": "software_package"
      },
      {
        "title": "Interpreting patient-Specific risk prediction using contextual decomposition of BiLSTMs: Application to children with asthma",
        "url": "https://core.ac.uk/download/294758884.pdf",
        "source_type": "technical_paper",
        "authors": ["Alsaad R.", "Boughorbel S.", "Janahi I.", "Malluhi Q."],
        "publication_date": "2019-01-01T00:00:00"
      },
      {
        "title": "Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models",
        "url": "http://arxiv.org/pdf/1911.06194v2",
        "source_type": "technical_paper",
        "authors": [
          "Xisen Jin",
          "Zhongyu Wei",
          "Junyi Du",
          "Xiangyang Xue",
          "Xiang Ren"
        ],
        "publication_date": "2019-11-08"
      },
      {
        "title": "Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition",
        "url": "http://arxiv.org/pdf/2407.00886v3",
        "source_type": "technical_paper",
        "authors": [
          "Aliyah R. Hsu",
          "Georgia Zhou",
          "Yeshwanth Cherapanamjeri",
          "Yaxuan Huang",
          "Anobel Y. Odisho",
          "Peter R. Carroll",
          "Bin Yu"
        ],
        "publication_date": "2024-07-01"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [9, 11, 26]
  },
  {
    "id": 9,
    "name": "Taylor Decomposition",
    "description": "Taylor Decomposition is a mathematical technique that explains neural network predictions by computing first-order and higher-order derivatives of the network's output with respect to input features. It decomposes the prediction into relevance scores that indicate how much each input feature and feature interaction contributes to the final decision. The method uses Layer-wise Relevance Propagation (LRP) principles to trace prediction contributions backwards through the network layers, providing precise mathematical attributions for each input element.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/neural-network",
      "applicable-models/cnn",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analyzing which pixels in an image contribute most to a convolutional neural network's classification decision, showing both positive and negative relevance scores for different regions of the input image.",
        "goal": "Explainability"
      },
      {
        "description": "Understanding how different word embeddings in a sentiment analysis model contribute to the final sentiment score, revealing which terms drive positive vs negative predictions.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Mathematically complex requiring deep understanding of calculus and neural network architectures."
      },
      {
        "description": "Computationally intensive as it requires computing gradients and higher-order derivatives through the entire network."
      },
      {
        "description": "Approximations used in practice may introduce errors that affect attribution accuracy."
      },
      {
        "description": "Limited tooling availability compared to other explainability methods, with most implementations being research-focused rather than production-ready."
      }
    ],
    "resources": [
      {
        "title": "Explaining NonLinear Classification Decisions with Deep Taylor Decomposition",
        "url": "http://arxiv.org/pdf/1512.02479v1",
        "source_type": "technical_paper",
        "authors": [
          "Gr\u00e9goire Montavon",
          "Sebastian Bach",
          "Alexander Binder",
          "Wojciech Samek",
          "Klaus-Robert M\u00fcller"
        ],
        "publication_date": "2015-12-08"
      },
      {
        "title": "A Rigorous Study Of The Deep Taylor Decomposition",
        "url": "http://arxiv.org/pdf/2211.08425v1",
        "source_type": "technical_paper",
        "authors": ["Leon Sixt", "Tim Landgraf"],
        "publication_date": "2022-11-14"
      },
      {
        "title": "sebastian-lapuschkin/lrp_toolbox",
        "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [8, 11, 26]
  },
  {
    "id": 10,
    "name": "Sobol Indices",
    "description": "Sobol Indices quantify how much each input feature contributes to the total variance in a model's predictions through global sensitivity analysis. The technique calculates first-order indices (individual feature contributions) and total-order indices (including all interaction effects involving that feature). By systematically sampling the input space and decomposing output variance, Sobol Indices reveal which features drive model uncertainty and which interactions between features are most important for predictions.",
    "assurance_goals": ["Explainability", "Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "assurance-goal-category/fairness",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing a climate prediction model to determine which atmospheric parameters (temperature, humidity, pressure) contribute most to rainfall forecast uncertainty, helping meteorologists understand which measurements need the highest precision.",
        "goal": "Explainability"
      },
      {
        "description": "Evaluating a financial risk model to identify which economic indicators (interest rates, inflation, GDP growth) drive the most variability in portfolio value predictions, enabling better risk management strategies.",
        "goal": "Explainability"
      },
      {
        "description": "Analysing a credit scoring model to quantify how much prediction variance stems from zip code (a potential proxy for race), helping identify features that may cause disparate impact across demographic groups.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive, requiring thousands of model evaluations to achieve stable variance estimates, making it impractical for very slow models."
      },
      {
        "description": "Assumes input features are independently distributed, which can lead to misleading results when features are correlated in real data."
      },
      {
        "description": "Curse of dimensionality makes the technique increasingly difficult and expensive to apply as the number of input features grows beyond 10-20."
      },
      {
        "description": "Requires defining appropriate probability distributions for input features, which may not accurately reflect real-world feature distributions."
      }
    ],
    "resources": [
      {
        "title": "Sobol Tensor Trains for Global Sensitivity Analysis",
        "url": "http://arxiv.org/pdf/1712.00233v1",
        "source_type": "technical_paper",
        "authors": [
          "Rafael Ballester-Ripoll",
          "Enrique G. Paredes",
          "Renato Pajarola"
        ],
        "publication_date": "2017-12-01"
      },
      {
        "title": "Sobol indices \u2014 UQpy v4.2.0 documentation",
        "url": "https://uqpyproject.readthedocs.io/en/latest/sensitivity/sobol.html",
        "source_type": "documentation"
      },
      {
        "title": "Sobol Indices to Measure Feature Importance | Towards Data Science",
        "url": "https://towardsdatascience.com/sobol-indices-to-measure-feature-importance-54cedc3281bc/",
        "source_type": "tutorial"
      },
      {
        "title": "Basics \u2014 SALib's documentation",
        "url": "https://salib.readthedocs.io/en/latest/user_guide/basics.html",
        "source_type": "documentation"
      },
      {
        "title": "UQpy (Uncertainty Quantification with python)",
        "url": "https://github.com/SURGroup/UQpy",
        "source_type": "software_package"
      },
      {
        "title": "SALib/SALib",
        "url": "https://github.com/SALib/SALib",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 5,
    "related_techniques": [2, 3, 4]
  },
  {
    "id": 11,
    "name": "Feature Interaction Detection (H-statistic)",
    "description": "The H-statistic measures the strength of interaction between two features in a model by comparing their combined effect on the prediction to the sum of their individual effects. A higher H-statistic for a feature pair means the model's prediction cannot be explained by additive effects of the two features alone, indicating a significant interaction.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Identifying significant interactions in healthcare predictive models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Only captures pairwise interactions (or one feature vs all); relies on partial dependence which can be misleading if features are correlated or have complex interactions."
      }
    ],
    "resources": [
      {
        "title": "An algorithmic approach for feature interaction detection",
        "url": "https://www.semanticscholar.org/paper/97b9a2054275f21816b805de1baae7b195fadfe5",
        "source_type": "technical_paper",
        "authors": [
          "K. Evers",
          "W. Lanowski",
          "L. Kersting",
          "I. Mokry",
          "H. Wagner"
        ]
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "related_techniques": [8, 9, 26]
  },
  {
    "id": 12,
    "name": "Local Interpretable Model-Agnostic Explanations (LIME)",
    "description": "LIME explains an individual prediction by training a simple surrogate model around that specific data point. It perturbs the input data point to create synthetic data, gets the complex model's predictions for these new points, and then fits an interpretable model (like a small linear model) on this local dataset. The surrogate's parameters (or rules) then highlight which features of the original data point influenced the prediction the most in that locality.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-training-data",
      "data-type/tabular",
      "evidence-type/qualitative-report",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining why a customer was denied a loan by approximating the model's decision locally.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Explanations can vary with repeated runs due to randomness; the linear surrogate may not be faithful if the model behavior is highly non-linear in that locality."
      }
    ],
    "resources": [
      {
        "title": "marcotcr/lime",
        "url": "https://github.com/marcotcr/lime",
        "source_type": "software_package"
      },
      {
        "title": "thomasp85/lime",
        "url": "https://github.com/thomasp85/lime",
        "source_type": "software_package"
      },
      {
        "title": "Local Interpretable Model-Agnostic Explanations (lime) \u2014 lime 0.1 ...",
        "url": "https://lime-ml.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems",
        "url": "https://www.semanticscholar.org/paper/e5c703aba8af983c36fedf08c32a6978eadd91b9",
        "source_type": "technical_paper",
        "authors": ["Muhammad Rehman Zafar", "N. Khan"]
      },
      {
        "title": "Enhanced LIME \u2014 ADS 2.6.5 documentation",
        "url": "https://accelerated-data-science.readthedocs.io/en/v2.6.5/user_guide/model_explainability/lime.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 3,
    "related_techniques": [1, 5, 6, 7, 27, 28]
  },
  {
    "id": 13,
    "name": "Ridge Regression Surrogates",
    "description": "This technique approximates a complex model by training a ridge regression (a linear model with L2 regularization) on the original model's predictions. The ridge regression serves as a global surrogate that balances fidelity and interpretability, capturing the main linear relationships that the complex model learned while ignoring noise due to regularization.",
    "assurance_goals": ["Explainability", "Transparency"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Approximating a complex ensemble model used for credit scoring with a ridge regression surrogate to identify the most influential features (income, credit history, debt-to-income ratio) and their linear relationships for regulatory compliance reporting.",
        "goal": "Explainability"
      },
      {
        "description": "Creating a ridge regression surrogate of a neural network used for medical diagnosis to understand which patient symptoms and biomarkers have the strongest linear predictive relationships with disease outcomes.",
        "goal": "Explainability"
      },
      {
        "description": "Creating an interpretable approximation of a complex insurance pricing model for regulatory compliance, enabling stakeholders to understand and validate the decision-making process through transparent linear relationships.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Linear approximation may miss important non-linear relationships and interactions captured by the original complex model."
      },
      {
        "description": "Requires a representative dataset to train the surrogate model, which may not be available or may be expensive to generate."
      },
      {
        "description": "Ridge regularisation may oversimplify the model by shrinking coefficients, potentially hiding important but less dominant features."
      },
      {
        "description": "Surrogate fidelity depends on how well linear relationships approximate the original model's behaviour across the entire input space."
      }
    ],
    "resources": [
      {
        "title": "scikit-learn Ridge Regression Documentation",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html",
        "source_type": "documentation"
      },
      {
        "title": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable",
        "url": "https://christophm.github.io/interpretable-ml-book/global.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [29, 64, 65]
  },
  {
    "id": 14,
    "name": "Partial Dependence Plots (PDP)",
    "description": "Partial Dependence Plots show how changing one or two features affects a model's predictions on average. The technique works by varying the selected feature(s) across their full range whilst keeping all other features fixed at their original values, then averaging the predictions. This creates a clear visualisation of whether increasing or decreasing a feature tends to increase or decrease predictions, and reveals patterns like linear trends, plateaus, or threshold effects that help explain model behaviour.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing how house prices change with property size in a real estate prediction model, revealing whether the relationship is linear or if there are diminishing returns for very large properties.",
        "goal": "Explainability"
      },
      {
        "description": "Examining how customer age affects predicted loan default probability in a credit scoring model, showing whether risk increases steadily with age or has specific age ranges with higher risk.",
        "goal": "Explainability"
      },
      {
        "description": "Visualising how temperature affects crop yield predictions in agricultural models, identifying optimal temperature ranges and potential threshold effects.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Assumes features are independent when averaging, which can be misleading when features are highly correlated."
      },
      {
        "description": "Shows only average effects across all instances, potentially hiding important variations in how different subgroups respond to feature changes."
      },
      {
        "description": "Cannot reveal instance-specific effects or interactions between the plotted feature and other features."
      },
      {
        "description": "May be computationally expensive for large datasets since it requires making predictions across the full range of feature values."
      }
    ],
    "resources": [
      {
        "title": "DanielKerrigan/PDPilot",
        "url": "https://github.com/DanielKerrigan/PDPilot",
        "source_type": "software_package"
      },
      {
        "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation",
        "url": "http://arxiv.org/pdf/1309.6392v2",
        "source_type": "technical_paper",
        "authors": [
          "Alex Goldstein",
          "Adam Kapelner",
          "Justin Bleich",
          "Emil Pitkin"
        ],
        "publication_date": "2013-09-25"
      },
      {
        "title": "SauceCat/PDPbox",
        "url": "https://github.com/SauceCat/PDPbox",
        "source_type": "software_package"
      },
      {
        "title": "iPDP: On Partial Dependence Plots in Dynamic Modeling Scenarios",
        "url": "http://arxiv.org/pdf/2306.07775v1",
        "source_type": "technical_paper",
        "authors": [
          "Maximilian Muschalik",
          "Fabian Fumagalli",
          "Rohit Jagtani",
          "Barbara Hammer",
          "Eyke H\u00fcllermeier"
        ],
        "publication_date": "2023-06-13"
      },
      {
        "title": "How to Interpret Models: PDP and ICE | Towards Data Science",
        "url": "https://towardsdatascience.com/how-to-interpret-models-pdp-and-ice-eabed0062e2c/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [15, 16]
  },
  {
    "id": 16,
    "name": "Individual Conditional Expectation Plots (ICE)",
    "description": "ICE plots display the predicted output for individual instances as a function of a feature, with all other features held fixed for each instance. Each line on an ICE plot represents one instance's prediction trajectory as the feature of interest changes, revealing whether different instances are affected differently by that feature.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/low",
      "explanatory-scope/local",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/visualization"
    ],
    "example_use_cases": [
      {
        "description": "Examining how house price predictions vary with property age for individual properties, revealing that whilst most houses follow a declining price trend with age, historic properties (built before 1900) show different patterns due to heritage value.",
        "goal": "Explainability"
      },
      {
        "description": "Analysing how individual patients' diabetes risk predictions change with BMI, showing that whilst most patients follow the expected increasing risk pattern, some patients with specific genetic markers show different response curves.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Plots can become cluttered and difficult to interpret when displaying many instances simultaneously."
      },
      {
        "description": "Does not provide automatic summarisation of overall effects, requiring manual visual inspection to identify patterns."
      },
      {
        "description": "Still assumes all other features remain fixed at their observed values, which may not reflect realistic scenarios."
      },
      {
        "description": "Cannot reveal interactions between the plotted feature and other features for individual instances."
      }
    ],
    "resources": [
      {
        "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation",
        "url": "http://arxiv.org/pdf/1309.6392v2",
        "source_type": "technical_paper",
        "authors": [
          "Alex Goldstein",
          "Adam Kapelner",
          "Justin Bleich",
          "Emil Pitkin"
        ],
        "publication_date": "2013-09-25"
      },
      {
        "title": "Bringing a Ruler Into the Black Box: Uncovering Feature Impact from Individual Conditional Expectation Plots",
        "url": "http://arxiv.org/pdf/2109.02724v1",
        "source_type": "technical_paper",
        "authors": ["Andrew Yeh", "Anhthy Ngo"],
        "publication_date": "2021-09-06"
      },
      {
        "title": "Explainable AI(XAI) - A guide to 7 packages in Python to explain ...",
        "url": "https://towardsdatascience.com/explainable-ai-xai-a-guide-to-7-packages-in-python-to-explain-your-models-932967f0634b/",
        "source_type": "tutorial"
      },
      {
        "title": "Communicating Uncertainty in Machine Learning Explanations: A Visualization Analytics Approach for Predictive Process Monitoring",
        "url": "https://www.semanticscholar.org/paper/3d0090df2b73369b502559eb49fd6d1ae432b952",
        "source_type": "technical_paper",
        "authors": ["Nijat Mehdiyev", "Maxim Majlatow", "Peter Fettke"]
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [14, 15]
  },
  {
    "id": 17,
    "name": "Saliency Maps",
    "description": "Saliency maps are visual explanations for image classification models that highlight which pixels in an image most strongly influence the model's prediction. Computed by calculating gradients of the model's output with respect to input pixels, saliency maps produce heatmaps where brighter regions indicate pixels that, when changed, would most significantly affect the prediction. This technique helps users understand which parts of an image the model is 'looking at' when making decisions.",
    "assurance_goals": ["Explainability", "Fairness"],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-type/image",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing X-ray images in a pneumonia detection model to verify that the algorithm focuses on lung regions showing inflammatory patterns rather than irrelevant areas like medical equipment or patient positioning markers.",
        "goal": "Explainability"
      },
      {
        "description": "Examining skin lesion classification models to ensure the algorithm identifies diagnostic features (irregular borders, colour variation) rather than artifacts like rulers, hair, or skin markings that shouldn't influence medical decisions.",
        "goal": "Explainability"
      },
      {
        "description": "Auditing a dermatology AI system to verify it focuses on medical symptoms rather than skin colour when diagnosing conditions, ensuring equitable treatment across racial groups by revealing inappropriate attention to demographic features.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Saliency maps are often noisy and can change dramatically with small input perturbations, making them unstable."
      },
      {
        "description": "Highlighted regions may not correspond to semantically meaningful or human-understandable features."
      },
      {
        "description": "Only indicates local gradient information, not causal importance or actual decision-making logic."
      },
      {
        "description": "May highlight irrelevant pixels that happen to have high gradients due to model artifacts rather than meaningful patterns."
      }
    ],
    "resources": [
      {
        "title": "utkuozbulak/pytorch-cnn-visualizations",
        "url": "https://github.com/utkuozbulak/pytorch-cnn-visualizations",
        "source_type": "software_package"
      },
      {
        "title": "Concepts of Saliency and Explainability in AI",
        "url": "https://xaitk-saliency.readthedocs.io/en/latest/xaitk_explanation.html",
        "source_type": "documentation"
      },
      {
        "title": "Occlusion Saliency Example",
        "url": "https://xaitk-saliency.readthedocs.io/en/latest/examples/OcclusionSaliency.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [18, 19]
  },
  {
    "id": 18,
    "name": "Gradient-weighted Class Activation Mapping (Grad-CAM)",
    "description": "Grad-CAM creates visual heatmaps showing which regions of an image a convolutional neural network focuses on when making a specific classification. Unlike pixel-level techniques, Grad-CAM produces coarser region-based explanations by using gradients from the predicted class to weight the CNN's final feature maps, then projecting these weighted activations back to create an overlay on the original image. This provides intuitive visual explanations of where the model is 'looking' for evidence of different classes.",
    "assurance_goals": ["Explainability", "Fairness"],
    "tags": [
      "applicable-models/cnn",
      "assurance-goal-category/explainability",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-type/image",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Validating that a melanoma detection model focuses on the actual skin lesion rather than surrounding healthy skin, medical equipment, or artifacts when making cancer/benign classifications.",
        "goal": "Explainability"
      },
      {
        "description": "Debugging an autonomous vehicle's traffic sign recognition system by visualising whether the model correctly focuses on the sign itself rather than background objects, shadows, or irrelevant visual elements.",
        "goal": "Explainability"
      },
      {
        "description": "Auditing a medical imaging system for racial bias by examining whether diagnostic predictions inappropriately focus on skin tone regions rather than actual pathological features, ensuring equitable healthcare AI deployment.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires access to the CNN's internal feature maps and gradients, limiting use to white-box scenarios."
      },
      {
        "description": "Resolution is constrained by the final convolutional layer's feature map size, producing coarser localisation than pixel-level methods."
      },
      {
        "description": "Only applicable to CNN architectures with clearly defined convolutional layers, not suitable for other neural network types."
      },
      {
        "description": "May highlight regions that correlate with the class but aren't causally important for the model's decision-making process."
      }
    ],
    "resources": [
      {
        "title": "jacobgil/pytorch-grad-cam",
        "url": "https://github.com/jacobgil/pytorch-grad-cam",
        "source_type": "software_package"
      },
      {
        "title": "Grad-CAM: Visualize class activation maps with Keras, TensorFlow ...",
        "url": "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/",
        "source_type": "tutorial"
      },
      {
        "title": "kazuto1011/grad-cam-pytorch",
        "url": "https://github.com/kazuto1011/grad-cam-pytorch",
        "source_type": "software_package"
      },
      {
        "title": "A Tutorial on Explainable Image Classification for Dementia Stages Using Convolutional Neural Network and Gradient-weighted Class Activation Mapping",
        "url": "https://www.semanticscholar.org/paper/8b1139cb06cfe5ba69e2fd05e1450b43df031a02",
        "source_type": "introductory_paper",
        "authors": ["Kevin Kam Fung Yuen"]
      },
      {
        "title": "A Guide to Grad-CAM in Deep Learning - Analytics Vidhya",
        "url": "https://www.analyticsvidhya.com/blog/2023/12/grad-cam-in-deep-learning/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [17, 19]
  },
  {
    "id": 19,
    "name": "Occlusion Sensitivity",
    "description": "Occlusion sensitivity tests which parts of the input are important by occluding (masking or removing) them and seeing how the model's prediction changes. For example, portions of an image can be covered up in a sliding window fashion; if the model's confidence drops significantly when a certain region is occluded, that region was important for the prediction.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/image",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Testing which regions of a chest X-ray are critical for pneumonia detection by systematically covering different areas with grey patches and measuring how much the model's confidence drops for each occluded region.",
        "goal": "Explainability"
      },
      {
        "description": "Evaluating whether a facial recognition system relies on specific facial features by masking eyes, nose, mouth, or other regions to identify which areas cause the biggest drop in recognition accuracy.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive as it requires running inference multiple times for each region tested, scaling poorly with input size."
      },
      {
        "description": "Choice of occlusion size and shape can significantly bias results - too small may miss important features, too large may occlude multiple relevant regions simultaneously."
      },
      {
        "description": "Cannot capture interactions between multiple regions that jointly contribute to the prediction but are individually less important."
      },
      {
        "description": "Results may be misleading if the model adapts to occlusion patterns or if occluded regions are filled with unrealistic pixel values."
      }
    ],
    "resources": [
      {
        "title": "kazuto1011/grad-cam-pytorch",
        "url": "https://github.com/kazuto1011/grad-cam-pytorch",
        "source_type": "software_package"
      },
      {
        "title": "Occlusion Sensitivity Analysis with Augmentation Subspace Perturbation in Deep Feature Space",
        "url": "http://arxiv.org/pdf/2311.15022v1",
        "source_type": "technical_paper",
        "authors": ["Pedro Valois", "Koichiro Niinuma", "Kazuhiro Fukui"],
        "publication_date": "2023-11-25"
      },
      {
        "title": "Occlusion Sensitivity \u2014 tf-explain documentation",
        "url": "https://tf-explain.readthedocs.io/en/latest/methods.html#occlusion-sensitivity",
        "source_type": "documentation"
      },
      {
        "title": "Adaptive occlusion sensitivity analysis for visually explaining video recognition networks",
        "url": "http://arxiv.org/pdf/2207.12859v2",
        "source_type": "technical_paper",
        "authors": [
          "Tomoki Uchiyama",
          "Naoya Sogi",
          "Satoshi Iizuka",
          "Koichiro Niinuma",
          "Kazuhiro Fukui"
        ],
        "publication_date": "2022-07-26"
      },
      {
        "title": "sicara/tf-explain",
        "url": "https://github.com/sicara/tf-explain",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 4,
    "related_techniques": [17, 18]
  },
  {
    "id": 20,
    "name": "Classical Attention Analysis in Neural Networks",
    "description": "Classical attention mechanisms in RNNs and CNNs create alignment matrices and temporal attention patterns that show how models focus on different input elements over time or space. This technique analyses these traditional attention patterns, particularly in encoder-decoder architectures and sequence-to-sequence models, where attention weights reveal which source elements influence each output step. Unlike transformer self-attention analysis, this focuses on understanding alignment patterns, temporal dependencies, and encoder-decoder attention dynamics in classical neural architectures.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/rnn",
      "applicable-models/cnn",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing encoder-decoder attention in a neural machine translation model to verify the alignment between source and target words, ensuring the model learns proper translation correspondences rather than positional biases.",
        "goal": "Explainability"
      },
      {
        "description": "Examining temporal attention patterns in an RNN-based image captioning model to understand how attention moves across different image regions as it generates each word of the caption description.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Attention weights are not always strongly correlated with feature importance for the final prediction."
      },
      {
        "description": "High attention does not necessarily imply causal influence - models can attend to irrelevant but correlated features."
      },
      {
        "description": "Only applicable to neural network architectures that explicitly use attention mechanisms."
      },
      {
        "description": "Interpretation can be misleading without understanding the specific attention mechanism implementation and training dynamics."
      }
    ],
    "resources": [
      {
        "title": "An Attentive Survey of Attention Models",
        "url": "https://www.semanticscholar.org/paper/a8427ce5aee6d62800c725588e89940ed4910e0d",
        "source_type": "review_paper",
        "authors": [
          "S. Chaudhari",
          "Gungor Polatkan",
          "R. Ramanath",
          "Varun Mithal"
        ]
      },
      {
        "title": "Attention, please! A survey of neural attention models in deep learning",
        "url": "https://www.semanticscholar.org/paper/44930df2a3186edb58c4d6f6e5ed828c5d6a0089",
        "source_type": "review_paper",
        "authors": ["Alana de Santana Correia", "E. Colombini"]
      },
      {
        "title": "ecco - Explain, Analyze, and Visualize NLP Language Models",
        "url": "https://github.com/jalammar/ecco",
        "source_type": "software_package"
      },
      {
        "title": "Enhancing Sentiment Analysis of Twitter Data Using Recurrent Neural Networks with Attention Mechanism",
        "url": "https://www.semanticscholar.org/paper/c59e0158280a567114ae8ca64a932eefd127e0aa",
        "source_type": "technical_paper",
        "authors": [
          "S. Nithya",
          "X. A. Presskila",
          "B. Sakthivel",
          "R. Krishnan",
          "K. Narayanan",
          "S. Sundararajan"
        ]
      },
      {
        "title": "Can Neural Networks Develop Attention? Google Thinks they Can ...",
        "url": "https://www.kdnuggets.com/2019/11/neural-networks-develop-attention-google.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [72]
  },
  {
    "id": 21,
    "name": "Factor Analysis",
    "description": "Factor analysis identifies hidden themes or patterns that explain why certain features in data tend to vary together. It works by finding a smaller number of underlying factors that can explain the relationships between many observed features. For example, it might discover that several test scores are all influenced by a single underlying 'intelligence' factor.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Discovering underlying factors in psychological survey data for social science research.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Assumes linear relationships and normality; results (factors) can be abstract and not directly interpretable, and requires deciding on number of factors and rotation method."
      }
    ],
    "resources": [
      {
        "title": "Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey",
        "url": "http://arxiv.org/pdf/2101.00734v2",
        "source_type": "review_paper",
        "authors": [
          "Benyamin Ghojogh",
          "Ali Ghodsi",
          "Fakhri Karray",
          "Mark Crowley"
        ],
        "publication_date": "2021-01-04"
      },
      {
        "title": "Exploratory Factor Analysis (efa) \u2014 jamovi Documentation",
        "url": "https://jamovi.readthedocs.io/ko/latest/jmv/jmv_efa.html",
        "source_type": "documentation"
      },
      {
        "title": "Factor Analysis in R Course | DataCamp",
        "url": "https://www.datacamp.com/courses/factor-analysis-in-r",
        "source_type": "tutorial"
      },
      {
        "title": "Welcome to the FactorAnalyzer documentation! \u2014 factor_analyzer ...",
        "url": "https://factor-analyzer.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "Confirmatory Factor Analysis Fundamentals | Towards Data Science",
        "url": "https://towardsdatascience.com/confirmatory-factor-analysis-theory-aac11af008a6/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [22, 23, 24]
  },
  {
    "id": 22,
    "name": "Principal Component Analysis (PCA)",
    "description": "Principal Component Analysis transforms high-dimensional data into a lower-dimensional representation by finding the directions (principal components) that capture the maximum variance in the data. Each component is a linear combination of original features, with the first component explaining the most variance, the second component the most remaining variance orthogonal to the first, and so on. This technique reveals underlying patterns in data structure, enables visualization of complex datasets, and helps identify which combinations of features drive the most variation in the data.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/visualization",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analyzing customer behavior data with dozens of variables (purchase frequency, spending patterns, demographics) to identify the 2-3 main dimensions that explain customer segmentation, revealing whether customers cluster by spending level, product preferences, or shopping frequency.",
        "goal": "Explainability"
      },
      {
        "description": "Reducing dimensionality of image data for facial recognition systems by finding the principal components that capture the most variation in face shapes and expressions, helping understand which facial features contribute most to distinguishing between individuals.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Principal components are abstract linear combinations of original features that often lack clear real-world interpretation or meaning."
      },
      {
        "description": "Only captures linear relationships between features, missing non-linear patterns and complex interactions in the data."
      },
      {
        "description": "Results are highly sensitive to feature scaling - features with larger numerical ranges can dominate the principal components."
      },
      {
        "description": "Information loss is inherent when reducing dimensions, and choosing the optimal number of components requires balancing simplicity with retained variance."
      }
    ],
    "resources": [
      {
        "title": "erdogant/pca",
        "url": "https://github.com/erdogant/pca",
        "source_type": "software_package"
      },
      {
        "title": "How to Calculate Principal Component Analysis (PCA) from Scratch ...",
        "url": "https://www.machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/",
        "source_type": "tutorial"
      },
      {
        "title": "A One-Stop Shop for Principal Component Analysis | Towards Data ...",
        "url": "https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c/",
        "source_type": "tutorial"
      },
      {
        "title": "Principal Component Analysis (PCA) with Scikit-Learn - KDnuggets",
        "url": "https://www.kdnuggets.com/2023/05/principal-component-analysis-pca-scikitlearn.html",
        "source_type": "tutorial"
      },
      {
        "title": "willtownes/glmpca-py",
        "url": "https://github.com/willtownes/glmpca-py",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [21, 23, 24]
  },
  {
    "id": 23,
    "name": "t-SNE",
    "description": "t-SNE (t-Distributed Stochastic Neighbour Embedding) is a non-linear dimensionality reduction technique that creates 2D or 3D visualisations of high-dimensional data by preserving local neighbourhood relationships. The algorithm converts similarities between data points into joint probabilities in the high-dimensional space, then tries to minimise the divergence between these probabilities and those in the low-dimensional embedding. This approach excels at revealing cluster structures and local patterns, making it particularly effective for exploratory data analysis and understanding complex data relationships that linear methods like PCA might miss.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/visualization"
    ],
    "example_use_cases": [
      {
        "description": "Analyzing genomic data with thousands of gene expression features to visualize how different cancer subtypes cluster together, revealing which tumors have similar molecular signatures and potentially similar treatment responses.",
        "goal": "Explainability"
      },
      {
        "description": "Exploring deep learning model embeddings to understand how a neural network represents different categories of images, showing whether the model groups similar objects (cars, animals, furniture) in meaningful clusters in its internal feature space.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Non-deterministic algorithm produces different results on each run, making it difficult to reproduce exact visualizations or compare results across studies."
      },
      {
        "description": "Prioritizes preserving local neighborhood structure at the expense of global relationships, potentially creating misleading impressions about overall data topology."
      },
      {
        "description": "Computationally expensive with O(n²) complexity, making it impractical for datasets with more than ~10,000 points without approximation methods."
      },
      {
        "description": "Sensitive to hyperparameter choices (perplexity, learning rate, iterations) that can dramatically affect clustering patterns and require domain expertise to tune appropriately."
      }
    ],
    "resources": [
      {
        "title": "pavlin-policar/openTSNE",
        "url": "https://github.com/pavlin-policar/openTSNE",
        "source_type": "software_package"
      },
      {
        "title": "openTSNE: Extensible, parallel implementations of t-SNE ...",
        "url": "https://opentsne.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "How t-SNE works \u2014 openTSNE 1.0.0 documentation",
        "url": "https://opentsne.readthedocs.io/en/stable/tsne_algorithm.html",
        "source_type": "documentation"
      },
      {
        "title": "t-SNE from Scratch (ft. NumPy) | Towards Data Science",
        "url": "https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [21, 22, 24]
  },
  {
    "id": 24,
    "name": "UMAP",
    "description": "UMAP (Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique that creates 2D or 3D visualisations of high-dimensional data by constructing a mathematical model of the data's underlying manifold structure. Unlike t-SNE, UMAP preserves both local neighbourhood relationships and global topology more effectively, using techniques from topological data analysis and Riemannian geometry. This approach often produces more interpretable cluster layouts while maintaining meaningful distances between clusters, making it particularly valuable for exploratory data analysis and understanding complex dataset structures.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/visualization"
    ],
    "example_use_cases": [
      {
        "description": "Analysing single-cell RNA sequencing data to visualise how different cell types cluster based on gene expression patterns, revealing developmental trajectories and identifying previously unknown cell subtypes in tissue samples.",
        "goal": "Explainability"
      },
      {
        "description": "Exploring customer segmentation by reducing hundreds of behavioural and demographic features to 2D space, showing how different customer groups relate to each other and identifying transition zones where customers might move between segments.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Hyperparameter choices (n_neighbors, min_dist, metric) significantly influence the embedding structure and can lead to very different interpretations of the same data."
      },
      {
        "description": "While preserving global structure better than t-SNE, distances in the reduced space still don't directly correspond to distances in the original feature space."
      },
      {
        "description": "Performance can be sensitive to the choice of distance metric, which may not be obvious for complex or mixed data types."
      },
      {
        "description": "Like other manifold learning techniques, it assumes the data lies on a lower-dimensional manifold, which may not hold for all datasets."
      }
    ],
    "resources": [
      {
        "title": "lmcinnes/umap",
        "url": "https://github.com/lmcinnes/umap",
        "source_type": "software_package"
      },
      {
        "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
        "url": "http://arxiv.org/pdf/1802.03426v3",
        "source_type": "technical_paper",
        "authors": ["Leland McInnes", "John Healy", "James Melville"],
        "publication_date": "2018-02-09"
      },
      {
        "title": "Uniform Manifold Approximation and Projection (UMAP) and its Variants: Tutorial and Survey",
        "url": "http://arxiv.org/pdf/2109.02508v1",
        "source_type": "review_paper",
        "authors": [
          "Benyamin Ghojogh",
          "Ali Ghodsi",
          "Fakhri Karray",
          "Mark Crowley"
        ],
        "publication_date": "2021-08-25"
      },
      {
        "title": "How UMAP Works \u2014 umap 0.5.8 documentation",
        "url": "https://umap-learn.readthedocs.io/en/latest/how_umap_works.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [21, 22, 23]
  },
  {
    "id": 25,
    "name": "Prototype and Criticism Models",
    "description": "Prototype and Criticism Models provide data understanding by identifying two complementary sets of examples: prototypes represent the most typical instances that best summarise common patterns in the data, whilst criticisms are outliers or edge cases that are poorly represented by the prototypes. For example, in a dataset of customer transactions, prototypes might be the most representative buying patterns (frequent small purchases, occasional large purchases), whilst criticisms could be unusual behaviors (bulk buyers, one-time high-value customers). This dual approach reveals both what is normal and what is exceptional, helping understand data coverage and model blind spots.",
    "assurance_goals": ["Explainability", "Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "assurance-goal-category/fairness",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing medical imaging datasets to identify prototype scans that represent typical healthy tissue patterns and criticism examples showing rare disease presentations, helping radiologists understand what the model considers 'normal' versus cases requiring special attention.",
        "goal": "Explainability"
      },
      {
        "description": "Evaluating credit scoring models by finding prototype borrowers who represent typical low-risk profiles and criticism cases showing unusual but legitimate financial patterns that the model might misclassify, ensuring fair treatment of edge cases.",
        "goal": "Explainability"
      },
      {
        "description": "Evaluating representation bias in hiring datasets by examining whether prototypes systematically exclude certain demographic groups and criticisms disproportionately represent minorities, revealing data collection inequities.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Selection of prototypes and criticisms is highly dependent on the choice of distance metric or similarity measure, which may not capture all meaningful relationships in the data."
      },
      {
        "description": "Computational complexity can become prohibitive for very large datasets, as the method often requires pairwise comparisons or optimisation over the entire dataset."
      },
      {
        "description": "The number of prototypes and criticisms to select is typically a hyperparameter that requires domain expertise to set appropriately."
      },
      {
        "description": "Results may not generalise well if the training data distribution differs significantly from the deployment data distribution."
      }
    ],
    "resources": [
      {
        "title": "Examples are not Enough, Learn to Criticize! Criticism for Interpretability",
        "url": "https://proceedings.neurips.cc/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf",
        "source_type": "technical_paper",
        "authors": ["Been Kim", "Rajiv Khanna", "Oluwasanmi O. Koyejo"],
        "publication_date": "2016-12-05"
      },
      {
        "title": "SeldonIO/alibi",
        "url": "https://github.com/SeldonIO/alibi",
        "source_type": "software_package"
      },
      {
        "title": "Prototype Selection for Interpretable Classification",
        "url": "http://arxiv.org/pdf/1202.5933v1",
        "source_type": "technical_paper",
        "authors": ["Oscar Reyes", "Carlos Morell", "Sebastian Ventura"],
        "publication_date": "2012-02-27"
      },
      {
        "title": "Alibi Explain Documentation",
        "url": "https://docs.seldon.io/projects/alibi/en/stable/",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [73, 77]
  },
  {
    "id": 26,
    "name": "Influence Functions",
    "description": "Influence functions quantify how much each training example influenced a model's predictions by computing the change in prediction that would occur if that training example were removed and the model retrained. Using calculus and the implicit function theorem, they approximate this 'leave-one-out' effect without actually retraining the model by computing gradients and Hessian information. This mathematical approach reveals which specific training examples were most responsible for pushing the model toward or away from particular predictions, enabling practitioners to trace problematic outputs back to their root causes in the training data.",
    "assurance_goals": ["Explainability", "Fairness", "Privacy"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "assurance-goal-category/fairness",
      "assurance-goal-category/privacy",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Investigating why a medical diagnosis model misclassified a patient by identifying which specific training cases most influenced the incorrect prediction, revealing potential mislabelled examples or problematic patterns in the training data.",
        "goal": "Explainability"
      },
      {
        "description": "Analysing a spam detection system that falsely flagged legitimate emails by tracing the prediction back to influential training examples, discovering that certain training emails contained misleading patterns that caused the model to overfit.",
        "goal": "Explainability"
      },
      {
        "description": "Auditing a loan approval model for discriminatory patterns by identifying which training examples most influenced rejections of minority applicants, revealing whether biased historical decisions are driving current unfair outcomes.",
        "goal": "Fairness"
      },
      {
        "description": "Assessing membership inference risks in a medical model by identifying whether certain patient records have disproportionate influence on predictions, indicating potential data leakage vulnerabilities.",
        "goal": "Privacy"
      }
    ],
    "limitations": [
      {
        "description": "Computationally intensive, requiring Hessian matrix computations that become intractable for very large models with millions of parameters."
      },
      {
        "description": "Requires access to the complete training dataset and training process, making it impossible to apply to pre-trained models without access to original training data."
      },
      {
        "description": "Accuracy degrades for highly non-convex models where the linear approximation underlying influence functions breaks down."
      },
      {
        "description": "Results can be sensitive to hyperparameter choices and may not generalise well across different model architectures or training procedures."
      }
    ],
    "resources": [
      {
        "title": "Understanding Black-box Predictions via Influence Functions",
        "url": "https://www.semanticscholar.org/paper/08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
        "source_type": "technical_paper",
        "authors": ["Pang Wei Koh", "Percy Liang"]
      },
      {
        "title": "nimarb/pytorch_influence_functions",
        "url": "https://github.com/nimarb/pytorch_influence_functions",
        "source_type": "software_package"
      },
      {
        "title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions",
        "url": "https://www.semanticscholar.org/paper/f33f3dece9f34c1ec5417dccf9e0acf592d8e8cb",
        "source_type": "technical_paper",
        "authors": [
          "Sang Keun Choe",
          "Hwijeen Ahn",
          "Juhan Bae",
          "Kewen Zhao",
          "Minsoo Kang",
          "Youngseog Chung",
          "Adithya Pratapa",
          "W. Neiswanger",
          "Emma Strubell",
          "Teruko Mitamura",
          "Jeff G. Schneider",
          "Eduard H. Hovy",
          "Roger B. Grosse",
          "Eric P. Xing"
        ]
      },
      {
        "title": "Scaling Up Influence Functions",
        "url": "https://www.semanticscholar.org/paper/ef2a773c3c7848a6cc16b18164be5f8876a310af",
        "source_type": "technical_paper",
        "authors": [
          "Andrea Schioppa",
          "Polina Zablotskaia",
          "David Vilar",
          "Artem Sokolov"
        ]
      },
      {
        "title": "Welcome to torch-influence's API Reference! \u2014 torch-influence 0.1.0 ...",
        "url": "https://torch-influence.readthedocs.io/",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "related_techniques": [8, 9, 11]
  },
  {
    "id": 27,
    "name": "Contrastive Explanation Method (CEM)",
    "description": "CEM explains model decisions by producing contrastive examples: it finds the minimal changes to an input that would switch the model's prediction. It outputs pertinent negatives (what could be removed from the input to change the prediction) and pertinent positives (what minimal additional features would be needed to reach the same decision) as a way to highlight what is essential in the input for that prediction.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining loan rejections by showing what changes would lead to approval.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Solving for pertinent positives/negatives requires iterative optimization per instance; results can be sensitive to parameter settings and might yield unrealistic contrastive inputs if constraints are not tight."
      }
    ],
    "resources": [
      {
        "title": "Towards Robust Contrastive Explanations for Human-Neural Multi-agent Systems",
        "url": "https://www.semanticscholar.org/paper/24613859807ca8e187705909e936a503999ae37e",
        "source_type": "technical_paper",
        "authors": ["Francesco Leofante", "A. Lomuscio"]
      },
      {
        "title": "Risk-Sensitive Counterfactual Explanations for AI Model Predictions",
        "url": "https://core.ac.uk/download/621981491.pdf",
        "source_type": "technical_paper",
        "authors": ["Asrzad, Amir", "Li, Xiaobai", "Sarkar, Sumit"],
        "publication_date": "2024-12-15T08:00:00"
      },
      {
        "title": "Benchmarking and survey of explanation methods for black box models",
        "url": "https://core.ac.uk/download/599106733.pdf",
        "source_type": "review_paper",
        "authors": [
          "Bodria Francesco",
          "Giannotti Fosca",
          "Guidotti R.",
          "Naretto Francesca",
          "Pedreschi Dino",
          "Rinzivillo S."
        ],
        "publication_date": "2023-01-01T00:00:00"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [1, 5, 6, 7, 12, 28]
  },
  {
    "id": 28,
    "name": "ANCHOR",
    "description": "ANCHOR generates precision if-then rules as explanations for individual predictions. It finds a minimal set of conditions (on input features) that 'anchor' the prediction, meaning that if those conditions are met, the model will almost always give the same prediction. These anchor rules are designed to be easily understood and highly predictive for that specific instance.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Generating rules to explain individual predictions in text classification.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Only provides explanations for individual predictions (local rules); may struggle with continuous features (usually requires discretization) and might not find an anchor if conditions are too strict."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [1, 5, 6, 7, 12, 27]
  },
  {
    "id": 29,
    "name": "RuleFit",
    "description": "RuleFit is a method that creates an interpretable model by combining linear terms with decision rules. It first extracts potential rules from ensemble trees, then builds a sparse linear model where those rules (binary conditions) and original features are used as predictors, with regularization to keep the model simple. The final model is a linear combination of a small set of rules and original features, balancing interpretability with predictive power.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Building interpretable models for predicting customer churn with rule-based explanations.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "The resulting model can still have many rules, complicating interpretability; performance may lag behind black-box models if too few rules are allowed for simplicity."
      }
    ],
    "resources": [
      {
        "title": "christophM/rulefit",
        "url": "https://github.com/christophM/rulefit",
        "source_type": "software_package"
      },
      {
        "title": "Tree Ensembles with Rule Structured Horseshoe Regularization",
        "url": "http://arxiv.org/pdf/1702.05008v2",
        "source_type": "technical_paper",
        "authors": ["Malte Nalenz", "Mattias Villani"],
        "publication_date": "2017-02-16"
      },
      {
        "title": "Safe RuleFit: Learning Optimal Sparse Rule Model by Meta Safe Screening",
        "url": "http://arxiv.org/pdf/1810.01683v2",
        "source_type": "technical_paper",
        "authors": ["Hiroki Kato", "Hiroyuki Hanada", "Ichiro Takeuchi"],
        "publication_date": "2018-10-03"
      },
      {
        "title": "csinva/imodels",
        "url": "https://github.com/csinva/imodels",
        "source_type": "software_package"
      },
      {
        "title": "Getting More From Regression Models with RuleFit | Towards Data ...",
        "url": "https://towardsdatascience.com/getting-more-from-regression-models-with-rulefit-2e6be8d77432/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [13, 64, 65]
  },
  {
    "id": 30,
    "name": "Monte Carlo Dropout",
    "description": "Monte Carlo Dropout uses dropout at prediction time to estimate model uncertainty. By running multiple forward passes with random dropout activated and observing the variation in outputs, it provides a distribution of predictions. A consistent prediction across runs indicates high confidence, while widely varying predictions indicate uncertainty.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Estimating prediction uncertainty in medical diagnosis models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Only captures model uncertainty, not data uncertainty; requires multiple forward passes and results depend on dropout rate, which must be same as training to be meaningful."
      }
    ],
    "resources": [
      {
        "title": "mattiasegu/uncertainty_estimation_deep_learning",
        "url": "https://github.com/mattiasegu/uncertainty_estimation_deep_learning",
        "source_type": "software_package"
      },
      {
        "title": "uzh-rpg/deep_uncertainty_estimation",
        "url": "https://github.com/uzh-rpg/deep_uncertainty_estimation",
        "source_type": "software_package"
      },
      {
        "title": "mourga/variational-lstm",
        "url": "https://github.com/mourga/variational-lstm",
        "source_type": "software_package"
      },
      {
        "title": "How certain are tansformers in image classification: uncertainty analysis with Monte Carlo dropout",
        "url": "https://www.semanticscholar.org/paper/d7ff734c5b62a4a140fd560373d890e43d5b36cf",
        "source_type": "technical_paper",
        "authors": [
          "Md. Farhadul Islam",
          "Sarah Zabeen",
          "Md. Azharul Islam",
          "Fardin Bin Rahman",
          "Anushua Ahmed",
          "Dewan Ziaul Karim",
          "Annajiat Alim Rasel",
          "Meem Arafat Manab"
        ]
      },
      {
        "title": "Tutorial 2: Comparison to other methods of uncertainty quantification ...",
        "url": "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Bayesian_Neural_Networks/dl2_bnn_tut2_student_with_answers.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [42, 43, 44, 47, 48, 49]
  },
  {
    "id": 31,
    "name": "Out-of-DIstribution detector for Neural networks (ODIN)",
    "description": "ODIN detects when a neural network receives unusual inputs that are very different from its training data. It works by making small, controlled changes to inputs and adjusting the model's confidence scores to better distinguish between familiar and unfamiliar data. When the adjusted confidence score is low, ODIN flags the input as potentially problematic or out-of-scope for the model.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-training-data",
      "data-type/graph",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Detecting unusual inputs that a classifier wasn't trained on, like identifying when an image recognition model is shown a completely new object class.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires tuning (temperature and perturbation magnitude) which may vary across different out-of-distribution types; may still struggle with adversarial examples or inputs very similar to training data."
      }
    ],
    "resources": [
      {
        "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
        "url": "http://arxiv.org/pdf/1706.02690v5",
        "source_type": "technical_paper",
        "authors": ["Shiyu Liang", "Yixuan Li", "R. Srikant"],
        "publication_date": "2017-06-08"
      },
      {
        "title": "facebookresearch/odin",
        "url": "https://github.com/facebookresearch/odin",
        "source_type": "software_package"
      },
      {
        "title": "Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data",
        "url": "http://arxiv.org/pdf/2002.11297v2",
        "source_type": "technical_paper",
        "authors": ["Yen-Chang Hsu", "Yilin Shen", "Hongxia Jin", "Zsolt Kira"],
        "publication_date": "2020-02-26"
      },
      {
        "title": "Detection of out-of-distribution samples using binary neuron activation\n  patterns",
        "url": "http://arxiv.org/abs/2212.14268",
        "source_type": "technical_paper",
        "authors": [
          "Chachu\u0142a, Krystian",
          "Olber, Bartlomiej",
          "Popowicz, Adam",
          "Radlak, Krystian",
          "Szczepankiewicz, Michal"
        ],
        "publication_date": "2023-03-24T00:00:00"
      },
      {
        "title": "Self-supervised out-of-distribution detection in wireless capsule endoscopy images.",
        "url": "https://diposit.ub.edu/dspace/bitstream/2445/207762/1/828150.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Laiz Trece\u00f1o, Pablo",
          "Quind\u00f3s S\u00e1nchez, Arnau",
          "Segu\u00ed Mesquida, Santi",
          "Vitri\u00e0 i Marca, Jordi"
        ],
        "publication_date": "2024-02-19T10:29:28"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [55, 103]
  },
  {
    "id": 32,
    "name": "Permutation Tests",
    "description": "Permutation tests assess the significance of an observed result (like model accuracy or feature importance) by comparing it to what would happen purely by chance. This is done by randomly shuffling labels or data many times and calculating the result each time, building a distribution of outcomes under the null hypothesis (no real relationship). If the actual result is far out in the tail of this distribution, it is deemed statistically significant.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Validating that a feature is truly important by comparing its importance score to what would be expected from random noise.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive, especially for large datasets or models with long inference times; requires many permutations to get reliable p-values for strict significance thresholds."
      }
    ],
    "resources": [
      {
        "title": "An Empirical Comparison of Parametric and Permutation Tests for Regression Analysis of Randomized Experiments",
        "url": "http://arxiv.org/pdf/1702.04851v2",
        "source_type": "technical_paper",
        "authors": ["Kellie Ottoboni", "Fraser Lewis", "Luigi Salmaso"],
        "publication_date": "2017-02-16"
      },
      {
        "title": "Permutation Tests for Classification",
        "url": "https://core.ac.uk/download/4383831.pdf",
        "source_type": "technical_paper",
        "authors": ["Golland, Polina", "Mukherjee, Sayan", "Panchenko, Dmitry"],
        "publication_date": "2003-01-01T00:00:00"
      },
      {
        "title": "How to use Permutation Tests | Towards Data Science",
        "url": "https://towardsdatascience.com/how-to-use-permutation-tests-bacc79f45749/",
        "source_type": "tutorial"
      },
      {
        "title": "Permutation test in R | Towards Data Science",
        "url": "https://towardsdatascience.com/permutation-test-in-r-77d551a9f891/",
        "source_type": "tutorial"
      },
      {
        "title": "The Exchangeability Assumption for Permutation Tests of Multiple Regression Models: Implications for Statistics and Data Science Educators",
        "url": "http://arxiv.org/pdf/2406.07756v2",
        "source_type": "introductory_paper",
        "authors": [
          "Johanna Hardin",
          "Lauren Quesada",
          "Julie Ye",
          "Nicholas J. Horton"
        ],
        "publication_date": "2024-06-11"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [50, 51, 52]
  },
  {
    "id": 33,
    "name": "Demographic Parity Assessment",
    "description": "Measures whether prediction rates are equal across different demographic groups (e.g., loan approval rates). This is often done by calculating the difference in positive outcome rates (Statistical Parity Difference) or the ratio of rates (Disparate Impact ratio) between groups. This technique helps identify potential group-level biases by comparing outcome distributions.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Checking if a job candidate screening algorithm selects candidates from different gender groups at equal rates.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Can force ignoring relevant features that legitimately correlate with protected attributes; may reduce accuracy and can conflict with individual fairness notions."
      }
    ],
    "resources": [
      {
        "title": "A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection",
        "url": "https://www.semanticscholar.org/paper/ce96e451a2685485c05f06fb0d991e29a9c43dae",
        "source_type": "technical_paper",
        "authors": [
          "Julia Ive",
          "Paulina Bondaronek",
          "Vishal Yadav",
          "D. Santel",
          "Tracy Glauser",
          "Tina Cheng",
          "Jeffrey R. Strawn",
          "G. Agasthya",
          "Jordan Tschida",
          "Sanghyun Choo",
          "Mayanka Chandrashekar",
          "Anuj J. Kapadia",
          "J. Pestian"
        ]
      }
    ],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [101, 102]
  },
  {
    "id": 34,
    "name": "Adversarial Debiasing",
    "description": "Adversarial debiasing reduces bias by training a model using a competitive setup. The main model learns to make accurate predictions while a competing 'bias detector' tries to identify protected characteristics (like race or gender) from the model's internal processing. The main model learns to hide these characteristics from the bias detector, forcing it to make decisions without relying on demographic information.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Training a resume screening model that makes hiring recommendations while ensuring representations don't enable prediction of applicants' gender or ethnicity.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "More complex to implement than standard models; may require careful tuning to balance task performance and fairness objectives, and effectiveness depends on adversary quality."
      }
    ],
    "resources": [
      {
        "title": "On the Fairness ROAD: Robust Optimization for Adversarial Debiasing",
        "url": "https://www.semanticscholar.org/paper/0c887592d781538a1b5c2168eae541b563c0ba9a",
        "source_type": "technical_paper",
        "authors": [
          "Vincent Grari",
          "Thibault Laugel",
          "Tatsunori B. Hashimoto",
          "S. Lamprier",
          "Marcin Detyniecki"
        ]
      },
      {
        "title": "aif360.sklearn.inprocessing.AdversarialDebiasing \u2014 aif360 0.6.1 ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.inprocessing.AdversarialDebiasing.html",
        "source_type": "documentation"
      },
      {
        "title": "aif360.algorithms.inprocessing.AdversarialDebiasing \u2014 aif360 0.6.1 ...",
        "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.inprocessing.AdversarialDebiasing.html",
        "source_type": "documentation"
      },
      {
        "title": "Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing",
        "url": "http://arxiv.org/pdf/2103.06179v1",
        "source_type": "technical_paper",
        "authors": [
          "Christian Reimers",
          "Paul Bodesheim",
          "Jakob Runge",
          "Joachim Denzler"
        ],
        "publication_date": "2021-03-10"
      },
      {
        "title": "Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing",
        "url": "https://www.semanticscholar.org/paper/8a5fd223fd3bbe45afec4b3ab81963170b36fdd7",
        "source_type": "technical_paper",
        "authors": [
          "Christian Reimers",
          "P. Bodesheim",
          "Jakob Runge",
          "Joachim Denzler"
        ]
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [87, 88, 89, 90, 91, 92, 93, 94]
  },
  {
    "id": 35,
    "name": "Counterfactual Fairness Assessment",
    "description": "Counterfactual Fairness Assessment evaluates whether a model's predictions would remain unchanged if an individual's protected attributes (race, gender, age) were different, whilst keeping all other causally legitimate factors constant. The technique requires constructing a causal graph that maps relationships between variables, then using do-calculus or structural causal models to simulate counterfactual scenarios. For example, it asks: 'Would this loan application still be approved if the applicant were a different race, holding constant their actual qualifications and economic circumstances?' This individual-level fairness criterion helps identify when decisions depend improperly on protected characteristics.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "fairness-approach/causal",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating a hiring algorithm by testing whether qualified candidates would receive the same evaluation scores if their gender were different, whilst controlling for actual skills, experience, and education, revealing whether gender bias affects recruitment decisions.",
        "goal": "Fairness"
      },
      {
        "description": "Assessing a criminal sentencing model by examining whether defendants with identical criminal histories and case circumstances would receive the same sentence recommendations regardless of their race, identifying potential discriminatory patterns in judicial AI systems.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires explicit specification of causal relationships between variables, which involves subjective assumptions about what constitutes legitimate versus illegitimate causal pathways."
      },
      {
        "description": "May be mathematically impossible to satisfy simultaneously with other fairness criteria (like statistical parity), forcing practitioners to choose between competing fairness definitions."
      },
      {
        "description": "Implementation complexity is high, requiring sophisticated causal inference techniques and structural causal models that are difficult to construct and validate."
      },
      {
        "description": "Depends heavily on the quality and completeness of the causal graph, which may be incorrect or missing important confounding variables."
      }
    ],
    "resources": [
      {
        "title": "Counterfactual Fairness",
        "url": "https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Matt J. Kusner",
          "Joshua Loftus",
          "Chris Russell",
          "Ricardo Silva"
        ],
        "publication_date": "2017-12-04"
      },
      {
        "title": "fairlearn/fairlearn",
        "url": "https://github.com/fairlearn/fairlearn",
        "source_type": "software_package"
      },
      {
        "title": "Counterfactual Fairness in Text Classification through Robustness",
        "url": "http://arxiv.org/pdf/1809.10610v2",
        "source_type": "technical_paper",
        "authors": [
          "Sahaj Garg",
          "Vincent Perot",
          "Nicole Limtiaco",
          "Ankur Taly",
          "Ed H. Chi",
          "Alex Beutel"
        ],
        "publication_date": "2018-09-27"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [105, 106]
  },
  {
    "id": 37,
    "name": "Sensitivity Analysis for Fairness",
    "description": "Sensitivity Analysis for Fairness systematically evaluates how model predictions change when sensitive attributes or their proxies are perturbed whilst holding other factors constant. The technique involves creating counterfactual instances by modifying potentially discriminatory features (race, gender, age) or their correlates (zip code, names, education institutions) and measuring the resulting prediction differences. This controlled perturbation approach quantifies the degree to which protected characteristics influence model decisions, helping detect both direct discrimination and indirect bias through proxy variables even when sensitive attributes are not explicitly used as model inputs.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/local",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Testing whether a lending model's decisions change significantly when only the applicant's zip code (which may correlate with race) is altered, while keeping all other factors constant.",
        "goal": "Fairness"
      },
      {
        "description": "Evaluating a recruitment algorithm by systematically changing candidate names from stereotypically male to female names (whilst keeping qualifications identical) to measure whether gender bias affects hiring recommendations, revealing discrimination through name-based proxies.",
        "goal": "Fairness"
      },
      {
        "description": "Assessing a healthcare resource allocation model by varying patient zip codes across different socioeconomic areas to determine whether geographic proxies for race and income inappropriately influence treatment recommendations.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires domain expertise to identify relevant proxy variables for sensitive attributes, which may not be obvious or comprehensive."
      },
      {
        "description": "Computationally intensive for complex models when testing many feature combinations or perturbation ranges."
      },
      {
        "description": "Choice of perturbation ranges and comparison points involves subjective decisions that can significantly affect results and conclusions."
      },
      {
        "description": "May miss subtle or interaction-based forms of discrimination that only manifest under specific combinations of features."
      }
    ],
    "resources": [
      {
        "title": "The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning",
        "url": "http://arxiv.org/pdf/2410.09600v2",
        "source_type": "technical_paper",
        "authors": [
          "Jake Fawkes",
          "Nic Fishman",
          "Mel Andrews",
          "Zachary C. Lipton"
        ],
        "publication_date": "2024-10-12"
      },
      {
        "title": "Fair SA: Sensitivity Analysis for Fairness in Face Recognition",
        "url": "http://arxiv.org/pdf/2202.03586v2",
        "source_type": "technical_paper",
        "authors": [
          "Aparna R. Joshi",
          "Xavier Suau",
          "Nivedha Sivakumar",
          "Luca Zappella",
          "Nicholas Apostoloff"
        ],
        "publication_date": "2022-02-08"
      },
      {
        "title": "fairlearn/fairlearn",
        "url": "https://github.com/fairlearn/fairlearn",
        "source_type": "software_package"
      },
      {
        "title": "Aequitas: Bias Audit Toolkit",
        "url": "https://github.com/dssg/aequitas",
        "source_type": "software_package"
      },
      {
        "title": "Fairness Through Sensitivity Analysis - Towards Data Science",
        "url": "https://towardsdatascience.com/fairness-through-sensitivity-analysis-3ea1b4d79e6c",
        "source_type": "tutorial"
      },
      {
        "title": "User Guide - Fairlearn documentation",
        "url": "https://fairlearn.org/v0.8.0/user_guide/assessment.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [82, 86, 104, 107, 108]
  },
  {
    "id": 38,
    "name": "Synthetic Data Generation",
    "description": "Synthetic data generation creates artificial datasets that maintain the statistical properties and patterns of real data without containing actual information from real individuals. By using techniques like differential privacy or generative models, teams can develop, test, and share ML models without exposing sensitive information. This approach balances data utility with privacy protection, enabling collaboration while minimizing re-identification risks.",
    "assurance_goals": ["Privacy"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/privacy",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/statistical-test",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Creating realistic but synthetic electronic health records for developing and testing medical diagnosis algorithms without exposing real patient data.",
        "goal": "Privacy"
      }
    ],
    "limitations": [
      {
        "description": "May not capture all nuances of real data, potentially reducing model performance; generating high-quality synthetic data can be challenging, and some approaches may still leak information."
      }
    ],
    "resources": [
      {
        "title": "sdv-dev/SDV",
        "url": "https://github.com/sdv-dev/SDV",
        "source_type": "software_package"
      },
      {
        "title": "An evaluation framework for synthetic data generation models",
        "url": "http://arxiv.org/pdf/2404.08866v1",
        "source_type": "technical_paper",
        "authors": [
          "Ioannis E. Livieris",
          "Nikos Alimpertis",
          "George Domalis",
          "Dimitris Tsakalidis"
        ],
        "publication_date": "2024-04-13"
      },
      {
        "title": "An evaluation framework for synthetic data generation models",
        "url": "http://arxiv.org/abs/2404.08866",
        "source_type": "technical_paper",
        "authors": [
          "Alimpertis, Nikos",
          "Domalis, George",
          "Livieris, Ioannis E.",
          "Tsakalidis, Dimitris"
        ],
        "publication_date": "2024-04-12T01:00:00"
      },
      {
        "title": "Synthetic Data \u2014 SecureML 0.2.2 documentation",
        "url": "https://secureml.readthedocs.io/en/latest/user_guide/synthetic_data.html",
        "source_type": "documentation"
      },
      {
        "title": "How to Generate Real-World Synthetic Data with CTGAN | Towards ...",
        "url": "https://towardsdatascience.com/how-to-generate-real-world-synthetic-data-with-ctgan-af41b4d60fde/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "related_techniques": [39, 40, 41]
  },
  {
    "id": 39,
    "name": "Federated Learning",
    "description": "Federated learning trains models across multiple devices or servers without exchanging the underlying data, only sharing model updates. Data remains on the local devices, preserving privacy while still enabling collaborative learning from distributed datasets. By keeping sensitive information local and only sharing model parameters or gradients, organizations can build effective models while respecting privacy boundaries and data sovereignty requirements.",
    "assurance_goals": ["Privacy"],
    "tags": [
      "assurance-goal-category/privacy",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Developing a smartphone keyboard prediction model by learning from users' typing patterns without their text ever leaving their devices.",
        "goal": "Privacy"
      }
    ],
    "limitations": [
      {
        "description": "Communication overhead can be significant; heterogeneous client data may lead to training instability, and the approach still has potential privacy vulnerabilities to inference attacks."
      }
    ],
    "resources": [
      {
        "title": "Open Federated Learning (OpenFL) Tutorials \u2014 OpenFL 2024.10 ...",
        "url": "https://openfl.readthedocs.io/en/v1.6/developer_guide/running_the_federation.tutorial.html",
        "source_type": "tutorial"
      },
      {
        "title": "Federated Learning - DeepLearning.AI",
        "url": "https://www.deeplearning.ai/short-courses/intro-to-federated-learning/",
        "source_type": "tutorial"
      },
      {
        "title": "A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection",
        "url": "http://arxiv.org/pdf/1907.09693v7",
        "source_type": "review_paper",
        "authors": [
          "Qinbin Li",
          "Zeyi Wen",
          "Zhaomin Wu",
          "Sixu Hu",
          "Naibo Wang",
          "Yuan Li",
          "Xu Liu",
          "Bingsheng He"
        ],
        "publication_date": "2019-07-23"
      },
      {
        "title": "Federated learning with hybrid differential privacy for secure and reliable cross-IoT platform knowledge sharing",
        "url": "https://core.ac.uk/download/603345619.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Algburi, S.",
          "Algburi, S.",
          "Anupallavi, S.",
          "Anupallavi, S.",
          "Ashokkumar, S. R.",
          "Ashokkumar, S. R.",
          "Elmedany, W.",
          "Elmedany, W.",
          "Khalaf, O. I.",
          "Khalaf, O. I.",
          "Selvaraj, D.",
          "Selvaraj, D.",
          "Sharif, M. S.",
          "Sharif, M. S."
        ],
        "publication_date": "2024-01-01T00:00:00"
      },
      {
        "title": "Welcome to the Open Federated Learning (OpenFL) Documentation ...",
        "url": "https://openfl.readthedocs.io/en/v1.6/",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 5,
    "related_techniques": [38, 40, 41]
  },
  {
    "id": 40,
    "name": "Applying Differential Privacy Mechanisms",
    "description": "Differential privacy protects individual privacy by adding controlled random noise to data or model results. This makes it mathematically impossible to determine whether any specific person's information was included in the dataset. Users can adjust the amount of noise added to balance privacy protection with data usefulness - more noise provides stronger privacy but may reduce accuracy.",
    "assurance_goals": ["Privacy"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/privacy",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adding privacy protection to a census data analysis model to ensure individual households cannot be identified while still obtaining accurate population statistics.",
        "goal": "Privacy"
      }
    ],
    "limitations": [
      {
        "description": "Adds noise that reduces model accuracy; setting the privacy budget requires careful consideration, and strong privacy guarantees may significantly impact utility."
      }
    ],
    "resources": [
      {
        "title": "Differentially Private Trajectory Analysis for Points-of-Interest Recommendation",
        "url": "https://core.ac.uk/download/85137324.pdf",
        "source_type": "technical_paper",
        "authors": ["Joshi, J", "Li, C", "Palanisamy, B"],
        "publication_date": "2017-09-07T00:00:00"
      },
      {
        "title": "Privacy-preserving machine learning and data aggregation for Internet of Things",
        "url": "https://core.ac.uk/download/201212159.pdf",
        "source_type": "technical_paper",
        "authors": ["Lyu, Lingjuan"],
        "publication_date": "2018-01-01T00:00:00"
      },
      {
        "title": "Private Graph Data Release: A Survey",
        "url": "http://arxiv.org/abs/2107.04245",
        "source_type": "review_paper",
        "authors": [
          "Li, Yang",
          "Ng, Kee Siong",
          "Purcell, Michael",
          "Rakotoarivelo, Thierry",
          "Ranbaduge, Thilina",
          "Smith, David"
        ],
        "publication_date": "2021-07-09T01:00:00"
      },
      {
        "title": "Estudio de modelos de privacidad de datos",
        "url": "https://core.ac.uk/download/487529044.pdf",
        "source_type": "technical_paper",
        "authors": ["Visca, Ramiro"],
        "publication_date": "2021-09-01T01:00:00"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [38, 39, 41]
  },
  {
    "id": 41,
    "name": "Homomorphic Encryption",
    "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
    "assurance_goals": ["Privacy"],
    "tags": [
      "assurance-goal-category/privacy",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Enabling a cloud-based medical diagnosis service to process encrypted patient data and return encrypted results without ever seeing the actual medical information.",
        "goal": "Privacy"
      }
    ],
    "limitations": [
      {
        "description": "Extremely computationally expensive, often orders of magnitude slower than unencrypted computation; limited operations supported efficiently, and implementation requires cryptographic expertise."
      }
    ],
    "resources": [
      {
        "title": "zama-ai/concrete-ml",
        "url": "https://github.com/zama-ai/concrete-ml",
        "source_type": "software_package"
      },
      {
        "title": "Survey on Fully Homomorphic Encryption, Theory, and Applications",
        "url": "https://core.ac.uk/download/579858842.pdf",
        "source_type": "review_paper",
        "authors": [
          "Chiara Marcolla",
          "Frank H.P. Fitzek",
          "Marc Manzano",
          "Najwa Aaraj",
          "Riccardo Bassoli",
          "Victor Sucasas"
        ],
        "publication_date": "2022-10-06T01:00:00"
      },
      {
        "title": "Welcome to OpenFHE's documentation! \u2014 OpenFHE documentation",
        "url": "https://openfhe-development.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "Evaluation of Privacy-Preserving Support Vector Machine (SVM) Learning Using Homomorphic Encryption",
        "url": "https://core.ac.uk/download/656115203.pdf",
        "source_type": "technical_paper",
        "authors": ["Ali, Hisham", "Buchanan, William J."],
        "publication_date": "2025-01-01T00:00:00"
      },
      {
        "title": "microsoft/SEAL",
        "url": "https://github.com/microsoft/SEAL",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "related_techniques": [38, 39, 40]
  },
  {
    "id": 42,
    "name": "Prediction Intervals",
    "description": "Prediction intervals provide a range within which a future observation is likely to fall with a specified probability. Unlike single-point predictions, they quantify the uncertainty in a model's predictions, giving upper and lower bounds that account for both the inherent noise in the data and the model's uncertainty. This helps users understand how precise and reliable a prediction really is.",
    "assurance_goals": ["Reliability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Providing realistic ranges for sales forecasts to help business planning under uncertainty.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Requires assumptions about error distribution; can be overconfident if model is miscalibrated, data distribution shifts, or assumptions are violated."
      }
    ],
    "resources": [
      {
        "title": "scikit-learn-contrib/MAPIE",
        "url": "https://github.com/scikit-learn-contrib/MAPIE",
        "source_type": "software_package"
      },
      {
        "title": "MAPIE - Model Agnostic Prediction Interval Estimator \u2014 MAPIE 1.0.1 ...",
        "url": "https://mapie.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "Nested cross-validation for estimating prediction intervals \u2014 MAPIE ...",
        "url": "https://mapie.readthedocs.io/en/v0.6.5/examples_regression/2-advanced-analysis/plot_nested-cv.html",
        "source_type": "tutorial"
      },
      {
        "title": "Estimating prediction intervals of time series forecast \u2014 MAPIE 0.8.0 ...",
        "url": "https://mapie.readthedocs.io/en/v0.8.0/examples_regression/1-quickstart/plot_timeseries_example.html",
        "source_type": "tutorial"
      },
      {
        "title": "valeman/awesome-conformal-prediction",
        "url": "https://github.com/valeman/awesome-conformal-prediction",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [30, 43, 44, 47, 48, 49]
  },
  {
    "id": 43,
    "name": "Quantile Regression",
    "description": "Quantile regression estimates different percentiles of the prediction's conditional distribution rather than just the mean. By modeling multiple quantiles (e.g., 10th, 50th, and 90th percentiles), it provides insights into the full range of possible outcomes and how features differently affect various parts of the outcome distribution. This helps understand prediction uncertainty and how the relationships might vary across different segments of the data.",
    "assurance_goals": ["Reliability"],
    "tags": [
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Modeling how housing prices vary across different market segments, by estimating how factors affect low-end, median, and luxury properties differently.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Can be computationally intensive to fit multiple quantiles; may yield crossing quantiles without constraints, creating logically inconsistent prediction intervals."
      }
    ],
    "resources": [
      {
        "title": "statsmodels/statsmodels",
        "url": "https://github.com/statsmodels/statsmodels",
        "source_type": "software_package"
      },
      {
        "title": "Quantile Regression in Machine Learning: A Survey",
        "url": "https://www.semanticscholar.org/paper/01cd143c5a054b85afc9b99d473f84422ace7e05",
        "source_type": "review_paper",
        "authors": [
          "Anshul Kumar",
          "Rajesh Wadhvani",
          "A. Rasool",
          "Muktesh Gupta"
        ]
      },
      {
        "title": "Tutorial for conformalized quantile regression (CQR) \u2014 MAPIE 0.8.5 ...",
        "url": "https://mapie.readthedocs.io/en/v0.8.5/examples_regression/4-tutorials/plot_cqr_tutorial.html",
        "source_type": "tutorial"
      },
      {
        "title": "Quantile Regression Forest \u2014 sklearn_quantile 0.1.1 documentation",
        "url": "https://sklearn-quantile.readthedocs.io/en/latest/methods.html",
        "source_type": "documentation"
      },
      {
        "title": "Quantile machine learning models for python \u2014 sklearn_quantile ...",
        "url": "https://sklearn-quantile.readthedocs.io/",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [30, 42, 44, 47, 48, 49]
  },
  {
    "id": 44,
    "name": "Conformal Prediction",
    "description": "Conformal prediction creates prediction sets that contain the true outcome with a guaranteed coverage probability. Unlike traditional confidence intervals, conformal prediction makes minimal assumptions and works with any model type. It provides rigorous uncertainty quantification by using past prediction errors on similar examples to calibrate the size of prediction intervals, ensuring they have the specified coverage rate (e.g., 95% of intervals contain the true value).",
    "assurance_goals": ["Reliability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Creating prediction intervals for medical test results that have a provable 95% coverage rate, regardless of the underlying model complexity.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Intervals can be unnecessarily wide if nonconformity scores vary greatly across the data; requires a held-out calibration set which reduces data available for training."
      }
    ],
    "resources": [
      {
        "title": "A tutorial on conformal prediction",
        "url": "http://arxiv.org/pdf/0706.3188v1",
        "source_type": "introductory_paper",
        "authors": ["Glenn Shafer", "Vladimir Vovk"],
        "publication_date": "2007-06-21"
      },
      {
        "title": "valeman/awesome-conformal-prediction",
        "url": "https://github.com/valeman/awesome-conformal-prediction",
        "source_type": "software_package"
      },
      {
        "title": "scikit-learn-contrib/MAPIE",
        "url": "https://github.com/scikit-learn-contrib/MAPIE",
        "source_type": "software_package"
      },
      {
        "title": "Tutorial for classification \u2014 MAPIE 0.8.6 documentation",
        "url": "https://mapie.readthedocs.io/en/v0.8.6/examples_classification/4-tutorials/plot_main-tutorial-classification.html",
        "source_type": "tutorial"
      },
      {
        "title": "Conformal Prediction: a Unified Review of Theory and New Challenges",
        "url": "http://arxiv.org/pdf/2005.07972v2",
        "source_type": "review_paper",
        "authors": ["Matteo Fontana", "Gianluca Zeni", "Simone Vantini"],
        "publication_date": "2020-05-16"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 2,
    "related_techniques": [30, 42, 43, 47, 48, 49]
  },
  {
    "id": 45,
    "name": "Empirical Calibration",
    "description": "Empirical calibration adjusts a model's predicted probabilities to match observed frequencies. For example, if events predicted with 80% confidence only occur 60% of the time, calibration would correct this overconfidence. Common techniques include Platt scaling and isotonic regression, which learn transformations that map the model's raw scores to well-calibrated probabilities, improving the reliability of confidence measures for downstream decisions.",
    "assurance_goals": ["Reliability", "Transparency", "Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "assurance-goal-category/fairness",
      "data-requirements/calibration-set",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adjusting a credit default prediction model's probabilities to ensure that loan applicants with a predicted 30% default risk actually default 30% of the time, improving decision-making.",
        "goal": "Reliability"
      },
      {
        "description": "Calibrating a medical diagnosis model's confidence scores so that stakeholders can meaningfully interpret probability outputs, enabling doctors to make informed decisions about treatment urgency based on reliable confidence estimates.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring that a hiring algorithm's confidence scores are equally well-calibrated across different demographic groups, preventing systematically overconfident predictions for certain populations that could lead to biased decision-making.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires a separate held-out calibration dataset, which reduces the amount of data available for model training."
      },
      {
        "description": "Calibration performance can degrade over time if the underlying data distribution shifts, requiring periodic recalibration."
      },
      {
        "description": "May sacrifice some discriminative power in favour of calibration, potentially reducing the model's ability to distinguish between classes."
      },
      {
        "description": "Calibration methods assume that the calibration set is representative of future data, which may not hold in dynamic environments."
      }
    ],
    "resources": [
      {
        "title": "google/empirical_calibration",
        "url": "https://github.com/google/empirical_calibration",
        "source_type": "software_package"
      },
      {
        "title": "A Python Library For Empirical Calibration",
        "url": "http://arxiv.org/pdf/1906.11920v2",
        "source_type": "technical_paper",
        "authors": ["Xiaojing Wang", "Jingang Miao", "Yunting Sun"],
        "publication_date": "2019-07-25"
      },
      {
        "title": "Assessing the effectiveness of empirical calibration under different bias scenarios",
        "url": "http://arxiv.org/pdf/2111.04233v2",
        "source_type": "technical_paper",
        "authors": ["Hon Hwang", "Juan C Quiroz", "Blanca Gallego"],
        "publication_date": "2021-11-08"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [46]
  },
  {
    "id": 46,
    "name": "Temperature Scaling",
    "description": "Temperature scaling adjusts a model's confidence by applying a single parameter (temperature) to its predictions. When a model is too confident in its wrong answers, temperature scaling can fix this by making the predictions more realistic. It works by dividing the model's outputs by the temperature value before converting them to probabilities. Higher temperatures make the model less confident, whilst lower temperatures increase confidence. The technique maintains the model's accuracy whilst ensuring that when it says it's 90% confident, it's actually right about 90% of the time.",
    "assurance_goals": ["Reliability", "Transparency", "Fairness"],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-requirements/calibration-set",
      "data-requirements/validation-set",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "explanatory-scope/global",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adjusting a deep learning image classifier's confidence scores to be realistic, ensuring that when it's 90% confident, it's right 90% of the time.",
        "goal": "Reliability"
      },
      {
        "description": "Making medical diagnosis model predictions more trustworthy by providing realistic confidence scores that doctors can interpret and use to make informed decisions about patient care.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring fair treatment across patient demographics by calibrating confidence scores equally across different groups, preventing systematic over-confidence in predictions for certain populations.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Only addresses calibration at the overall dataset level, not subgroup-specific miscalibration issues."
      },
      {
        "description": "Does not improve the rank ordering or accuracy of predictions, only adjusts confidence levels."
      },
      {
        "description": "Assumes that calibration errors are consistent across different types of inputs and feature values."
      },
      {
        "description": "Requires a separate validation set for temperature parameter optimisation, which may not be available in small datasets."
      }
    ],
    "resources": [
      {
        "title": "gpleiss/temperature_scaling",
        "url": "https://github.com/gpleiss/temperature_scaling",
        "source_type": "software_package"
      },
      {
        "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
        "url": "http://arxiv.org/pdf/2502.20604v1",
        "source_type": "technical_paper",
        "authors": ["Hao Xuan", "Bokai Yang", "Xingyu Li"],
        "publication_date": "2025-02-28"
      },
      {
        "title": "Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration",
        "url": "http://arxiv.org/pdf/2209.11604v2",
        "source_type": "technical_paper",
        "authors": ["Yung-Chen Tang", "Pin-Yu Chen", "Tsung-Yi Ho"],
        "publication_date": "2024-07-24"
      },
      {
        "title": "On Calibration of Modern Neural Networks | arXiv",
        "url": "https://arxiv.org/abs/1706.04599",
        "source_type": "technical_paper",
        "authors": [
          "Chuan Guo",
          "Geoff Pleiss",
          "Yu Sun",
          "Kilian Q. Weinberger"
        ],
        "publication_date": "2017-06-14"
      },
      {
        "title": "On the Limitations of Temperature Scaling for Distributions with Overlaps",
        "url": "http://arxiv.org/pdf/2306.00740v3",
        "source_type": "technical_paper",
        "authors": ["Muthu Chidambaram", "Rong Ge"],
        "publication_date": "2023-06-01"
      }
    ],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [45]
  },
  {
    "id": 47,
    "name": "Deep Ensembles",
    "description": "Deep ensembles combine predictions from multiple neural networks trained independently with different random initializations. By capturing model uncertainty through the diversity of the ensemble's predictions, they provide more reliable uncertainty estimates and better out-of-distribution detection than single models. The disagreement between ensemble members naturally indicates prediction uncertainty, improving both accuracy and calibration.",
    "assurance_goals": ["Reliability"],
    "tags": [
      "assurance-goal-category/reliability",
      "data-requirements/calibration-set",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Improving self-driving car safety by using multiple neural networks to detect obstacles, where disagreement between models signals uncertainty and triggers extra caution.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive to train and deploy multiple complete models; may still provide overconfident predictions for inputs far from the training distribution."
      }
    ],
    "resources": [
      {
        "title": "ENSTA-U2IS-AI/awesome-uncertainty-deeplearning",
        "url": "https://github.com/ENSTA-U2IS-AI/awesome-uncertainty-deeplearning",
        "source_type": "software_package"
      },
      {
        "title": "ENSTA-U2IS-AI/torch-uncertainty",
        "url": "https://github.com/ENSTA-U2IS-AI/torch-uncertainty",
        "source_type": "software_package"
      },
      {
        "title": "Deep Ensembles Secretly Perform Empirical Bayes",
        "url": "http://arxiv.org/pdf/2501.17917v1",
        "source_type": "technical_paper",
        "authors": [
          "Gabriel Loaiza-Ganem",
          "Valentin Villecroze",
          "Yixin Wang"
        ],
        "publication_date": "2025-01-29"
      },
      {
        "title": "Joint Training of Deep Ensembles Fails Due to Learner Collusion",
        "url": "https://www.semanticscholar.org/paper/672b6c1e0ec7e920ec7caa103df50b16944b8fa6",
        "source_type": "technical_paper",
        "authors": [
          "Alan Jeffares",
          "Tennison Liu",
          "Jonathan Crabbe",
          "M. Schaar"
        ]
      },
      {
        "title": "Deep Ensembles: A Loss Landscape Perspective",
        "url": "http://arxiv.org/pdf/1912.02757v2",
        "source_type": "technical_paper",
        "authors": ["Stanislav Fort", "Huiyi Hu", "Balaji Lakshminarayanan"],
        "publication_date": "2019-12-05"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 5,
    "related_techniques": [30, 42, 43, 44, 48, 49]
  },
  {
    "id": 48,
    "name": "Bootstrapping",
    "description": "Bootstrapping estimates uncertainty by repeatedly resampling the original dataset with replacement to create many new training sets, training a model on each sample, and analysing the variation in predictions. This approach provides confidence intervals and stability measures without making strong statistical assumptions. By showing how predictions change with different random samples of the data, it reveals how sensitive the model is to the specific training examples and provides robust uncertainty estimates.",
    "assurance_goals": ["Reliability", "Transparency", "Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "explanatory-scope/global",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Estimating uncertainty in financial risk models by resampling historical data to understand how predictions might vary under different historical scenarios.",
        "goal": "Reliability"
      },
      {
        "description": "Providing confidence intervals for medical diagnosis predictions to help doctors understand the reliability of AI recommendations and make more informed treatment decisions.",
        "goal": "Transparency"
      },
      {
        "description": "Assessing whether prediction uncertainty is consistent across different demographic groups in hiring algorithms, identifying if the model is systematically more uncertain for certain populations.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive as it requires training multiple models on resampled datasets."
      },
      {
        "description": "Does not account for uncertainty in model structure or architecture choices."
      },
      {
        "description": "Cannot detect systematically missing data patterns or biases present in the original dataset."
      },
      {
        "description": "Assumes that the original dataset is representative of the population of interest."
      }
    ],
    "resources": [
      {
        "title": "Deterministic bootstrapping for a class of bootstrap methods",
        "url": "http://arxiv.org/pdf/1903.10816v2",
        "source_type": "technical_paper",
        "authors": ["Thomas Pitschel"],
        "publication_date": "2019-03-26"
      },
      {
        "title": "A Gentle Introduction to the Bootstrap Method ...",
        "url": "https://www.machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/",
        "source_type": "tutorial"
      },
      {
        "title": "scipy.stats.bootstrap",
        "url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html",
        "source_type": "software_package"
      },
      {
        "title": "Bootstrapping and bagging \u2014 modAL documentation",
        "url": "https://modal-python.readthedocs.io/en/latest/content/examples/bootstrapping_and_bagging.html",
        "source_type": "tutorial"
      },
      {
        "title": "Machine Learning: What is Bootstrapping? - KDnuggets",
        "url": "https://www.kdnuggets.com/2023/03/bootstrapping.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [30, 42, 43, 44, 47, 49]
  },
  {
    "id": 49,
    "name": "Jackknife Resampling",
    "description": "Jackknife resampling assesses model stability by systematically leaving out one (or a few) data points at a time and retraining the model. This approach reveals how individual points influence results and provides an estimate of the standard error. By examining the distribution of predictions across these leave-one-out models, it identifies unusually influential points and characterizes prediction uncertainty.",
    "assurance_goals": ["Reliability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating how removing individual countries from a global climate model affects predictions, to identify which regions have outsized influence on the results.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Extremely computationally intensive for large datasets, as it requires training n models for n data points; may underestimate uncertainty compared to other methods."
      }
    ],
    "resources": [
      {
        "title": "Accelerating jackknife resampling for the Canonical Polyadic Decomposition",
        "url": "http://arxiv.org/pdf/2112.03985v1",
        "source_type": "technical_paper",
        "authors": [
          "Christos Psarras",
          "Lars Karlsson",
          "Rasmus Bro",
          "Paolo Bientinesi"
        ],
        "publication_date": "2021-12-07"
      },
      {
        "title": "Accelerating jackknife resampling for the Canonical Polyadic\n  Decomposition",
        "url": "http://arxiv.org/abs/2112.03985",
        "source_type": "technical_paper",
        "authors": [
          "Bientinesi, Paolo",
          "Bro, Rasmus",
          "Karlsson, Lars",
          "Psarras, Christos"
        ],
        "publication_date": "2021-12-07T00:00:00"
      },
      {
        "title": "Jackknife resampling technique on mocks: an alternative method for covariance matrix estimation",
        "url": "http://arxiv.org/pdf/1606.00233v1",
        "source_type": "technical_paper",
        "authors": [
          "S. Escoffier",
          "M. -C. Cousinou",
          "A. Tilquin",
          "A. Pisani",
          "A. Aguichine",
          "S. de la Torre",
          "A. Ealet",
          "W. Gillard",
          "E. Jullo"
        ],
        "publication_date": "2016-06-01"
      },
      {
        "title": "Robust Regression Estimation: A Doubly Weighted M-Estimation Approach with Generalized Jackknife Resampling",
        "url": "https://www.semanticscholar.org/paper/0d103450b5b2b51de0f42be37e3c0362f5f75e15",
        "source_type": "technical_paper",
        "authors": ["A. J. Adjekukor", "C. O. Aronu"]
      },
      {
        "title": "Theoretical Description Regression : contents \u2014 MAPIE 1.0.1 ...",
        "url": "https://mapie.readthedocs.io/en/latest/theoretical_description_regression.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 5,
    "related_techniques": [30, 42, 43, 44, 47, 48]
  },
  {
    "id": 50,
    "name": "Cross-validation",
    "description": "Cross-validation evaluates model performance and stability by partitioning data into multiple subsets. The model is trained and tested repeatedly on different train-test splits, revealing how performance varies across different subsamples of data. This provides a realistic assessment of how well the model will generalize to new data and helps quantify prediction uncertainty, making it a fundamental technique for reliable model evaluation.",
    "assurance_goals": ["Reliability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/validation-set",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Using 10-fold cross-validation to estimate a healthcare prediction model's true accuracy and assess if performance is consistent across different patient subgroups.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Time-consuming for large datasets or complex models; standard cross-validation can give optimistic estimates if there are dependencies in the data (e.g., time series)."
      }
    ],
    "resources": [
      {
        "title": "Cross-validation: what does it estimate and how well does it do it?",
        "url": "http://arxiv.org/pdf/2104.00673v4",
        "source_type": "technical_paper",
        "authors": ["Stephen Bates", "Trevor Hastie", "Robert Tibshirani"],
        "publication_date": "2021-04-01"
      },
      {
        "title": "Blocked Cross-Validation: A Precise and Efficient Method for Hyperparameter Tuning",
        "url": "http://arxiv.org/pdf/2306.06591v2",
        "source_type": "technical_paper",
        "authors": ["Giovanni Maria Merola"],
        "publication_date": "2023-06-11"
      },
      {
        "title": "Approximate Cross-validation: Guarantees for Model Assessment and Selection",
        "url": "http://arxiv.org/pdf/2003.00617v2",
        "source_type": "technical_paper",
        "authors": ["Ashia Wilson", "Maximilian Kasy", "Lester Mackey"],
        "publication_date": "2020-03-02"
      },
      {
        "title": "Cross-Validation in Machine Learning: How to Do It Right",
        "url": "https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right",
        "source_type": "tutorial"
      },
      {
        "title": "From Train-Test to Cross-Validation: Advancing Your Model's ...",
        "url": "https://machinelearningmastery.com/from-train-test-to-cross-validation-advancing-your-models-evaluation/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 3,
    "related_techniques": [32, 51, 52]
  },
  {
    "id": 51,
    "name": "Area Under Precision-Recall Curve (AUPRC)",
    "description": "AUPRC measures model performance by plotting precision against recall at various classification thresholds and calculating the area under the resulting curve. Unlike accuracy or AUC-ROC, it's particularly valuable for imbalanced datasets where the minority class is of primary interest. By focusing on the trade-off between precision and recall, it provides a more informative assessment for use cases where false positives and false negatives have different costs.",
    "assurance_goals": ["Reliability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/labelled-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating fraud detection models where genuine transactions far outnumber fraudulent ones, to ensure fraud is caught without overwhelming reviewers with false alarms.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "More sensitive to class distribution than ROC curves; comparing models across datasets with different class balances can be misleading."
      }
    ],
    "resources": [
      {
        "title": "Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence",
        "url": "https://www.semanticscholar.org/paper/c1b5b9dfc7d6e024097f63947aa5db06e1c192d8",
        "source_type": "technical_paper",
        "authors": [
          "Qi Qi",
          "Youzhi Luo",
          "Zhao Xu",
          "Shuiwang Ji",
          "Tianbao Yang"
        ]
      },
      {
        "title": "Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence",
        "url": "http://arxiv.org/pdf/2104.08736v5",
        "source_type": "technical_paper",
        "authors": [
          "Qi Qi",
          "Youzhi Luo",
          "Zhao Xu",
          "Shuiwang Ji",
          "Tianbao Yang"
        ],
        "publication_date": "2021-04-18"
      },
      {
        "title": "A Closer Look at AUROC and AUPRC under Class Imbalance",
        "url": "http://arxiv.org/pdf/2401.06091v4",
        "source_type": "technical_paper",
        "authors": [
          "Matthew B. A. McDermott",
          "Haoran Zhang",
          "Lasse Hyldig Hansen",
          "Giovanni Angelotti",
          "Jack Gallifant"
        ],
        "publication_date": "2024-01-11"
      },
      {
        "title": "DominikRafacz/auprc",
        "url": "https://github.com/DominikRafacz/auprc",
        "source_type": "software_package"
      },
      {
        "title": "Stochastic Optimization of Area Under Precision-Recall Curve for Deep Learning with Provable Convergence",
        "url": "https://www.semanticscholar.org/paper/00e644260effd47bbae038f376dcc7c6586d025f",
        "source_type": "technical_paper",
        "authors": [
          "Qi Qi",
          "Youzhi Luo",
          "Zhao Xu",
          "Shuiwang Ji",
          "Tianbao Yang"
        ]
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [32, 50, 52]
  },
  {
    "id": 52,
    "name": "Precision Metrics for High-Risk Domains",
    "description": "In high-risk domains, specialized metrics focus on worst-case performance and extreme error cases rather than averages. These include metrics like maximum error, 99th percentile error, and failure rate above critical thresholds. By explicitly measuring performance in the most challenging situations, these metrics ensure models meet safety requirements and help identify potentially catastrophic failure modes.",
    "assurance_goals": ["Reliability"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating an autonomous vehicle's detection system by measuring its worst-case performance in challenging visibility conditions, rather than average-case performance.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "May require more test cases and data to accurately estimate tail performance; optimizing for extreme cases can sometimes harm average performance."
      }
    ],
    "resources": [
      {
        "title": "A Text Intelligence-Based Approach for Automatic Generation of Fault Trees in Nuclear Power Plants",
        "url": "https://www.semanticscholar.org/paper/7d6e79ae233feebb85603eb0747df4b45267379e",
        "source_type": "technical_paper",
        "authors": [
          "Xingyu Xiao",
          "Songlin Liu",
          "Zhiyong Zuo",
          "Peng Chen",
          "Ben Qi",
          "Jingang Liang",
          "Jiejuan Tong"
        ]
      },
      {
        "title": "Spectral Features Analysis for Print Quality Prediction in Additive Manufacturing: An Acoustics-Based Approach",
        "url": "https://www.semanticscholar.org/paper/571f0ff39200a03f23696a79160d38e5d36f8e30",
        "source_type": "technical_paper",
        "authors": [
          "Michael Olowe",
          "Michael Ogunsanya",
          "Brian Best",
          "Yousef Hanif",
          "Saurabh Bajaj",
          "Varalakshmi Vakkalagadda",
          "Olukayode Fatoki",
          "Salil S. Desai"
        ]
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [32, 50, 51]
  },
  {
    "id": 53,
    "name": "Internal Review Boards",
    "description": "Internal review boards evaluate ML projects for ethical and safety concerns before development or deployment. Comprised of cross-functional experts (technical, legal, ethics, domain specialists), they review use cases, potential harms, mitigation strategies, and monitoring plans. By formalizing ethical review processes, they help organizations identify risks early and ensure responsible AI development practices.",
    "assurance_goals": ["Safety"],
    "tags": [
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/qualitative-report",
      "expertise-needed/ml-engineering",
      "expertise-needed/regulatory-compliance",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "example_use_cases": [
      {
        "description": "Reviewing a proposed criminal risk assessment tool to evaluate potential discriminatory impacts and privacy implications before development begins.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Can slow development timelines; effectiveness depends on board composition and authority, and organizations may face pressure to approve revenue-generating projects."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 1,
    "related_techniques": [54, 56, 57, 58]
  },
  {
    "id": 54,
    "name": "Red Teaming",
    "description": "Red teaming involves dedicated adversarial testing of ML systems by specialists who try to find flaws, vulnerabilities, harmful outputs, or ways to circumvent safety measures. Drawing on security practices, red teams probe systems from multiple angles, including prompt engineering, adversarial examples, and edge case testing. This approach reveals non-obvious risks before deployment and helps build more robust, safer AI systems.",
    "assurance_goals": ["Safety"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/graph",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Having a specialized team attempt to make a content moderation AI generate harmful outputs by using creative prompting techniques.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Requires specialized expertise and significant resources; can only find issues that testers think to look for, and systems may remain vulnerable to novel attack types."
      }
    ],
    "resources": [
      {
        "title": "Red Teaming LLM Applications - DeepLearning.AI",
        "url": "https://www.deeplearning.ai/short-courses/red-teaming-llm-applications/",
        "source_type": "tutorial"
      },
      {
        "title": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models",
        "url": "https://www.semanticscholar.org/paper/a2fb135fc4bfa323bc92dd498ba45bcaf7259a02",
        "source_type": "technical_paper",
        "authors": [
          "Alberto Purpura",
          "Sahil Wadhwa",
          "Jesse Zymet",
          "Akshay Gupta",
          "Andy Luo",
          "Melissa Kazemi Rad",
          "Swapnil Shinde",
          "M. Sorower"
        ]
      },
      {
        "title": "Red Teaming LLM Applications - DeepLearning.AI",
        "url": "https://learn.deeplearning.ai/courses/red-teaming-llm-applications/lesson/t1tp1/introduction",
        "source_type": "tutorial"
      },
      {
        "title": "Effective Automation to Support the Human Infrastructure in AI Red Teaming",
        "url": "https://www.semanticscholar.org/paper/c42dcb3a795f970d657ee46537553634eea2b014",
        "source_type": "technical_paper",
        "authors": ["Alice Qian Zhang", "Jina Suh", "Mary L. Gray", "Hong Shen"]
      },
      {
        "title": "Trojan Activation Attack: Red-Teaming Large Language Models using Steering Vectors for Safety-Alignment",
        "url": "https://www.semanticscholar.org/paper/598df44f1d21a5d1fe3940c0bb2a6128a62c1c15",
        "source_type": "technical_paper",
        "authors": ["Haoran Wang", "Kai Shu"]
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [53, 56, 57, 58]
  },
  {
    "id": 55,
    "name": "Anomaly Detection",
    "description": "Anomaly detection identifies unusual behaviors, inputs, or outputs that deviate significantly from normal patterns. Applied to ML systems, it can flag unexpected model predictions, suspicious inputs, or drift in behavior. By continuously monitoring for anomalies in production, organizations can catch potential issues early, investigate causes, and prevent harm from system misuse or malfunction.",
    "assurance_goals": ["Safety"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Detecting unusual patterns of API calls to a machine translation service that might indicate attempts to extract harmful outputs or attack the model.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Setting appropriate thresholds is challenging; may generate false alarms in legitimate edge cases, and novel anomalies might not match patterns the system is trained to detect."
      }
    ],
    "resources": [
      {
        "title": "Overview of Anomaly Detection Techniques across Different Domains: A Systematic Review",
        "url": "https://www.semanticscholar.org/paper/3f1b79dc9c60ee219a9375057e11cbc207d7e363",
        "source_type": "review_paper",
        "authors": ["Venkatraman Umbalacheri Ramasamy"]
      },
      {
        "title": "Anomaly Detection Toolkit (ADTK) \u2014 ADTK 0.6.2 documentation",
        "url": "https://adtk.readthedocs.io/en/stable/",
        "source_type": "documentation"
      },
      {
        "title": "TimeEval: Evaluation Tool for Anomaly Detection Algorithms on ...",
        "url": "https://timeeval.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "DeepOD documentation",
        "url": "https://deepod.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "A Beginner's Guide to Anomaly Detection Techniques in Data ...",
        "url": "https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [31, 103]
  },
  {
    "id": 56,
    "name": "Human-in-the-Loop Safeguards",
    "description": "Human-in-the-loop safeguards integrate human oversight into model operations, requiring human review of high-risk or uncertain predictions before actions are taken. This approach combines model efficiency with human judgment for sensitive decisions. By designating appropriate intervention points and routing challenging cases to human experts, organizations can better manage the risks of automated decisions while still benefiting from ML capabilities.",
    "assurance_goals": ["Safety"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Requiring human moderator approval before taking down content that an AI system has flagged for potential policy violations.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Scales poorly with high request volumes; introduces latency into the decision process, and humans may experience fatigue or defer too easily to automation."
      }
    ],
    "resources": [
      {
        "title": "Improving the Applicability of AI for Psychiatric Applications through Human-in-the-loop Methodologies",
        "url": "https://core.ac.uk/download/544064129.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Chandler, Chelsea",
          "Elvev\u00e5g, Brita",
          "Foltz, Peter W."
        ],
        "publication_date": "2022-01-01T00:00:00"
      },
      {
        "title": "Medical AI and Contextual Bias",
        "url": "https://core.ac.uk/download/475682943.pdf",
        "source_type": "technical_paper",
        "authors": ["Price, W. Nicholson, II"],
        "publication_date": "2019-09-01T08:00:00"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [53, 54, 57, 58]
  },
  {
    "id": 57,
    "name": "Confidence Thresholding",
    "description": "Confidence thresholding routes predictions to different handling paths based on the model's confidence level. High-confidence predictions proceed automatically, while low-confidence cases receive extra scrutiny, human review, or fallback handling. By establishing appropriate thresholds for different risk levels, organizations can balance automation benefits with prudent safeguards, ensuring greater oversight where uncertainty is high.",
    "assurance_goals": ["Safety"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Only allowing a medical diagnosis system to make recommendations automatically when its confidence exceeds 95%, routing less certain cases to human physicians.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Many models have poorly calibrated confidence scores; threshold selection can be challenging, and some risky predictions may still have high confidence."
      }
    ],
    "resources": [
      {
        "title": "A Novel Dynamic Confidence Threshold Estimation AI Algorithm for Enhanced Object Detection",
        "url": "https://www.semanticscholar.org/paper/93cda7adfa043c969639e094d6c27b1c4d507208",
        "source_type": "technical_paper",
        "authors": ["Mounika Thatikonda", "M. Pk", "Fathi H. Amsaad"]
      },
      {
        "title": "Improving speech recognition accuracy with multi-confidence thresholding",
        "url": "https://www.semanticscholar.org/paper/bef1c8668115675f786e5a3c6d165f268e399e9d",
        "source_type": "technical_paper",
        "authors": ["Shuangyu Chang"]
      },
      {
        "title": "Improving the Robustness and Generalization of Deep Neural Network with Confidence Threshold Reduction",
        "url": "http://arxiv.org/pdf/2206.00913v2",
        "source_type": "technical_paper",
        "authors": [
          "Xiangyuan Yang",
          "Jie Lin",
          "Hanlin Zhang",
          "Xinyu Yang",
          "Peng Zhao"
        ],
        "publication_date": "2022-06-02"
      },
      {
        "title": "Adaptive Confidence Threshold for ByteTrack in Multi-Object Tracking",
        "url": "http://arxiv.org/pdf/2312.01650v2",
        "source_type": "technical_paper",
        "authors": [
          "Linh Van Ma",
          "Muhammad Ishfaq Hussain",
          "JongHyun Park",
          "Jeongbae Kim",
          "Moongu Jeon"
        ],
        "publication_date": "2023-12-04"
      },
      {
        "title": "A Low-Complexity R-peak Detection Algorithm with Adaptive Thresholding for Wearable Devices",
        "url": "https://www.semanticscholar.org/paper/674fa06d150e3807ec41bc96399484c5b2894696",
        "source_type": "technical_paper",
        "authors": [
          "Tiago Rodrigues",
          "Sirisack Samoutphonh",
          "H. Silva",
          "A. Fred"
        ]
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [53, 54, 56, 58]
  },
  {
    "id": 58,
    "name": "Runtime Monitoring and Circuit Breakers",
    "description": "Runtime monitoring tracks key system metrics and prediction patterns in production, with automated circuit breakers that can throttle, disable, or revert ML systems when anomalies exceed thresholds. This approach provides real-time protection against unexpected behavior. By continuously monitoring inputs, outputs, and system health, organizations can quickly detect and respond to potential issues before they cause significant harm.",
    "assurance_goals": ["Safety"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Automatically disabling an automated trading system if it starts making a volume of trades that exceeds historical patterns by a large margin.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Setting appropriate thresholds requires careful calibration; false alarms can disrupt service unnecessarily, and some harmful behaviors may still fall within monitored metrics ranges."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [53, 54, 56, 57]
  },
  {
    "id": 59,
    "name": "Model Cards",
    "description": "Model cards are standardized documentation templates that provide essential information about ML models, including their intended uses, performance metrics across different conditions and demographic groups, training data characteristics, and known limitations. By creating transparency about a model's behavior and appropriate contexts, they help prevent misuse and enable users to make informed decisions about when and how to apply the model.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/regulatory-compliance",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Creating comprehensive documentation for a facial recognition API that clearly describes performance differences across skin tones and lighting conditions.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Creating thorough model cards requires significant effort; information may become outdated as models are updated, and some organizations may provide incomplete information."
      }
    ],
    "resources": [
      {
        "title": "Model Cards for Model Reporting",
        "url": "http://arxiv.org/pdf/1810.03993v2",
        "source_type": "technical_paper",
        "authors": [
          "Margaret Mitchell",
          "Simone Wu",
          "Andrew Zaldivar",
          "Parker Barnes",
          "Lucy Vasserman",
          "Ben Hutchinson",
          "Elena Spitzer",
          "Inioluwa Deborah Raji",
          "Timnit Gebru"
        ],
        "publication_date": "2018-10-05"
      },
      {
        "title": "Model Card Guidebook",
        "url": "https://huggingface.co/docs/hub/en/model-card-guidebook",
        "source_type": "tutorial"
      },
      {
        "title": "scikit-learn model cards \u2014 skops 0.10 documentation",
        "url": "https://skops.readthedocs.io/en/v0.10/auto_examples/plot_model_card.html",
        "source_type": "tutorial"
      },
      {
        "title": "Model Cards",
        "url": "https://huggingface.co/docs/hub/en/model-cards",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [60, 61, 62, 63]
  },
  {
    "id": 60,
    "name": "Datasheets for Datasets",
    "description": "Datasheets for datasets document a dataset's creation, composition, intended uses, and maintenance. They include information about collection methods, preprocessing steps, recommended uses, potential biases, and legal/ethical considerations. By providing this context, datasheets improve transparency, help users make informed decisions about dataset suitability, and encourage dataset creators to reflect on responsible practices throughout the data lifecycle.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/documentation",
      "expertise-needed/low",
      "expertise-needed/regulatory-compliance",
      "lifecycle-stage/data-handling",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Creating comprehensive documentation for a public facial image dataset, detailing consent procedures, demographic representation, and appropriate use guidelines.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Requires time and effort to create and maintain; information may become outdated if the dataset evolves, and there's no enforced standard for completeness."
      }
    ],
    "resources": [
      {
        "title": "Structured dataset documentation: a datasheet for CheXpert",
        "url": "http://arxiv.org/pdf/2105.03020v1",
        "source_type": "technical_paper",
        "authors": [
          "Christian Garbin",
          "Pranav Rajpurkar",
          "Jeremy Irvin",
          "Matthew P. Lungren",
          "Oge Marques"
        ],
        "publication_date": "2021-05-07"
      },
      {
        "title": "Datasheets for AI and medical datasets (DAIMS): a data validation and documentation framework before machine learning analysis in medical research",
        "url": "http://arxiv.org/pdf/2501.14094v1",
        "source_type": "technical_paper",
        "authors": [
          "Ramtin Zargari Marandi",
          "Anne Svane Frahm",
          "Maja Milojevic"
        ],
        "publication_date": "2025-01-23"
      },
      {
        "title": "MT-Adapted Datasheets for Datasets: Template and Repository",
        "url": "http://arxiv.org/pdf/2005.13156v1",
        "source_type": "technical_paper",
        "authors": [
          "Marta R. Costa-juss\u00e0",
          "Roger Creus",
          "Oriol Domingo",
          "Albert Dom\u00ednguez",
          "Miquel Escobar",
          "Cayetana L\u00f3pez",
          "Marina Garcia",
          "Margarita Geleta"
        ],
        "publication_date": "2020-05-27"
      },
      {
        "title": "xgi-org/xgi-data",
        "url": "https://github.com/xgi-org/xgi-data",
        "source_type": "software_package"
      },
      {
        "title": "Understanding Machine Learning Practitioners' Data Documentation Perceptions, Needs, Challenges, and Desiderata",
        "url": "http://arxiv.org/pdf/2206.02923v2",
        "source_type": "technical_paper",
        "authors": [
          "Amy K. Heger",
          "Liz B. Marquis",
          "Mihaela Vorvoreanu",
          "Hanna Wallach",
          "Jennifer Wortman Vaughan"
        ],
        "publication_date": "2022-06-06"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [59, 61, 62, 63]
  },
  {
    "id": 61,
    "name": "System Documentation Templates",
    "description": "System documentation templates provide standardized frameworks for describing ML systems beyond just models and datasets. They capture information about infrastructure, deployment environments, monitoring systems, failure modes, and maintenance procedures. By ensuring comprehensive documentation of the entire ML pipeline, they support better governance, reproducibility, and safety, serving both technical and non-technical stakeholders.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/low",
      "lifecycle-stage/project-design",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Documenting an automated trading system's components, monitoring protocols, and emergency shutdown procedures to support operational safety and regulatory compliance.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Maintaining comprehensive documentation can be time-consuming; different stakeholders may require different levels of detail, and templates may not fit all types of ML systems equally well."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 1,
    "related_techniques": [59, 60, 62, 63]
  },
  {
    "id": 62,
    "name": "Lineage Tracking for ML Systems",
    "description": "ML system lineage tracks the complete history of models, datasets, and experiments through a system's lifecycle. It records which datasets were used to train each model version, what hyperparameters were set, who approved changes, and when models were deployed. By maintaining this audit trail, organizations can reproduce past results, track sources of performance issues, and demonstrate regulatory compliance when needed.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/regulatory-compliance",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "example_use_cases": [
      {
        "description": "Maintaining complete records of model versions and datasets for a medical diagnosis system to support regulatory audits and trace the origin of any prediction issues.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Implementing comprehensive lineage tracking requires dedicated infrastructure; can generate large volumes of metadata, and requires discipline from all team members to maintain."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [59, 60, 61, 63]
  },
  {
    "id": 63,
    "name": "Automated Documentation Generation",
    "description": "Automated documentation generation extracts information directly from code, models, and data pipelines to create and maintain up-to-date documentation. It can capture model architectures, data schemas, feature importance, performance metrics, and lineage information without manual writing. By reducing the burden of documentation maintenance, it helps teams keep comprehensive records that remain accurate as systems evolve.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/documentation",
      "expertise-needed/software-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Automatically generating model cards each time a new model version is trained, with updated performance metrics and data statistics.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "May miss context and insights that human documentation would include; quality depends on instrumentation comprehensiveness, and unstructured information is harder to capture."
      }
    ],
    "resources": [
      {
        "title": "daynin/fundoc",
        "url": "https://github.com/daynin/fundoc",
        "source_type": "software_package"
      },
      {
        "title": "Generating Method Documentation Using Concrete Values from Executions",
        "url": "https://core.ac.uk/download/132422652.pdf",
        "source_type": "technical_paper",
        "publication_date": "2017-01-01T00:00:00"
      },
      {
        "title": "Generative AI for Software Development - DeepLearning.AI",
        "url": "https://www.deeplearning.ai/courses/generative-ai-for-software-development/",
        "source_type": "tutorial"
      },
      {
        "title": "Documentation Generator \u2014 Wiser 0.1 documentation",
        "url": "https://chiplicity.readthedocs.io/en/latest/On_Software/DocumentationGenerator.html",
        "source_type": "documentation"
      },
      {
        "title": "pyTooling/sphinx-reports",
        "url": "https://github.com/pyTooling/sphinx-reports",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [59, 60, 61, 62]
  },
  {
    "id": 64,
    "name": "Model Distillation",
    "description": "Model distillation compresses a large, complex model (the teacher) into a smaller, simpler model (the student) that approximates the original's behavior. The student model learns from the teacher's outputs rather than the raw data. This technique makes models more interpretable, deployable, and efficient while preserving most of the original performance. It helps balance the benefits of complex models with the practical requirements of responsible deployment.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Compressing a large computer vision model with billions of parameters into a smaller model that can run on mobile devices while being easier to inspect.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Student models typically lose some performance compared to teacher models; the distillation process may still produce black-box models if not combined with interpretable architectures."
      }
    ],
    "resources": [
      {
        "title": "airaria/TextBrewer",
        "url": "https://github.com/airaria/TextBrewer",
        "source_type": "software_package"
      },
      {
        "title": "Main features \u2014 TextBrewer 0.2.1.post1 documentation",
        "url": "https://textbrewer.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "A Generic Approach for Reproducible Model Distillation",
        "url": "http://arxiv.org/abs/2211.12631",
        "source_type": "technical_paper",
        "authors": ["Hooker, Giles", "Xu, Peiru", "Zhou, Yunzhe"],
        "publication_date": "2023-04-27T01:00:00"
      },
      {
        "title": "dkozlov/awesome-knowledge-distillation",
        "url": "https://github.com/dkozlov/awesome-knowledge-distillation",
        "source_type": "software_package"
      },
      {
        "title": "Dynamic Knowledge Elicitation:  Leveraging Student Feedback For Improved Language Model Distillation",
        "url": "https://core.ac.uk/download/650174370.pdf",
        "source_type": "technical_paper",
        "authors": ["Muller, Reuven"],
        "publication_date": "2024-11-20T08:00:00"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [13, 29, 65, 71]
  },
  {
    "id": 65,
    "name": "Model Extraction",
    "description": "Model extraction creates a simpler, interpretable model (like a decision tree) that approximates a complex black-box model's behavior. Unlike distillation which transfers knowledge during training, extraction works with already-trained models by analyzing their inputs and outputs. This technique helps understand what patterns a black-box model has learned and provides explanations that stakeholders can understand without technical expertise.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Converting a complex neural network credit score model into a set of human-readable rules to explain to regulators and applicants how decisions are made.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Extracted models may not faithfully represent the original if it's highly complex; accuracy and fidelity trade off against simplicity, and training data for extraction may not be diverse enough."
      }
    ],
    "resources": [
      {
        "title": "ftramer/Steal-ML",
        "url": "https://github.com/ftramer/Steal-ML",
        "source_type": "software_package"
      },
      {
        "title": "DNN model extraction attacks using prediction interfaces",
        "url": "https://core.ac.uk/download/162135863.pdf",
        "source_type": "technical_paper",
        "authors": ["Dmitrenko, Alexey"],
        "publication_date": "2018-08-20T01:00:00"
      },
      {
        "title": "pbiecek/xai_resources",
        "url": "https://github.com/pbiecek/xai_resources",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [13, 29, 64]
  },
  {
    "id": 66,
    "name": "Monotonicity Constraints",
    "description": "Monotonicity constraints ensure that a model's predictions always increase (or decrease) as a specific feature increases, enforcing a consistent directional relationship. For instance, income can only positively affect loan approval chances. This technique makes models more intuitive and transparent, as users can understand how changing a particular input will affect the output, without unexpected reversals or non-linear behaviors.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring a credit scoring model always treats higher income as a positive factor (or at least never as a negative factor) for creditworthiness assessment.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "May reduce model accuracy if true relationships are non-monotonic; requires domain knowledge to identify which features should be monotonic, and increases training complexity."
      }
    ],
    "resources": [
      {
        "title": "Monotonic Constraints \u2014 xgboost 3.1.0-dev documentation",
        "url": "https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html",
        "source_type": "documentation"
      },
      {
        "title": "NONPARAMETRIC KERNEL REGRESSION SUBJECT TO MONOTONICITY CONSTRAINTS",
        "url": "https://www.semanticscholar.org/paper/28e2be532d66694d3fe3486671f5c0217f58892d",
        "source_type": "technical_paper",
        "authors": ["P. Hall", "Li-Shan Huang"]
      },
      {
        "title": "Fit XGBoost Model \u2014 xgboost \u2022 xgboost",
        "url": "https://xgboost.readthedocs.io/en/release_3.0.0/r_docs/R-package/docs/reference/xgboost.html",
        "source_type": "documentation"
      },
      {
        "title": "High-dimensional additive Gaussian processes under monotonicity constraints",
        "url": "https://www.semanticscholar.org/paper/4d4f1e2de3742735dcc47d2e51cc572a4415231e",
        "source_type": "technical_paper",
        "authors": ["Andr'es F. L'opez-Lopera", "F. Bachoc", "O. Roustant"]
      },
      {
        "title": "cagrell/gp_constr",
        "url": "https://github.com/cagrell/gp_constr",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [67, 68, 69, 70]
  },
  {
    "id": 67,
    "name": "Decision Trees and Rule Lists",
    "description": "Decision trees and rule lists create models that make predictions through a series of interpretable if-then rules arranged in a flowchart-like structure. Each decision point is based on a clear condition about a feature, and the path to any prediction can be traced and explained in natural language. These inherently transparent models allow stakeholders to understand exactly how inputs lead to outputs.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "applicable-models/tree-based",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/structured-output",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/project-design",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Building a loan approval system using a decision tree with clear rules based on income, credit history, and debt ratio that can be explained to applicants.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Often less accurate than complex models for difficult problems; deep trees can still be hard to interpret, and training can be unstable with small data changes."
      }
    ],
    "resources": [
      {
        "title": "Optimal Decision Tree Policies for Markov Decision Processes",
        "url": "http://arxiv.org/pdf/2301.13185v2",
        "source_type": "technical_paper",
        "authors": ["Dani\u00ebl Vos", "Sicco Verwer"],
        "publication_date": "2023-01-30"
      },
      {
        "title": "Probabilistic Dataset Reconstruction from Interpretable Models",
        "url": "http://arxiv.org/pdf/2308.15099v2",
        "source_type": "technical_paper",
        "authors": [
          "Julien Ferry",
          "Ulrich A\u00efvodji",
          "S\u00e9bastien Gambs",
          "Marie-Jos\u00e9 Huguet",
          "Mohamed Siala"
        ],
        "publication_date": "2023-08-29"
      },
      {
        "title": "Learning Global Transparent Models Consistent with Local Contrastive Explanations",
        "url": "http://arxiv.org/pdf/2002.08247v4",
        "source_type": "technical_paper",
        "authors": [
          "Tejaswini Pedapati",
          "Avinash Balakrishnan",
          "Karthikeyan Shanmugam",
          "Amit Dhurandhar"
        ],
        "publication_date": "2020-02-19"
      },
      {
        "title": "Programs as Black-Box Explanations",
        "url": "http://arxiv.org/pdf/1611.07579v1",
        "source_type": "technical_paper",
        "authors": ["Sameer Singh", "Marco Tulio Ribeiro", "Carlos Guestrin"],
        "publication_date": "2016-11-22"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [66, 68, 69, 70]
  },
  {
    "id": 68,
    "name": "Linear/Logistic Models with Few Features",
    "description": "Linear and logistic regression models with a small number of carefully selected features provide transparent predictions through simple, weighted combinations of inputs. Each coefficient represents a feature's impact, and the entire model can be expressed as a single equation. By prioritizing simplicity and selecting the most important features, these models balance interpretability with adequate performance for many applications.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Using a simple logistic regression with 5-10 key features for a medical screening test, where clinicians need to understand and explain the factors behind each risk assessment.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "May miss complex nonlinear relationships or interactions; predictive performance often lower than more complex models for difficult problems."
      }
    ],
    "resources": [
      {
        "title": "Feature Selection Optimization for Heart Rate Variability",
        "url": "https://www.semanticscholar.org/paper/9f14301d7fb29475790ad1a65a5865b33fc7ea1e",
        "source_type": "technical_paper",
        "authors": [
          "A. Antunes",
          "Joaquim P. Silva",
          "A. Braga",
          "Joaquim Gon\u00e7alves"
        ]
      },
      {
        "title": "PermDroid a framework developed using proposed feature selection approach and machine learning techniques for Android malware detection",
        "url": "https://www.semanticscholar.org/paper/1db10b0e8801ebf14f568bcdb705a3317514ba42",
        "source_type": "technical_paper",
        "authors": [
          "Arvind Mahindru",
          "Himani Arora",
          "Abhinav Kumar",
          "Sachin Kumar Gupta",
          "Shubham Mahajan",
          "Seifedine Kadry",
          "Jungeun Kim"
        ]
      },
      {
        "title": "Optimizing Pre-Trained Models for Medical Dataset Classification with a Fine-Tuning Approach",
        "url": "https://www.semanticscholar.org/paper/40e3d616f8c67a9ce284c4dbbac5c20d4795af23",
        "source_type": "technical_paper",
        "authors": ["N. Kumar", "T. Christopher"]
      }
    ],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [66, 67, 69, 70]
  },
  {
    "id": 69,
    "name": "Generalized Additive Models (GAMs)",
    "description": "GAMs extend linear models by allowing flexible, nonlinear relationships between individual features and the target while maintaining the additive structure that keeps them interpretable. Each feature's effect is modeled separately as a smooth function, visualized as a curve showing how the feature influences predictions across its range. This approach balances the transparency of linear models with the ability to capture more complex patterns.",
    "assurance_goals": ["Transparency"],
    "tags": [
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Predicting hospital readmission risk with a GAM that shows how the risk varies nonlinearly with patient age while still keeping the model transparent enough for clinical use.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Cannot capture complex interactions between features unless explicitly modeled; setup requires deciding which features need nonlinear treatment, and fitting process is more complex than linear models."
      }
    ],
    "resources": [
      {
        "title": "zzzace2000/GAMs",
        "url": "https://github.com/zzzace2000/GAMs",
        "source_type": "software_package"
      },
      {
        "title": "zzzace2000/GAMs_models",
        "url": "https://github.com/zzzace2000/GAMs_models",
        "source_type": "software_package"
      },
      {
        "title": "A Tour of pyGAM \u2014 pyGAM documentation",
        "url": "https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html",
        "source_type": "tutorial"
      },
      {
        "title": "noamross/gams-in-r-course",
        "url": "https://github.com/noamross/gams-in-r-course",
        "source_type": "software_package"
      },
      {
        "title": "nrennie/f4sg-gams",
        "url": "https://github.com/nrennie/f4sg-gams",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [66, 67, 68, 70]
  },
  {
    "id": 71,
    "name": "Model Pruning",
    "description": "Simplifies neural networks by removing less important weights or neurons, reducing complexity while retaining performance.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Reducing model size for deployment on mobile devices without significant loss in accuracy.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Over-pruning can significantly reduce accuracy; finding the right pruning threshold is trial-and-error, and pruned models may still be complex to interpret if remaining structure is not simple."
      }
    ],
    "resources": [
      {
        "title": "horseee/LLM-Pruner",
        "url": "https://github.com/horseee/LLM-Pruner",
        "source_type": "software_package"
      },
      {
        "title": "Pruning Quickstart \u2014 Neural Network Intelligence",
        "url": "https://nni.readthedocs.io/en/stable/tutorials/pruning_quick_start.html",
        "source_type": "tutorial"
      },
      {
        "title": "Overview of NNI Model Pruning \u2014 Neural Network Intelligence",
        "url": "https://nni.readthedocs.io/en/v2.8/compression/pruning.html",
        "source_type": "documentation"
      },
      {
        "title": "Overview of NNI Model Pruning \u2014 Neural Network Intelligence",
        "url": "https://nni.readthedocs.io/en/stable/compression/pruning.html",
        "source_type": "documentation"
      },
      {
        "title": "coldlarry/YOLOv3-complete-pruning",
        "url": "https://github.com/coldlarry/YOLOv3-complete-pruning",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [64]
  },
  {
    "id": 72,
    "name": "Attention Visualisation in Transformers",
    "description": "Attention Visualisation in Transformers specialises in analysing multi-head self-attention patterns unique to transformer architectures. This technique examines token-to-token attention relationships across multiple attention heads and layers, revealing how transformers build contextual representations through self-attention mechanisms. Unlike classical encoder-decoder attention analysis, this focuses on understanding positional encodings, attention head specialisation, layer-wise attention evolution, and cross-attention patterns in models like BERT, GPT, and T5. The visualisation reveals attention flow between all token pairs simultaneously.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "applicable-models/transformer",
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing attention head specialisation in BERT to discover that certain heads focus on syntactic relationships (subject-object dependencies) whilst others capture semantic similarities between contextually related tokens.",
        "goal": "Explainability"
      },
      {
        "description": "Examining layer-wise attention evolution in GPT to understand how early layers focus on local token relationships whilst deeper layers develop longer-range dependencies and abstract reasoning patterns.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "High attention weights do not necessarily correlate with feature importance for the final prediction."
      },
      {
        "description": "Only applicable to transformer-based models with attention mechanisms."
      },
      {
        "description": "Visualisation only shows attention patterns but doesn't explain the underlying reasoning or decision-making process."
      },
      {
        "description": "Attention patterns can be difficult to interpret in models with many attention heads and layers."
      }
    ],
    "resources": [
      {
        "title": "jessevig/bertviz",
        "url": "https://github.com/jessevig/bertviz",
        "source_type": "software_package"
      },
      {
        "title": "Attention is All You Need",
        "url": "https://arxiv.org/abs/1706.03762",
        "source_type": "technical_paper"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [20]
  },
  {
    "id": 73,
    "name": "Neuron Activation Analysis",
    "description": "Analyzes activation patterns of neurons in large language models (LLMs) to interpret their roles and the concepts they represent.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Identifying neurons responsible for syntax or semantics in language models.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Interpreting individual neurons requires analyzing large numbers of activations; insights are often qualitative, and important behavior may be distributed across many neurons rather than single ones."
      }
    ],
    "resources": [
      {
        "title": "jalammar/ecco",
        "url": "https://github.com/jalammar/ecco",
        "source_type": "software_package"
      },
      {
        "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models",
        "url": "http://arxiv.org/pdf/2504.21053v1",
        "source_type": "technical_paper",
        "authors": [
          "Yi Zhou",
          "Wenpeng Xing",
          "Dezhang Kong",
          "Changting Lin",
          "Meng Han"
        ],
        "publication_date": "2025-04-29"
      },
      {
        "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron Activation Analysis",
        "url": "http://arxiv.org/pdf/2404.13567v1",
        "source_type": "technical_paper",
        "authors": [
          "Abhilekha Dalal",
          "Rushrukh Rayan",
          "Adrita Barua",
          "Eugene Y. Vasserman",
          "Md Kamruzzaman Sarker",
          "Pascal Hitzler"
        ],
        "publication_date": "2024-04-21"
      },
      {
        "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron\n  Activation Analysis",
        "url": "http://arxiv.org/abs/2404.13567",
        "source_type": "technical_paper",
        "authors": [
          "Barua, Adrita",
          "Dalal, Abhilekha",
          "Hitzler, Pascal",
          "Rayan, Rushrukh",
          "Sarker, Md Kamruzzaman",
          "Vasserman, Eugene Y."
        ],
        "publication_date": "2024-04-21T01:00:00"
      },
      {
        "title": "Ecco",
        "url": "https://ecco.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "Tracing the Thoughts in Language Models",
        "url": "https://www.anthropic.com/news/tracing-thoughts-language-model",
        "source_type": "blog_post"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [25, 77]
  },
  {
    "id": 74,
    "name": "Prompt Sensitivity Analysis",
    "description": "Studies how variations in input prompts affect LLM outputs to understand model behavior and sensitivity.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating how different phrasings influence an LLM's answers in question-answering tasks.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Only surfaces sensitivity to tested prompt variations; may not cover all aspects of model behavior, and systematic prompt generation can be time-consuming or incomplete."
      }
    ],
    "resources": [
      {
        "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
        "url": "https://www.semanticscholar.org/paper/17a6116e5bbd8b87082cbb2e795885567300c483",
        "source_type": "technical_paper",
        "authors": [
          "Melanie Sclar",
          "Yejin Choi",
          "Yulia Tsvetkov",
          "Alane Suhr"
        ]
      },
      {
        "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts",
        "url": "http://arxiv.org/pdf/2505.12592v1",
        "source_type": "technical_paper",
        "authors": [
          "Sullam Jeoung",
          "Yueyan Chen",
          "Yi Zhang",
          "Shuai Wang",
          "Haibo Ding",
          "Lin Lee Cheong"
        ],
        "publication_date": "2025-05-19"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [75, 76, 78]
  },
  {
    "id": 75,
    "name": "Causal Mediation Analysis in Language Models",
    "description": "Investigates causal relationships within LLMs by assessing how interventions on specific components affect outputs.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Understanding how adjusting embeddings changes model responses in language generation tasks.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires defining interventions on internal components, which needs expert knowledge of model architecture; results depend on correctness of causal assumptions and can be challenging to interpret conclusively."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [74, 76, 78]
  },
  {
    "id": 76,
    "name": "Feature Attribution with Integrated Gradients in NLP",
    "description": "Applies Integrated Gradients to attribute importance of input tokens in LLMs for specific predictions, often producing visualizations.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Identifying words influencing text sentiment classification or topic modeling.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Long text inputs mean integrating over many steps which is slow; attributions can be diffuse across many tokens, and choosing a neutral baseline (e.g., empty or padding text) is non-trivial."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [74, 75, 78]
  },
  {
    "id": 77,
    "name": "Concept Activation Vectors (CAVs)",
    "description": "Concept Activation Vectors (CAVs) help explain how neural networks make decisions by identifying specific concepts that the model uses internally. They work by finding mathematical directions in the model's internal representation space that correspond to human-understandable concepts (like 'stripes' or 'young'). This allows researchers to test how much these concepts influence the model's predictions.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/access-to-model-internals",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Assessing how concepts like 'negativity' affect language model outputs.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Relies on having clearly defined concept examples; concept directions might not exist clearly in the model's internal space, and one must choose which layer to examine, affecting results."
      }
    ],
    "resources": [
      {
        "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks",
        "url": "http://arxiv.org/pdf/2505.17883v1",
        "source_type": "technical_paper",
        "authors": [
          "Laines Schmalwasser",
          "Niklas Penzel",
          "Joachim Denzler",
          "Julia Niebling"
        ],
        "publication_date": "2025-05-23"
      },
      {
        "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
        "url": "http://arxiv.org/pdf/2311.15303v1",
        "source_type": "technical_paper",
        "authors": ["Avani Gupta", "Saurabh Saini", "P J Narayanan"],
        "publication_date": "2023-11-26"
      },
      {
        "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations",
        "url": "http://arxiv.org/pdf/2503.05522v1",
        "source_type": "technical_paper",
        "authors": [
          "Eren Erogullari",
          "Sebastian Lapuschkin",
          "Wojciech Samek",
          "Frederik Pahde"
        ],
        "publication_date": "2025-03-07"
      },
      {
        "title": "Concept Gradient: Concept-based Interpretation Without Linear Assumption",
        "url": "http://arxiv.org/pdf/2208.14966v2",
        "source_type": "technical_paper",
        "authors": [
          "Andrew Bai",
          "Chih-Kuan Yeh",
          "Pradeep Ravikumar",
          "Neil Y. C. Lin",
          "Cho-Jui Hsieh"
        ],
        "publication_date": "2022-08-31"
      },
      {
        "title": "SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation",
        "url": "http://arxiv.org/pdf/2310.07698v1",
        "source_type": "technical_paper",
        "authors": ["Bo Pan", "Zhenke Liu", "Yifei Zhang", "Liang Zhao"],
        "publication_date": "2023-10-11"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [25, 73]
  },
  {
    "id": 78,
    "name": "In-Context Learning Analysis",
    "description": "Examines how LLMs learn from examples provided in the input prompt, revealing capacity for few-shot learning.",
    "assurance_goals": ["Explainability"],
    "tags": [
      "assurance-goal-category/explainability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analyzing the effect of examples on an LLM's ability to perform a new task like translation.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Observational analysis without guaranteed causal insight; any findings can be specific to the tested tasks or prompts, making general conclusions about model behavior difficult."
      }
    ],
    "resources": [
      {
        "title": "The Mystery of In-Context Learning: A Comprehensive Survey on Interpretation and Analysis",
        "url": "https://www.semanticscholar.org/paper/ae16932164b3be704671f25af7989f2346a689a5",
        "source_type": "review_paper",
        "authors": [
          "Yuxiang Zhou",
          "Jiazheng Li",
          "Yanzheng Xiang",
          "Hanqi Yan",
          "Lin Gui",
          "Yulan He"
        ]
      },
      {
        "title": "When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models",
        "url": "https://www.semanticscholar.org/paper/bc3965a843b80e720aa03116d3f2cf3896e40592",
        "source_type": "review_paper",
        "authors": [
          "Xianzheng Ma",
          "Yash Bhalgat",
          "Brandon Smart",
          "Shuai Chen",
          "Xinghui Li",
          "Jian Ding",
          "Jindong Gu",
          "Dave Zhenyu Chen",
          "Songyou Peng",
          "Jiawang Bian",
          "Philip H. S. Torr",
          "Marc Pollefeys",
          "Matthias Nie\u00dfner",
          "Ian D Reid",
          "Angel X. Chang",
          "Iro Laina",
          "Victor Prisacariu"
        ]
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 2,
    "related_techniques": [74, 75, 76]
  },
  {
    "id": 79,
    "name": "Reweighing",
    "description": "Assigns weights to instances in the training data to ensure different groups are equally represented in all labels.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Balancing gender representation in credit approval datasets before training a classifier.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Does not remove bias present in features; simply changing weights can lead to higher variance if some groups are underrepresented, and it assumes labels are unbiased which might not hold."
      }
    ],
    "resources": [
      {
        "title": "Achieving Fairness at No Utility Cost via Data Reweighing with Influence",
        "url": "http://arxiv.org/pdf/2202.00787v2",
        "source_type": "technical_paper",
        "authors": ["Peizhao Li", "Hongfu Liu"],
        "publication_date": "2022-02-01"
      },
      {
        "title": "aif360.sklearn.preprocessing.Reweighing \u2014 aif360 0.6.1 ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.preprocessing.Reweighing.html",
        "source_type": "documentation"
      },
      {
        "title": "brandeis-machine-learning/influence-fairness",
        "url": "https://github.com/brandeis-machine-learning/influence-fairness",
        "source_type": "software_package"
      },
      {
        "title": "Boosting Fair Classifier Generalization through Adaptive Priority Reweighing",
        "url": "http://arxiv.org/pdf/2309.08375v3",
        "source_type": "technical_paper",
        "authors": [
          "Zhihao Hu",
          "Yiran Xu",
          "Mengnan Du",
          "Jindong Gu",
          "Xinmei Tian",
          "Fengxiang He"
        ],
        "publication_date": "2023-09-15"
      },
      {
        "title": "aif360.sklearn.preprocessing.ReweighingMeta \u2014 aif360 0.6.1 ...",
        "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.sklearn.preprocessing.ReweighingMeta.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [80, 81, 84, 85]
  },
  {
    "id": 80,
    "name": "Disparate Impact Remover",
    "description": "Disparate Impact Remover is a preprocessing technique that transforms feature values in a dataset to reduce statistical dependence between features and protected attributes (like race or gender). The method modifies non-protected features through mathematical transformations that preserve the utility of the data whilst reducing correlations that could lead to discriminatory outcomes. This approach specifically targets the '80% rule' disparate impact threshold by adjusting feature distributions to ensure more equitable treatment across demographic groups in downstream model predictions.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Transforming features in a credit scoring dataset where variables like 'years of employment' and 'education level' are correlated with race, applying mathematical transformations to reduce these correlations whilst preserving the predictive value for creditworthiness assessment.",
        "goal": "Fairness"
      },
      {
        "description": "Preprocessing a recruitment dataset where features like 'previous job titles' and 'university attended' correlate with gender, modifying these features to ensure the '80% rule' is met whilst maintaining useful information for predicting job performance.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Feature transformations may reduce model accuracy by removing or distorting important predictive information during the debiasing process."
      },
      {
        "description": "Only addresses measured protected attributes and cannot eliminate bias that operates through unmeasured proxy variables."
      },
      {
        "description": "Effectiveness depends on the specific transformation method chosen and may not generalise well to different datasets or domains."
      },
      {
        "description": "May create artificial feature distributions that don't reflect real-world data patterns, potentially causing issues in model deployment."
      }
    ],
    "resources": [
      {
        "title": "holistic-ai/holisticai",
        "url": "https://github.com/holistic-ai/holisticai",
        "source_type": "software_package"
      },
      {
        "title": "Disparate Impact Remover \u2014 holisticai documentation",
        "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/preprocessing/bc_disparate_impact_remover_disparate_impact_remover.html",
        "source_type": "tutorial"
      },
      {
        "title": "Trusted-AI/AIF360",
        "url": "https://github.com/Trusted-AI/AIF360",
        "source_type": "software_package"
      },
      {
        "title": "aif360.algorithms.preprocessing.DisparateImpactRemover \u2014 aif360 ...",
        "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.DisparateImpactRemover.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [79, 81, 84, 85]
  },
  {
    "id": 81,
    "name": "Learning Fair Representations",
    "description": "Learns latent representations that encode data well but obfuscate information about protected attributes.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Creating unbiased data representations for hiring algorithms.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires training a complex model (encoder) with adversarial or constrained objectives; balancing reconstruction and fairness is tricky and may lead to loss of useful information."
      }
    ],
    "resources": [
      {
        "title": "Learning Fair Representations",
        "url": "https://www.semanticscholar.org/paper/37c3303d173c055592ef923235837e1cbc6bd986",
        "source_type": "technical_paper",
        "authors": [
          "R. Zemel",
          "Ledell Yu Wu",
          "Kevin Swersky",
          "T. Pitassi",
          "C. Dwork"
        ]
      },
      {
        "title": "zjelveh/learning-fair-representations",
        "url": "https://github.com/zjelveh/learning-fair-representations",
        "source_type": "software_package"
      },
      {
        "title": "aif360.algorithms.preprocessing.lfr \u2014 aif360 0.1.0 documentation",
        "url": "https://aif360.readthedocs.io/en/v0.2.3/_modules/aif360/algorithms/preprocessing/lfr.html",
        "source_type": "documentation"
      },
      {
        "title": "aif360.algorithms.preprocessing.LFR \u2014 aif360 0.6.1 documentation",
        "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.LFR.html",
        "source_type": "documentation"
      },
      {
        "title": "Inherent Tradeoffs in Learning Fair Representations",
        "url": "http://arxiv.org/pdf/1906.08386v6",
        "source_type": "technical_paper",
        "authors": ["Han Zhao", "Geoffrey J. Gordon"],
        "publication_date": "2019-06-19"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "related_techniques": [79, 80, 84, 85]
  },
  {
    "id": 82,
    "name": "Fairness GAN",
    "description": "Employs Generative Adversarial Networks to generate fair representations of data that obfuscate protected attributes.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Creating unbiased datasets for training fair image recognition models.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "GAN training is notoriously difficult to stabilize; ensuring fairness might come at the cost of utility, and it needs a large dataset to train the generator and discriminator effectively."
      }
    ],
    "resources": [
      {
        "title": "Fairness GAN",
        "url": "http://arxiv.org/pdf/1805.09910v1",
        "source_type": "technical_paper",
        "authors": [
          "Prasanna Sattigeri",
          "Samuel C. Hoffman",
          "Vijil Chenthamarakshan",
          "Kush R. Varshney"
        ],
        "publication_date": "2018-05-24"
      },
      {
        "title": "Fairness GAN",
        "url": "https://www.semanticscholar.org/paper/665265289471d08a4b472329eb42965b51ac485a",
        "source_type": "technical_paper",
        "authors": [
          "P. Sattigeri",
          "Samuel C. Hoffman",
          "Vijil Chenthamarakshan",
          "Kush R. Varshney"
        ]
      },
      {
        "title": "Fair GANs through model rebalancing for extremely imbalanced class distributions",
        "url": "http://arxiv.org/pdf/2308.08638v2",
        "source_type": "technical_paper",
        "authors": ["Anubhav Jain", "Nasir Memon", "Julian Togelius"],
        "publication_date": "2023-08-16"
      },
      {
        "title": "Inclusive GAN: Improving Data and Minority Coverage in Generative Models",
        "url": "http://arxiv.org/abs/2004.03355",
        "source_type": "technical_paper",
        "authors": [
          "Davis, Larry",
          "Fritz, Mario",
          "Li, Ke",
          "Malik, Jitendra",
          "Yu, Ning",
          "Zhou, Peng"
        ],
        "publication_date": "2020-01-01T00:00:00"
      },
      {
        "title": "Biases and Fairness in Deep Learning Models: A Survey on Inculcating Fairness in Generative Models",
        "url": "https://www.semanticscholar.org/paper/8342bbf56c3f096449125fc1507e41265d66fa11",
        "source_type": "review_paper",
        "authors": ["Grishma Deshmukh"]
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "related_techniques": [37, 86, 104, 107, 108]
  },
  {
    "id": 84,
    "name": "Relabelling",
    "description": "Changes labels of certain instances in training data to reduce bias, often based on fairness constraints.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Modifying labels in loan default datasets to mitigate historical biases.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Altering labels risks introducing new biases or reducing prediction accuracy; deciding which instances to relabel can require a fairness criterion and ground truth fairness assessment."
      }
    ],
    "resources": [
      {
        "title": "Google-Health/med-gemini-medqa-relabelling",
        "url": "https://github.com/Google-Health/med-gemini-medqa-relabelling",
        "source_type": "software_package"
      },
      {
        "title": "Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation",
        "url": "http://arxiv.org/pdf/2406.00725v1",
        "source_type": "technical_paper",
        "authors": ["Xiaocong Chen", "Siyu Wang", "Lina Yao"],
        "publication_date": "2024-06-02"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [79, 80, 81, 85]
  },
  {
    "id": 85,
    "name": "Preferential Sampling",
    "description": "Preferential Sampling addresses dataset imbalances by re-sampling training data with preference for underrepresented groups to achieve fair representation. This preprocessing technique modifies the training distribution to ensure that all demographic groups have adequate representation, helping prevent models from learning biased patterns due to skewed data. The approach can involve oversampling minority groups, undersampling majority groups, or using sophisticated sampling strategies that balance representation while maintaining data quality.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Oversampling minority groups in medical data to train unbiased models.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Over-sampling can cause overfitting to minority examples; under-sampling can drop important data from majority group, and it doesn't adjust the model if it inherently treats groups differently."
      }
    ],
    "resources": [],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [79, 80, 81, 84]
  },
  {
    "id": 86,
    "name": "Attribute Removal (Fairness Through Unawareness)",
    "description": "Attribute Removal (Fairness Through Unawareness) ensures fairness by completely excluding protected attributes such as race, gender, or age from the model's input features. While this approach prevents direct discrimination, it may not eliminate bias if other features are correlated with protected attributes (proxy discrimination). This technique represents the most basic fairness intervention but often needs to be combined with other approaches to address indirect bias through seemingly neutral features.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Removing gender as a feature in employee promotion predictions.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Ignoring protected attributes doesn't guarantee fairness; proxies for the protected attribute in other features can still lead to biased outcomes."
      }
    ],
    "resources": [
      {
        "title": "Discrimination-free Insurance Pricing with Privatized Sensitive Attributes",
        "url": "https://www.semanticscholar.org/paper/eab49649660fc01e899a9da5da540fc8150df716",
        "source_type": "technical_paper",
        "authors": ["Tianhe Zhang", "Suhan Liu", "Peng Shi"]
      }
    ],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [37, 82, 104, 107, 108]
  },
  {
    "id": 87,
    "name": "Adversarial Debiasing for Text",
    "description": "Applies adversarial debiasing techniques specifically to textual data to mitigate biases in language models.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Reducing gender bias in sentiment analysis models by adversarial training on text data.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Text data can carry subtle biases in language; adversarial removal of bias might strip out important linguistic context, and the technique inherits all challenges of adversarial training."
      }
    ],
    "resources": [
      {
        "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens",
        "url": "https://www.semanticscholar.org/paper/9bdb7d93eafeff015864fb18d2fc92391920e252",
        "source_type": "technical_paper",
        "authors": ["Pranav Narayanan Venkit"]
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "related_techniques": [34, 88, 89, 90, 91, 92, 93, 94]
  },
  {
    "id": 88,
    "name": "Fair Adversarial Networks",
    "description": "Fair Adversarial Networks extend the adversarial debiasing framework specifically for deep learning architectures. The technique uses adversarial training where a main network learns to make predictions while an adversarial network attempts to predict sensitive attributes from the main network's representations. This competitive process forces the main network to learn representations that are predictive for the task but uninformative about protected attributes, thereby reducing bias in deep learning models.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Reducing bias in facial recognition systems with adversarial networks.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Extending adversarial debiasing to deep networks can be very complex to implement; requires careful tuning of loss trade-offs, and interpretations of fairness improvement can be opaque."
      }
    ],
    "resources": [
      {
        "title": "Fair Adversarial Networks",
        "url": "http://arxiv.org/pdf/2002.12144v1",
        "source_type": "technical_paper",
        "authors": ["George Cevora"],
        "publication_date": "2020-02-23"
      },
      {
        "title": "Fair Adversarial Networks",
        "url": "https://www.semanticscholar.org/paper/c0cf06bdb81bb020408ecd588913a0e8c245b847",
        "source_type": "technical_paper",
        "authors": ["G. Cevora"]
      },
      {
        "title": "Demonstrating Rosa: the fairness solution for any Data Analytic pipeline",
        "url": "http://arxiv.org/pdf/2003.00899v2",
        "source_type": "technical_paper",
        "authors": ["Kate Wilkinson", "George Cevora"],
        "publication_date": "2020-02-28"
      },
      {
        "title": "Triangular Trade-off between Robustness, Accuracy, and Fairness in Deep Neural Networks: A Survey",
        "url": "https://www.semanticscholar.org/paper/13b0444d079bea1c8c57a6082200b67ab5f4616e",
        "source_type": "review_paper",
        "authors": ["Jingyang Li", "Guoqiang Li"]
      },
      {
        "title": "Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks",
        "url": "https://www.semanticscholar.org/paper/6995779ac582c5f2436cfb82a3c8cf5ca72bae2f",
        "source_type": "technical_paper",
        "authors": [
          "R. Ramachandranpillai",
          "Md Fahim Sikder",
          "David Bergstrom",
          "Fredrik Heintz"
        ]
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [34, 87, 89, 90, 91, 92, 93, 94]
  },
  {
    "id": 89,
    "name": "Prejudice Remover Regulariser",
    "description": "Incorporates a fairness penalty into the learning objective to penalize models that encode biases with respect to protected attributes.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Training logistic regression models with fairness constraints for university admissions.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires setting a hyperparameter for fairness penalty; too high can severely hurt accuracy, too low has little effect. Only applicable to models that can incorporate such a regularizer (e.g., logistic regression)."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [34, 87, 88, 90, 91, 92, 93, 94]
  },
  {
    "id": 90,
    "name": "Meta Fair Classifier",
    "description": "Meta Fair Classifier is a meta-learning approach that can modify any existing classifier to optimize for fairness metrics while maintaining predictive performance. The technique learns how to adjust model parameters or decision boundaries to satisfy fairness constraints such as demographic parity or equalized odds. This approach is particularly useful when you have a pre-trained model that performs well but exhibits bias, as it can retrofit fairness without requiring complete retraining.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Applying fairness optimization to models in employee evaluation systems.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Meta-learning approach can be complex to implement and require extensive hyperparameter tuning; may result in longer training times and complexity that makes the method less accessible."
      }
    ],
    "resources": [
      {
        "title": "\u03c1-Fair Method \u2014 holisticai documentation",
        "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/inprocessing/bc_meta_fair_classifier_rho_fair.html",
        "source_type": "documentation"
      },
      {
        "title": "aif360.algorithms.inprocessing \u2014 aif360 0.1.0 documentation",
        "url": "https://aif360.readthedocs.io/en/v0.2.3/modules/inprocessing.html",
        "source_type": "documentation"
      },
      {
        "title": "Welcome to AI Fairness 360's documentation! \u2014 aif360 0.1.0 ...",
        "url": "https://aif360.readthedocs.io/en/v0.2.3/",
        "source_type": "documentation"
      },
      {
        "title": "A benchmark study on methods to ensure fair algorithmic decisions for\n  credit scoring",
        "url": "http://arxiv.org/abs/2209.07912",
        "source_type": "technical_paper",
        "authors": ["Moldovan, Darie"],
        "publication_date": "2022-09-16T01:00:00"
      },
      {
        "title": "The Importance of Modeling Data Missingness in Algorithmic Fairness: A\n  Causal Perspective",
        "url": "http://arxiv.org/abs/2012.11448",
        "source_type": "technical_paper",
        "authors": [
          "Amayuelas, Alfonso",
          "Deshpande, Amit",
          "Goel, Naman",
          "Sharma, Amit"
        ],
        "publication_date": "2020-12-21T00:00:00"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [34, 87, 88, 89, 91, 92, 93, 94]
  },
  {
    "id": 91,
    "name": "Exponentiated Gradient Reduction",
    "description": "Exponentiated Gradient Reduction creates fair machine learning models by treating fairness as a mathematical constraint that must be satisfied during training. It uses a specific optimization algorithm that gradually adjusts the model to balance both accuracy and fairness requirements. The method systematically finds the best possible trade-off between making correct predictions and treating different groups fairly.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Training fair classifiers for employment screening processes.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Involves iterative retraining of a classifier with adjusted weights; might require a convex base learner for theoretical guarantees, and can be sensitive to convergence criteria."
      }
    ],
    "resources": [
      {
        "title": "aif360.sklearn.inprocessing.ExponentiatedGradientReduction ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.inprocessing.ExponentiatedGradientReduction.html",
        "source_type": "documentation"
      },
      {
        "title": "aif360.algorithms.inprocessing.ExponentiatedGradientReduction ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.algorithms.inprocessing.ExponentiatedGradientReduction.html",
        "source_type": "documentation"
      },
      {
        "title": "Bias Measuring and Mitigation in Regression Tasks \u2014 holisticai ...",
        "url": "https://holisticai.readthedocs.io/en/latest/gallery/tutorials/bias/mitigating_bias/regression/examples/example_us_crime.html",
        "source_type": "tutorial"
      },
      {
        "title": "In-processing Methods \u2014 holisticai documentation",
        "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/inprocessing.html",
        "source_type": "documentation"
      },
      {
        "title": "scikit-learn-Compatible API Reference \u2014 aif360 0.6.1 documentation",
        "url": "https://aif360.readthedocs.io/en/latest/modules/sklearn.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [34, 87, 88, 89, 90, 92, 93, 94]
  },
  {
    "id": 92,
    "name": "Fair Transfer Learning",
    "description": "Fair Transfer Learning adapts pre-trained models from one domain to another while explicitly preserving fairness constraints across different contexts or populations. This technique addresses the challenge that fairness properties may not transfer when adapting models to new domains with different demographic compositions or data distributions. The approach typically involves constraint-aware fine-tuning or domain adaptation techniques that maintain equitable performance across groups in the target domain.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Transferring fairness-aware models from one region's data to another in healthcare analytics.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Fairness achieved in one domain might not directly translate to another if data distributions differ; approach can be complicated to design and may need careful tuning to preserve fairness across domains."
      }
    ],
    "resources": [
      {
        "title": "Segmenting across places: The need for fair transfer learning with satellite imagery",
        "url": "http://arxiv.org/pdf/2204.04358v3",
        "source_type": "technical_paper",
        "authors": [
          "Miao Zhang",
          "Harvineet Singh",
          "Lazarus Chok",
          "Rumi Chunara"
        ],
        "publication_date": "2022-04-09"
      },
      {
        "title": "brandeis-machine-learning/awesome-ml-fairness",
        "url": "https://github.com/brandeis-machine-learning/awesome-ml-fairness",
        "source_type": "software_package"
      },
      {
        "title": "Trustworthy Transfer Learning: A Survey",
        "url": "https://www.semanticscholar.org/paper/7ee5c5b58ed0b4e585e0c30790c206bea07faacf",
        "source_type": "review_paper",
        "authors": ["Jun Wu", "Jingrui He"]
      },
      {
        "title": "Sharing to Learn and Learning to Share; Fitting Together Meta, Multi-Task, and Transfer Learning: A Meta Review",
        "url": "https://www.semanticscholar.org/paper/702642e36982b4ace71bbbb2069b10eab351f326",
        "source_type": "review_paper",
        "authors": [
          "Richa Upadhyay",
          "R. Phlypo",
          "Rajkumar Saini",
          "M. Liwicki"
        ]
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [34, 87, 88, 89, 90, 91, 93, 94]
  },
  {
    "id": 93,
    "name": "Adaptive Sensitive Reweighting",
    "description": "Adaptive Sensitive Reweighting dynamically adjusts instance weights during the training process based on the model's current performance across different demographic groups. Unlike static reweighting approaches, this technique continuously monitors bias indicators and adaptively modifies the importance of training examples to reduce disparities. The method helps prevent the model from learning biased patterns by emphasizing examples from underperforming groups as training progresses.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Balancing performance in speech recognition across accents and dialects.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires monitoring model performance across groups in training, which can introduce instability; if not tuned properly, could oscillate or focus too much on one group at a time."
      }
    ],
    "resources": [
      {
        "title": "brandeis-machine-learning/awesome-ml-fairness",
        "url": "https://github.com/brandeis-machine-learning/awesome-ml-fairness",
        "source_type": "software_package"
      },
      {
        "title": "Reducing Bias in Predictive Models Serving Analytics Users: Novel Approaches and their Implications",
        "url": "https://core.ac.uk/download/654637364.pdf",
        "source_type": "technical_paper",
        "authors": ["Alok Gupta"],
        "publication_date": "2021-11-30T00:00:00"
      },
      {
        "title": "Non-Invasive Fairness in Learning through the Lens of Data Drift",
        "url": "http://arxiv.org/abs/2303.17566",
        "source_type": "technical_paper",
        "authors": ["Meliou, Alexandra", "Yang, Ke"],
        "publication_date": "2023-04-05T01:00:00"
      },
      {
        "title": "Assessing and Enforcing Fairness in the AI Lifecycle",
        "url": "https://core.ac.uk/download/611892706.pdf",
        "source_type": "review_paper",
        "authors": [
          "Calegari R.",
          "Castane G. G.",
          "Milano M.",
          "O'Sullivan B."
        ],
        "publication_date": "2023-01-01T00:00:00"
      },
      {
        "title": "Investigating Trade-offs For Fair Machine Learning Systems",
        "url": "https://core.ac.uk/download/554844280.pdf",
        "source_type": "technical_paper",
        "authors": ["Hort, Max"],
        "publication_date": "2023-01-28T00:00:00"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [34, 87, 88, 89, 90, 91, 92, 94]
  },
  {
    "id": 94,
    "name": "Multi-Accuracy Boosting",
    "description": "Improves accuracy uniformly across groups by correcting errors where the model performs poorly for certain groups.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Enhancing model performance for underrepresented groups in disease prediction.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Targets error patterns in subgroups which requires identifying those groups or error regions; could increase complexity of the model and may overfit if very granular corrections are made."
      }
    ],
    "resources": [
      {
        "title": "mcboost: Multi-Calibration Boosting for R",
        "url": "https://core.ac.uk/download/479152372.pdf",
        "source_type": "software_package",
        "authors": [
          "Bischl, Bernd",
          "Dandl, Susanne",
          "Kern, Christoph",
          "Kim, Michael P.",
          "Pfisterer, Florian",
          "Sun, Matthew"
        ],
        "publication_date": "2021-01-01T00:00:00"
      },
      {
        "title": "Multigroup Robustness",
        "url": "http://arxiv.org/abs/2405.00614",
        "source_type": "technical_paper",
        "authors": ["Hu, Lunjia", "Peale, Charlotte", "Shen, Judy Hanwen"],
        "publication_date": "2024-05-01T01:00:00"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [34, 87, 88, 89, 90, 91, 92, 93]
  },
  {
    "id": 95,
    "name": "Equalised Odds Post-Processing",
    "description": "Equalized Odds Post-Processing is a technique that modifies a model's output probabilities after training to ensure equal true positive rates and false positive rates across different demographic groups. This post-processing approach allows practitioners to achieve fairness without retraining the model, making it particularly useful when the original training process cannot be modified. The technique typically involves finding optimal threshold adjustments for each group.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring fairness in recidivism risk assessments used in judicial decisions.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Adjusting outputs can reduce model confidence or require randomization in decisions; may sacrifice individual consistency (similar cases get different outcomes to satisfy group rates)."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [96, 97, 98]
  },
  {
    "id": 96,
    "name": "Threshold Optimiser",
    "description": "Threshold Optimizer adjusts decision thresholds for different demographic groups after model training to satisfy specific fairness constraints. This post-processing technique allows practitioners to achieve fairness goals like demographic parity or equalized opportunity without modifying the underlying model. The optimizer finds optimal threshold values for each group that balance fairness requirements with overall model performance, making it particularly useful when fairness considerations arise after model deployment.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/process"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring equal acceptance rates in college admissions across demographics.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires a held-out set to determine thresholds per group; if distributions shift, thresholds may need recalibration. Also, using different thresholds per group can raise legal or ethical concerns in deployment."
      }
    ],
    "resources": [
      {
        "title": "Fair and Private Data Preprocessing through Microaggregation",
        "url": "https://core.ac.uk/download/595548411.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Gonz\u00e1lez-Zelaya, Vladimiro",
          "Meg\u00edas, David",
          "Missier, Paolo",
          "Salas, Juli\u00e1n"
        ],
        "publication_date": "2023-08-24T01:00:00"
      },
      {
        "title": "Fair and Private Data Preprocessing through Microaggregation",
        "url": "https://core.ac.uk/download/598040083.pdf",
        "source_type": "technical_paper",
        "authors": ["Gonzalez-Zelaya V", "Megias D", "Missier P", "Salas J"],
        "publication_date": "2023-12-09T00:00:00"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 1,
    "related_techniques": [95, 97, 98]
  },
  {
    "id": 97,
    "name": "Reject Option Classification",
    "description": "Changes decisions where the model is least certain, favoring the disadvantaged group within this uncertain region.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Mitigating bias in hiring decisions by adjusting uncertain predictions.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Only applicable when model uncertainty can be estimated; choosing the 'reject' region and how to reassign decisions can be subjective and might reject too many instances if tuned conservatively."
      }
    ],
    "resources": [
      {
        "title": "Machine Learning with a Reject Option: A survey",
        "url": "https://www.semanticscholar.org/paper/24864a7f899718477c04ede9c0bea906c5dc2667",
        "source_type": "review_paper",
        "authors": [
          "Kilian Hendrickx",
          "Lorenzo Perini",
          "Dries Van der Plas",
          "Wannes Meert",
          "Jesse Davis"
        ]
      },
      {
        "title": "aif360.sklearn.postprocessing.RejectOptionClassifier \u2014 aif360 0.6.1 ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.postprocessing.RejectOptionClassifier.html?ref=evoacademy.cl",
        "source_type": "documentation"
      },
      {
        "title": "aif360.algorithms.postprocessing.RejectOptionClassification ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html",
        "source_type": "documentation"
      },
      {
        "title": "aif360.algorithms.postprocessing.reject_option_classification ...",
        "url": "https://aif360.readthedocs.io/en/v0.2.3/_modules/aif360/algorithms/postprocessing/reject_option_classification.html",
        "source_type": "documentation"
      },
      {
        "title": "Survey on Leveraging Uncertainty Estimation Towards Trustworthy Deep Neural Networks: The Case of Reject Option and Post-training Processing",
        "url": "https://www.semanticscholar.org/paper/e939c6ac58e08b539ae8a7dc54216bceb775b085",
        "source_type": "review_paper",
        "authors": [
          "M. Hasan",
          "Moloud Abdar",
          "A. Khosravi",
          "U. Aickelin",
          "Pietro Lio'",
          "Ibrahim Hossain",
          "Ashikur Rahman",
          "Saeid Nahavandi"
        ]
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [95, 96, 98]
  },
  {
    "id": 98,
    "name": "Calibration with Equality of Opportunity",
    "description": "Adjusts probabilities to achieve equal true positive rates across groups while maintaining calibration within each group.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/calibration-set",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Balancing opportunity in credit scoring across different ethnic groups.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Achieving calibration within each group while equalizing true positive rates can be at odds with overall calibration; it may involve solving for probabilities in a way that lowers overall model calibration or accuracy."
      }
    ],
    "resources": [],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [95, 96, 97]
  },
  {
    "id": 101,
    "name": "Equal Opportunity Difference",
    "description": "Computes the difference in true positive rates between groups, assessing fairness in terms of equal opportunity.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Assessing fairness in medical diagnosis models across age groups.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Addresses only true positive rates, ignoring false positive disparities; requires accurate ground truth labels for the positive class, and improving TPR for one group might increase FPR for that group."
      }
    ],
    "resources": [
      {
        "title": "aif360.sklearn.metrics.equal_opportunity_difference \u2014 aif360 0.6.1 ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.metrics.equal_opportunity_difference.html",
        "source_type": "documentation"
      },
      {
        "title": "Welcome to TransparentAI's documentation! \u2014 TransparentAI 0.1.0 ...",
        "url": "https://transparentai.readthedocs.io/en/latest/",
        "source_type": "documentation"
      },
      {
        "title": "lale.lib.aif360.util module \u2014 LALE 0.9.0-dev documentation",
        "url": "https://lale.readthedocs.io/en/latest/modules/lale.lib.aif360.util.html",
        "source_type": "documentation"
      },
      {
        "title": "IBM/bias-mitigation-of-machine-learning-models-using-aif360",
        "url": "https://github.com/IBM/bias-mitigation-of-machine-learning-models-using-aif360",
        "source_type": "software_package"
      },
      {
        "title": "aif360.algorithms.postprocessing.RejectOptionClassification ...",
        "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [33, 102]
  },
  {
    "id": 102,
    "name": "Average Odds Difference",
    "description": "Average Odds Difference measures fairness by calculating the average difference in both false positive rates and true positive rates between different demographic groups. This metric captures how consistently a model performs across groups for both positive and negative predictions. A value of 0 indicates perfect fairness under the equalized odds criterion, while larger absolute values indicate greater disparities in model performance between groups.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Measuring bias in criminal risk assessment tools.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Combines multiple error rates which may obscure specific issues (a model could have one high and one low disparity and still average out); still needs ground truth labels and a balanced trade-off with accuracy."
      }
    ],
    "resources": [
      {
        "title": "aif360.sklearn.metrics.average_odds_difference \u2014 aif360 0.6.1 ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.metrics.average_odds_difference.html",
        "source_type": "documentation"
      },
      {
        "title": "FairBalance: How to Achieve Equalized Odds With Data Pre-processing",
        "url": "http://arxiv.org/pdf/2107.08310v5",
        "source_type": "technical_paper",
        "authors": ["Zhe Yu", "Joymallya Chakraborty", "Tim Menzies"],
        "publication_date": "2021-07-17"
      },
      {
        "title": "Binary Classification \u2014 holisticai documentation",
        "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/metrics/binary_classification.html",
        "source_type": "documentation"
      },
      {
        "title": "aif360.algorithms.postprocessing.RejectOptionClassification ...",
        "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html",
        "source_type": "documentation"
      },
      {
        "title": "IBM/bias-mitigation-of-machine-learning-models-using-aif360",
        "url": "https://github.com/IBM/bias-mitigation-of-machine-learning-models-using-aif360",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [33, 101]
  },
  {
    "id": 103,
    "name": "Measuring Individual Fairness (Consistency)",
    "description": "Evaluates whether similar individuals receive similar predictions, assessing fairness at an individual level.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/individual",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring similar credit applicants receive similar loan decisions.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires a domain-specific similarity metric between individuals; hard to define and validate, and ensuring consistency can conflict with achieving good group-level metrics."
      }
    ],
    "resources": [
      {
        "title": "Fairness in recommender systems: research landscape and future directions",
        "url": "https://www.semanticscholar.org/paper/bb5c2e2cb4abd175dc43e5606ded5c5dbdb840eb",
        "source_type": "review_paper",
        "authors": [
          "Yashar Deldjoo",
          "D. Jannach",
          "Alejandro Bellog\u00edn",
          "Alessandro Difonzo",
          "Dario Zanzonelli"
        ]
      },
      {
        "title": "Approaching Fairness in Digital Governance: A Multi-Dimensional, Multi-Layered Framework",
        "url": "https://www.semanticscholar.org/paper/b49470594261e6468660d72933ac7747f2871b97",
        "source_type": "technical_paper",
        "authors": ["Yasaman Yousefi"]
      },
      {
        "title": "Towards ethical pricing fairness enhancement using mean penalty reduction method",
        "url": "https://www.semanticscholar.org/paper/b8fa9ced9452d1b96bee7ab79b410d5b7f11daae",
        "source_type": "technical_paper",
        "authors": ["Lakshmi Lineshah", "Sunil Vadera"]
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [31, 55]
  },
  {
    "id": 104,
    "name": "Algorithmic Fairness using K-NN",
    "description": "Uses K-nearest neighbors to assess individual fairness by comparing predictions among similar instances.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/individual",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating fairness in personalized recommendation systems.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Relies on the quality of the distance metric; high-dimensional data can make nearest neighbor comparisons noisy (curse of dimensionality), and it doesn't directly fix the model, just evaluates it."
      }
    ],
    "resources": [],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [37, 82, 86, 107, 108]
  },
  {
    "id": 105,
    "name": "Path-Specific Counterfactual Fairness Assessment",
    "description": "Path-Specific Counterfactual Fairness Assessment evaluates fairness by examining specific causal pathways in a model's decision-making process. Unlike general counterfactual fairness, this approach allows practitioners to identify and intervene on particular causal paths that may introduce bias, while preserving other legitimate pathways. The technique helps distinguish between direct discrimination (through protected attributes) and indirect discrimination (through seemingly neutral factors that correlate with protected attributes).",
    "assurance_goals": ["Fairness"],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "fairness-approach/causal",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Modeling fair decisions in advertising without altering legitimate causal effects.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires identifying which causal pathways are 'allowable' and which are not\u2014a subjective decision; analyzing specific paths adds complexity to the causal model and the fairness criterion."
      }
    ],
    "resources": [
      {
        "title": "Path-Specific Counterfactual Fairness via Dividend Correction",
        "url": "https://www.semanticscholar.org/paper/197367ee337e8838fd2ef1a887101ddc84eb0612",
        "source_type": "technical_paper",
        "authors": ["Daisuke Hatano", "Satoshi Hara", "Hiromi Arai"]
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [35, 106]
  },
  {
    "id": 106,
    "name": "Causal Fairness Assessment with Do-Calculus",
    "description": "Utilizes causal inference techniques to assess and mitigate bias by computing interventional distributions.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "fairness-approach/causal",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Understanding bias in hiring decisions through causal relationships.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Strongly dependent on having a correct causal graph; using do-calculus in practice can be computationally intense and challenging outside of relatively simple models or well-specified causal relationships."
      }
    ],
    "resources": [
      {
        "title": "Integrating Deep Learning and Counterfactual Methods for Causal Inference in Genomics",
        "url": "https://www.semanticscholar.org/paper/38c75ef310dc901149d6c39eb35a2ff766a72903",
        "source_type": "technical_paper",
        "authors": ["Tshepo Kitso Gobonamang", "Dimane Mpoeleng"]
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [35, 105]
  },
  {
    "id": 107,
    "name": "Diversity Constraints in Recommendations",
    "description": "Incorporates diversity and fairness constraints in recommendation systems for varied and fair content exposure.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Ensuring fair representation of genres in music recommendation platforms.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "May reduce accuracy or relevance of recommendations if forced diversity conflicts with user preferences; implementing constraints can complicate the recommendation algorithm and objective function."
      }
    ],
    "resources": [
      {
        "title": "Relevance Meets Diversity: A User-Centric Framework for Knowledge Exploration Through Recommendations",
        "url": "https://www.semanticscholar.org/paper/c1943da4bb66534892d6dbc506dc6dfa1f9d723d",
        "source_type": "technical_paper",
        "authors": ["Erica Coppolillo", "Giuseppe Manco", "A. Gionis"]
      },
      {
        "title": "Transcending Information Cocoons: Integrating TransR Embeddings with Tensor Decomposition in Recommender Systems",
        "url": "https://www.semanticscholar.org/paper/cff28f48d19f426351802217538f478e9d86ebdf",
        "source_type": "technical_paper",
        "authors": ["Xin Hong"]
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [37, 82, 86, 104, 108]
  },
  {
    "id": 108,
    "name": "Bayesian Fairness Regularization",
    "description": "Applies Bayesian methods to include fairness as a prior, allowing probabilistic interpretation of fairness constraints.",
    "assurance_goals": ["Fairness"],
    "tags": [
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Applying fairness regularization in Bayesian models for credit risk assessment.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Selecting appropriate prior distributions for fairness is non-trivial; Bayesian methods can be computationally intensive (e.g., requiring sampling) and outcomes can be sensitive to prior assumptions."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [37, 82, 86, 104, 107]
  }
]
