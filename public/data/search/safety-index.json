[
  {
    "slug": "out-of-distribution-detector-for-neural-networks",
    "name": "Out-of-DIstribution detector for Neural networks",
    "description": "ODIN (Out-of-Distribution Detector for Neural Networks) identifies when a neural network encounters inputs significantly different from its training distribution. It enhances detection by applying temperature scaling to soften the model's output distribution and adding small, carefully calibrated perturbations to the input that push in-distribution samples towards higher confidence predictions. By measuring the maximum softmax probability after these adjustments, ODIN can effectively distinguish between in-distribution and out-of-distribution inputs, flagging potentially unreliable predictions before they cause downstream errors.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "out-of-distribution detector for neural networks odin (out-of-distribution detector for neural networks) identifies when a neural network encounters inputs significantly different from its training distribution. it enhances detection by applying temperature scaling to soften the model's output distribution and adding small, carefully calibrated perturbations to the input that push in-distribution samples towards higher confidence predictions. by measuring the maximum softmax probability after these adjustments, odin can effectively distinguish between in-distribution and out-of-distribution inputs, flagging potentially unreliable predictions before they cause downstream errors. explainability reliability safety applicable-models/neural-network assurance-goal-category/explainability assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics explanatory-scope/global lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "synthetic-data-generation",
    "name": "Synthetic Data Generation",
    "description": "Synthetic data generation creates artificial datasets that aim to preserve the statistical properties, distributions, and relationships of real data whilst containing no actual records from real individuals. The technique encompasses various approaches including generative adversarial networks (GANs), variational autoencoders (VAEs), statistical sampling methods, and privacy-preserving techniques like differential privacy. Beyond privacy protection, synthetic data serves multiple purposes: augmenting limited datasets, balancing class distributions, testing model robustness, enabling data sharing across organisations, and supporting fairness assessments by generating representative samples for underrepresented groups.",
    "assurance_goals": [
      "Privacy",
      "Fairness",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/privacy",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/data-handling",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "synthetic data generation synthetic data generation creates artificial datasets that aim to preserve the statistical properties, distributions, and relationships of real data whilst containing no actual records from real individuals. the technique encompasses various approaches including generative adversarial networks (gans), variational autoencoders (vaes), statistical sampling methods, and privacy-preserving techniques like differential privacy. beyond privacy protection, synthetic data serves multiple purposes: augmenting limited datasets, balancing class distributions, testing model robustness, enabling data sharing across organisations, and supporting fairness assessments by generating representative samples for underrepresented groups. privacy fairness reliability safety applicable-models/agnostic assurance-goal-category/privacy assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/structured-output expertise-needed/cryptography expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/data-handling lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "federated-learning",
    "name": "Federated Learning",
    "description": "Federated learning enables collaborative model training across multiple distributed parties (devices, organisations, or data centres) without requiring centralised data sharing. Participants train models locally on their private datasets and only share model updates (gradients, weights, or aggregated statistics) with a central coordinator. This distributed approach serves multiple purposes: preserving data privacy and sovereignty, reducing communication costs, enabling learning from diverse data sources, improving model robustness through heterogeneous training, and facilitating compliance with data protection regulations whilst maintaining model performance comparable to centralised training.",
    "assurance_goals": [
      "Privacy",
      "Reliability",
      "Safety",
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/privacy",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/fairness",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "federated learning federated learning enables collaborative model training across multiple distributed parties (devices, organisations, or data centres) without requiring centralised data sharing. participants train models locally on their private datasets and only share model updates (gradients, weights, or aggregated statistics) with a central coordinator. this distributed approach serves multiple purposes: preserving data privacy and sovereignty, reducing communication costs, enabling learning from diverse data sources, improving model robustness through heterogeneous training, and facilitating compliance with data protection regulations whilst maintaining model performance comparable to centralised training. privacy reliability safety fairness applicable-models/agnostic assurance-goal-category/privacy assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/fairness data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/software-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "homomorphic-encryption",
    "name": "Homomorphic Encryption",
    "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
    "assurance_goals": [
      "Privacy",
      "Safety",
      "Transparency",
      "Security"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/privacy",
      "assurance-goal-category/privacy/formal-guarantee",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/privacy-guarantee",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "homomorphic encryption homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. this enables secure outsourced computation where sensitive data remains encrypted throughout processing. by allowing ml operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information. privacy safety transparency security applicable-models/agnostic assurance-goal-category/privacy assurance-goal-category/privacy/formal-guarantee assurance-goal-category/safety assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric evidence-type/privacy-guarantee expertise-needed/cryptography expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "deep-ensembles",
    "name": "Deep Ensembles",
    "description": "Deep ensembles combine predictions from multiple neural networks trained independently with different random initializations to capture epistemic uncertainty (model uncertainty). By training several models on the same data with different starting points, the ensemble reveals how much the model's predictions depend on training randomness. The disagreement between ensemble members naturally indicates prediction uncertainty - when models agree, confidence is high; when they disagree, uncertainty is revealed. This approach provides more reliable uncertainty estimates, better out-of-distribution detection, and improved calibration compared to single models.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Safety"
    ],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/transparency",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/prediction-interval",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "deep ensembles deep ensembles combine predictions from multiple neural networks trained independently with different random initializations to capture epistemic uncertainty (model uncertainty). by training several models on the same data with different starting points, the ensemble reveals how much the model's predictions depend on training randomness. the disagreement between ensemble members naturally indicates prediction uncertainty - when models agree, confidence is high; when they disagree, uncertainty is revealed. this approach provides more reliable uncertainty estimates, better out-of-distribution detection, and improved calibration compared to single models. reliability transparency safety applicable-models/neural-network assurance-goal-category/reliability assurance-goal-category/reliability/uncertainty-quantification assurance-goal-category/transparency assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric evidence-type/prediction-interval expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "safety-envelope-testing",
    "name": "Safety Envelope Testing",
    "description": "Safety envelope testing systematically evaluates AI system performance at the boundaries of its intended operational domain to identify potential failure modes before deployment. The technique involves defining the system's operational design domain (ODD), creating test scenarios that approach or exceed these boundaries, and measuring performance degradation as conditions become more challenging. By testing edge cases, environmental extremes, and boundary conditions, it reveals where the system transitions from safe to unsafe operation, enabling the establishment of clear operational limits and safety margins for deployment.",
    "assurance_goals": [
      "Safety",
      "Reliability"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "data-requirements/test-scenarios",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/boundary-analysis",
      "expertise-needed/domain-expertise",
      "expertise-needed/safety-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/testing"
    ],
    "searchText": "safety envelope testing safety envelope testing systematically evaluates ai system performance at the boundaries of its intended operational domain to identify potential failure modes before deployment. the technique involves defining the system's operational design domain (odd), creating test scenarios that approach or exceed these boundaries, and measuring performance degradation as conditions become more challenging. by testing edge cases, environmental extremes, and boundary conditions, it reveals where the system transitions from safe to unsafe operation, enabling the establishment of clear operational limits and safety margins for deployment. safety reliability applicable-models/agnostic assurance-goal-category/safety assurance-goal-category/reliability data-requirements/test-scenarios data-type/any evidence-type/quantitative-metric evidence-type/boundary-analysis expertise-needed/domain-expertise expertise-needed/safety-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/testing"
  },
  {
    "slug": "internal-review-boards",
    "name": "Internal Review Boards",
    "description": "Internal Review Boards (IRBs) provide independent, systematic evaluation of AI/ML projects throughout their lifecycle to identify ethical, safety, and societal risks before they materialise. Typically composed of multidisciplinary experts including ethicists, domain specialists, legal counsel, community representatives, and technical staff, IRBs review project proposals, assess potential harms to individuals and communities, evaluate mitigation strategies, and establish ongoing monitoring requirements. Unlike traditional research ethics committees, AI-focused IRBs address algorithmic bias, fairness concerns, privacy implications, and societal impact at scale, providing essential governance for responsible AI development and deployment.",
    "assurance_goals": [
      "Safety",
      "Fairness",
      "Transparency"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "assurance-goal-category/fairness",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "evidence-type/governance-framework",
      "expertise-needed/ethics",
      "expertise-needed/legal",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/project-planning",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "searchText": "internal review boards internal review boards (irbs) provide independent, systematic evaluation of ai/ml projects throughout their lifecycle to identify ethical, safety, and societal risks before they materialise. typically composed of multidisciplinary experts including ethicists, domain specialists, legal counsel, community representatives, and technical staff, irbs review project proposals, assess potential harms to individuals and communities, evaluate mitigation strategies, and establish ongoing monitoring requirements. unlike traditional research ethics committees, ai-focused irbs address algorithmic bias, fairness concerns, privacy implications, and societal impact at scale, providing essential governance for responsible ai development and deployment. safety fairness transparency applicable-models/agnostic assurance-goal-category/safety assurance-goal-category/fairness assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/qualitative-report evidence-type/governance-framework expertise-needed/ethics expertise-needed/legal expertise-needed/domain-expertise lifecycle-stage/project-planning lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/process"
  },
  {
    "slug": "red-teaming",
    "name": "Red Teaming",
    "description": "Red teaming involves systematic adversarial testing of AI/ML systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. Drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. Unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "assurance-goal-category/fairness",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "expertise-needed/security",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/procedural"
    ],
    "searchText": "red teaming red teaming involves systematic adversarial testing of ai/ml systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment. safety reliability fairness security applicable-models/agnostic assurance-goal-category/safety assurance-goal-category/reliability assurance-goal-category/fairness data-requirements/no-special-requirements data-type/any evidence-type/qualitative-report expertise-needed/security expertise-needed/ml-engineering lifecycle-stage/system-deployment-and-use technique-type/procedural"
  },
  {
    "slug": "anomaly-detection",
    "name": "Anomaly Detection",
    "description": "Anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. Applied to AI/ML systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. By establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "assurance-goal-category/safety/monitoring/anomaly-detection",
      "assurance-goal-category/reliability",
      "assurance-goal-category/fairness",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "searchText": "anomaly detection anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. applied to ai/ml systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. by establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm. safety reliability fairness security applicable-models/agnostic assurance-goal-category/safety assurance-goal-category/safety/monitoring/anomaly-detection assurance-goal-category/reliability assurance-goal-category/fairness data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/statistics expertise-needed/ml-engineering lifecycle-stage/system-deployment-and-use lifecycle-stage/system-deployment-and-use/monitoring technique-type/algorithmic"
  },
  {
    "slug": "human-in-the-loop-safeguards",
    "name": "Human-in-the-Loop Safeguards",
    "description": "Human-in-the-loop safeguards establish systematic checkpoints where human experts review, validate, or override AI/ML system decisions before they take effect. This governance approach combines automated efficiency with human judgement by defining clear intervention criteria (such as uncertainty thresholds, risk levels, or sensitive contexts) that trigger mandatory human oversight. By incorporating domain expertise, ethical considerations, and contextual understanding that machines may lack, these safeguards help ensure that critical decisions maintain appropriate human accountability whilst preserving the benefits of automated processing for routine cases.",
    "assurance_goals": [
      "Safety",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "assurance-goal-category/fairness",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "expertise-needed/domain-knowledge",
      "expertise-needed/stakeholder-engagement",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "searchText": "human-in-the-loop safeguards human-in-the-loop safeguards establish systematic checkpoints where human experts review, validate, or override ai/ml system decisions before they take effect. this governance approach combines automated efficiency with human judgement by defining clear intervention criteria (such as uncertainty thresholds, risk levels, or sensitive contexts) that trigger mandatory human oversight. by incorporating domain expertise, ethical considerations, and contextual understanding that machines may lack, these safeguards help ensure that critical decisions maintain appropriate human accountability whilst preserving the benefits of automated processing for routine cases. safety transparency fairness applicable-models/agnostic assurance-goal-category/safety assurance-goal-category/transparency assurance-goal-category/fairness data-requirements/no-special-requirements data-type/any evidence-type/qualitative-report expertise-needed/domain-knowledge expertise-needed/stakeholder-engagement lifecycle-stage/system-deployment-and-use technique-type/process"
  },
  {
    "slug": "confidence-thresholding",
    "name": "Confidence Thresholding",
    "description": "Confidence thresholding creates decision boundaries based on model uncertainty scores, routing predictions into different handling workflows depending on their confidence levels. High-confidence predictions (e.g., above 95%) proceed automatically, whilst medium-confidence cases (e.g., 70-95%) may trigger additional validation or human review, and low-confidence predictions (below 70%) receive extensive oversight or default to conservative fallback actions. This technique enables organisations to maintain automated efficiency for clear-cut cases whilst ensuring appropriate human intervention for uncertain decisions, balancing operational speed with risk management across safety-critical applications.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "confidence thresholding confidence thresholding creates decision boundaries based on model uncertainty scores, routing predictions into different handling workflows depending on their confidence levels. high-confidence predictions (e.g., above 95%) proceed automatically, whilst medium-confidence cases (e.g., 70-95%) may trigger additional validation or human review, and low-confidence predictions (below 70%) receive extensive oversight or default to conservative fallback actions. this technique enables organisations to maintain automated efficiency for clear-cut cases whilst ensuring appropriate human intervention for uncertain decisions, balancing operational speed with risk management across safety-critical applications. safety reliability transparency applicable-models/agnostic assurance-goal-category/safety assurance-goal-category/reliability assurance-goal-category/reliability/uncertainty-quantification assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/statistics expertise-needed/ml-engineering lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "runtime-monitoring-and-circuit-breakers",
    "name": "Runtime Monitoring and Circuit Breakers",
    "description": "Runtime monitoring and circuit breakers establish continuous surveillance of AI/ML systems in production, tracking critical metrics such as prediction accuracy, response times, input characteristics, output distributions, and system resource usage. When monitored parameters exceed predefined safety thresholds or exhibit anomalous patterns, automated circuit breakers immediately trigger protective actions including request throttling, service degradation, system shutdown, or failover to backup mechanisms. This approach provides real-time defensive capabilities that prevent cascading failures, ensure consistent service reliability, and maintain transparent operation status for stakeholders monitoring system health.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/safety",
      "assurance-goal-category/safety/monitoring/anomaly-detection",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/software-engineering",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "searchText": "runtime monitoring and circuit breakers runtime monitoring and circuit breakers establish continuous surveillance of ai/ml systems in production, tracking critical metrics such as prediction accuracy, response times, input characteristics, output distributions, and system resource usage. when monitored parameters exceed predefined safety thresholds or exhibit anomalous patterns, automated circuit breakers immediately trigger protective actions including request throttling, service degradation, system shutdown, or failover to backup mechanisms. this approach provides real-time defensive capabilities that prevent cascading failures, ensure consistent service reliability, and maintain transparent operation status for stakeholders monitoring system health. safety reliability transparency applicable-models/agnostic assurance-goal-category/safety assurance-goal-category/safety/monitoring/anomaly-detection assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/software-engineering expertise-needed/ml-engineering lifecycle-stage/system-deployment-and-use lifecycle-stage/system-deployment-and-use/monitoring technique-type/algorithmic"
  },
  {
    "slug": "model-cards",
    "name": "Model Cards",
    "description": "Model cards are standardised documentation frameworks that systematically document machine learning models through structured templates. The templates cover intended use cases, performance metrics across different demographic groups and operating conditions, training data characteristics, evaluation procedures, limitations, and ethical considerations. They serve as comprehensive technical specifications that enable informed model selection, prevent inappropriate deployment, support regulatory compliance, and facilitate fair assessment by providing transparent reporting of model capabilities and constraints across diverse populations and scenarios.",
    "assurance_goals": [
      "Transparency",
      "Fairness",
      "Safety"
    ],
    "tags": [
      "applicable-models/agnostic",
      "assurance-goal-category/transparency",
      "assurance-goal-category/transparency/documentation/model-card",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/regulatory-compliance",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/documentation"
    ],
    "searchText": "model cards model cards are standardised documentation frameworks that systematically document machine learning models through structured templates. the templates cover intended use cases, performance metrics across different demographic groups and operating conditions, training data characteristics, evaluation procedures, limitations, and ethical considerations. they serve as comprehensive technical specifications that enable informed model selection, prevent inappropriate deployment, support regulatory compliance, and facilitate fair assessment by providing transparent reporting of model capabilities and constraints across diverse populations and scenarios. transparency fairness safety applicable-models/agnostic assurance-goal-category/transparency assurance-goal-category/transparency/documentation/model-card assurance-goal-category/fairness assurance-goal-category/safety data-requirements/access-to-training-data data-requirements/sensitive-attributes data-type/any evidence-type/documentation expertise-needed/ml-engineering expertise-needed/regulatory-compliance expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/documentation"
  },
  {
    "slug": "model-distillation",
    "name": "Model Distillation",
    "description": "Model distillation transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) by training the student to mimic the teacher's behaviour. The student learns from the teacher's soft predictions and intermediate representations rather than just hard labels, capturing nuanced decision boundaries and uncertainty. This produces models that are faster, require less memory, and are often more interpretable whilst maintaining much of the original performance. Beyond compression, distillation can improve model reliability by regularising training and enable deployment in resource-constrained environments.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/deployment",
      "technique-type/algorithmic"
    ],
    "searchText": "model distillation model distillation transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) by training the student to mimic the teacher's behaviour. the student learns from the teacher's soft predictions and intermediate representations rather than just hard labels, capturing nuanced decision boundaries and uncertainty. this produces models that are faster, require less memory, and are often more interpretable whilst maintaining much of the original performance. beyond compression, distillation can improve model reliability by regularising training and enable deployment in resource-constrained environments. explainability reliability safety applicable-models/neural-network assurance-goal-category/explainability assurance-goal-category/reliability assurance-goal-category/safety data-requirements/access-to-training-data data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/deployment technique-type/algorithmic"
  },
  {
    "slug": "model-pruning",
    "name": "Model Pruning",
    "description": "Model pruning systematically removes less important weights, neurons, or entire layers from neural networks to create smaller, more efficient models whilst maintaining performance. This process involves iterative removal based on importance criteria (weight magnitudes, gradient information, activation patterns) followed by fine-tuning. Pruning can be structured (removing entire neurons/channels) or unstructured (removing individual weights), with structured pruning providing greater computational benefits and interpretability through simplified architectures.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/neural-network",
      "assurance-goal-category/explainability",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-model-internals",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-optimization",
      "technique-type/algorithmic"
    ],
    "searchText": "model pruning model pruning systematically removes less important weights, neurons, or entire layers from neural networks to create smaller, more efficient models whilst maintaining performance. this process involves iterative removal based on importance criteria (weight magnitudes, gradient information, activation patterns) followed by fine-tuning. pruning can be structured (removing entire neurons/channels) or unstructured (removing individual weights), with structured pruning providing greater computational benefits and interpretability through simplified architectures. explainability reliability safety applicable-models/neural-network assurance-goal-category/explainability assurance-goal-category/reliability assurance-goal-category/safety data-requirements/access-to-model-internals data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/model-optimization technique-type/algorithmic"
  },
  {
    "slug": "neuron-activation-analysis",
    "name": "Neuron Activation Analysis",
    "description": "Neuron activation analysis examines the firing patterns of individual neurons in neural networks by probing them with diverse inputs and analysing their activation responses. This technique helps understand what concepts, features, or patterns different neurons have learned to recognise, providing insights into the model's internal representations. For large language models, this can reveal neurons specialised for linguistic concepts, semantic categories, or even potentially harmful patterns, enabling targeted interventions and deeper model understanding.",
    "assurance_goals": [
      "Explainability",
      "Safety",
      "Fairness"
    ],
    "tags": [
      "applicable-models/neural-network",
      "applicable-models/llm",
      "applicable-models/transformer",
      "assurance-goal-category/explainability",
      "assurance-goal-category/safety",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "evidence-type/visualisation",
      "explanatory-scope/local",
      "explanatory-scope/global",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/testing",
      "lifecycle-stage/monitoring",
      "technique-type/algorithmic"
    ],
    "searchText": "neuron activation analysis neuron activation analysis examines the firing patterns of individual neurons in neural networks by probing them with diverse inputs and analysing their activation responses. this technique helps understand what concepts, features, or patterns different neurons have learned to recognise, providing insights into the model's internal representations. for large language models, this can reveal neurons specialised for linguistic concepts, semantic categories, or even potentially harmful patterns, enabling targeted interventions and deeper model understanding. explainability safety fairness applicable-models/neural-network applicable-models/llm applicable-models/transformer assurance-goal-category/explainability assurance-goal-category/safety assurance-goal-category/fairness data-requirements/access-to-model-internals data-type/text evidence-type/quantitative-metric evidence-type/visualisation explanatory-scope/local explanatory-scope/global expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/testing lifecycle-stage/monitoring technique-type/algorithmic"
  },
  {
    "slug": "prompt-sensitivity-analysis",
    "name": "Prompt Sensitivity Analysis",
    "description": "Prompt Sensitivity Analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. This technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. It encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. The analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/llm",
      "assurance-goal-category/explainability",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "expertise-needed/linguistics",
      "expertise-needed/experimental-design",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "technique-type/experimental"
    ],
    "searchText": "prompt sensitivity analysis prompt sensitivity analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. this technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. it encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. the analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations. explainability reliability safety applicable-models/llm assurance-goal-category/explainability assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/text evidence-type/quantitative-metric expertise-needed/statistics expertise-needed/linguistics expertise-needed/experimental-design lifecycle-stage/model-development lifecycle-stage/model-evaluation technique-type/experimental"
  },
  {
    "slug": "causal-mediation-analysis-in-language-models",
    "name": "Causal Mediation Analysis in Language Models",
    "description": "Causal mediation analysis in language models is a mechanistic interpretability technique that systematically investigates how specific internal components (neurons, attention heads, or layers) causally contribute to model outputs. By performing controlled interventions—such as activating, deactivating, or modifying specific components—researchers can trace the causal pathways through which information flows and transforms within the model. This approach goes beyond correlation to establish causal relationships, enabling researchers to understand not just what features influence outputs, but how and why they do so through specific computational pathways.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/llm",
      "applicable-models/transformer",
      "assurance-goal-category/explainability",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/causal-analysis",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/post-deployment",
      "technique-type/mechanistic-interpretability"
    ],
    "searchText": "causal mediation analysis in language models causal mediation analysis in language models is a mechanistic interpretability technique that systematically investigates how specific internal components (neurons, attention heads, or layers) causally contribute to model outputs. by performing controlled interventions—such as activating, deactivating, or modifying specific components—researchers can trace the causal pathways through which information flows and transforms within the model. this approach goes beyond correlation to establish causal relationships, enabling researchers to understand not just what features influence outputs, but how and why they do so through specific computational pathways. explainability reliability safety applicable-models/llm applicable-models/transformer assurance-goal-category/explainability assurance-goal-category/reliability assurance-goal-category/safety data-requirements/access-to-model-internals data-type/text evidence-type/causal-analysis expertise-needed/causal-inference expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/post-deployment technique-type/mechanistic-interpretability"
  },
  {
    "slug": "feature-attribution-with-integrated-gradients-in-nlp",
    "name": "Feature Attribution with Integrated Gradients in NLP",
    "description": "Applies Integrated Gradients to natural language processing models to attribute prediction importance to individual input tokens, words, or subword units. This technique computes gradients along a straight-line path from a baseline input (typically all-zeros, padding tokens, or neutral text) to the actual input, integrating these gradients to obtain attribution scores. Unlike vanilla gradient methods, Integrated Gradients satisfies axioms of sensitivity and implementation invariance, making it particularly valuable for understanding transformer-based language models where token interactions are complex.",
    "assurance_goals": [
      "Explainability",
      "Fairness",
      "Safety"
    ],
    "tags": [
      "applicable-models/transformer",
      "applicable-models/llm",
      "assurance-goal-category/explainability",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "evidence-type/visualisation",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/testing",
      "technique-type/gradient-based"
    ],
    "searchText": "feature attribution with integrated gradients in nlp applies integrated gradients to natural language processing models to attribute prediction importance to individual input tokens, words, or subword units. this technique computes gradients along a straight-line path from a baseline input (typically all-zeros, padding tokens, or neutral text) to the actual input, integrating these gradients to obtain attribution scores. unlike vanilla gradient methods, integrated gradients satisfies axioms of sensitivity and implementation invariance, making it particularly valuable for understanding transformer-based language models where token interactions are complex. explainability fairness safety applicable-models/transformer applicable-models/llm assurance-goal-category/explainability assurance-goal-category/fairness assurance-goal-category/safety data-requirements/no-special-requirements data-type/text evidence-type/quantitative-metric evidence-type/visualisation expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/testing technique-type/gradient-based"
  }
]