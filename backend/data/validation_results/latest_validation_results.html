<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>URL Validation Results</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        h1, h2 { color: #333; }
        .summary { margin-bottom: 30px; background-color: #f5f5f5; padding: 15px; border-radius: 5px; }
        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; cursor: pointer; position: relative; }
        th:hover { background-color: #ddd; }
        th::after { content: ""; position: absolute; right: 5px; }
        th.sorted-asc::after { content: "▲"; }
        th.sorted-desc::after { content: "▼"; }
        tr:nth-child(even) { background-color: #f9f9f9; }
        tr:hover { background-color: #f1f1f1; }
        .pass { background-color: #dff0d8; }
        .fail { background-color: #f2dede; }
        .filters { margin-bottom: 15px; }
        .filters input, .filters select { margin-right: 10px; padding: 5px; }
        .priority-high { background-color: #ffcccc; }
        .priority-medium { background-color: #ffffcc; }
        .priority-low { background-color: #ccffcc; }
        .error-type { font-weight: bold; }
    </style>
</head>
<body>
    <h1>URL Validation Results</h1>

    <div class="summary">
        <h2>Summary</h2>
        <p>Generated on: 2025-03-03 07:21:17</p>
        <p>Total rows: 79</p>
        <p>Total resources: 85</p>
        <p>Successful checks: 30 (35.3%)</p>
        <p>Failed checks: 55 (64.7%)</p>
        
        <h3>Failure Types:</h3>
        <ul>
            <li>Http Error: 10 (18.2%)</li>
            <li>No Title: 1 (1.8%)</li>
            <li>Title Mismatch: 40 (72.7%)</li>
            <li>Request Error: 4 (7.3%)</li>

        </ul>
    </div>

    <div class="filters">
        <input type="text" id="searchInput" placeholder="Search...">
        <select id="statusFilter">
            <option value="all">All Status</option>
            <option value="PASS">Pass</option>
            <option value="FAIL">Fail</option>
        </select>
        <select id="priorityFilter">
            <option value="all">All Priorities</option>
            <option value="high">High Priority</option>
            <option value="medium">Medium Priority</option>
            <option value="low">Low Priority</option>
        </select>
        <button onclick="resetFilters()">Reset Filters</button>
    </div>

    <table id="resultsTable">
        <thead>
            <tr>
                <th onclick="sortTable(0)">Row ID</th>
                <th onclick="sortTable(1)">Resource Title</th>
                <th onclick="sortTable(2)">URL</th>
                <th onclick="sortTable(3)">Status</th>
                <th onclick="sortTable(4)">Actual Title</th>
                <th onclick="sortTable(5)">Similarity</th>
                <th onclick="sortTable(6)">Keyword Match</th>
                <th onclick="sortTable(7)" class="sorted-desc">Priority</th>
                <th onclick="sortTable(8)">Suggestion</th>
            </tr>
        </thead>
        <tbody>

            <tr class="fail priority-high" data-priority="60" data-status="FAIL">
                <td>10</td>
                <td>ContextualDecomposition Repository</td>
                <td><a href="https://github.com/jamie-murdoch/ContextualDecomposition" target="_blank">https://github.com/jamie-murdoch/ContextualDecomposition</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>60</td>
                <td>Connection error - verify URL is accessible</td>
            </tr>

            <tr class="fail priority-high" data-priority="60" data-status="FAIL">
                <td>23</td>
                <td>Factor Analysis Explanation (Statistics Textbook)</td>
                <td><a href="https://www.statsoft.com/Textbook/Factor-Analysis" target="_blank">https://www.statsoft.com/Textbook/Factor-Analysis</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>60</td>
                <td>Connection error - verify URL is accessible</td>
            </tr>

            <tr class="fail priority-high" data-priority="60" data-status="FAIL">
                <td>32</td>
                <td>RuleFit: An Interpretable Model (Friedman & Popescu, 2008)</td>
                <td><a href="http://statweb.stanford.edu/~jhf/ftp/RuleFit.pdf" target="_blank">http://statweb.stanford.edu/~jhf/ftp/RuleFit.pdf</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>60</td>
                <td>Connection error - verify URL is accessible</td>
            </tr>

            <tr class="fail priority-high" data-priority="60" data-status="FAIL">
                <td>65</td>
                <td>Reject Option Classification for Fairness (Kamiran et al., 2012)</td>
                <td><a href="https://ieeexplore.ieee.org/document/6233097" target="_blank">https://ieeexplore.ieee.org/document/6233097</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>60</td>
                <td>Connection error - verify URL is accessible</td>
            </tr>

            <tr class="fail priority-high" data-priority="50" data-status="FAIL">
                <td>1</td>
                <td>SHAP Library</td>
                <td><a href="https://github.com/slundberg/shap" target="_blank">https://github.com/slundberg/shap</a></td>
                <td>FAIL</td>
                <td>GitHub - shap/shap: A game theoretic approach to explain the output of any machine learning model.</td>
                <td>0.16</td>
                <td>0.00</td>
                <td>50</td>
                <td>Consider updating expected title to: 'GitHub - shap/shap: A game theoretic approach to explain the output of any machine learning model.'</td>
            </tr>

            <tr class="fail priority-high" data-priority="50" data-status="FAIL">
                <td>24</td>
                <td>PCA - Sklearn User Guide</td>
                <td><a href="https://scikit-learn.org/stable/modules/decomposition.html#pca" target="_blank">https://scikit-learn.org/stable/modules/decomposition.html#pca</a></td>
                <td>FAIL</td>
                <td>2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.6.1 documentation</td>
                <td>0.13</td>
                <td>0.00</td>
                <td>50</td>
                <td>Consider updating expected title to: '2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.6.1 documentation'</td>
            </tr>

            <tr class="fail priority-high" data-priority="50" data-status="FAIL">
                <td>57</td>
                <td>Fairness-Aware Classifier with Prejudice Remover Regularizer (Kamishima et al., 2012)</td>
                <td><a href="https://arxiv.org/abs/1205.2999" target="_blank">https://arxiv.org/abs/1205.2999</a></td>
                <td>FAIL</td>
                <td>[1205.2999] Towards a new brain science: lessons from the economic collapse</td>
                <td>0.20</td>
                <td>0.00</td>
                <td>50</td>
                <td>Consider updating expected title to: '[1205.2999] Towards a new brain science: lessons from the economic collapse'</td>
            </tr>

            <tr class="fail priority-high" data-priority="50" data-status="FAIL">
                <td>79</td>
                <td>Explaining Bias: SHAP values for Fairness Analysis</td>
                <td><a href="https://arxiv.org/abs/2011.03045" target="_blank">https://arxiv.org/abs/2011.03045</a></td>
                <td>FAIL</td>
                <td>[2011.03045] Maximal correlation and monotonicity of free entropy and Stein discrepancy</td>
                <td>0.20</td>
                <td>0.00</td>
                <td>50</td>
                <td>Consider updating expected title to: '[2011.03045] Maximal correlation and monotonicity of free entropy and Stein discrepancy'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>13</td>
                <td>Interpretable ML Book – Friedman's H-Statistic</td>
                <td><a href="https://christophm.github.io/interpretable-ml-book/interaction.html" target="_blank">https://christophm.github.io/interpretable-ml-book/interaction.html</a></td>
                <td>FAIL</td>
                <td>8.3 Feature Interaction | Interpretable Machine Learning</td>
                <td>0.39</td>
                <td>0.25</td>
                <td>45</td>
                <td>Consider updating expected title to: '8.3 Feature Interaction | Interpretable Machine Learning'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>14</td>
                <td>Why Should I Trust You? (Ribeiro et al., 2016)</td>
                <td><a href="https://dl.acm.org/doi/10.1145/2939672.2939778" target="_blank">https://dl.acm.org/doi/10.1145/2939672.2939778</a></td>
                <td>FAIL</td>
                <td>"Why Should I Trust You?" | Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</td>
                <td>0.38</td>
                <td>0.50</td>
                <td>45</td>
                <td>Consider updating expected title to: '"Why Should I Trust You?" | Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>14</td>
                <td>LIME Library</td>
                <td><a href="https://github.com/marcotcr/lime" target="_blank">https://github.com/marcotcr/lime</a></td>
                <td>FAIL</td>
                <td>GitHub - marcotcr/lime: Lime: Explaining the predictions of any machine learning classifier</td>
                <td>0.26</td>
                <td>0.50</td>
                <td>45</td>
                <td>Consider updating expected title to: 'GitHub - marcotcr/lime: Lime: Explaining the predictions of any machine learning classifier'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>17</td>
                <td>ALE Plots – A Better PDP (Medium)</td>
                <td><a href="https://medium.com/@parrt/accumulated-local-effects-ale-plots-better-than-pdp-72a72fc88749" target="_blank">https://medium.com/@parrt/accumulated-local-effects-ale-plots-better-than-pdp-72a72fc88749</a></td>
                <td>FAIL</td>
                <td>Medium</td>
                <td>0.34</td>
                <td>0.33</td>
                <td>45</td>
                <td>Consider updating expected title to: 'Medium'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>18</td>
                <td>Interpreting ICE Plots</td>
                <td><a href="https://christophm.github.io/interpretable-ml-book/ice.html" target="_blank">https://christophm.github.io/interpretable-ml-book/ice.html</a></td>
                <td>FAIL</td>
                <td>9.1 Individual Conditional Expectation (ICE) | Interpretable Machine Learning</td>
                <td>0.25</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '9.1 Individual Conditional Expectation (ICE) | Interpretable Machine Learning'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>26</td>
                <td>UMAP How-To Guide</td>
                <td><a href="https://umap-learn.readthedocs.io/en/latest/" target="_blank">https://umap-learn.readthedocs.io/en/latest/</a></td>
                <td>FAIL</td>
                <td>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction — umap 0.5.8 documentation</td>
                <td>0.26</td>
                <td>0.50</td>
                <td>45</td>
                <td>Consider updating expected title to: 'UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction — umap 0.5.8 documentation'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>36</td>
                <td>Fairness Definitions Explained (Verma & Rubin, 2018)</td>
                <td><a href="https://arxiv.org/abs/1802.04896" target="_blank">https://arxiv.org/abs/1802.04896</a></td>
                <td>FAIL</td>
                <td>[1802.04896] Non-normal purely log terminal centres in characteristic $p \geq 3$</td>
                <td>0.29</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[1802.04896] Non-normal purely log terminal centres in characteristic $p \geq 3$'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>40</td>
                <td>Interpretability in the Wild: Neural Network Foci (Bau et al., 2020)</td>
                <td><a href="https://arxiv.org/abs/2007.09710" target="_blank">https://arxiv.org/abs/2007.09710</a></td>
                <td>FAIL</td>
                <td>[2007.09710] Boundary complexes of moduli spaces of curves in higher genus</td>
                <td>0.23</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[2007.09710] Boundary complexes of moduli spaces of curves in higher genus'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>42</td>
                <td>Investigating Gender Bias in LMs using Causal Mediation (Vig et al., 2020)</td>
                <td><a href="https://arxiv.org/abs/2004.12265" target="_blank">https://arxiv.org/abs/2004.12265</a></td>
                <td>FAIL</td>
                <td>[2004.12265] Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias</td>
                <td>0.33</td>
                <td>0.67</td>
                <td>45</td>
                <td>Consider updating expected title to: '[2004.12265] Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>45</td>
                <td>Investigating Few-Shot Learning in GPT-3 (Zhao et al., 2021)</td>
                <td><a href="https://arxiv.org/abs/2108.13888" target="_blank">https://arxiv.org/abs/2108.13888</a></td>
                <td>FAIL</td>
                <td>[2108.13888] Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning</td>
                <td>0.21</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[2108.13888] Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>52</td>
                <td>Preferential Sampling for Fair Classification (Kamal et al., 2010)</td>
                <td><a href="https://link.springer.com/chapter/10.1007/978-3-642-15880-3_3" target="_blank">https://link.springer.com/chapter/10.1007/978-3-642-15880-3_3</a></td>
                <td>FAIL</td>
                <td>Intelligent Interaction with the Real World | SpringerLink</td>
                <td>0.36</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: 'Intelligent Interaction with the Real World | SpringerLink'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>55</td>
                <td>Gender Bias in Text: Debiasing Word Embeddings with Adversaries (Zhao et al., 2018)</td>
                <td><a href="https://arxiv.org/abs/1807.11714" target="_blank">https://arxiv.org/abs/1807.11714</a></td>
                <td>FAIL</td>
                <td>[1807.11714] Gender Bias in Neural Natural Language Processing</td>
                <td>0.36</td>
                <td>0.22</td>
                <td>45</td>
                <td>Consider updating expected title to: '[1807.11714] Gender Bias in Neural Natural Language Processing'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>56</td>
                <td>Fairness in Deep Learning via Adversarial Networks</td>
                <td><a href="https://arxiv.org/abs/1707.00075" target="_blank">https://arxiv.org/abs/1707.00075</a></td>
                <td>FAIL</td>
                <td>[1707.00075] Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations</td>
                <td>0.38</td>
                <td>0.40</td>
                <td>45</td>
                <td>Consider updating expected title to: '[1707.00075] Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>58</td>
                <td>Mitigating Bias in Classification via Meta-Algorithm (Celis et al., 2019)</td>
                <td><a href="https://arxiv.org/abs/1905.03850" target="_blank">https://arxiv.org/abs/1905.03850</a></td>
                <td>FAIL</td>
                <td>[1905.03850] Solving zero-sum extensive-form games with arbitrary payoff uncertainty models</td>
                <td>0.22</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[1905.03850] Solving zero-sum extensive-form games with arbitrary payoff uncertainty models'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>60</td>
                <td>Fair Transfer Learning with Missing Demographics (Madras et al., 2018)</td>
                <td><a href="https://arxiv.org/abs/1809.10050" target="_blank">https://arxiv.org/abs/1809.10050</a></td>
                <td>FAIL</td>
                <td>[1809.10050] An Iterative Regularized Incremental Projected Subgradient Method for a Class of Bilevel Optimization Problems</td>
                <td>0.30</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[1809.10050] An Iterative Regularized Incremental Projected Subgradient Method for a Class of Bilevel Optimization Problems'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>61</td>
                <td>Adaptive Reweighting for Fair Classification (Biswas et al., 2020)</td>
                <td><a href="https://arxiv.org/abs/2010.00078" target="_blank">https://arxiv.org/abs/2010.00078</a></td>
                <td>FAIL</td>
                <td>[2010.00078] Pathways for producing binary black holes with large misaligned spins in the isolated formation channel</td>
                <td>0.30</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[2010.00078] Pathways for producing binary black holes with large misaligned spins in the isolated formation channel'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>66</td>
                <td>Equalized Odds and Calibration (Pleiss et al., 2017)</td>
                <td><a href="https://arxiv.org/abs/1707.00010" target="_blank">https://arxiv.org/abs/1707.00010</a></td>
                <td>FAIL</td>
                <td>[1707.00010] From Parity to Preference-based Notions of Fairness in Classification</td>
                <td>0.32</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[1707.00010] From Parity to Preference-based Notions of Fairness in Classification'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>67</td>
                <td>Fairness Definitions Explained (Verma & Rubin, 2018)</td>
                <td><a href="https://arxiv.org/abs/1802.04896" target="_blank">https://arxiv.org/abs/1802.04896</a></td>
                <td>FAIL</td>
                <td>[1802.04896] Non-normal purely log terminal centres in characteristic $p \geq 3$</td>
                <td>0.29</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[1802.04896] Non-normal purely log terminal centres in characteristic $p \geq 3$'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>71</td>
                <td>Fairness Metrics in ML (Chouldechova, 2017)</td>
                <td><a href="https://arxiv.org/abs/1703.00056" target="_blank">https://arxiv.org/abs/1703.00056</a></td>
                <td>FAIL</td>
                <td>[1703.00056] Fair prediction with disparate impact: A study of bias in recidivism prediction instruments</td>
                <td>0.25</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[1703.00056] Fair prediction with disparate impact: A study of bias in recidivism prediction instruments'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>76</td>
                <td>Causal Fairness Analysis (Nabi & Shpitser, 2018)</td>
                <td><a href="https://arxiv.org/abs/1707.00010" target="_blank">https://arxiv.org/abs/1707.00010</a></td>
                <td>FAIL</td>
                <td>[1707.00010] From Parity to Preference-based Notions of Fairness in Classification</td>
                <td>0.30</td>
                <td>0.17</td>
                <td>45</td>
                <td>Consider updating expected title to: '[1707.00010] From Parity to Preference-based Notions of Fairness in Classification'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>77</td>
                <td>Controlling Fairness and Diversity in Recommenders (Patro et al., 2020)</td>
                <td><a href="https://arxiv.org/abs/2005.11736" target="_blank">https://arxiv.org/abs/2005.11736</a></td>
                <td>FAIL</td>
                <td>[2005.11736] Efficient Intervention Design for Causal Discovery with Latents</td>
                <td>0.34</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[2005.11736] Efficient Intervention Design for Causal Discovery with Latents'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="45" data-status="FAIL">
                <td>78</td>
                <td>Bayesian Fairness: Posterior Regularization for Fair ML (Crawford et al., 2018)</td>
                <td><a href="https://arxiv.org/abs/1805.12564" target="_blank">https://arxiv.org/abs/1805.12564</a></td>
                <td>FAIL</td>
                <td>[1805.12564] Modeling 4D fMRI Data via Spatio-Temporal Convolutional Neural Networks (ST-CNN)</td>
                <td>0.29</td>
                <td>0.00</td>
                <td>45</td>
                <td>Consider updating expected title to: '[1805.12564] Modeling 4D fMRI Data via Spatio-Temporal Convolutional Neural Networks (ST-CNN)'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>15</td>
                <td>Global Surrogate Models for Explainability</td>
                <td><a href="https://christophm.github.io/interpretable-ml-book/global.html" target="_blank">https://christophm.github.io/interpretable-ml-book/global.html</a></td>
                <td>FAIL</td>
                <td>8.6 Global Surrogate | Interpretable Machine Learning</td>
                <td>0.52</td>
                <td>0.50</td>
                <td>40</td>
                <td>Consider updating expected title to: '8.6 Global Surrogate | Interpretable Machine Learning'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>16</td>
                <td>Partial Dependence Plot Tutorial (Scikit-learn)</td>
                <td><a href="https://scikit-learn.org/stable/modules/partial_dependence.html" target="_blank">https://scikit-learn.org/stable/modules/partial_dependence.html</a></td>
                <td>FAIL</td>
                <td>4.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.6.1 documentation</td>
                <td>0.43</td>
                <td>0.50</td>
                <td>40</td>
                <td>Consider updating expected title to: '4.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.6.1 documentation'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>19</td>
                <td>Deep Inside Convolutional Networks (Simonyan et al., 2014)</td>
                <td><a href="https://arxiv.org/abs/1312.6034" target="_blank">https://arxiv.org/abs/1312.6034</a></td>
                <td>FAIL</td>
                <td>[1312.6034] Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</td>
                <td>0.58</td>
                <td>0.67</td>
                <td>40</td>
                <td>Consider updating expected title to: '[1312.6034] Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>20</td>
                <td>Grad-CAM: Visual Explanations (Selvaraju et al., 2017)</td>
                <td><a href="https://arxiv.org/abs/1610.02391" target="_blank">https://arxiv.org/abs/1610.02391</a></td>
                <td>FAIL</td>
                <td>[1610.02391] Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</td>
                <td>0.53</td>
                <td>0.60</td>
                <td>40</td>
                <td>Consider updating expected title to: '[1610.02391] Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>21</td>
                <td>Visualizing and Understanding CNNs (Zeiler & Fergus, 2014)</td>
                <td><a href="https://arxiv.org/abs/1311.2901" target="_blank">https://arxiv.org/abs/1311.2901</a></td>
                <td>FAIL</td>
                <td>[1311.2901] Visualizing and Understanding Convolutional Networks</td>
                <td>0.59</td>
                <td>0.33</td>
                <td>40</td>
                <td>Consider updating expected title to: '[1311.2901] Visualizing and Understanding Convolutional Networks'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>30</td>
                <td>bnlearn Package (Bayesian Networks in R)</td>
                <td><a href="https://cran.r-project.org/web/packages/bnlearn/index.html" target="_blank">https://cran.r-project.org/web/packages/bnlearn/index.html</a></td>
                <td>FAIL</td>
                <td>CRAN: Package bnlearn</td>
                <td>0.48</td>
                <td>0.50</td>
                <td>40</td>
                <td>Consider updating expected title to: 'CRAN: Package bnlearn'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>34</td>
                <td>ODIN: Out-of-Distribution Detector (Liang et al., 2018)</td>
                <td><a href="https://arxiv.org/abs/1706.02690" target="_blank">https://arxiv.org/abs/1706.02690</a></td>
                <td>FAIL</td>
                <td>[1706.02690] Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks</td>
                <td>0.51</td>
                <td>0.20</td>
                <td>40</td>
                <td>Consider updating expected title to: '[1706.02690] Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>37</td>
                <td>Learning to Prune Deep Neural Networks (Han et al., 2015)</td>
                <td><a href="https://arxiv.org/abs/1506.02626" target="_blank">https://arxiv.org/abs/1506.02626</a></td>
                <td>FAIL</td>
                <td>[1506.02626] Learning both Weights and Connections for Efficient Neural Networks</td>
                <td>0.53</td>
                <td>0.50</td>
                <td>40</td>
                <td>Consider updating expected title to: '[1506.02626] Learning both Weights and Connections for Efficient Neural Networks'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>39</td>
                <td>BERTViz (Transformer Attention Visualizer)</td>
                <td><a href="https://github.com/jessevig/bertviz" target="_blank">https://github.com/jessevig/bertviz</a></td>
                <td>FAIL</td>
                <td>GitHub - jessevig/bertviz: BertViz: Visualize Attention in NLP Models (BERT, GPT2, BART, etc.)</td>
                <td>0.50</td>
                <td>0.50</td>
                <td>40</td>
                <td>Consider updating expected title to: 'GitHub - jessevig/bertviz: BertViz: Visualize Attention in NLP Models (BERT, GPT2, BART, etc.)'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>43</td>
                <td>Captum Integrated Gradients (Text Example)</td>
                <td><a href="https://captum.ai/tutorials/IMDB_TorchText_Interpret" target="_blank">https://captum.ai/tutorials/IMDB_TorchText_Interpret</a></td>
                <td>FAIL</td>
                <td>Captum · Model Interpretability for PyTorch</td>
                <td>0.42</td>
                <td>0.20</td>
                <td>40</td>
                <td>Consider updating expected title to: 'Captum · Model Interpretability for PyTorch'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>46</td>
                <td>AIF360 Reweighing Documentation</td>
                <td><a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html" target="_blank">https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html</a></td>
                <td>FAIL</td>
                <td>aif360.algorithms.preprocessing.Reweighing — aif360 0.6.1 documentation</td>
                <td>0.47</td>
                <td>0.67</td>
                <td>40</td>
                <td>Consider updating expected title to: 'aif360.algorithms.preprocessing.Reweighing — aif360 0.6.1 documentation'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>47</td>
                <td>AIF360 DisparateImpactRemover Documentation</td>
                <td><a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.DisparateImpactRemover.html" target="_blank">https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.DisparateImpactRemover.html</a></td>
                <td>FAIL</td>
                <td>aif360.algorithms.preprocessing.DisparateImpactRemover — aif360 0.6.1 documentation</td>
                <td>0.60</td>
                <td>0.67</td>
                <td>40</td>
                <td>Consider updating expected title to: 'aif360.algorithms.preprocessing.DisparateImpactRemover — aif360 0.6.1 documentation'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>53</td>
                <td>Fairness Through Unawareness Principle (Dwork et al., 2012)</td>
                <td><a href="https://arxiv.org/abs/1104.3913" target="_blank">https://arxiv.org/abs/1104.3913</a></td>
                <td>FAIL</td>
                <td>[1104.3913] Fairness Through Awareness</td>
                <td>0.57</td>
                <td>0.33</td>
                <td>40</td>
                <td>Consider updating expected title to: '[1104.3913] Fairness Through Awareness'</td>
            </tr>

            <tr class="fail priority-medium" data-priority="40" data-status="FAIL">
                <td>64</td>
                <td>Fairlearn ThresholdOptimizer Documentation</td>
                <td><a href="https://fairlearn.org/v0.7.0/api_reference/fairlearn.postprocessing.html#fairlearn.postprocessing.ThresholdOptimizer" target="_blank">https://fairlearn.org/v0.7.0/api_reference/fairlearn.postprocessing.html#fairlearn.postprocessing.ThresholdOptimizer</a></td>
                <td>FAIL</td>
                <td>fairlearn.postprocessing package — Fairlearn 0.7.0 documentation</td>
                <td>0.43</td>
                <td>0.33</td>
                <td>40</td>
                <td>Consider updating expected title to: 'fairlearn.postprocessing package — Fairlearn 0.7.0 documentation'</td>
            </tr>

            <tr class="pass priority-medium" data-priority="30" data-status="PASS">
                <td>12</td>
                <td>SALib (Python Sensitivity Analysis Library)</td>
                <td><a href="https://github.com/SALib/SALib" target="_blank">https://github.com/SALib/SALib</a></td>
                <td>PASS</td>
                <td>GitHub - SALib/SALib: Sensitivity Analysis Library in Python. Contains Sobol, Morris, FAST, and other methods.</td>
                <td>0.45</td>
                <td>0.80</td>
                <td>30</td>
                <td></td>
            </tr>

            <tr class="pass priority-medium" data-priority="30" data-status="PASS">
                <td>44</td>
                <td>TCAV: Concept Activation Vectors (Kim et al., 2018)</td>
                <td><a href="https://arxiv.org/abs/1711.11279" target="_blank">https://arxiv.org/abs/1711.11279</a></td>
                <td>PASS</td>
                <td>[1711.11279] Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)</td>
                <td>0.45</td>
                <td>0.80</td>
                <td>30</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>1</td>
                <td>A Unified Approach to Interpreting Model Predictions (NIPS 2017)</td>
                <td><a href="https://arxiv.org/abs/1705.07874" target="_blank">https://arxiv.org/abs/1705.07874</a></td>
                <td>PASS</td>
                <td>[1705.07874] A Unified Approach to Interpreting Model Predictions</td>
                <td>0.91</td>
                <td>0.71</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>2</td>
                <td>Permutation Feature Importance - Scikit-learn Guide</td>
                <td><a href="https://scikit-learn.org/stable/modules/permutation_importance.html" target="_blank">https://scikit-learn.org/stable/modules/permutation_importance.html</a></td>
                <td>PASS</td>
                <td>4.2. Permutation feature importance — scikit-learn 1.6.1 documentation</td>
                <td>0.72</td>
                <td>0.50</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>3</td>
                <td>Feature Importance Measures in Random Forests (Scikit-learn)</td>
                <td><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html" target="_blank">https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html</a></td>
                <td>PASS</td>
                <td>Feature importances with a forest of trees — scikit-learn 1.6.1 documentation</td>
                <td>0.62</td>
                <td>0.29</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>4</td>
                <td>Feature Importance Measures in Random Forests (Scikit-learn)</td>
                <td><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html" target="_blank">https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html</a></td>
                <td>PASS</td>
                <td>Feature importances with a forest of trees — scikit-learn 1.6.1 documentation</td>
                <td>0.62</td>
                <td>0.29</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>6</td>
                <td>Axiomatic Attribution for Deep Networks (Integrated Gradients, 2017)</td>
                <td><a href="https://arxiv.org/abs/1703.01365" target="_blank">https://arxiv.org/abs/1703.01365</a></td>
                <td>PASS</td>
                <td>[1703.01365] Axiomatic Attribution for Deep Networks</td>
                <td>0.75</td>
                <td>0.57</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>7</td>
                <td>Learning Important Features Through Propagating Activation Differences (DeepLIFT, 2017)</td>
                <td><a href="https://arxiv.org/abs/1704.02685" target="_blank">https://arxiv.org/abs/1704.02685</a></td>
                <td>PASS</td>
                <td>[1704.02685] Learning Important Features Through Propagating Activation Differences</td>
                <td>0.91</td>
                <td>0.78</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>8</td>
                <td>On Pixel-Wise Explanations by Layer-Wise Relevance Propagation (Bach et al., 2015)</td>
                <td><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140" target="_blank">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140</a></td>
                <td>PASS</td>
                <td>On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation | PLOS One</td>
                <td>0.70</td>
                <td>0.78</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>9</td>
                <td>Feature Importance Measures in Random Forests (Scikit-learn)</td>
                <td><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html" target="_blank">https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html</a></td>
                <td>PASS</td>
                <td>Feature importances with a forest of trees — scikit-learn 1.6.1 documentation</td>
                <td>0.62</td>
                <td>0.29</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>10</td>
                <td>Beyond Word Importance: Contextual Decomposition (Murdoch et al., 2018)</td>
                <td><a href="https://openreview.net/forum?id=rkRwGg-0Z" target="_blank">https://openreview.net/forum?id=rkRwGg-0Z</a></td>
                <td>PASS</td>
                <td>Beyond Word Importance:  Contextual Decomposition to Extract Interactions from LSTMs | OpenReview</td>
                <td>0.74</td>
                <td>0.71</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>11</td>
                <td>Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition (Montavon et al., 2017)</td>
                <td><a href="https://arxiv.org/abs/1512.02479" target="_blank">https://arxiv.org/abs/1512.02479</a></td>
                <td>PASS</td>
                <td>[1512.02479] Explaining NonLinear Classification Decisions with Deep Taylor Decomposition</td>
                <td>0.88</td>
                <td>0.78</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>22</td>
                <td>Attention is not Explanation (Serrano & Smith, 2019)</td>
                <td><a href="https://arxiv.org/abs/1902.10186" target="_blank">https://arxiv.org/abs/1902.10186</a></td>
                <td>PASS</td>
                <td>[1902.10186] Attention is not Explanation</td>
                <td>0.75</td>
                <td>0.40</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>25</td>
                <td>How to Use t-SNE Effectively (Distill.pub)</td>
                <td><a href="https://distill.pub/2016/misread-tsne" target="_blank">https://distill.pub/2016/misread-tsne</a></td>
                <td>PASS</td>
                <td>How to Use t-SNE Effectively</td>
                <td>0.82</td>
                <td>0.50</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>28</td>
                <td>Understanding Black-box Predictions via Influence Functions (Koh & Liang, 2017)</td>
                <td><a href="https://arxiv.org/abs/1703.04730" target="_blank">https://arxiv.org/abs/1703.04730</a></td>
                <td>PASS</td>
                <td>[1703.04730] Understanding Black-box Predictions via Influence Functions</td>
                <td>0.89</td>
                <td>0.71</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>33</td>
                <td>Dropout as a Bayesian Approximation (Gal & Ghahramani, 2016)</td>
                <td><a href="https://arxiv.org/abs/1506.02142" target="_blank">https://arxiv.org/abs/1506.02142</a></td>
                <td>PASS</td>
                <td>[1506.02142] Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</td>
                <td>0.62</td>
                <td>0.60</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>38</td>
                <td>Distilling the Knowledge in a Neural Network (Hinton et al., 2015)</td>
                <td><a href="https://arxiv.org/abs/1503.02531" target="_blank">https://arxiv.org/abs/1503.02531</a></td>
                <td>PASS</td>
                <td>[1503.02531] Distilling the Knowledge in a Neural Network</td>
                <td>0.83</td>
                <td>0.67</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>47</td>
                <td>Certifying and Removing Disparate Impact (Feldman et al., 2015)</td>
                <td><a href="https://arxiv.org/abs/1412.3756" target="_blank">https://arxiv.org/abs/1412.3756</a></td>
                <td>PASS</td>
                <td>[1412.3756] Certifying and removing disparate impact</td>
                <td>0.73</td>
                <td>0.67</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>48</td>
                <td>Learning Fair Representations (Zemel et al., 2013)</td>
                <td><a href="https://proceedings.mlr.press/v28/zemel13.html" target="_blank">https://proceedings.mlr.press/v28/zemel13.html</a></td>
                <td>PASS</td>
                <td>Learning Fair Representations</td>
                <td>0.77</td>
                <td>0.60</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>49</td>
                <td>FairGAN: Fairness-aware Generative Adversarial Networks (Xu et al., 2018)</td>
                <td><a href="https://arxiv.org/abs/1805.11202" target="_blank">https://arxiv.org/abs/1805.11202</a></td>
                <td>PASS</td>
                <td>[1805.11202] FairGAN: Fairness-aware Generative Adversarial Networks</td>
                <td>0.89</td>
                <td>0.86</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>50</td>
                <td>Optimized Preprocessing for Discrimination Prevention (Calmon et al., 2017)</td>
                <td><a href="https://arxiv.org/abs/1704.03354" target="_blank">https://arxiv.org/abs/1704.03354</a></td>
                <td>PASS</td>
                <td>[1704.03354] Optimized Data Pre-Processing for Discrimination Prevention</td>
                <td>0.82</td>
                <td>0.50</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>54</td>
                <td>Mitigating Unwanted Biases with Adversarial Learning (Zhang et al., 2018)</td>
                <td><a href="https://arxiv.org/abs/1801.07593" target="_blank">https://arxiv.org/abs/1801.07593</a></td>
                <td>PASS</td>
                <td>[1801.07593] Mitigating Unwanted Biases with Adversarial Learning</td>
                <td>0.86</td>
                <td>0.71</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>59</td>
                <td>A Reductions Approach to Fair Classification (Agarwal et al., 2018)</td>
                <td><a href="https://arxiv.org/abs/1803.02453" target="_blank">https://arxiv.org/abs/1803.02453</a></td>
                <td>PASS</td>
                <td>[1803.02453] A Reductions Approach to Fair Classification</td>
                <td>0.82</td>
                <td>0.67</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>63</td>
                <td>Equality of Opportunity in Supervised Learning (Hardt et al., 2016)</td>
                <td><a href="https://arxiv.org/abs/1610.02413" target="_blank">https://arxiv.org/abs/1610.02413</a></td>
                <td>PASS</td>
                <td>[1610.02413] Equality of Opportunity in Supervised Learning</td>
                <td>0.84</td>
                <td>0.67</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>68</td>
                <td>Uniform Guidelines on Employee Selection (80% rule)</td>
                <td><a href="https://www.gpo.gov/fdsys/pkg/CFR-2016-title29-vol4/xml/CFR-2016-title29-vol4-part1607.xml" target="_blank">https://www.gpo.gov/fdsys/pkg/CFR-2016-title29-vol4/xml/CFR-2016-title29-vol4-part1607.xml</a></td>
                <td>PASS</td>
                <td>UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978)</td>
                <td>0.81</td>
                <td>0.80</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>70</td>
                <td>Equality of Opportunity (Hardt et al., 2016)</td>
                <td><a href="https://arxiv.org/abs/1610.02413" target="_blank">https://arxiv.org/abs/1610.02413</a></td>
                <td>PASS</td>
                <td>[1610.02413] Equality of Opportunity in Supervised Learning</td>
                <td>0.60</td>
                <td>0.50</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>72</td>
                <td>Fairness Through Awareness (Dwork et al., 2012)</td>
                <td><a href="https://arxiv.org/abs/1104.3913" target="_blank">https://arxiv.org/abs/1104.3913</a></td>
                <td>PASS</td>
                <td>[1104.3913] Fairness Through Awareness</td>
                <td>0.66</td>
                <td>0.60</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>74</td>
                <td>Counterfactual Fairness (Kusner et al., 2017)</td>
                <td><a href="https://arxiv.org/abs/1703.06856" target="_blank">https://arxiv.org/abs/1703.06856</a></td>
                <td>PASS</td>
                <td>[1703.06856] Counterfactual Fairness</td>
                <td>0.72</td>
                <td>0.50</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="20" data-status="PASS">
                <td>75</td>
                <td>Path-Specific Counterfactual Fairness (Chiappa & Gillam, 2018)</td>
                <td><a href="https://arxiv.org/abs/1802.08139" target="_blank">https://arxiv.org/abs/1802.08139</a></td>
                <td>PASS</td>
                <td>[1802.08139] Path-Specific Counterfactual Fairness</td>
                <td>0.79</td>
                <td>0.57</td>
                <td>20</td>
                <td></td>
            </tr>

            <tr class="pass priority-low" data-priority="10" data-status="PASS">
                <td>5</td>
                <td>Interpreting Regression Coefficients</td>
                <td><a href="https://www.theanalysisfactor.com/interpreting-regression-coefficients/" target="_blank">https://www.theanalysisfactor.com/interpreting-regression-coefficients/</a></td>
                <td>PASS</td>
                <td>Interpreting Regression Coefficients - The Analysis Factor</td>
                <td>0.78</td>
                <td>1.00</td>
                <td>10</td>
                <td></td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>27</td>
                <td>MMD-Critic: Prototypes and Criticisms (Kim et al., 2016)</td>
                <td><a href="https://papers.nips.cc/paper/2016/file/5680522b8e2bb01943234bceabde35a1-Paper.pdf" target="_blank">https://papers.nips.cc/paper/2016/file/5680522b8e2bb01943234bceabde35a1-Paper.pdf</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Check if URL is correct (HTTP 404)</td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>29</td>
                <td>Towards Contrastive Explanations with Pertinent Negatives (Dhurandhar et al., 2018)</td>
                <td><a href="https://papers.nips.cc/paper/2018/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf" target="_blank">https://papers.nips.cc/paper/2018/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Check if URL is correct (HTTP 404)</td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>31</td>
                <td>Anchors: High-Precision Model-Agnostic Explanations (Ribeiro et al., 2018)</td>
                <td><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982/16015" target="_blank">https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982/16015</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Check if URL is correct (HTTP 403)</td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>35</td>
                <td>Permutation Tests in ML (Blog)</td>
                <td><a href="https://towardsdatascience.com/understanding-permutation-tests-391aff0aff7d" target="_blank">https://towardsdatascience.com/understanding-permutation-tests-391aff0aff7d</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Check if URL is correct (HTTP 404)</td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>40</td>
                <td>OpenAI: Circuits & Neurons Analysis</td>
                <td><a href="https://openai.com/blog/automated-interpretability/" target="_blank">https://openai.com/blog/automated-interpretability/</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Check if URL is correct (HTTP 403)</td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>41</td>
                <td>Prompt Sensitivity Index (Zhang et al., 2022)</td>
                <td><a href="https://aclanthology.org/2022.findings-emnlp.167.pdf" target="_blank">https://aclanthology.org/2022.findings-emnlp.167.pdf</a></td>
                <td>FAIL</td>
                <td>None</td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Page has no title element</td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>46</td>
                <td>Data Preprocessing Techniques for Classification Without Discrimination (Kamiran & Calders, 2012)</td>
                <td><a href="https://link.springer.com/article/10.1007/s10115-010-0352-x" target="_blank">https://link.springer.com/article/10.1007/s10115-010-0352-x</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Check if URL is correct (HTTP 404)</td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>51</td>
                <td>Classification with No Discrimination via Massaging (Kamiran et al., 2012)</td>
                <td><a href="https://ieeexplore.ieee.org/document/6233097" target="_blank">https://ieeexplore.ieee.org/document/6233097</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Check if URL is correct (HTTP 404)</td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>62</td>
                <td>Multiaccuracy: Black-Box Post-Processing for Fairness in Classification (Hebert-Johnson et al., 2018)</td>
                <td><a href="https://papers.nips.cc/paper/2018/hash/9a49a25d845a3cdfd8b3d0cce985d545-Abstract.html" target="_blank">https://papers.nips.cc/paper/2018/hash/9a49a25d845a3cdfd8b3d0cce985d545-Abstract.html</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Check if URL is correct (HTTP 404)</td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>69</td>
                <td>Understanding Demographic Parity in ML</td>
                <td><a href="https://fairmlbook.org/pdf/demographic_parity.pdf" target="_blank">https://fairmlbook.org/pdf/demographic_parity.pdf</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Check if URL is correct (HTTP 404)</td>
            </tr>

            <tr class="fail priority-low" data-priority="0" data-status="FAIL">
                <td>73</td>
                <td>k-NN Consistency as Fairness Metric</td>
                <td><a href="https://openreview.net/pdf?id=SJxUjjR9tX" target="_blank">https://openreview.net/pdf?id=SJxUjjR9tX</a></td>
                <td>FAIL</td>
                <td></td>
                <td>0.00</td>
                <td>0.00</td>
                <td>0</td>
                <td>Check if URL is correct (HTTP 404)</td>
            </tr>

        </tbody>
    </table>

    <script>
        // Initial sort by priority (column 7) in descending order
        window.onload = function() {
            sortTable(7, true); // Sort by priority column in descending order
        };
        
        function sortTable(columnIndex, initialSort = false) {
            const table = document.getElementById('resultsTable');
            const header = table.getElementsByTagName('th')[columnIndex];
            const tbody = table.getElementsByTagName('tbody')[0];
            const rows = Array.from(tbody.getElementsByTagName('tr'));
            
            // Check current sort direction
            let sortDirection = 'asc';
            if (!initialSort) {
                if (header.classList.contains('sorted-asc')) {
                    sortDirection = 'desc';
                }
            } else {
                sortDirection = 'desc'; // For initial load
            }
            
            // Remove sorting classes from all headers
            const headers = table.getElementsByTagName('th');
            for (let i = 0; i < headers.length; i++) {
                headers[i].classList.remove('sorted-asc', 'sorted-desc');
            }
            
            // Add appropriate class to the current header
            header.classList.add(sortDirection === 'asc' ? 'sorted-asc' : 'sorted-desc');
            
            // Sort the rows
            rows.sort((a, b) => {
                let aValue = a.getElementsByTagName('td')[columnIndex].textContent;
                let bValue = b.getElementsByTagName('td')[columnIndex].textContent;
                
                // Handle numeric values
                if (!isNaN(aValue) && !isNaN(bValue)) {
                    aValue = parseFloat(aValue) || 0;
                    bValue = parseFloat(bValue) || 0;
                }
                
                // Compare values
                if (aValue < bValue) {
                    return sortDirection === 'asc' ? -1 : 1;
                } else if (aValue > bValue) {
                    return sortDirection === 'asc' ? 1 : -1;
                }
                return 0;
            });
            
            // Update the table
            rows.forEach(row => tbody.appendChild(row));
            
            // Apply filters after sorting
            applyFilters();
        }
        
        function applyFilters() {
            const searchTerm = document.getElementById('searchInput').value.toLowerCase();
            const statusFilter = document.getElementById('statusFilter').value;
            const priorityFilter = document.getElementById('priorityFilter').value;
            
            const rows = document.querySelectorAll('#resultsTable tbody tr');
            
            rows.forEach(row => {
                let showRow = true;
                
                // Check search term
                if (searchTerm) {
                    const text = row.textContent.toLowerCase();
                    if (!text.includes(searchTerm)) {
                        showRow = false;
                    }
                }
                
                // Check status filter
                if (statusFilter !== 'all') {
                    const rowStatus = row.getAttribute('data-status');
                    if (rowStatus !== statusFilter) {
                        showRow = false;
                    }
                }
                
                // Check priority filter
                if (priorityFilter !== 'all') {
                    const priority = parseInt(row.getAttribute('data-priority'));
                    if (priorityFilter === 'high' && priority < 50) showRow = false;
                    if (priorityFilter === 'medium' && (priority < 30 || priority >= 50)) showRow = false;
                    if (priorityFilter === 'low' && priority >= 30) showRow = false;
                }
                
                // Show or hide the row
                row.style.display = showRow ? '' : 'none';
            });
        }
        
        function resetFilters() {
            document.getElementById('searchInput').value = '';
            document.getElementById('statusFilter').value = 'all';
            document.getElementById('priorityFilter').value = 'all';
            applyFilters();
        }
        
        // Attach event listeners
        document.getElementById('searchInput').addEventListener('input', applyFilters);
        document.getElementById('statusFilter').addEventListener('change', applyFilters);
        document.getElementById('priorityFilter').addEventListener('change', applyFilters);
    </script>

</body>
</html>
