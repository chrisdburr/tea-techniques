{
  "tag": {
    "name": "assurance-goal-category/explainability/explains/internal-mechanisms",
    "slug": "assurance-goal-category-explainability-explains-internal-mechanisms",
    "count": 14,
    "category": "assurance-goal-category"
  },
  "techniques": [
    {
      "slug": "layer-wise-relevance-propagation",
      "name": "Layer-wise Relevance Propagation",
      "description": "Layer-wise Relevance Propagation (LRP) explains neural network predictions by working backwards through the network to show how much each input feature contributed to the final decision. It follows a simple conservation rule: the total contribution scores always add up to the original prediction. Starting from the output, LRP distributes 'relevance' backwards through each layer using different rules depending on the layer type. This creates a detailed breakdown showing which input features helped or hindered the prediction, making it easier to understand why the network made a particular decision.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/model-specific",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/explainability/property/consistency",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Identifying which pixels in chest X-rays contribute to pneumonia detection, helping radiologists verify AI diagnoses by highlighting anatomical regions the model considers relevant.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging a neural network's misclassification of handwritten digits by tracing relevance through layers to identify which input pixels caused the error and which network layers amplified it.",
          "goal": "Explainability"
        },
        {
          "description": "Providing transparent explanations for automated credit scoring decisions by showing which financial features received positive or negative relevance scores, enabling clear regulatory reporting.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires different propagation rules for each layer type, making implementation complex for new architectures."
        },
        {
          "description": "Can produce negative relevance scores which may be difficult to interpret intuitively."
        },
        {
          "description": "Rule selection (LRP-ε, LRP-γ, etc.) significantly affects results and requires domain expertise."
        },
        {
          "description": "Limited to feedforward networks and may not work well with modern architectures like transformers without substantial modifications."
        }
      ],
      "resources": [
        {
          "title": "rachtibat/LRP-eXplains-Transformers",
          "url": "https://github.com/rachtibat/LRP-eXplains-Transformers",
          "source_type": "software_package"
        },
        {
          "title": "sebastian-lapuschkin/lrp_toolbox",
          "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
          "source_type": "software_package"
        },
        {
          "title": "Getting started — zennit documentation",
          "url": "https://zennit.readthedocs.io/en/latest/getting-started.html",
          "source_type": "documentation"
        },
        {
          "title": "Layer-wise Relevance Propagation eXplains Transformers (LXT) documentation",
          "url": "https://lxt.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation",
          "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140",
          "source_type": "technical_paper",
          "authors": [
            "Sebastian Bach",
            "Alexander Binder",
            "Grégoire Montavon",
            "Frederick Klauschen",
            "Klaus-Robert Müller",
            "Wojciech Samek"
          ],
          "publication_date": "2015-07-10"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 3,
      "acronym": "LRP",
      "related_techniques": [
        "shapley-additive-explanations",
        "integrated-gradients",
        "deeplift",
        "local-interpretable-model-agnostic-explanations",
        "contrastive-explanation-method",
        "anchor"
      ]
    },
    {
      "slug": "contextual-decomposition",
      "name": "Contextual Decomposition",
      "description": "Contextual Decomposition explains LSTM and RNN predictions by decomposing the final hidden state into contributions from individual inputs and their interactions. Unlike simpler attribution methods, it separates the direct contribution of specific words or phrases from the contextual effects of surrounding words. This is particularly useful for understanding how sequential models process language, as it can identify whether a word's influence comes from its individual meaning or from its interaction with nearby words in the sequence.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/recurrent-neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/attribution-methods/model-specific",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing why an LSTM-based spam filter flagged an email by decomposing contributions from individual words ('free', 'urgent') versus their contextual interactions ('free trial' together).",
          "goal": "Explainability"
        },
        {
          "description": "Understanding how a medical text classifier diagnoses conditions from clinical notes by separating direct symptom mentions from contextual medical reasoning patterns.",
          "goal": "Explainability"
        },
        {
          "description": "Providing transparent explanations for automated content moderation decisions by showing which words and phrase interactions contributed to toxicity detection.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Primarily designed for LSTM and simple RNN architectures, not suitable for modern transformers or attention-based models."
        },
        {
          "description": "Not widely implemented in standard machine learning libraries, often requiring custom implementation."
        },
        {
          "description": "Computational overhead increases significantly with sequence length and model depth."
        },
        {
          "description": "May not scale well to very complex models or capture all types of feature interactions in deep networks."
        }
      ],
      "resources": [
        {
          "title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs",
          "url": "http://arxiv.org/pdf/1801.05453v2",
          "source_type": "technical_paper",
          "authors": [
            "W. James Murdoch",
            "Peter J. Liu",
            "Bin Yu"
          ],
          "publication_date": "2018-01-16"
        },
        {
          "title": "FredericGodin/ContextualDecomposition-NLP",
          "url": "https://github.com/FredericGodin/ContextualDecomposition-NLP",
          "source_type": "software_package"
        },
        {
          "title": "Interpreting patient-Specific risk prediction using contextual decomposition of BiLSTMs: Application to children with asthma",
          "url": "https://core.ac.uk/download/294758884.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Alsaad R.",
            "Boughorbel S.",
            "Janahi I.",
            "Malluhi Q."
          ],
          "publication_date": "2019-01-01"
        },
        {
          "title": "Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models",
          "url": "http://arxiv.org/pdf/1911.06194v2",
          "source_type": "technical_paper",
          "authors": [
            "Xisen Jin",
            "Zhongyu Wei",
            "Junyi Du",
            "Xiangyang Xue",
            "Xiang Ren"
          ],
          "publication_date": "2019-11-08"
        },
        {
          "title": "Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition",
          "url": "http://arxiv.org/pdf/2407.00886v3",
          "source_type": "technical_paper",
          "authors": [
            "Aliyah R. Hsu",
            "Georgia Zhou",
            "Yeshwanth Cherapanamjeri",
            "Yaxuan Huang",
            "Anobel Y. Odisho",
            "Peter R. Carroll",
            "Bin Yu"
          ],
          "publication_date": "2024-07-01"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 3,
      "related_techniques": [
        "taylor-decomposition",
        "influence-functions"
      ]
    },
    {
      "slug": "taylor-decomposition",
      "name": "Taylor Decomposition",
      "description": "Taylor Decomposition is a mathematical technique that explains neural network predictions by computing first-order and higher-order derivatives of the network's output with respect to input features. It decomposes the prediction into relevance scores that indicate how much each input feature and feature interaction contributes to the final decision. The method uses Layer-wise Relevance Propagation (LRP) principles to trace prediction contributions backwards through the network layers, providing precise mathematical attributions for each input element.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/completeness",
        "assurance-goal-category/explainability/property/fidelity",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analyzing which pixels in an image contribute most to a convolutional neural network's classification decision, showing both positive and negative relevance scores for different regions of the input image.",
          "goal": "Explainability"
        },
        {
          "description": "Understanding how different word embeddings in a sentiment analysis model contribute to the final sentiment score, revealing which terms drive positive vs negative predictions.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Mathematically complex requiring deep understanding of calculus and neural network architectures."
        },
        {
          "description": "Computationally intensive as it requires computing gradients and higher-order derivatives through the entire network."
        },
        {
          "description": "Approximations used in practice may introduce errors that affect attribution accuracy."
        },
        {
          "description": "Limited tooling availability compared to other explainability methods, with most implementations being research-focused rather than production-ready."
        }
      ],
      "resources": [
        {
          "title": "Explaining NonLinear Classification Decisions with Deep Taylor Decomposition",
          "url": "http://arxiv.org/pdf/1512.02479v1",
          "source_type": "technical_paper",
          "authors": [
            "Grégoire Montavon",
            "Sebastian Bach",
            "Alexander Binder",
            "Wojciech Samek",
            "Klaus-Robert Müller"
          ],
          "publication_date": "2015-12-08"
        },
        {
          "title": "A Rigorous Study Of The Deep Taylor Decomposition",
          "url": "http://arxiv.org/pdf/2211.08425v1",
          "source_type": "technical_paper",
          "authors": [
            "Leon Sixt",
            "Tim Landgraf"
          ],
          "publication_date": "2022-11-14"
        },
        {
          "title": "sebastian-lapuschkin/lrp_toolbox",
          "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "contextual-decomposition",
        "influence-functions"
      ]
    },
    {
      "slug": "gradient-weighted-class-activation-mapping",
      "name": "Gradient-weighted Class Activation Mapping",
      "description": "Grad-CAM creates visual heatmaps showing which regions of an image a convolutional neural network focuses on when making a specific classification. Unlike pixel-level techniques, Grad-CAM produces coarser region-based explanations by using gradients from the predicted class to weight the CNN's final feature maps, then projecting these weighted activations back to create an overlay on the original image. This provides intuitive visual explanations of where the model is 'looking' for evidence of different classes.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/visualization-methods/activation-maps",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Validating that a melanoma detection model focuses on the actual skin lesion rather than surrounding healthy skin, medical equipment, or artifacts when making cancer/benign classifications.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging an autonomous vehicle's traffic sign recognition system by visualising whether the model correctly focuses on the sign itself rather than background objects, shadows, or irrelevant visual elements.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a medical imaging system for racial bias by examining whether diagnostic predictions inappropriately focus on skin tone regions rather than actual pathological features, ensuring equitable healthcare AI deployment.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires access to the CNN's internal feature maps and gradients, limiting use to white-box scenarios."
        },
        {
          "description": "Resolution is constrained by the final convolutional layer's feature map size, producing coarser localisation than pixel-level methods."
        },
        {
          "description": "Only applicable to CNN architectures with clearly defined convolutional layers, not suitable for other neural network types."
        },
        {
          "description": "May highlight regions that correlate with the class but aren't causally important for the model's decision-making process."
        }
      ],
      "resources": [
        {
          "title": "jacobgil/pytorch-grad-cam",
          "url": "https://github.com/jacobgil/pytorch-grad-cam",
          "source_type": "software_package"
        },
        {
          "title": "Grad-CAM: Visualize class activation maps with Keras, TensorFlow ...",
          "url": "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/",
          "source_type": "tutorial"
        },
        {
          "title": "kazuto1011/grad-cam-pytorch",
          "url": "https://github.com/kazuto1011/grad-cam-pytorch",
          "source_type": "software_package"
        },
        {
          "title": "A Tutorial on Explainable Image Classification for Dementia Stages Using Convolutional Neural Network and Gradient-weighted Class Activation Mapping",
          "url": "https://www.semanticscholar.org/paper/8b1139cb06cfe5ba69e2fd05e1450b43df031a02",
          "source_type": "documentation",
          "authors": [
            "Kevin Kam Fung Yuen"
          ]
        },
        {
          "title": "A Guide to Grad-CAM in Deep Learning - Analytics Vidhya",
          "url": "https://www.analyticsvidhya.com/blog/2023/12/grad-cam-in-deep-learning/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "Grad-CAM",
      "related_techniques": [
        "saliency-maps",
        "occlusion-sensitivity"
      ]
    },
    {
      "slug": "classical-attention-analysis-in-neural-networks",
      "name": "Classical Attention Analysis in Neural Networks",
      "description": "Classical attention mechanisms in RNNs and CNNs create alignment matrices and temporal attention patterns that show how models focus on different input elements over time or space. This technique analyses these traditional attention patterns, particularly in encoder-decoder architectures and sequence-to-sequence models, where attention weights reveal which source elements influence each output step. Unlike transformer self-attention analysis, this focuses on understanding alignment patterns, temporal dependencies, and encoder-decoder attention dynamics in classical neural architectures.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/rnn",
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/visualization-methods/attention-patterns",
        "assurance-goal-category/explainability/representation-analysis/decomposition",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/fidelity",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing encoder-decoder attention in a neural machine translation model to verify the alignment between source and target words, ensuring the model learns proper translation correspondences rather than positional biases.",
          "goal": "Explainability"
        },
        {
          "description": "Examining temporal attention patterns in an RNN-based image captioning model to understand how attention moves across different image regions as it generates each word of the caption description.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Attention weights are not always strongly correlated with feature importance for the final prediction."
        },
        {
          "description": "High attention does not necessarily imply causal influence - models can attend to irrelevant but correlated features."
        },
        {
          "description": "Only applicable to neural network architectures that explicitly use attention mechanisms."
        },
        {
          "description": "Interpretation can be misleading without understanding the specific attention mechanism implementation and training dynamics."
        }
      ],
      "resources": [
        {
          "title": "An Attentive Survey of Attention Models",
          "url": "https://www.semanticscholar.org/paper/a8427ce5aee6d62800c725588e89940ed4910e0d",
          "source_type": "documentation",
          "authors": [
            "S. Chaudhari",
            "Gungor Polatkan",
            "R. Ramanath",
            "Varun Mithal"
          ]
        },
        {
          "title": "Attention, please! A survey of neural attention models in deep learning",
          "url": "https://www.semanticscholar.org/paper/44930df2a3186edb58c4d6f6e5ed828c5d6a0089",
          "source_type": "documentation",
          "authors": [
            "Alana de Santana Correia",
            "E. Colombini"
          ]
        },
        {
          "title": "ecco - Explain, Analyze, and Visualize NLP Language Models",
          "url": "https://github.com/jalammar/ecco",
          "source_type": "software_package"
        },
        {
          "title": "Enhancing Sentiment Analysis of Twitter Data Using Recurrent Neural Networks with Attention Mechanism",
          "url": "https://www.semanticscholar.org/paper/c59e0158280a567114ae8ca64a932eefd127e0aa",
          "source_type": "technical_paper",
          "authors": [
            "S. Nithya",
            "X. A. Presskila",
            "B. Sakthivel",
            "R. Krishnan",
            "K. Narayanan",
            "S. Sundararajan"
          ]
        },
        {
          "title": "Can Neural Networks Develop Attention? Google Thinks they Can ...",
          "url": "https://www.kdnuggets.com/2019/11/neural-networks-develop-attention-google.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "attention-visualisation-in-transformers"
      ]
    },
    {
      "slug": "factor-analysis",
      "name": "Factor Analysis",
      "description": "Factor analysis is a statistical technique that identifies latent variables (hidden factors) underlying observed correlations in data. It works by analysing how variables relate to each other, finding a smaller number of unobserved factors that explain patterns among multiple observed variables. Unlike PCA which maximises total variance, factor analysis focuses on shared variance (communalities - the variance variables have in common) whilst separating out unique variance and measurement error. After extracting factors, rotation methods like varimax (which creates uncorrelated factors) or oblimin (allowing correlated factors) help make factors more interpretable by aligning them with distinct groups of variables.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/structured-output",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing customer satisfaction surveys to identify key drivers (e.g., 'service quality', 'product value', 'convenience') from dozens of individual questions, helping businesses focus improvement efforts.",
          "goal": "Explainability"
        },
        {
          "description": "Reducing dimensionality of financial indicators to identify underlying economic factors (e.g., 'growth', 'inflation', 'credit risk') for more interpretable risk models.",
          "goal": "Explainability"
        },
        {
          "description": "Creating transparent feature groups for regulatory reporting by showing how multiple correlated features can be summarised into interpretable factors with clear business meaning.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Assumes linear relationships between variables and multivariate normality of data."
        },
        {
          "description": "Results can be abstract and require domain expertise to interpret meaningfully."
        },
        {
          "description": "Sensitive to the choice of number of factors and rotation method, which can significantly affect interpretability."
        },
        {
          "description": "Requires sufficiently large sample sizes relative to the number of variables for stable results."
        }
      ],
      "resources": [
        {
          "title": "Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey",
          "url": "http://arxiv.org/pdf/2101.00734v2",
          "source_type": "documentation",
          "authors": [
            "Benyamin Ghojogh",
            "Ali Ghodsi",
            "Fakhri Karray",
            "Mark Crowley"
          ],
          "publication_date": "2021-01-04"
        },
        {
          "title": "Factor Analysis in R Course | DataCamp",
          "url": "https://www.datacamp.com/courses/factor-analysis-in-r",
          "source_type": "tutorial"
        },
        {
          "title": "EducationalTestingService/factor_analyzer",
          "url": "https://github.com/EducationalTestingService/factor_analyzer",
          "source_type": "software_package"
        },
        {
          "title": "Confirmatory Factor Analysis Fundamentals | Towards Data Science",
          "url": "https://towardsdatascience.com/confirmatory-factor-analysis-theory-aac11af008a6/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "principal-component-analysis",
        "t-sne",
        "umap"
      ]
    },
    {
      "slug": "t-sne",
      "name": "t-SNE",
      "description": "t-SNE (t-Distributed Stochastic Neighbour Embedding) is a non-linear dimensionality reduction technique that creates 2D or 3D visualisations of high-dimensional data by preserving local neighbourhood relationships. The algorithm converts similarities between data points into joint probabilities in the high-dimensional space, then tries to minimise the divergence between these probabilities and those in the low-dimensional embedding. This approach excels at revealing cluster structures and local patterns, making it particularly effective for exploratory data analysis and understanding complex data relationships that linear methods like PCA might miss.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
        "assurance-goal-category/explainability/visualization-methods/feature-relationships",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/comprehensibility",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/visualization"
      ],
      "example_use_cases": [
        {
          "description": "Analyzing genomic data with thousands of gene expression features to visualize how different cancer subtypes cluster together, revealing which tumors have similar molecular signatures and potentially similar treatment responses.",
          "goal": "Explainability"
        },
        {
          "description": "Exploring deep learning model embeddings to understand how a neural network represents different categories of images, showing whether the model groups similar objects (cars, animals, furniture) in meaningful clusters in its internal feature space.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Non-deterministic algorithm produces different results on each run, making it difficult to reproduce exact visualizations or compare results across studies."
        },
        {
          "description": "Prioritizes preserving local neighborhood structure at the expense of global relationships, potentially creating misleading impressions about overall data topology."
        },
        {
          "description": "Computationally expensive with O(n²) complexity, making it impractical for datasets with more than ~10,000 points without approximation methods."
        },
        {
          "description": "Sensitive to hyperparameter choices (perplexity, learning rate, iterations) that can dramatically affect clustering patterns and require domain expertise to tune appropriately."
        }
      ],
      "resources": [
        {
          "title": "pavlin-policar/openTSNE",
          "url": "https://github.com/pavlin-policar/openTSNE",
          "source_type": "software_package"
        },
        {
          "title": "openTSNE: Extensible, parallel implementations of t-SNE ...",
          "url": "https://opentsne.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "How t-SNE works — openTSNE 1.0.0 documentation",
          "url": "https://opentsne.readthedocs.io/en/stable/tsne_algorithm.html",
          "source_type": "documentation"
        },
        {
          "title": "t-SNE from Scratch (ft. NumPy) | Towards Data Science",
          "url": "https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "related_techniques": [
        "factor-analysis",
        "principal-component-analysis",
        "umap"
      ]
    },
    {
      "slug": "model-distillation",
      "name": "Model Distillation",
      "description": "Model distillation transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) by training the student to mimic the teacher's behaviour. The student learns from the teacher's soft predictions and intermediate representations rather than just hard labels, capturing nuanced decision boundaries and uncertainty. This produces models that are faster, require less memory, and are often more interpretable whilst maintaining much of the original performance. Beyond compression, distillation can improve model reliability by regularising training and enable deployment in resource-constrained environments.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/model-simplification/knowledge-transfer",
        "assurance-goal-category/explainability/surrogate-models/global-surrogates",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/decision-boundaries",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/access-to-training-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/deployment",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Compressing a large medical diagnosis model into a smaller student model that can run on edge devices in resource-limited clinics, making the decision process more transparent for healthcare professionals whilst maintaining diagnostic accuracy for critical patient care.",
          "goal": "Explainability"
        },
        {
          "description": "Creating a compressed fraud detection model from a complex ensemble teacher that maintains detection performance whilst being more robust to adversarial attacks and data drift, ensuring consistent protection of financial transactions across varying conditions.",
          "goal": "Reliability"
        },
        {
          "description": "Distilling a large autonomous vehicle perception model into a smaller student model that can run with guaranteed inference times and lower computational requirements, ensuring predictable safety-critical decision-making under real-time constraints.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Student models typically achieve 90-95% of teacher performance, creating a trade-off between model efficiency and predictive accuracy that may be unacceptable for high-stakes applications requiring maximum precision."
        },
        {
          "description": "Distillation process can be computationally expensive, requiring extensive teacher model inference during training and careful hyperparameter tuning to balance knowledge transfer with student model capacity."
        },
        {
          "description": "Knowledge transfer quality depends heavily on teacher-student architecture compatibility and the chosen distillation objectives, with mismatched designs potentially leading to ineffective learning or mode collapse."
        },
        {
          "description": "Student models may inherit teacher model biases and vulnerabilities whilst potentially introducing new failure modes, requiring separate validation for fairness, robustness, and safety properties."
        },
        {
          "description": "Compressed models may lack the teacher's capability to handle edge cases or out-of-distribution inputs, potentially creating safety risks when deployed in environments different from the training distribution."
        }
      ],
      "resources": [
        {
          "title": "airaria/TextBrewer",
          "url": "https://github.com/airaria/TextBrewer",
          "source_type": "software_package",
          "description": "PyTorch-based knowledge distillation toolkit for natural language processing with support for transformer models, flexible distillation strategies, and multi-teacher approaches."
        },
        {
          "title": "Main features — TextBrewer 0.2.1.post1 documentation",
          "url": "https://textbrewer.readthedocs.io/",
          "source_type": "documentation",
          "description": "Comprehensive documentation for TextBrewer including tutorials, API reference, configuration guides, and experimental results for knowledge distillation in NLP tasks."
        },
        {
          "title": "A Generic Approach for Reproducible Model Distillation",
          "url": "http://arxiv.org/abs/2211.12631",
          "source_type": "technical_paper",
          "authors": [
            "Hooker, Giles",
            "Xu, Peiru",
            "Zhou, Yunzhe"
          ],
          "publication_date": "2023-04-27",
          "description": "Research paper presenting a framework for reproducible knowledge distillation with standardised evaluation protocols and benchmarking across different model architectures and distillation techniques."
        },
        {
          "title": "dkozlov/awesome-knowledge-distillation",
          "url": "https://github.com/dkozlov/awesome-knowledge-distillation",
          "source_type": "software_package",
          "description": "Curated collection of knowledge distillation resources including academic papers, implementation code across multiple frameworks (PyTorch, TensorFlow, Keras), and educational videos."
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "related_techniques": [
        "ridge-regression-surrogates",
        "rulefit",
        "model-pruning"
      ]
    },
    {
      "slug": "model-pruning",
      "name": "Model Pruning",
      "description": "Model pruning systematically removes less important weights, neurons, or entire layers from neural networks to create smaller, more efficient models whilst maintaining performance. This process involves iterative removal based on importance criteria (weight magnitudes, gradient information, activation patterns) followed by fine-tuning. Pruning can be structured (removing entire neurons/channels) or unstructured (removing individual weights), with structured pruning providing greater computational benefits and interpretability through simplified architectures.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/model-simplification/pruning",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-optimization",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Compressing a medical imaging model from 100MB to 15MB for deployment on edge devices in remote clinics, enabling healthcare professionals to audit the remaining critical feature detectors and understand which anatomical patterns drive diagnoses whilst maintaining diagnostic accuracy.",
          "goal": "Explainability"
        },
        {
          "description": "Pruning a financial fraud detection model by 70% to eliminate redundant pathways that amplify noise, creating a more robust system that maintains consistent predictions across different transaction types and reduces false positives during market volatility.",
          "goal": "Reliability"
        },
        {
          "description": "Reducing an autonomous vehicle perception model to ensure predictable inference times under 50ms for safety-critical decisions, removing non-essential neurons to guarantee consistent computational behaviour whilst maintaining object detection accuracy for pedestrian safety.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Determining optimal pruning ratios requires extensive experimentation as over-pruning can cause dramatic accuracy degradation, whilst under-pruning provides minimal benefits, making the process time-consuming and resource-intensive."
        },
        {
          "description": "Structured pruning often requires specific hardware or software framework support to realise computational benefits, limiting deployment flexibility and potentially necessitating model architecture changes."
        },
        {
          "description": "Pruned models may exhibit reduced robustness to out-of-distribution inputs or adversarial attacks, as removing neurons can eliminate defensive redundancy that helped handle edge cases."
        },
        {
          "description": "The iterative pruning and fine-tuning process can be computationally expensive, sometimes requiring more resources than training the original model, particularly for large-scale networks."
        },
        {
          "description": "Pruning criteria based on weight magnitudes or gradients may not align with interpretability goals, potentially removing neurons that contribute to model transparency whilst retaining complex, opaque pathways."
        }
      ],
      "resources": [
        {
          "title": "horseee/LLM-Pruner",
          "url": "https://github.com/horseee/LLM-Pruner",
          "source_type": "software_package",
          "description": "Structural pruning tool for large language models supporting Llama, BLOOM, and other LLMs with three-stage compression process requiring only 50,000 training samples for post-training recovery."
        },
        {
          "title": "Pruning Quickstart — Neural Network Intelligence",
          "url": "https://nni.readthedocs.io/en/stable/tutorials/pruning_quick_start.html",
          "source_type": "tutorial",
          "description": "Step-by-step tutorial for implementing model pruning using Microsoft's NNI toolkit, covering basic usage, pruning algorithms, and practical examples for neural network compression."
        },
        {
          "title": "Overview of NNI Model Pruning — Neural Network Intelligence",
          "url": "https://nni.readthedocs.io/en/stable/compression/pruning.html",
          "source_type": "documentation",
          "description": "Comprehensive documentation for NNI's pruning capabilities covering structured and unstructured pruning strategies, supported algorithms, and integration with popular deep learning frameworks."
        },
        {
          "title": "coldlarry/YOLOv3-complete-pruning",
          "url": "https://github.com/coldlarry/YOLOv3-complete-pruning",
          "source_type": "software_package",
          "description": "Complete pruning implementation for YOLOv3 object detection models demonstrating computer vision model compression with minimal accuracy loss for real-time inference applications."
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "model-distillation"
      ]
    },
    {
      "slug": "neuron-activation-analysis",
      "name": "Neuron Activation Analysis",
      "description": "Neuron activation analysis examines the firing patterns of individual neurons in neural networks by probing them with diverse inputs and analysing their activation responses. This technique helps understand what concepts, features, or patterns different neurons have learned to recognise, providing insights into the model's internal representations. For large language models, this can reveal neurons specialised for linguistic concepts, semantic categories, or even potentially harmful patterns, enabling targeted interventions and deeper model understanding.",
      "assurance_goals": [
        "Explainability",
        "Safety",
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/llm",
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/safety",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "evidence-type/visualisation",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "lifecycle-stage/monitoring",
        "technique-type/algorithmic",
        "assurance-goal-category/explainability/representation-analysis/concept-identification",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/property/comprehensibility"
      ],
      "example_use_cases": [
        {
          "description": "Analysing GPT-based models to identify specific neurons that activate on toxic or harmful content, enabling targeted interventions to reduce model toxicity whilst preserving general language capabilities for safer AI deployment.",
          "goal": "Safety"
        },
        {
          "description": "Examining activation patterns in multilingual language models to detect neurons that exhibit systematic biases when processing text from different linguistic communities, revealing implicit representation inequalities that could affect downstream applications.",
          "goal": "Fairness"
        },
        {
          "description": "Investigating individual neurons in medical language models to understand which clinical concepts and medical knowledge representations drive diagnostic suggestions, enabling healthcare professionals to validate the model's medical reasoning pathways.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Many neurons exhibit polysemantic behaviour, representing multiple unrelated concepts simultaneously, making it difficult to assign clear interpretable meanings to individual neural units."
        },
        {
          "description": "Important model behaviours are often distributed across many neurons rather than localised in single units, requiring analysis of neural circuits and interactions that can be exponentially complex."
        },
        {
          "description": "Computational costs scale dramatically with modern large language models containing billions of parameters, making comprehensive neuron-by-neuron analysis prohibitively expensive for complete model understanding."
        },
        {
          "description": "Neuron activation patterns are highly context-dependent, with the same neuron potentially serving different roles based on surrounding input context, complicating consistent interpretation across diverse scenarios."
        },
        {
          "description": "Interpretation of activation patterns often relies on subjective human analysis without rigorous validation methods, potentially leading to confirmation bias or misattribution of neural functions."
        }
      ],
      "resources": [
        {
          "title": "jalammar/ecco",
          "url": "https://github.com/jalammar/ecco",
          "source_type": "software_package"
        },
        {
          "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models",
          "url": "http://arxiv.org/pdf/2504.21053v1",
          "source_type": "technical_paper",
          "authors": [
            "Yi Zhou",
            "Wenpeng Xing",
            "Dezhang Kong",
            "Changting Lin",
            "Meng Han"
          ],
          "publication_date": "2025-04-29"
        },
        {
          "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron Activation Analysis",
          "url": "http://arxiv.org/pdf/2404.13567v1",
          "source_type": "technical_paper",
          "authors": [
            "Abhilekha Dalal",
            "Rushrukh Rayan",
            "Adrita Barua",
            "Eugene Y. Vasserman",
            "Md Kamruzzaman Sarker",
            "Pascal Hitzler"
          ],
          "publication_date": "2024-04-21"
        },
        {
          "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron\n  Activation Analysis",
          "url": "http://arxiv.org/abs/2404.13567",
          "source_type": "technical_paper",
          "authors": [
            "Barua, Adrita",
            "Dalal, Abhilekha",
            "Hitzler, Pascal",
            "Rayan, Rushrukh",
            "Sarker, Md Kamruzzaman",
            "Vasserman, Eugene Y."
          ],
          "publication_date": "2024-04-21"
        },
        {
          "title": "Ecco",
          "url": "https://ecco.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "Tracing the Thoughts in Language Models",
          "url": "https://www.anthropic.com/news/tracing-thoughts-language-model",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "prototype-and-criticism-models",
        "concept-activation-vectors"
      ]
    },
    {
      "slug": "prompt-sensitivity-analysis",
      "name": "Prompt Sensitivity Analysis",
      "description": "Prompt Sensitivity Analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. This technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. It encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. The analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/llm",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "expertise-needed/linguistics",
        "expertise-needed/experimental-design",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "technique-type/experimental",
        "assurance-goal-category/explainability/uncertainty-analysis/sensitivity-testing",
        "assurance-goal-category/explainability/attribution-methods/perturbation-based",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/prediction-confidence",
        "assurance-goal-category/explainability/property/consistency",
        "assurance-goal-category/explainability/property/fidelity"
      ],
      "example_use_cases": [
        {
          "description": "Testing medical diagnosis LLMs with semantically equivalent but syntactically different symptom descriptions to ensure consistent diagnostic recommendations across different patient communication styles, identifying potential failure modes where slight phrasing changes could lead to dangerous misdiagnoses.",
          "goal": "Safety"
        },
        {
          "description": "Analysing how variations in candidate descriptions (gendered language, cultural references, educational institution prestige indicators) affect LLM-based CV screening recommendations to identify potential discriminatory patterns and ensure equitable treatment across diverse applicant backgrounds.",
          "goal": "Reliability"
        },
        {
          "description": "Examining how different ways of framing financial questions (formal vs informal language, technical vs layperson terminology) affect investment advice generated by LLMs to improve user understanding and model transparency whilst ensuring consistent advisory quality.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Analysis is inherently limited to the specific prompt variations tested, potentially missing important sensitivity patterns that weren't anticipated during study design, making comprehensive coverage challenging."
        },
        {
          "description": "Systematic exploration of prompt variations can be computationally expensive, particularly for large-scale sensitivity analysis across multiple dimensions of variation, requiring significant resources for thorough evaluation."
        },
        {
          "description": "Ensuring that prompt variations maintain semantic equivalence whilst introducing meaningful perturbations requires careful linguistic expertise and validation, which can be subjective and domain-dependent."
        },
        {
          "description": "Results may reveal sensitivity patterns that are difficult to interpret or act upon, particularly when multiple types of variations interact in complex ways, limiting practical applicability of findings."
        },
        {
          "description": "The meaningfulness of sensitivity measurements depends heavily on the choice of baseline prompts and variation strategies, which can introduce methodological biases and affect the generalisability of conclusions."
        }
      ],
      "resources": [
        {
          "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
          "url": "https://www.semanticscholar.org/paper/17a6116e5bbd8b87082cbb2e795885567300c483",
          "source_type": "technical_paper",
          "authors": [
            "Melanie Sclar",
            "Yejin Choi",
            "Yulia Tsvetkov",
            "Alane Suhr"
          ]
        },
        {
          "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts",
          "url": "http://arxiv.org/pdf/2505.12592v1",
          "source_type": "technical_paper",
          "authors": [
            "Sullam Jeoung",
            "Yueyan Chen",
            "Yi Zhang",
            "Shuai Wang",
            "Haibo Ding",
            "Lin Lee Cheong"
          ],
          "publication_date": "2025-05-19"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "causal-mediation-analysis-in-language-models",
        "feature-attribution-with-integrated-gradients-in-nlp"
      ]
    },
    {
      "slug": "causal-mediation-analysis-in-language-models",
      "name": "Causal Mediation Analysis in Language Models",
      "description": "Causal mediation analysis in language models is a mechanistic interpretability technique that systematically investigates how specific internal components (neurons, attention heads, or layers) causally contribute to model outputs. By performing controlled interventions—such as activating, deactivating, or modifying specific components—researchers can trace the causal pathways through which information flows and transforms within the model. This approach goes beyond correlation to establish causal relationships, enabling researchers to understand not just what features influence outputs, but how and why they do so through specific computational pathways.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/llm",
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "evidence-type/causal-analysis",
        "expertise-needed/causal-inference",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/post-deployment",
        "technique-type/mechanistic-interpretability",
        "assurance-goal-category/explainability/causal-analysis/mediation-analysis",
        "assurance-goal-category/explainability/explains/causal-pathways",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/causality",
        "assurance-goal-category/explainability/property/fidelity"
      ],
      "example_use_cases": [
        {
          "description": "Investigating causal pathways in content moderation models to understand how specific attention mechanisms contribute to flagging potentially harmful content, enabling verification that safety decisions rely on appropriate features rather than spurious correlations and ensuring robust content filtering.",
          "goal": "Safety"
        },
        {
          "description": "Identifying specific neurons or attention heads that causally contribute to biased outputs in hiring or lending language models, enabling targeted interventions to reduce discriminatory behaviour whilst preserving model performance on legitimate tasks and ensuring fair treatment across demographics.",
          "goal": "Reliability"
        },
        {
          "description": "Tracing causal pathways in large language models performing mathematical reasoning tasks to understand how intermediate steps are computed and stored, revealing which components are responsible for different aspects of logical inference and enabling validation of reasoning processes.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Requires sophisticated understanding of model architecture to design meaningful interventions, as poorly chosen intervention points may yield misleading causal conclusions or fail to capture relevant computational pathways."
        },
        {
          "description": "Results are highly dependent on the validity of underlying causal assumptions, which can be difficult to verify in complex, high-dimensional neural network spaces where multiple causal pathways may interact."
        },
        {
          "description": "Comprehensive causal analysis requires extensive computational resources, particularly for large models, as each intervention requires separate forward passes and multiple intervention combinations for robust conclusions."
        },
        {
          "description": "Distinguishing between direct causal effects and indirect effects mediated through other components can be challenging, potentially leading to oversimplified causal narratives that miss important intermediate processes."
        },
        {
          "description": "Causal relationships identified in specific contexts or datasets may not generalise to different domains, tasks, or model versions, requiring careful validation across diverse scenarios to ensure robust findings."
        }
      ],
      "resources": [],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "prompt-sensitivity-analysis",
        "feature-attribution-with-integrated-gradients-in-nlp"
      ]
    },
    {
      "slug": "concept-activation-vectors",
      "name": "Concept Activation Vectors",
      "description": "Concept Activation Vectors (CAVs), also known as Testing with Concept Activation Vectors (TCAV), identify mathematical directions in neural network representation space that correspond to human-understandable concepts such as 'stripes', 'young', or 'medical equipment'. The technique works by finding linear directions that separate activations of concept examples from non-concept examples, then measuring how much these concept directions influence the model's predictions. This provides quantitative answers to questions like 'How much does the concept of youth affect this model's hiring decisions?' enabling systematic bias detection and model understanding.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/transformer",
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/visualization",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-knowledge",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use/auditing",
        "technique-type/algorithmic",
        "assurance-goal-category/explainability/representation-analysis/concept-identification",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/causal-pathways",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/causality"
      ],
      "example_use_cases": [
        {
          "description": "Auditing a medical imaging model to verify it focuses on diagnostic features (like 'tumour characteristics') rather than irrelevant concepts (like 'scanner type' or 'patient positioning') when classifying chest X-rays, ensuring clinical decisions rely on medically relevant information.",
          "goal": "Explainability"
        },
        {
          "description": "Testing whether a hiring algorithm's resume screening decisions are influenced by concepts related to protected characteristics such as 'gender-associated names', 'prestigious universities', or 'employment gaps', enabling systematic bias detection and compliance verification.",
          "goal": "Fairness"
        },
        {
          "description": "Providing regulatory-compliant explanations for financial lending decisions by quantifying how concepts like 'debt-to-income ratio', 'employment stability', and 'credit history length' influence loan approval models, with precise sensitivity scores for audit documentation.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires clearly defined concept examples and non-concept examples, which can be challenging to obtain for abstract or subjective concepts."
        },
        {
          "description": "Assumes that meaningful concept directions exist as linear separable directions in the model's internal representation space, which may not hold for all concepts."
        },
        {
          "description": "Results depend heavily on which network layer is examined, as different layers capture different levels of abstraction and concept representation."
        },
        {
          "description": "Computational cost grows significantly with model size and number of concepts tested, though recent advances like FastCAV address this limitation."
        },
        {
          "description": "Interpretation requires domain expertise to define meaningful concepts and understand the significance of sensitivity scores in practical contexts."
        }
      ],
      "resources": [
        {
          "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks",
          "url": "http://arxiv.org/pdf/2505.17883v1",
          "source_type": "technical_paper",
          "authors": [
            "Laines Schmalwasser",
            "Niklas Penzel",
            "Joachim Denzler",
            "Julia Niebling"
          ],
          "publication_date": "2025-05-23"
        },
        {
          "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
          "url": "http://arxiv.org/pdf/2311.15303v1",
          "source_type": "technical_paper",
          "authors": [
            "Avani Gupta",
            "Saurabh Saini",
            "P J Narayanan"
          ],
          "publication_date": "2023-11-26"
        },
        {
          "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations",
          "url": "http://arxiv.org/pdf/2503.05522v1",
          "source_type": "technical_paper",
          "authors": [
            "Eren Erogullari",
            "Sebastian Lapuschkin",
            "Wojciech Samek",
            "Frederik Pahde"
          ],
          "publication_date": "2025-03-07"
        },
        {
          "title": "Concept Gradient: Concept-based Interpretation Without Linear Assumption",
          "url": "http://arxiv.org/pdf/2208.14966v2",
          "source_type": "technical_paper",
          "authors": [
            "Andrew Bai",
            "Chih-Kuan Yeh",
            "Pradeep Ravikumar",
            "Neil Y. C. Lin",
            "Cho-Jui Hsieh"
          ],
          "publication_date": "2022-08-31"
        },
        {
          "title": "SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation",
          "url": "http://arxiv.org/pdf/2310.07698v1",
          "source_type": "technical_paper",
          "authors": [
            "Bo Pan",
            "Zhenke Liu",
            "Yifei Zhang",
            "Liang Zhao"
          ],
          "publication_date": "2023-10-11"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "acronym": "CAVs",
      "related_techniques": [
        "prototype-and-criticism-models",
        "neuron-activation-analysis"
      ]
    },
    {
      "slug": "attention-visualisation-in-transformers",
      "name": "Attention Visualisation in Transformers",
      "description": "Attention Visualisation in Transformers analyses the multi-head self-attention mechanisms that enable transformers to process sequences by attending to different positions simultaneously. The technique visualises attention weights as heatmaps showing how strongly each token attends to every other token across different heads and layers. By examining these attention patterns, practitioners can understand how models like BERT, GPT, and T5 build contextual representations, identify which tokens influence predictions most strongly, and detect potential biases in how the model processes different types of input. This provides insights into positional encoding effects, head specialisation patterns, and the evolution of attention from local to global context across layers.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/feature-analysis",
        "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "technique-type/algorithmic",
        "assurance-goal-category/explainability/visualization-methods/attention-patterns",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/efficiency"
      ],
      "example_use_cases": [
        {
          "description": "Examining attention patterns in a medical language model processing clinical notes to verify it focuses on relevant symptoms and conditions rather than irrelevant demographic identifiers, revealing that certain attention heads specialise in medical terminology whilst others track syntactic relationships between diagnoses and treatments.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a sentiment analysis model for customer reviews by visualising how attention weights differ when processing reviews from different demographic groups, discovering that the model pays disproportionate attention to certain cultural expressions or colloquialisms that could lead to biased sentiment predictions.",
          "goal": "Fairness"
        },
        {
          "description": "Creating visual explanations for regulatory compliance in a financial document classification system, showing which specific words and phrases in loan applications or contracts triggered particular risk assessments, enabling auditors to verify that decisions are based on legitimate financial factors rather than discriminatory language patterns.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "High attention weights do not necessarily indicate causal importance for predictions, as models may attend strongly to tokens that serve structural rather than semantic purposes."
        },
        {
          "description": "The sheer number of attention heads and layers in modern transformers creates visualisation overload, making it difficult to identify meaningful patterns without systematic analysis tools."
        },
        {
          "description": "Attention patterns can be misleading when models use residual connections and layer normalisation, as the final representation incorporates information beyond what attention weights suggest."
        },
        {
          "description": "Different transformer architectures (encoder-only, decoder-only, encoder-decoder) exhibit fundamentally different attention patterns, limiting the generalisability of insights across model types."
        },
        {
          "description": "The technique cannot explain the reasoning process within feed-forward layers or how attention patterns translate into specific predictions, providing only a partial view of model behaviour."
        }
      ],
      "resources": [
        {
          "title": "jessevig/bertviz",
          "url": "https://github.com/jessevig/bertviz",
          "source_type": "software_package",
          "description": "Interactive tool for visualising attention patterns in transformer language models including BERT, GPT-2, and T5"
        },
        {
          "title": "Attention is All You Need",
          "url": "https://arxiv.org/abs/1706.03762",
          "source_type": "technical_paper",
          "description": "Foundational paper introducing the transformer architecture and self-attention mechanism"
        },
        {
          "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
          "url": "https://arxiv.org/abs/1905.09418",
          "source_type": "technical_paper",
          "description": "Research showing how different attention heads specialise in distinct linguistic phenomena"
        },
        {
          "title": "What Does BERT Look At? An Analysis of BERT's Attention",
          "url": "https://arxiv.org/abs/1906.04341",
          "source_type": "technical_paper",
          "description": "Comprehensive analysis of attention patterns in BERT revealing syntactic and semantic specialisation"
        },
        {
          "title": "Transformer Explainability Beyond Attention Visualization",
          "url": "https://arxiv.org/abs/2012.09838",
          "source_type": "technical_paper",
          "description": "Methods for attribution beyond raw attention weights including relevancy propagation and gradient-based approaches"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "integrated-gradients",
        "layer-wise-relevance-propagation",
        "saliency-maps",
        "gradient-weighted-class-activation-mapping",
        "classical-attention-analysis-in-neural-networks",
        "contrastive-explanation-method"
      ]
    }
  ],
  "count": 14
}