{
  "tag": {
    "name": "applicable-models/llm",
    "slug": "applicable-models-llm",
    "count": 4,
    "category": "applicable-models"
  },
  "techniques": [
    {
      "slug": "neuron-activation-analysis",
      "name": "Neuron Activation Analysis",
      "description": "Neuron activation analysis examines the firing patterns of individual neurons in neural networks by probing them with diverse inputs and analysing their activation responses. This technique helps understand what concepts, features, or patterns different neurons have learned to recognise, providing insights into the model's internal representations. For large language models, this can reveal neurons specialised for linguistic concepts, semantic categories, or even potentially harmful patterns, enabling targeted interventions and deeper model understanding.",
      "assurance_goals": [
        "Explainability",
        "Safety",
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/llm",
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/safety",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "evidence-type/visualisation",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "lifecycle-stage/monitoring",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing GPT-based models to identify specific neurons that activate on toxic or harmful content, enabling targeted interventions to reduce model toxicity whilst preserving general language capabilities for safer AI deployment.",
          "goal": "Safety"
        },
        {
          "description": "Examining activation patterns in multilingual language models to detect neurons that exhibit systematic biases when processing text from different linguistic communities, revealing implicit representation inequalities that could affect downstream applications.",
          "goal": "Fairness"
        },
        {
          "description": "Investigating individual neurons in medical language models to understand which clinical concepts and medical knowledge representations drive diagnostic suggestions, enabling healthcare professionals to validate the model's medical reasoning pathways.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Many neurons exhibit polysemantic behaviour, representing multiple unrelated concepts simultaneously, making it difficult to assign clear interpretable meanings to individual neural units."
        },
        {
          "description": "Important model behaviours are often distributed across many neurons rather than localised in single units, requiring analysis of neural circuits and interactions that can be exponentially complex."
        },
        {
          "description": "Computational costs scale dramatically with modern large language models containing billions of parameters, making comprehensive neuron-by-neuron analysis prohibitively expensive for complete model understanding."
        },
        {
          "description": "Neuron activation patterns are highly context-dependent, with the same neuron potentially serving different roles based on surrounding input context, complicating consistent interpretation across diverse scenarios."
        },
        {
          "description": "Interpretation of activation patterns often relies on subjective human analysis without rigorous validation methods, potentially leading to confirmation bias or misattribution of neural functions."
        }
      ],
      "resources": [
        {
          "title": "jalammar/ecco",
          "url": "https://github.com/jalammar/ecco",
          "source_type": "software_package"
        },
        {
          "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models",
          "url": "http://arxiv.org/pdf/2504.21053v1",
          "source_type": "technical_paper",
          "authors": [
            "Yi Zhou",
            "Wenpeng Xing",
            "Dezhang Kong",
            "Changting Lin",
            "Meng Han"
          ],
          "publication_date": "2025-04-29"
        },
        {
          "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron Activation Analysis",
          "url": "http://arxiv.org/pdf/2404.13567v1",
          "source_type": "technical_paper",
          "authors": [
            "Abhilekha Dalal",
            "Rushrukh Rayan",
            "Adrita Barua",
            "Eugene Y. Vasserman",
            "Md Kamruzzaman Sarker",
            "Pascal Hitzler"
          ],
          "publication_date": "2024-04-21"
        },
        {
          "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron\n  Activation Analysis",
          "url": "http://arxiv.org/abs/2404.13567",
          "source_type": "technical_paper",
          "authors": [
            "Barua, Adrita",
            "Dalal, Abhilekha",
            "Hitzler, Pascal",
            "Rayan, Rushrukh",
            "Sarker, Md Kamruzzaman",
            "Vasserman, Eugene Y."
          ],
          "publication_date": "2024-04-21"
        },
        {
          "title": "Ecco",
          "url": "https://ecco.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "Tracing the Thoughts in Language Models",
          "url": "https://www.anthropic.com/news/tracing-thoughts-language-model",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "prototype-and-criticism-models",
        "concept-activation-vectors"
      ]
    },
    {
      "slug": "prompt-sensitivity-analysis",
      "name": "Prompt Sensitivity Analysis",
      "description": "Prompt Sensitivity Analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. This technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. It encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. The analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/llm",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "expertise-needed/linguistics",
        "expertise-needed/experimental-design",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "technique-type/experimental"
      ],
      "example_use_cases": [
        {
          "description": "Testing medical diagnosis LLMs with semantically equivalent but syntactically different symptom descriptions to ensure consistent diagnostic recommendations across different patient communication styles, identifying potential failure modes where slight phrasing changes could lead to dangerous misdiagnoses.",
          "goal": "Safety"
        },
        {
          "description": "Analysing how variations in candidate descriptions (gendered language, cultural references, educational institution prestige indicators) affect LLM-based CV screening recommendations to identify potential discriminatory patterns and ensure equitable treatment across diverse applicant backgrounds.",
          "goal": "Reliability"
        },
        {
          "description": "Examining how different ways of framing financial questions (formal vs informal language, technical vs layperson terminology) affect investment advice generated by LLMs to improve user understanding and model transparency whilst ensuring consistent advisory quality.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Analysis is inherently limited to the specific prompt variations tested, potentially missing important sensitivity patterns that weren't anticipated during study design, making comprehensive coverage challenging."
        },
        {
          "description": "Systematic exploration of prompt variations can be computationally expensive, particularly for large-scale sensitivity analysis across multiple dimensions of variation, requiring significant resources for thorough evaluation."
        },
        {
          "description": "Ensuring that prompt variations maintain semantic equivalence whilst introducing meaningful perturbations requires careful linguistic expertise and validation, which can be subjective and domain-dependent."
        },
        {
          "description": "Results may reveal sensitivity patterns that are difficult to interpret or act upon, particularly when multiple types of variations interact in complex ways, limiting practical applicability of findings."
        },
        {
          "description": "The meaningfulness of sensitivity measurements depends heavily on the choice of baseline prompts and variation strategies, which can introduce methodological biases and affect the generalisability of conclusions."
        }
      ],
      "resources": [
        {
          "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
          "url": "https://www.semanticscholar.org/paper/17a6116e5bbd8b87082cbb2e795885567300c483",
          "source_type": "technical_paper",
          "authors": [
            "Melanie Sclar",
            "Yejin Choi",
            "Yulia Tsvetkov",
            "Alane Suhr"
          ]
        },
        {
          "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts",
          "url": "http://arxiv.org/pdf/2505.12592v1",
          "source_type": "technical_paper",
          "authors": [
            "Sullam Jeoung",
            "Yueyan Chen",
            "Yi Zhang",
            "Shuai Wang",
            "Haibo Ding",
            "Lin Lee Cheong"
          ],
          "publication_date": "2025-05-19"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "causal-mediation-analysis-in-language-models",
        "feature-attribution-with-integrated-gradients-in-nlp"
      ]
    },
    {
      "slug": "causal-mediation-analysis-in-language-models",
      "name": "Causal Mediation Analysis in Language Models",
      "description": "Causal mediation analysis in language models is a mechanistic interpretability technique that systematically investigates how specific internal components (neurons, attention heads, or layers) causally contribute to model outputs. By performing controlled interventions—such as activating, deactivating, or modifying specific components—researchers can trace the causal pathways through which information flows and transforms within the model. This approach goes beyond correlation to establish causal relationships, enabling researchers to understand not just what features influence outputs, but how and why they do so through specific computational pathways.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/llm",
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "evidence-type/causal-analysis",
        "expertise-needed/causal-inference",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/post-deployment",
        "technique-type/mechanistic-interpretability"
      ],
      "example_use_cases": [
        {
          "description": "Investigating causal pathways in content moderation models to understand how specific attention mechanisms contribute to flagging potentially harmful content, enabling verification that safety decisions rely on appropriate features rather than spurious correlations and ensuring robust content filtering.",
          "goal": "Safety"
        },
        {
          "description": "Identifying specific neurons or attention heads that causally contribute to biased outputs in hiring or lending language models, enabling targeted interventions to reduce discriminatory behaviour whilst preserving model performance on legitimate tasks and ensuring fair treatment across demographics.",
          "goal": "Reliability"
        },
        {
          "description": "Tracing causal pathways in large language models performing mathematical reasoning tasks to understand how intermediate steps are computed and stored, revealing which components are responsible for different aspects of logical inference and enabling validation of reasoning processes.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Requires sophisticated understanding of model architecture to design meaningful interventions, as poorly chosen intervention points may yield misleading causal conclusions or fail to capture relevant computational pathways."
        },
        {
          "description": "Results are highly dependent on the validity of underlying causal assumptions, which can be difficult to verify in complex, high-dimensional neural network spaces where multiple causal pathways may interact."
        },
        {
          "description": "Comprehensive causal analysis requires extensive computational resources, particularly for large models, as each intervention requires separate forward passes and multiple intervention combinations for robust conclusions."
        },
        {
          "description": "Distinguishing between direct causal effects and indirect effects mediated through other components can be challenging, potentially leading to oversimplified causal narratives that miss important intermediate processes."
        },
        {
          "description": "Causal relationships identified in specific contexts or datasets may not generalise to different domains, tasks, or model versions, requiring careful validation across diverse scenarios to ensure robust findings."
        }
      ],
      "resources": [],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "prompt-sensitivity-analysis",
        "feature-attribution-with-integrated-gradients-in-nlp"
      ]
    },
    {
      "slug": "feature-attribution-with-integrated-gradients-in-nlp",
      "name": "Feature Attribution with Integrated Gradients in NLP",
      "description": "Applies Integrated Gradients to natural language processing models to attribute prediction importance to individual input tokens, words, or subword units. This technique computes gradients along a straight-line path from a baseline input (typically all-zeros, padding tokens, or neutral text) to the actual input, integrating these gradients to obtain attribution scores. Unlike vanilla gradient methods, Integrated Gradients satisfies axioms of sensitivity and implementation invariance, making it particularly valuable for understanding transformer-based language models where token interactions are complex.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Safety"
      ],
      "tags": [
        "applicable-models/transformer",
        "applicable-models/llm",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "evidence-type/visualisation",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "technique-type/gradient-based"
      ],
      "example_use_cases": [
        {
          "description": "In a clinical decision support system processing doctor's notes to predict patient risk, Integrated Gradients identifies which medical terms, symptoms, or phrases most strongly influence risk predictions, enabling clinicians to verify that the model focuses on clinically relevant information rather than spurious correlations and supporting regulatory compliance in healthcare AI.",
          "goal": "Safety"
        },
        {
          "description": "For automated loan approval systems processing free-text application descriptions, Integrated Gradients reveals which words or phrases drive acceptance decisions, supporting fairness audits by highlighting whether protected characteristics inadvertently influence decisions and enabling transparent explanations to customers about application outcomes.",
          "goal": "Fairness"
        },
        {
          "description": "In content moderation systems flagging potentially harmful posts, Integrated Gradients identifies which specific words or linguistic patterns trigger safety classifications, enabling platform teams to debug false positives and validate that models focus on genuinely problematic language rather than demographic markers.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Computational overhead scales significantly with document length as processing requires computing gradients across many integration steps (typically 20-300), making real-time applications or large-scale document processing challenging."
        },
        {
          "description": "Choice of baseline input (zero embeddings, padding tokens, neutral text, or average embeddings) substantially affects attribution results, but optimal baseline selection remains domain-specific and often requires extensive experimentation."
        },
        {
          "description": "In transformer models with attention mechanisms, importance often spreads across many tokens, making it difficult to identify clear, actionable insights, especially for complex reasoning tasks where multiple tokens contribute collectively."
        },
        {
          "description": "Modern NLP models use subword tokenisation (BPE, WordPiece), making attribution results difficult to interpret at the word level, as single words may split across multiple tokens with varying attribution scores."
        },
        {
          "description": "While Integrated Gradients identifies correlative relationships between tokens and predictions, it cannot establish causal relationships or distinguish between spurious correlations and meaningful semantic dependencies in the input text."
        }
      ],
      "resources": [
        {
          "title": "Captum: Model Interpretability for PyTorch",
          "url": "https://captum.ai/",
          "source_type": "software_package",
          "description": "Open-source PyTorch library implementing Integrated Gradients with multi-modal support including text, featuring easy integration with transformer models and comprehensive NLP tutorials (BERT SQUAD, IMDB classification, Llama2 attribution)."
        },
        {
          "title": "Axiomatic Attribution for Deep Networks",
          "url": "https://arxiv.org/abs/1703.01365",
          "source_type": "technical_paper",
          "authors": [
            "Mukund Sundararajan",
            "Ankur Taly",
            "Qiqi Yan"
          ],
          "publication_date": "2017-03-19",
          "description": "Original paper introducing Integrated Gradients method with fundamental axioms of sensitivity and implementation invariance, demonstrating applications across text models and providing theoretical foundations for attribution methods."
        },
        {
          "title": "The Building Blocks of Interpretability",
          "url": "https://distill.pub/2020/attribution-baselines/",
          "source_type": "tutorial",
          "description": "Interactive Distill article providing comprehensive guidance on baseline selection for Integrated Gradients, exploring different baseline types and their impact on feature attribution quality with transferable principles for NLP applications."
        },
        {
          "title": "transformers-interpret",
          "url": "https://github.com/cdpierse/transformers-interpret",
          "source_type": "software_package",
          "description": "Model explainability library designed for Hugging Face transformers, enabling transformer model explanation in two lines of code with HTML visualisations and support for sequence classification, multi-label classification, and computer vision models."
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "prompt-sensitivity-analysis",
        "causal-mediation-analysis-in-language-models"
      ]
    }
  ],
  "count": 4
}