{
  "slug": "individual-conditional-expectation-plots",
  "name": "Individual Conditional Expectation Plots",
  "description": "ICE plots display the predicted output for individual instances as a function of a feature, with all other features held fixed for each instance. Each line on an ICE plot represents one instance's prediction trajectory as the feature of interest changes, revealing whether different instances are affected differently by that feature.",
  "assurance_goals": [
    "Explainability"
  ],
  "tags": [
    "applicable-models/agnostic",
    "assurance-goal-category/explainability",
    "assurance-goal-category/explainability/visualization-methods/feature-relationships",
    "assurance-goal-category/explainability/explains/feature-importance",
    "assurance-goal-category/explainability/explains/data-patterns",
    "assurance-goal-category/explainability/property/fidelity",
    "data-requirements/no-special-requirements",
    "data-type/any",
    "evidence-type/visualization",
    "expertise-needed/low",
    "explanatory-scope/local",
    "explanatory-scope/global",
    "lifecycle-stage/model-development",
    "technique-type/visualization"
  ],
  "example_use_cases": [
    {
      "description": "Examining how house price predictions vary with property age for individual properties, revealing that whilst most houses follow a declining price trend with age, historic properties (built before 1900) show different patterns due to heritage value.",
      "goal": "Explainability"
    },
    {
      "description": "Analysing how individual patients' diabetes risk predictions change with BMI, showing that whilst most patients follow the expected increasing risk pattern, some patients with specific genetic markers show different response curves.",
      "goal": "Explainability"
    }
  ],
  "limitations": [
    {
      "description": "Plots can become cluttered and difficult to interpret when displaying many instances simultaneously."
    },
    {
      "description": "Does not provide automatic summarisation of overall effects, requiring manual visual inspection to identify patterns."
    },
    {
      "description": "Still assumes all other features remain fixed at their observed values, which may not reflect realistic scenarios."
    },
    {
      "description": "Cannot reveal interactions between the plotted feature and other features for individual instances."
    }
  ],
  "resources": [
    {
      "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation",
      "url": "http://arxiv.org/pdf/1309.6392v2",
      "source_type": "technical_paper",
      "authors": [
        "Alex Goldstein",
        "Adam Kapelner",
        "Justin Bleich",
        "Emil Pitkin"
      ],
      "publication_date": "2013-09-25"
    },
    {
      "title": "Bringing a Ruler Into the Black Box: Uncovering Feature Impact from Individual Conditional Expectation Plots",
      "url": "http://arxiv.org/pdf/2109.02724v1",
      "source_type": "technical_paper",
      "authors": [
        "Andrew Yeh",
        "Anhthy Ngo"
      ],
      "publication_date": "2021-09-06"
    },
    {
      "title": "Explainable AI(XAI) - A guide to 7 packages in Python to explain ...",
      "url": "https://towardsdatascience.com/explainable-ai-xai-a-guide-to-7-packages-in-python-to-explain-your-models-932967f0634b/",
      "source_type": "tutorial"
    },
    {
      "title": "Communicating Uncertainty in Machine Learning Explanations: A Visualization Analytics Approach for Predictive Process Monitoring",
      "url": "https://www.semanticscholar.org/paper/3d0090df2b73369b502559eb49fd6d1ae432b952",
      "source_type": "technical_paper",
      "authors": [
        "Nijat Mehdiyev",
        "Maxim Majlatow",
        "Peter Fettke"
      ]
    }
  ],
  "complexity_rating": 2,
  "computational_cost_rating": 2,
  "acronym": "ICE",
  "related_techniques": [
    "partial-dependence-plots"
  ]
}