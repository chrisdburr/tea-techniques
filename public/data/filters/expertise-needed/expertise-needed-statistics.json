{
  "tag": {
    "name": "expertise-needed/statistics",
    "slug": "expertise-needed-statistics",
    "count": 49,
    "category": "expertise-needed"
  },
  "techniques": [
    {
      "slug": "shapley-additive-explanations",
      "name": "SHapley Additive exPlanations",
      "description": "SHAP explains model predictions by quantifying how much each input feature contributes to the outcome. It assigns an importance score to every feature, indicating whether it pushes the prediction towards or away from the average. The method systematically evaluates how predictions change as features are included or excluded, drawing on game theory concepts to ensure a fair distribution of contributions.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/feature-analysis",
        "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing a customer churn prediction model to understand why a specific high-value customer was flagged as likely to leave, revealing that recent support ticket interactions and declining purchase frequency were the main drivers.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a loan approval model by comparing SHAP values for applicants from different demographic groups, ensuring that protected characteristics like race or gender do not have an undue influence on credit decisions.",
          "goal": "Fairness"
        },
        {
          "description": "Validating a medical diagnosis model by confirming that its predictions are based on relevant clinical features (e.g., blood pressure, cholesterol levels) rather than spurious correlations (e.g., patient ID or appointment time), thereby improving model reliability.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Assumes feature independence, which can produce misleading explanations when features are highly correlated, as the model may attribute importance to features that are merely proxies for others."
        },
        {
          "description": "Computationally expensive for models with many features or large datasets, as the number of required predictions grows exponentially with the number of features."
        },
        {
          "description": "The choice of background dataset for generating explanations can significantly influence the results, requiring careful selection to ensure a representative baseline."
        },
        {
          "description": "Global explanations derived from averaging local SHAP values may obscure important heterogeneous effects where features impact subgroups of the population differently."
        }
      ],
      "resources": [
        {
          "title": "shap/shap",
          "url": "https://github.com/shap/shap",
          "source_type": "software_package"
        },
        {
          "title": "Introduction to SHapley Additive exPlanations (SHAP) — XAI Tutorials",
          "url": "https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/shap.html",
          "source_type": "tutorial"
        },
        {
          "title": "An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models",
          "url": "http://arxiv.org/pdf/2204.11351v3",
          "source_type": "technical_paper",
          "authors": [
            "Han Yuan",
            "Mingxuan Liu",
            "Lican Kang",
            "Chenkui Miao",
            "Ying Wu"
          ],
          "publication_date": "2022-04-24"
        },
        {
          "title": "SHAP: Shapley Additive Explanations | Towards Data Science",
          "url": "https://towardsdatascience.com/shap-shapley-additive-explanations-5a2a271ed9c3/",
          "source_type": "tutorial"
        },
        {
          "title": "MAIF/shapash",
          "url": "https://github.com/MAIF/shapash",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "acronym": "SHAP",
      "related_techniques": [
        "integrated-gradients",
        "deeplift",
        "layer-wise-relevance-propagation",
        "local-interpretable-model-agnostic-explanations",
        "contrastive-explanation-method",
        "anchor"
      ]
    },
    {
      "slug": "permutation-importance",
      "name": "Permutation Importance",
      "description": "Permutation Importance quantifies a feature's contribution to a model's performance by randomly shuffling its values and measuring the resulting drop in predictive accuracy. If shuffling a feature significantly degrades the model's performance, that feature is considered important. This model-agnostic technique helps identify which inputs are genuinely driving predictions, rather than just being correlated with the outcome.",
      "assurance_goals": [
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/feature-analysis",
        "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Assessing which patient characteristics (e.g., age, blood pressure, cholesterol) are most critical for a medical diagnosis model by observing the performance drop when each characteristic's values are randomly shuffled, ensuring the model relies on clinically relevant factors.",
          "goal": "Explainability"
        },
        {
          "description": "Validating the robustness of a fraud detection model by permuting features like transaction amount or location, and confirming that the model's ability to detect fraud significantly decreases only for truly important features, thereby improving confidence in its reliability.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Can be misleading when features are highly correlated, as shuffling one feature might indirectly affect others, leading to an overestimation of its importance."
        },
        {
          "description": "Computationally expensive for large datasets or complex models, as it requires re-evaluating the model many times for each feature."
        },
        {
          "description": "Does not account for interactions between features; it measures the marginal importance of a feature, assuming other features remain unchanged."
        },
        {
          "description": "The choice of metric for evaluating performance drop (e.g., accuracy, F1-score) can influence the perceived importance of features."
        }
      ],
      "resources": [
        {
          "title": "Asymptotic Unbiasedness of the Permutation Importance Measure in Random Forest Models",
          "url": "http://arxiv.org/pdf/1912.03306v1",
          "source_type": "technical_paper",
          "authors": [
            "Burim Ramosaj",
            "Markus Pauly"
          ],
          "publication_date": "2019-12-05"
        },
        {
          "title": "eli5.permutation_importance — ELI5 0.15.0 documentation",
          "url": "https://eli5.readthedocs.io/en/latest/autodocs/permutation_importance.html",
          "source_type": "documentation"
        },
        {
          "title": "Permutation Importance — PermutationImportance 1.2.1.5 ...",
          "url": "https://permutationimportance.readthedocs.io/en/latest/permutation.html",
          "source_type": "documentation"
        },
        {
          "title": "parrt/random-forest-importances",
          "url": "https://github.com/parrt/random-forest-importances",
          "source_type": "software_package"
        },
        {
          "title": "Statistically Valid Variable Importance Assessment through Conditional Permutations",
          "url": "http://arxiv.org/pdf/2309.07593v2",
          "source_type": "technical_paper",
          "authors": [
            "Ahmad Chamma",
            "Denis A. Engemann",
            "Bertrand Thirion"
          ],
          "publication_date": "2023-09-14"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 3,
      "related_techniques": [
        "mean-decrease-impurity",
        "coefficient-magnitudes-in-linear-models",
        "sobol-indices"
      ]
    },
    {
      "slug": "mean-decrease-impurity",
      "name": "Mean Decrease Impurity",
      "description": "Mean Decrease Impurity (MDI) quantifies a feature's importance in tree-based models (e.g., Random Forests, Gradient Boosting Machines) by measuring the total reduction in impurity (e.g., Gini impurity, entropy) across all splits where the feature is used. Features that lead to larger, more consistent reductions in impurity are considered more important, indicating their effectiveness in creating homogeneous child nodes and improving predictive accuracy.",
      "assurance_goals": [
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/tree-based",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Determining the most influential genetic markers in a decision tree model predicting disease susceptibility, by identifying which markers consistently lead to the purest splits between healthy and diseased patient groups.",
          "goal": "Explainability"
        },
        {
          "description": "Assessing the key factors driving customer purchasing decisions in an e-commerce random forest model, revealing which product attributes or customer demographics are most effective in segmenting buyers.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "MDI is inherently biased towards features with more unique values or those that allow for more splits, potentially overestimating their true importance."
        },
        {
          "description": "It is only applicable to tree-based models and cannot be directly used with other model architectures."
        },
        {
          "description": "The importance scores can be unstable, varying significantly with small changes in the training data or model parameters."
        },
        {
          "description": "MDI does not account for feature interactions, meaning it might not accurately reflect the importance of features that are only relevant when combined with others."
        }
      ],
      "resources": [
        {
          "title": "Trees, forests, and impurity-based variable importance",
          "url": "http://arxiv.org/pdf/2001.04295v3",
          "source_type": "technical_paper",
          "authors": [
            "Erwan Scornet"
          ],
          "publication_date": "2020-01-13"
        },
        {
          "title": "A Debiased MDI Feature Importance Measure for Random Forests",
          "url": "http://arxiv.org/pdf/1906.10845v2",
          "source_type": "technical_paper",
          "authors": [
            "Xiao Li",
            "Yu Wang",
            "Sumanta Basu",
            "Karl Kumbier",
            "Bin Yu"
          ],
          "publication_date": "2019-06-26"
        },
        {
          "title": "Variable Importance in Random Forests | Towards Data Science",
          "url": "https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0/",
          "source_type": "tutorial"
        },
        {
          "title": "Interpreting Deep Forest through Feature Contribution and MDI Feature Importance",
          "url": "http://arxiv.org/pdf/2305.00805v1",
          "source_type": "technical_paper",
          "authors": [
            "Yi-Xiao He",
            "Shen-Huan Lyu",
            "Yuan Jiang"
          ],
          "publication_date": "2023-05-01"
        },
        {
          "title": "optuna.importance.MeanDecreaseImpurityImportanceEvaluator ...",
          "url": "https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.MeanDecreaseImpurityImportanceEvaluator.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "permutation-importance",
        "coefficient-magnitudes-in-linear-models",
        "sobol-indices"
      ]
    },
    {
      "slug": "integrated-gradients",
      "name": "Integrated Gradients",
      "description": "Integrated Gradients is an attribution technique that explains a model's prediction by quantifying the contribution of each input feature. It works by accumulating gradients along a straight path from a user-defined baseline input (e.g., a black image or an all-zero vector) to the actual input. This path integral ensures that the attributions satisfy fundamental axioms like completeness (attributions sum up to the difference between the prediction and the baseline prediction) and sensitivity (non-zero attributions for features that change the prediction). The output is a set of importance scores, often visualised as heatmaps, indicating which parts of the input were most influential for the model's decision.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "data-requirements/labelled-data",
        "data-requirements/reference-dataset",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing a medical image classification model to understand which specific pixels or regions in an X-ray image contribute most to a diagnosis of pneumonia, ensuring the model focuses on relevant pathological features rather than artifacts.",
          "goal": "Explainability"
        },
        {
          "description": "Explaining the sentiment prediction of a natural language processing model by highlighting which words or phrases in a review most strongly influenced its classification as positive or negative, revealing the model's interpretative focus.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Requires a carefully chosen and meaningful baseline input; an inappropriate baseline can lead to misleading or uninformative attributions."
        },
        {
          "description": "The model must be differentiable, which limits its direct application to models with non-differentiable components or discrete inputs without workarounds."
        },
        {
          "description": "Computationally more expensive than simple gradient-based methods, as it requires multiple gradient calculations along the integration path."
        },
        {
          "description": "While satisfying completeness, the attributions can sometimes be visually noisy or difficult for humans to interpret intuitively, especially for complex inputs."
        }
      ],
      "resources": [
        {
          "title": "ankurtaly/Integrated-Gradients",
          "url": "https://github.com/ankurtaly/Integrated-Gradients",
          "source_type": "software_package"
        },
        {
          "title": "pytorch/captum",
          "url": "https://github.com/pytorch/captum",
          "source_type": "software_package"
        },
        {
          "title": "Maximum Entropy Baseline for Integrated Gradients",
          "url": "http://arxiv.org/pdf/2204.05948v1",
          "source_type": "technical_paper",
          "authors": [
            "Hanxiao Tan"
          ],
          "publication_date": "2022-04-12"
        },
        {
          "title": "Integrated Gradients from Scratch | Towards Data Science",
          "url": "https://towardsdatascience.com/integrated-gradients-from-scratch-b46311e4ab4/",
          "source_type": "tutorial"
        },
        {
          "title": "Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution",
          "url": "http://arxiv.org/pdf/2004.10484v2",
          "source_type": "technical_paper",
          "authors": [
            "Gary S. W. Goh",
            "Sebastian Lapuschkin",
            "Leander Weber",
            "Wojciech Samek",
            "Alexander Binder"
          ],
          "publication_date": "2020-04-22"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "shapley-additive-explanations",
        "deeplift",
        "layer-wise-relevance-propagation",
        "local-interpretable-model-agnostic-explanations",
        "contrastive-explanation-method",
        "anchor"
      ]
    },
    {
      "slug": "sobol-indices",
      "name": "Sobol Indices",
      "description": "Sobol Indices quantify how much each input feature contributes to the total variance in a model's predictions through global sensitivity analysis. The technique calculates first-order indices (individual feature contributions) and total-order indices (including all interaction effects involving that feature). By systematically sampling the input space and decomposing output variance, Sobol Indices reveal which features drive model uncertainty and which interactions between features are most important for predictions.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing a climate prediction model to determine which atmospheric parameters (temperature, humidity, pressure) contribute most to rainfall forecast uncertainty, helping meteorologists understand which measurements need the highest precision.",
          "goal": "Explainability"
        },
        {
          "description": "Evaluating a financial risk model to identify which economic indicators (interest rates, inflation, GDP growth) drive the most variability in portfolio value predictions, enabling better risk management strategies.",
          "goal": "Explainability"
        },
        {
          "description": "Analysing a credit scoring model to quantify how much prediction variance stems from zip code (a potential proxy for race), helping identify features that may cause disparate impact across demographic groups.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive, requiring thousands of model evaluations to achieve stable variance estimates, making it impractical for very slow models."
        },
        {
          "description": "Assumes input features are independently distributed, which can lead to misleading results when features are correlated in real data."
        },
        {
          "description": "Curse of dimensionality makes the technique increasingly difficult and expensive to apply as the number of input features grows beyond 10-20."
        },
        {
          "description": "Requires defining appropriate probability distributions for input features, which may not accurately reflect real-world feature distributions."
        }
      ],
      "resources": [
        {
          "title": "Sobol Tensor Trains for Global Sensitivity Analysis",
          "url": "http://arxiv.org/pdf/1712.00233v1",
          "source_type": "technical_paper",
          "authors": [
            "Rafael Ballester-Ripoll",
            "Enrique G. Paredes",
            "Renato Pajarola"
          ],
          "publication_date": "2017-12-01"
        },
        {
          "title": "Sobol indices — UQpy v4.2.0 documentation",
          "url": "https://uqpyproject.readthedocs.io/en/latest/sensitivity/sobol.html",
          "source_type": "documentation"
        },
        {
          "title": "Sobol Indices to Measure Feature Importance | Towards Data Science",
          "url": "https://towardsdatascience.com/sobol-indices-to-measure-feature-importance-54cedc3281bc/",
          "source_type": "tutorial"
        },
        {
          "title": "Basics — SALib's documentation",
          "url": "https://salib.readthedocs.io/en/latest/user_guide/basics.html",
          "source_type": "documentation"
        },
        {
          "title": "UQpy (Uncertainty Quantification with python)",
          "url": "https://github.com/SURGroup/UQpy",
          "source_type": "software_package"
        },
        {
          "title": "SALib/SALib",
          "url": "https://github.com/SALib/SALib",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 5,
      "related_techniques": [
        "permutation-importance",
        "mean-decrease-impurity",
        "coefficient-magnitudes-in-linear-models"
      ]
    },
    {
      "slug": "factor-analysis",
      "name": "Factor Analysis",
      "description": "Factor analysis is a statistical technique that identifies latent variables (hidden factors) underlying observed correlations in data. It works by analysing how variables relate to each other, finding a smaller number of unobserved factors that explain patterns among multiple observed variables. Unlike PCA which maximises total variance, factor analysis focuses on shared variance (communalities - the variance variables have in common) whilst separating out unique variance and measurement error. After extracting factors, rotation methods like varimax (which creates uncorrelated factors) or oblimin (allowing correlated factors) help make factors more interpretable by aligning them with distinct groups of variables.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/structured-output",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing customer satisfaction surveys to identify key drivers (e.g., 'service quality', 'product value', 'convenience') from dozens of individual questions, helping businesses focus improvement efforts.",
          "goal": "Explainability"
        },
        {
          "description": "Reducing dimensionality of financial indicators to identify underlying economic factors (e.g., 'growth', 'inflation', 'credit risk') for more interpretable risk models.",
          "goal": "Explainability"
        },
        {
          "description": "Creating transparent feature groups for regulatory reporting by showing how multiple correlated features can be summarised into interpretable factors with clear business meaning.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Assumes linear relationships between variables and multivariate normality of data."
        },
        {
          "description": "Results can be abstract and require domain expertise to interpret meaningfully."
        },
        {
          "description": "Sensitive to the choice of number of factors and rotation method, which can significantly affect interpretability."
        },
        {
          "description": "Requires sufficiently large sample sizes relative to the number of variables for stable results."
        }
      ],
      "resources": [
        {
          "title": "Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey",
          "url": "http://arxiv.org/pdf/2101.00734v2",
          "source_type": "documentation",
          "authors": [
            "Benyamin Ghojogh",
            "Ali Ghodsi",
            "Fakhri Karray",
            "Mark Crowley"
          ],
          "publication_date": "2021-01-04"
        },
        {
          "title": "Factor Analysis in R Course | DataCamp",
          "url": "https://www.datacamp.com/courses/factor-analysis-in-r",
          "source_type": "tutorial"
        },
        {
          "title": "EducationalTestingService/factor_analyzer",
          "url": "https://github.com/EducationalTestingService/factor_analyzer",
          "source_type": "software_package"
        },
        {
          "title": "Confirmatory Factor Analysis Fundamentals | Towards Data Science",
          "url": "https://towardsdatascience.com/confirmatory-factor-analysis-theory-aac11af008a6/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "principal-component-analysis",
        "t-sne",
        "umap"
      ]
    },
    {
      "slug": "principal-component-analysis",
      "name": "Principal Component Analysis",
      "description": "Principal Component Analysis transforms high-dimensional data into a lower-dimensional representation by finding the directions (principal components) that capture the maximum variance in the data. Each component is a linear combination of original features, with the first component explaining the most variance, the second component the most remaining variance orthogonal to the first, and so on. This technique reveals underlying patterns in data structure, enables visualization of complex datasets, and helps identify which combinations of features drive the most variation in the data.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/visualization",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing customer behavior data with dozens of variables (purchase frequency, spending patterns, demographics) to identify the 2-3 main dimensions that explain customer segmentation, revealing whether customers cluster by spending level, product preferences, or shopping frequency.",
          "goal": "Explainability"
        },
        {
          "description": "Reducing dimensionality of image data for facial recognition systems by finding the principal components that capture the most variation in face shapes and expressions, helping understand which facial features contribute most to distinguishing between individuals.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Principal components are abstract linear combinations of original features that often lack clear real-world interpretation or meaning."
        },
        {
          "description": "Only captures linear relationships between features, missing non-linear patterns and complex interactions in the data."
        },
        {
          "description": "Results are highly sensitive to feature scaling - features with larger numerical ranges can dominate the principal components."
        },
        {
          "description": "Information loss is inherent when reducing dimensions, and choosing the optimal number of components requires balancing simplicity with retained variance."
        }
      ],
      "resources": [
        {
          "title": "erdogant/pca",
          "url": "https://github.com/erdogant/pca",
          "source_type": "software_package"
        },
        {
          "title": "How to Calculate Principal Component Analysis (PCA) from Scratch ...",
          "url": "https://www.machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/",
          "source_type": "tutorial"
        },
        {
          "title": "A One-Stop Shop for Principal Component Analysis | Towards Data ...",
          "url": "https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c/",
          "source_type": "tutorial"
        },
        {
          "title": "Principal Component Analysis (PCA) with Scikit-Learn - KDnuggets",
          "url": "https://www.kdnuggets.com/2023/05/principal-component-analysis-pca-scikitlearn.html",
          "source_type": "tutorial"
        },
        {
          "title": "willtownes/glmpca-py",
          "url": "https://github.com/willtownes/glmpca-py",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "acronym": "PCA",
      "related_techniques": [
        "factor-analysis",
        "t-sne",
        "umap"
      ]
    },
    {
      "slug": "t-sne",
      "name": "t-SNE",
      "description": "t-SNE (t-Distributed Stochastic Neighbour Embedding) is a non-linear dimensionality reduction technique that creates 2D or 3D visualisations of high-dimensional data by preserving local neighbourhood relationships. The algorithm converts similarities between data points into joint probabilities in the high-dimensional space, then tries to minimise the divergence between these probabilities and those in the low-dimensional embedding. This approach excels at revealing cluster structures and local patterns, making it particularly effective for exploratory data analysis and understanding complex data relationships that linear methods like PCA might miss.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/visualization"
      ],
      "example_use_cases": [
        {
          "description": "Analyzing genomic data with thousands of gene expression features to visualize how different cancer subtypes cluster together, revealing which tumors have similar molecular signatures and potentially similar treatment responses.",
          "goal": "Explainability"
        },
        {
          "description": "Exploring deep learning model embeddings to understand how a neural network represents different categories of images, showing whether the model groups similar objects (cars, animals, furniture) in meaningful clusters in its internal feature space.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Non-deterministic algorithm produces different results on each run, making it difficult to reproduce exact visualizations or compare results across studies."
        },
        {
          "description": "Prioritizes preserving local neighborhood structure at the expense of global relationships, potentially creating misleading impressions about overall data topology."
        },
        {
          "description": "Computationally expensive with O(n²) complexity, making it impractical for datasets with more than ~10,000 points without approximation methods."
        },
        {
          "description": "Sensitive to hyperparameter choices (perplexity, learning rate, iterations) that can dramatically affect clustering patterns and require domain expertise to tune appropriately."
        }
      ],
      "resources": [
        {
          "title": "pavlin-policar/openTSNE",
          "url": "https://github.com/pavlin-policar/openTSNE",
          "source_type": "software_package"
        },
        {
          "title": "openTSNE: Extensible, parallel implementations of t-SNE ...",
          "url": "https://opentsne.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "How t-SNE works — openTSNE 1.0.0 documentation",
          "url": "https://opentsne.readthedocs.io/en/stable/tsne_algorithm.html",
          "source_type": "documentation"
        },
        {
          "title": "t-SNE from Scratch (ft. NumPy) | Towards Data Science",
          "url": "https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "related_techniques": [
        "factor-analysis",
        "principal-component-analysis",
        "umap"
      ]
    },
    {
      "slug": "umap",
      "name": "UMAP",
      "description": "UMAP (Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique that creates 2D or 3D visualisations of high-dimensional data by constructing a mathematical model of the data's underlying manifold structure. Unlike t-SNE, UMAP preserves both local neighbourhood relationships and global topology more effectively, using techniques from topological data analysis and Riemannian geometry. This approach often produces more interpretable cluster layouts while maintaining meaningful distances between clusters, making it particularly valuable for exploratory data analysis and understanding complex dataset structures.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/visualization"
      ],
      "example_use_cases": [
        {
          "description": "Analysing single-cell RNA sequencing data to visualise how different cell types cluster based on gene expression patterns, revealing developmental trajectories and identifying previously unknown cell subtypes in tissue samples.",
          "goal": "Explainability"
        },
        {
          "description": "Exploring customer segmentation by reducing hundreds of behavioural and demographic features to 2D space, showing how different customer groups relate to each other and identifying transition zones where customers might move between segments.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Hyperparameter choices (n_neighbors, min_dist, metric) significantly influence the embedding structure and can lead to very different interpretations of the same data."
        },
        {
          "description": "While preserving global structure better than t-SNE, distances in the reduced space still don't directly correspond to distances in the original feature space."
        },
        {
          "description": "Performance can be sensitive to the choice of distance metric, which may not be obvious for complex or mixed data types."
        },
        {
          "description": "Like other manifold learning techniques, it assumes the data lies on a lower-dimensional manifold, which may not hold for all datasets."
        }
      ],
      "resources": [
        {
          "title": "lmcinnes/umap",
          "url": "https://github.com/lmcinnes/umap",
          "source_type": "software_package"
        },
        {
          "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
          "url": "http://arxiv.org/pdf/1802.03426v3",
          "source_type": "technical_paper",
          "authors": [
            "Leland McInnes",
            "John Healy",
            "James Melville"
          ],
          "publication_date": "2018-02-09"
        },
        {
          "title": "Uniform Manifold Approximation and Projection (UMAP) and its Variants: Tutorial and Survey",
          "url": "http://arxiv.org/pdf/2109.02508v1",
          "source_type": "documentation",
          "authors": [
            "Benyamin Ghojogh",
            "Ali Ghodsi",
            "Fakhri Karray",
            "Mark Crowley"
          ],
          "publication_date": "2021-08-25"
        },
        {
          "title": "How UMAP Works — umap 0.5.8 documentation",
          "url": "https://umap-learn.readthedocs.io/en/latest/how_umap_works.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "factor-analysis",
        "principal-component-analysis",
        "t-sne"
      ]
    },
    {
      "slug": "anchor",
      "name": "ANCHOR",
      "description": "ANCHOR generates high-precision if-then rules that explain individual predictions by identifying the minimal set of feature conditions that guarantee a specific prediction with high confidence. It searches for 'anchor' conditions (e.g., 'age > 30 AND income < £50k') that ensure the model gives the same prediction at least 95% of the time when those conditions are met. This creates human-readable rules that users can trust as sufficient conditions for understanding why a particular decision was made.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Explaining loan application decisions with rules like 'IF credit_score > 650 AND debt_ratio < 0.4 THEN approval = 95% likely', giving applicants clear, actionable conditions they can understand and potentially improve.",
          "goal": "Explainability"
        },
        {
          "description": "Generating diagnostic rules for medical predictions such as 'IF fever > 38.5°C AND white_blood_cells > 12,000 THEN infection = 92% likely', helping clinicians validate automated diagnoses with trusted clinical indicators.",
          "goal": "Explainability"
        },
        {
          "description": "Creating transparent hiring decisions with rules like 'IF experience >= 3_years AND degree = relevant THEN hire = 89% likely', providing clear justification for recruitment decisions that can be audited for fairness.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Limited to local explanations for individual instances, cannot provide global insights about overall model behaviour."
        },
        {
          "description": "Requires discretisation of continuous features, which can lose important nuanced information and create arbitrary thresholds."
        },
        {
          "description": "May fail to find suitable anchor rules if precision requirements are too strict or if the prediction space is highly complex."
        },
        {
          "description": "Computationally expensive as it requires extensive sampling to validate rule precision, especially for high-dimensional data."
        }
      ],
      "resources": [
        {
          "title": "Anchors: High-Precision Model-Agnostic Explanations",
          "url": "https://homes.cs.washington.edu/~marcotcr/aaai18.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Marco Tulio Ribeiro",
            "Sameer Singh",
            "Carlos Guestrin"
          ],
          "publication_date": "2018-01-01"
        },
        {
          "title": "marcotcr/anchor",
          "url": "https://github.com/marcotcr/anchor",
          "source_type": "software_package"
        },
        {
          "title": "alibi/alibi",
          "url": "https://github.com/SeldonIO/alibi",
          "source_type": "software_package"
        },
        {
          "title": "Interpretable Machine Learning - Anchors",
          "url": "https://christophm.github.io/interpretable-ml-book/anchors.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "shapley-additive-explanations",
        "integrated-gradients",
        "deeplift",
        "layer-wise-relevance-propagation",
        "local-interpretable-model-agnostic-explanations",
        "contrastive-explanation-method"
      ]
    },
    {
      "slug": "rulefit",
      "name": "RuleFit",
      "description": "RuleFit is a method that creates an interpretable model by combining linear terms with decision rules. It first extracts potential rules from ensemble trees, then builds a sparse linear model where those rules (binary conditions) and original features are used as predictors, with regularization to keep the model simple. The final model is a linear combination of a small set of rules and original features, balancing interpretability with predictive power.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Building customer churn prediction models with rules like 'IF contract_length < 12_months AND support_calls > 5 THEN churn_risk = high', allowing marketing teams to understand and act on the key drivers of customer attrition.",
          "goal": "Explainability"
        },
        {
          "description": "Creating credit scoring models that combine traditional linear factors (income, age) with interpretable rules (IF recent_missed_payments = 0 AND account_age > 2_years THEN creditworthy), providing transparent lending decisions.",
          "goal": "Explainability"
        },
        {
          "description": "Developing regulatory-compliant medical diagnosis models where treatment recommendations combine clinical measurements with clear decision rules (IF blood_pressure > 140 AND diabetes = true THEN high_risk), enabling audit trails for healthcare decisions.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Can generate large numbers of rules even with regularisation, potentially overwhelming users and reducing practical interpretability."
        },
        {
          "description": "Performance may be inferior to complex ensemble methods when rule complexity is constrained for interpretability."
        },
        {
          "description": "Rule extraction quality depends heavily on the underlying tree ensemble, which may miss important feature interactions if not properly configured."
        },
        {
          "description": "Requires careful hyperparameter tuning to balance between model complexity and interpretability, with no universal optimal setting."
        }
      ],
      "resources": [
        {
          "title": "christophM/rulefit",
          "url": "https://github.com/christophM/rulefit",
          "source_type": "software_package"
        },
        {
          "title": "Tree Ensembles with Rule Structured Horseshoe Regularization",
          "url": "http://arxiv.org/pdf/1702.05008v2",
          "source_type": "technical_paper",
          "authors": [
            "Malte Nalenz",
            "Mattias Villani"
          ],
          "publication_date": "2017-02-16"
        },
        {
          "title": "Safe RuleFit: Learning Optimal Sparse Rule Model by Meta Safe Screening",
          "url": "http://arxiv.org/pdf/1810.01683v2",
          "source_type": "technical_paper",
          "authors": [
            "Hiroki Kato",
            "Hiroyuki Hanada",
            "Ichiro Takeuchi"
          ],
          "publication_date": "2018-10-03"
        },
        {
          "title": "csinva/imodels",
          "url": "https://github.com/csinva/imodels",
          "source_type": "software_package"
        },
        {
          "title": "Getting More From Regression Models with RuleFit | Towards Data ...",
          "url": "https://towardsdatascience.com/getting-more-from-regression-models-with-rulefit-2e6be8d77432/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "ridge-regression-surrogates",
        "model-distillation"
      ]
    },
    {
      "slug": "monte-carlo-dropout",
      "name": "Monte Carlo Dropout",
      "description": "Monte Carlo Dropout estimates prediction uncertainty by applying dropout (randomly setting neural network weights to zero) during inference rather than just training. It performs multiple forward passes through the network with different random dropout patterns and collects the resulting predictions to form a distribution. Low variance across predictions indicates epistemic certainty (the model is confident), while high variance suggests epistemic uncertainty (the model is unsure). This technique transforms any dropout-trained neural network into a Bayesian approximation for uncertainty quantification.",
      "assurance_goals": [
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Quantifying diagnostic uncertainty in medical imaging models by running 50+ Monte Carlo forward passes to detect when a chest X-ray classification is highly uncertain, prompting radiologist review for borderline cases.",
          "goal": "Reliability"
        },
        {
          "description": "Estimating prediction confidence in autonomous vehicle perception systems, where high uncertainty in object detection (e.g., variance > 0.3 across MC samples) triggers more conservative driving behaviour or human handover.",
          "goal": "Reliability"
        },
        {
          "description": "Providing uncertainty estimates in financial fraud detection models, where high epistemic uncertainty (wide prediction variance) indicates the model lacks sufficient training data for similar transaction patterns, requiring manual review.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Only captures epistemic (model) uncertainty, not aleatoric (data) uncertainty, providing an incomplete picture of total prediction uncertainty."
        },
        {
          "description": "Computationally expensive as it requires multiple forward passes (typically 50-100) for each prediction, significantly increasing inference time."
        },
        {
          "description": "Results depend critically on dropout rate matching the training configuration, and poorly calibrated dropout can lead to misleading uncertainty estimates."
        },
        {
          "description": "Approximation quality varies with network architecture and dropout placement, with some configurations providing poor uncertainty calibration despite theoretical foundations."
        }
      ],
      "resources": [
        {
          "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
          "url": "http://arxiv.org/pdf/1506.02142v6",
          "source_type": "technical_paper",
          "authors": [
            "Yarin Gal",
            "Zoubin Ghahramani"
          ],
          "publication_date": "2016-06-06"
        },
        {
          "title": "mattiasegu/uncertainty_estimation_deep_learning",
          "url": "https://github.com/mattiasegu/uncertainty_estimation_deep_learning",
          "source_type": "software_package"
        },
        {
          "title": "uzh-rpg/deep_uncertainty_estimation",
          "url": "https://github.com/uzh-rpg/deep_uncertainty_estimation",
          "source_type": "software_package"
        },
        {
          "title": "How certain are tansformers in image classification: uncertainty analysis with Monte Carlo dropout",
          "url": "https://www.semanticscholar.org/paper/d7ff734c5b62a4a140fd560373d890e43d5b36cf",
          "source_type": "technical_paper",
          "authors": [
            "Md. Farhadul Islam",
            "Sarah Zabeen",
            "Md. Azharul Islam",
            "Fardin Bin Rahman",
            "Anushua Ahmed",
            "Dewan Ziaul Karim",
            "Annajiat Alim Rasel",
            "Meem Arafat Manab"
          ]
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "prediction-intervals",
        "quantile-regression",
        "conformal-prediction",
        "deep-ensembles",
        "bootstrapping",
        "jackknife-resampling"
      ]
    },
    {
      "slug": "out-of-distribution-detector-for-neural-networks",
      "name": "Out-of-DIstribution detector for Neural networks",
      "description": "ODIN (Out-of-Distribution Detector for Neural Networks) identifies when a neural network encounters inputs significantly different from its training distribution. It enhances detection by applying temperature scaling to soften the model's output distribution and adding small, carefully calibrated perturbations to the input that push in-distribution samples towards higher confidence predictions. By measuring the maximum softmax probability after these adjustments, ODIN can effectively distinguish between in-distribution and out-of-distribution inputs, flagging potentially unreliable predictions before they cause downstream errors.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Detecting anomalous medical images in diagnostic systems, where ODIN flags X-rays or scans containing rare pathologies or imaging artefacts not present in training data, preventing misdiagnosis and prompting specialist review.",
          "goal": "Reliability"
        },
        {
          "description": "Protecting autonomous vehicle perception systems by identifying novel road scenarios (e.g., unusual weather conditions, rare obstacle types) that fall outside the training distribution, triggering fallback safety mechanisms.",
          "goal": "Safety"
        },
        {
          "description": "Monitoring production ML systems for data drift by detecting when incoming customer behaviour patterns deviate significantly from training data, helping explain why model performance may degrade over time.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Requires careful tuning of temperature scaling and perturbation magnitude parameters, which may need adjustment for different types of out-of-distribution data."
        },
        {
          "description": "Performance degrades when out-of-distribution samples are very similar to training data, making near-distribution detection challenging."
        },
        {
          "description": "Vulnerable to adversarial examples specifically crafted to evade detection by mimicking in-distribution characteristics."
        },
        {
          "description": "Computational overhead from input preprocessing and perturbation generation can impact real-time inference applications."
        }
      ],
      "resources": [
        {
          "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
          "url": "http://arxiv.org/pdf/1706.02690v5",
          "source_type": "technical_paper",
          "authors": [
            "Shiyu Liang",
            "Yixuan Li",
            "R. Srikant"
          ],
          "publication_date": "2017-06-08"
        },
        {
          "title": "facebookresearch/odin",
          "url": "https://github.com/facebookresearch/odin",
          "source_type": "software_package"
        },
        {
          "title": "Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data",
          "url": "http://arxiv.org/pdf/2002.11297v2",
          "source_type": "technical_paper",
          "authors": [
            "Yen-Chang Hsu",
            "Yilin Shen",
            "Hongxia Jin",
            "Zsolt Kira"
          ],
          "publication_date": "2020-02-26"
        },
        {
          "title": "Detection of out-of-distribution samples using binary neuron activation patterns",
          "url": "http://arxiv.org/abs/2212.14268",
          "source_type": "technical_paper",
          "authors": [
            "Chachuła, Krystian",
            "Olber, Bartlomiej",
            "Popowicz, Adam",
            "Radlak, Krystian",
            "Szczepankiewicz, Michal"
          ],
          "publication_date": "2023-03-24"
        },
        {
          "title": "Out-of-Distribution Detection with ODIN - A Tutorial",
          "url": "https://medium.com/@abhaypatil2000/out-of-distribution-detection-using-odin-f1a3e9e6b3b8",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "acronym": "ODIN",
      "related_techniques": [
        "anomaly-detection"
      ]
    },
    {
      "slug": "permutation-tests",
      "name": "Permutation Tests",
      "description": "Permutation tests assess the statistical significance of observed results (such as model accuracy, feature importance, or group differences) by comparing them to what would occur purely by chance. The technique randomly shuffles labels or data thousands of times, recalculating the metric of interest each time to build an empirical null distribution. If the actual observed result falls in the extreme tail of this distribution (typically beyond the 95th or 99th percentile), it provides strong evidence that the relationship is genuine rather than due to random chance, without requiring parametric assumptions about data distributions.",
      "assurance_goals": [
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Validating feature importance in medical diagnosis models by permuting each feature 10,000 times to ensure that identified risk factors (e.g., blood pressure, cholesterol) have statistically significant predictive power beyond random chance.",
          "goal": "Reliability"
        },
        {
          "description": "Testing whether observed differences in loan approval rates between demographic groups are statistically significant by permuting group labels and calculating the approval rate difference distribution under the null hypothesis of no discrimination.",
          "goal": "Explainability"
        },
        {
          "description": "Verifying that a model's claimed 95% accuracy on test data is genuinely better than random guessing by permuting labels 5,000 times and confirming the actual accuracy falls beyond the 99th percentile of the null distribution.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive as it requires thousands of model evaluations or metric calculations, scaling poorly with dataset size and model complexity."
        },
        {
          "description": "Requires many permutations (typically 5,000-10,000) to achieve reliable p-values for strict significance thresholds like p < 0.01."
        },
        {
          "description": "Assumes exchangeability of observations under the null hypothesis, which may be violated in time series or hierarchical data structures."
        },
        {
          "description": "Cannot be easily parallelised for some metrics that require global model retraining, limiting scalability for complex machine learning pipelines."
        }
      ],
      "resources": [
        {
          "title": "Permutation Tests for Classification",
          "url": "https://core.ac.uk/download/4383831.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Golland, Polina",
            "Mukherjee, Sayan",
            "Panchenko, Dmitry"
          ],
          "publication_date": "2003-01-01"
        },
        {
          "title": "How to use Permutation Tests | Towards Data Science",
          "url": "https://towardsdatascience.com/how-to-use-permutation-tests-bacc79f45749/",
          "source_type": "tutorial"
        },
        {
          "title": "Permutation test in R | Towards Data Science",
          "url": "https://towardsdatascience.com/permutation-test-in-r-77d551a9f891/",
          "source_type": "tutorial"
        },
        {
          "title": "The Exchangeability Assumption for Permutation Tests of Multiple Regression Models: Implications for Statistics and Data Science Educators",
          "url": "http://arxiv.org/pdf/2406.07756v2",
          "source_type": "documentation",
          "authors": [
            "Johanna Hardin",
            "Lauren Quesada",
            "Julie Ye",
            "Nicholas J. Horton"
          ],
          "publication_date": "2024-06-11"
        },
        {
          "title": "scikit-learn permutation_importance",
          "url": "https://scikit-learn.org/stable/modules/permutation_importance.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "related_techniques": [
        "cross-validation",
        "area-under-precision-recall-curve"
      ]
    },
    {
      "slug": "demographic-parity-assessment",
      "name": "Demographic Parity Assessment",
      "description": "Demographic Parity Assessment evaluates whether a model produces equal positive prediction rates across different demographic groups, regardless of underlying differences in qualifications or base rates. It quantifies fairness using metrics like Statistical Parity Difference (the absolute difference in positive outcome rates between groups) or Disparate Impact ratio (the ratio of positive rates). Unlike techniques that modify data or models, this is purely a measurement approach that highlights when protected groups receive favourable outcomes at different rates, helping organisations identify and document potential discrimination.",
      "assurance_goals": [
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/fairness-metric",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating credit approval algorithms by calculating that loan approval rates for different racial groups must be within 20% of each other (0.8 disparate impact ratio), ensuring compliance with anti-discrimination regulations.",
          "goal": "Fairness"
        },
        {
          "description": "Monitoring hiring platforms by measuring that job recommendation rates for male vs female candidates remain statistically equivalent (Statistical Parity Difference < 0.05), preventing systemic gender bias in career opportunities.",
          "goal": "Fairness"
        },
        {
          "description": "Auditing healthcare triage systems to verify that urgent care assignment rates are equal across ethnic groups, ensuring that automated medical prioritisation doesn't disadvantage minority patients.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Purely observational - identifies discrimination but doesn't provide solutions for remediation or bias mitigation."
        },
        {
          "description": "May penalise models for legitimate differences in base rates between groups, potentially forcing artificial equality where none should exist."
        },
        {
          "description": "Can conflict with individual fairness principles, where similarly qualified individuals might receive different treatment to achieve group parity."
        },
        {
          "description": "Doesn't account for quality of outcomes or consider whether equal rates are actually desirable given different group needs or preferences."
        }
      ],
      "resources": [
        {
          "title": "Fairness through awareness",
          "url": "http://arxiv.org/pdf/1104.3913v1",
          "source_type": "technical_paper",
          "authors": [
            "Cynthia Dwork",
            "Moritz Hardt",
            "Toniann Pitassi",
            "Omer Reingold",
            "Richard Zemel"
          ],
          "publication_date": "2011-04-20"
        },
        {
          "title": "AI Fairness 360 Toolkit",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package"
        },
        {
          "title": "Fairlearn - Demographic Parity",
          "url": "https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html#demographic-parity",
          "source_type": "documentation"
        },
        {
          "title": "A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection",
          "url": "https://www.semanticscholar.org/paper/ce96e451a2685485c05f06fb0d991e29a9c43dae",
          "source_type": "technical_paper",
          "authors": [
            "Julia Ive",
            "Paulina Bondaronek",
            "Vishal Yadav",
            "D. Santel",
            "Tracy Glauser",
            "Tina Cheng",
            "Jeffrey R. Strawn",
            "G. Agasthya",
            "Jordan Tschida",
            "Sanghyun Choo",
            "Mayanka Chandrashekar",
            "Anuj J. Kapadia",
            "J. Pestian"
          ]
        }
      ],
      "complexity_rating": 1,
      "computational_cost_rating": 1,
      "related_techniques": [
        "equal-opportunity-difference",
        "average-odds-difference"
      ]
    },
    {
      "slug": "sensitivity-analysis-for-fairness",
      "name": "Sensitivity Analysis for Fairness",
      "description": "Sensitivity Analysis for Fairness systematically evaluates how model predictions change when sensitive attributes or their proxies are perturbed whilst holding other factors constant. The technique involves creating counterfactual instances by modifying potentially discriminatory features (race, gender, age) or their correlates (zip code, names, education institutions) and measuring the resulting prediction differences. This controlled perturbation approach quantifies the degree to which protected characteristics influence model decisions, helping detect both direct discrimination and indirect bias through proxy variables even when sensitive attributes are not explicitly used as model inputs.",
      "assurance_goals": [
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/local",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Testing whether a lending model's decisions change significantly when only the applicant's zip code (which may correlate with race) is altered, while keeping all other factors constant.",
          "goal": "Fairness"
        },
        {
          "description": "Evaluating a recruitment algorithm by systematically changing candidate names from stereotypically male to female names (whilst keeping qualifications identical) to measure whether gender bias affects hiring recommendations, revealing discrimination through name-based proxies.",
          "goal": "Fairness"
        },
        {
          "description": "Assessing a healthcare resource allocation model by varying patient zip codes across different socioeconomic areas to determine whether geographic proxies for race and income inappropriately influence treatment recommendations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires domain expertise to identify relevant proxy variables for sensitive attributes, which may not be obvious or comprehensive."
        },
        {
          "description": "Computationally intensive for complex models when testing many feature combinations or perturbation ranges."
        },
        {
          "description": "Choice of perturbation ranges and comparison points involves subjective decisions that can significantly affect results and conclusions."
        },
        {
          "description": "May miss subtle or interaction-based forms of discrimination that only manifest under specific combinations of features."
        }
      ],
      "resources": [
        {
          "title": "The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning",
          "url": "http://arxiv.org/pdf/2410.09600v2",
          "source_type": "technical_paper",
          "authors": [
            "Jake Fawkes",
            "Nic Fishman",
            "Mel Andrews",
            "Zachary C. Lipton"
          ],
          "publication_date": "2024-10-12"
        },
        {
          "title": "Fair SA: Sensitivity Analysis for Fairness in Face Recognition",
          "url": "http://arxiv.org/pdf/2202.03586v2",
          "source_type": "technical_paper",
          "authors": [
            "Aparna R. Joshi",
            "Xavier Suau",
            "Nivedha Sivakumar",
            "Luca Zappella",
            "Nicholas Apostoloff"
          ],
          "publication_date": "2022-02-08"
        },
        {
          "title": "fairlearn/fairlearn",
          "url": "https://github.com/fairlearn/fairlearn",
          "source_type": "software_package"
        },
        {
          "title": "Aequitas: Bias Audit Toolkit",
          "url": "https://github.com/dssg/aequitas",
          "source_type": "software_package"
        },
        {
          "title": "Fairness Through Sensitivity Analysis - Towards Data Science",
          "url": "https://towardsdatascience.com/fairness-through-sensitivity-analysis-3ea1b4d79e6c",
          "source_type": "tutorial"
        },
        {
          "title": "User Guide - Fairlearn documentation",
          "url": "https://fairlearn.org/v0.8.0/user_guide/assessment.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "fairness-gan",
        "attribute-removal-fairness-through-unawareness",
        "bayesian-fairness-regularization"
      ]
    },
    {
      "slug": "synthetic-data-generation",
      "name": "Synthetic Data Generation",
      "description": "Synthetic data generation creates artificial datasets that aim to preserve the statistical properties, distributions, and relationships of real data whilst containing no actual records from real individuals. The technique encompasses various approaches including generative adversarial networks (GANs), variational autoencoders (VAEs), statistical sampling methods, and privacy-preserving techniques like differential privacy. Beyond privacy protection, synthetic data serves multiple purposes: augmenting limited datasets, balancing class distributions, testing model robustness, enabling data sharing across organisations, and supporting fairness assessments by generating representative samples for underrepresented groups.",
      "assurance_goals": [
        "Privacy",
        "Fairness",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/privacy",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/structured-output",
        "expertise-needed/cryptography",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/data-handling",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Creating realistic but synthetic electronic health records for developing and testing medical diagnosis algorithms without exposing real patient data, enabling secure collaboration between healthcare institutions.",
          "goal": "Privacy"
        },
        {
          "description": "Generating synthetic samples for underrepresented demographic groups in a hiring dataset to train fair recruitment models, ensuring all groups have sufficient representation for bias testing and mitigation.",
          "goal": "Fairness"
        },
        {
          "description": "Augmenting limited training data for rare medical conditions by generating synthetic patient records, improving model reliability and performance on edge cases where real data is insufficient.",
          "goal": "Reliability"
        },
        {
          "description": "Creating synthetic financial transaction data for testing fraud detection systems in development environments, avoiding exposure of real customer financial information whilst maintaining realistic attack patterns.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "May not capture all subtle patterns, correlations, and edge cases present in real data, potentially leading to reduced model performance when deployed on actual data with different characteristics."
        },
        {
          "description": "Generating high-quality synthetic data that maintains both statistical fidelity and utility requires sophisticated techniques and substantial computational resources, especially for complex, high-dimensional datasets."
        },
        {
          "description": "Privacy-preserving approaches may still risk information leakage through statistical inference attacks, membership inference, or model inversion, requiring careful privacy budget management and validation."
        },
        {
          "description": "Synthetic data may inadvertently amplify existing biases in the original data or introduce new biases through the generation process, particularly in generative models trained on biased datasets."
        },
        {
          "description": "Validation and quality assessment of synthetic data is challenging, as traditional metrics may not adequately capture whether the synthetic data preserves the relationships and patterns needed for specific downstream tasks."
        }
      ],
      "resources": [
        {
          "title": "sdv-dev/SDV",
          "url": "https://github.com/sdv-dev/SDV",
          "source_type": "software_package"
        },
        {
          "title": "An evaluation framework for synthetic data generation models",
          "url": "http://arxiv.org/pdf/2404.08866v1",
          "source_type": "technical_paper",
          "authors": [
            "Ioannis E. Livieris",
            "Nikos Alimpertis",
            "George Domalis",
            "Dimitris Tsakalidis"
          ],
          "publication_date": "2024-04-13"
        },
        {
          "title": "Synthetic Data — SecureML 0.2.2 documentation",
          "url": "https://secureml.readthedocs.io/en/latest/user_guide/synthetic_data.html",
          "source_type": "documentation"
        },
        {
          "title": "How to Generate Real-World Synthetic Data with CTGAN | Towards ...",
          "url": "https://towardsdatascience.com/how-to-generate-real-world-synthetic-data-with-ctgan-af41b4d60fde/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 4,
      "related_techniques": [
        "federated-learning",
        "differential-privacy",
        "homomorphic-encryption"
      ]
    },
    {
      "slug": "differential-privacy",
      "name": "Differential Privacy",
      "description": "Differential privacy provides mathematically rigorous privacy protection by adding carefully calibrated random noise to data queries, statistical computations, or machine learning outputs. The technique works by ensuring that the presence or absence of any individual's data has minimal impact on the results - specifically, any query result should be nearly indistinguishable whether or not a particular person's data is included. This is achieved through controlled noise addition that scales with the query's sensitivity and a privacy budget (epsilon) that quantifies the privacy-utility trade-off. The smaller the epsilon, the more noise is added and the stronger the privacy guarantee, but at the cost of reduced accuracy.",
      "assurance_goals": [
        "Privacy",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/privacy",
        "assurance-goal-category/privacy/formal-guarantee/differential-privacy",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/privacy-guarantee",
        "expertise-needed/cryptography",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/data-handling",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Protecting individual privacy in census data analysis by adding calibrated noise to demographic statistics, ensuring households cannot be re-identified whilst maintaining accurate population insights for policy planning.",
          "goal": "Privacy"
        },
        {
          "description": "Publishing differentially private aggregate statistics about model performance across different demographic groups, enabling transparent bias auditing without exposing sensitive individual prediction details or group membership.",
          "goal": "Transparency"
        },
        {
          "description": "Enabling fair evaluation of lending algorithms by releasing differentially private performance metrics across protected groups, allowing regulatory compliance checking whilst protecting individual applicant privacy.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Adding noise inherently reduces the accuracy and utility of results, with stronger privacy guarantees (smaller epsilon values) leading to more significant degradation in data quality."
        },
        {
          "description": "Setting the privacy budget (epsilon) requires expertise and careful consideration of the privacy-utility trade-off, with no universal guidelines for appropriate values across different applications."
        },
        {
          "description": "Sequential queries consume the privacy budget cumulatively, potentially requiring careful query planning and potentially prohibiting future analyses once the budget is exhausted."
        },
        {
          "description": "Implementation complexity is high, requiring deep understanding of sensitivity analysis, noise mechanisms, and composition theorems to avoid inadvertent privacy violations."
        },
        {
          "description": "May not protect against all privacy attacks, particularly sophisticated adversaries with auxiliary information or when combined with other data sources that could aid re-identification."
        }
      ],
      "resources": [
        {
          "title": "Google Differential Privacy Library",
          "url": "https://github.com/google/differential-privacy",
          "source_type": "software_package",
          "description": "Open-source library providing implementations of differential privacy algorithms and utilities"
        },
        {
          "title": "The Algorithmic Foundations of Differential Privacy",
          "url": "https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Cynthia Dwork",
            "Aaron Roth"
          ],
          "description": "Foundational monograph on differential privacy theory and algorithms"
        },
        {
          "title": "Opacus: User-Friendly Differential Privacy Library in PyTorch",
          "url": "https://github.com/pytorch/opacus",
          "source_type": "software_package",
          "description": "PyTorch library for training neural networks with differential privacy"
        },
        {
          "title": "Programming Differential Privacy",
          "url": "https://programming-dp.com/",
          "source_type": "tutorial",
          "description": "Comprehensive online book and tutorial for learning differential privacy programming"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 3,
      "related_techniques": [
        "synthetic-data-generation",
        "federated-learning",
        "homomorphic-encryption"
      ]
    },
    {
      "slug": "prediction-intervals",
      "name": "Prediction Intervals",
      "description": "Prediction intervals provide a range of plausible values around a model's prediction, expressing uncertainty as 'the true value will likely fall between X and Y with Z% confidence'. For example, instead of predicting 'house price: £300,000', a prediction interval might say 'house price: £280,000 to £320,000 with 95% confidence'. This technique works by calculating upper and lower bounds that account for both model uncertainty (how confident the model is) and inherent randomness in the data. Prediction intervals are crucial for informed decision-making, as they help users understand the reliability and precision of predictions, enabling better risk assessment and planning.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/reliability/uncertainty-quantification",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/prediction-interval",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Providing realistic ranges for medical diagnosis predictions, such as 'patient survival time: 8-14 months with 90% confidence', enabling doctors to make informed treatment decisions and communicate uncertainty to patients and families.",
          "goal": "Reliability"
        },
        {
          "description": "Communicating uncertainty in automated loan approval systems by showing 'credit score prediction: 650-720 with 95% confidence' rather than a single score, helping loan officers understand prediction reliability and make transparent decisions.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring consistent prediction uncertainty across demographic groups in hiring algorithms, verifying that prediction intervals have similar widths for different protected groups to avoid unfair confidence disparities.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Relies on assumptions about the error distribution (often normality) which may not hold in practice, leading to inaccurate interval coverage when data exhibits heavy tails, skewness, or other non-standard patterns."
        },
        {
          "description": "Can be overconfident if the underlying model is poorly calibrated, producing intervals that are too narrow and fail to capture the true prediction uncertainty."
        },
        {
          "description": "Vulnerable to distribution shift between training and deployment data, where intervals calculated on historical data may not reflect uncertainty in new, unseen conditions."
        },
        {
          "description": "May require careful hyperparameter tuning and validation to achieve desired coverage rates, particularly when using advanced methods like conformal prediction or quantile regression."
        },
        {
          "description": "Computational overhead increases when generating intervals for large datasets or complex models, especially when using resampling-based methods like bootstrapping."
        }
      ],
      "resources": [
        {
          "title": "scikit-learn-contrib/MAPIE",
          "url": "https://github.com/scikit-learn-contrib/MAPIE",
          "source_type": "software_package",
          "description": "Open-source Python library for quantifying uncertainties using conformal prediction techniques, compatible with scikit-learn, TensorFlow, and PyTorch"
        },
        {
          "title": "MAPIE - Model Agnostic Prediction Interval Estimator",
          "url": "https://mapie.readthedocs.io/",
          "source_type": "documentation",
          "description": "Official documentation for MAPIE library implementing distribution-free uncertainty estimates for regression and classification tasks"
        },
        {
          "title": "valeman/awesome-conformal-prediction",
          "url": "https://github.com/valeman/awesome-conformal-prediction",
          "source_type": "software_package",
          "description": "Curated collection of conformal prediction resources including videos, tutorials, books, papers, and open-source libraries"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "monte-carlo-dropout",
        "quantile-regression",
        "conformal-prediction",
        "deep-ensembles",
        "bootstrapping",
        "jackknife-resampling"
      ]
    },
    {
      "slug": "quantile-regression",
      "name": "Quantile Regression",
      "description": "Quantile regression estimates specific percentiles (quantiles) of the target variable rather than just predicting the average outcome. For example, instead of predicting 'average house price = £300,000', it can predict 'there's a 10% chance the price will be below £250,000, 50% chance below £300,000, and 90% chance below £380,000'. This technique reveals how input features affect different parts of the outcome distribution - perhaps property size strongly influences luxury homes (90th percentile) but barely affects budget properties (10th percentile). By capturing the full conditional distribution, quantile regression provides rich uncertainty information and enables robust prediction intervals.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/reliability/uncertainty-quantification",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/prediction-interval",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Predicting patient recovery times after surgery by estimating multiple quantiles (e.g., 25th, 50th, 75th percentiles), enabling doctors to communicate realistic timeframes: 'Most patients recover within 2-4 weeks, but some may take up to 8 weeks', providing robust uncertainty estimates for treatment planning.",
          "goal": "Reliability"
        },
        {
          "description": "Revealing how income inequality affects different segments of society by showing how education's impact varies across income quantiles - demonstrating that education benefits high earners much more than low earners, providing transparent insights into systemic inequalities.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring equitable loan amount predictions across demographic groups by verifying that the spread of predicted loan amounts (difference between 90th and 10th percentiles) is consistent across protected groups, preventing discriminatory practices in lending ranges.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Computationally intensive when fitting multiple quantiles simultaneously, especially for large datasets or complex models, as each quantile requires separate optimization."
        },
        {
          "description": "May produce crossing quantiles without proper constraints, where predicted 90th percentile values are lower than 50th percentile values, creating logically inconsistent and unusable prediction intervals."
        },
        {
          "description": "Sensitive to outliers and heavy-tailed distributions, particularly in extreme quantiles (e.g., 5th or 95th percentiles), which can lead to unstable and unreliable estimates."
        },
        {
          "description": "Requires careful selection of quantile levels and may need domain expertise to interpret results meaningfully, as different quantiles may reveal conflicting patterns in feature relationships."
        },
        {
          "description": "Less effective with small datasets where extreme quantiles cannot be reliably estimated due to insufficient data points in the tails of the distribution."
        }
      ],
      "resources": [
        {
          "title": "statsmodels/statsmodels",
          "url": "https://github.com/statsmodels/statsmodels",
          "source_type": "software_package",
          "description": "Python package providing comprehensive statistical modeling capabilities including quantile regression alongside descriptive statistics and statistical inference"
        },
        {
          "title": "Quantile Regression in Machine Learning: A Survey",
          "url": "https://www.semanticscholar.org/paper/01cd143c5a054b85afc9b99d473f84422ace7e05",
          "source_type": "documentation",
          "authors": [
            "Anshul Kumar",
            "Rajesh Wadhvani",
            "A. Rasool",
            "Muktesh Gupta"
          ],
          "description": "Comprehensive survey covering quantile regression applications, methods, and developments in machine learning contexts"
        },
        {
          "title": "Tutorial for conformalized quantile regression (CQR) — MAPIE 0.8.5 ...",
          "url": "https://mapie.readthedocs.io/en/v0.8.5/examples_regression/4-tutorials/plot_cqr_tutorial.html",
          "source_type": "tutorial"
        },
        {
          "title": "Quantile Regression Forest — sklearn_quantile 0.1.1 documentation",
          "url": "https://sklearn-quantile.readthedocs.io/en/latest/methods.html",
          "source_type": "documentation"
        },
        {
          "title": "Quantile machine learning models for python — sklearn_quantile ...",
          "url": "https://sklearn-quantile.readthedocs.io/",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "monte-carlo-dropout",
        "prediction-intervals",
        "conformal-prediction",
        "deep-ensembles",
        "bootstrapping",
        "jackknife-resampling"
      ]
    },
    {
      "slug": "conformal-prediction",
      "name": "Conformal Prediction",
      "description": "Conformal prediction provides mathematically guaranteed uncertainty quantification by creating prediction sets that contain the true outcome with a specified probability (e.g., exactly 95% coverage). The technique works by measuring how 'strange' or 'nonconforming' new predictions are compared to calibration data - if a prediction seems unusual, it gets wider intervals. For example, in medical diagnosis, instead of saying 'likely cancer', it might say 'possible diagnoses: {cancer, benign tumour} with 95% confidence'. This distribution-free method works with any underlying model (neural networks, random forests, etc.) and requires no assumptions about data distribution, making it a robust framework for reliable uncertainty estimates in high-stakes applications.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/reliability/uncertainty-quantification",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/calibration-set",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/prediction-interval",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Creating prediction sets for drug discovery that guarantee 95% coverage, such as 'this compound will likely have activity against {target A, target B, target C}', ensuring reliable decision-making in costly experimental validation.",
          "goal": "Reliability"
        },
        {
          "description": "Providing transparent multi-class predictions in judicial risk assessment by showing all plausible risk categories with guaranteed coverage, enabling judges to see the full range of possibilities rather than just a single point estimate.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring fair uncertainty quantification across demographic groups in college admissions by verifying that prediction set sizes (number of possible outcomes) are consistent across protected groups, preventing discriminatory overconfidence for certain populations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Prediction sets can be unnecessarily wide when nonconformity scores vary greatly across the feature space, leading to conservative intervals that reduce practical utility."
        },
        {
          "description": "Requires a held-out calibration set separate from training data, reducing the amount of data available for model training, which can impact performance on small datasets."
        },
        {
          "description": "Guarantees only hold under the exchangeability assumption - if test data distribution differs significantly from calibration data, coverage guarantees may be violated."
        },
        {
          "description": "For multi-class problems, prediction sets may include many classes when the model is uncertain, making decisions difficult when sets contain opposing outcomes."
        },
        {
          "description": "Computational cost increases with the number of calibration samples, and efficient implementation requires careful design for large-scale or real-time applications."
        }
      ],
      "resources": [
        {
          "title": "A tutorial on conformal prediction",
          "url": "http://arxiv.org/pdf/0706.3188v1",
          "source_type": "documentation",
          "authors": [
            "Glenn Shafer",
            "Vladimir Vovk"
          ],
          "publication_date": "2007-06-21",
          "description": "Foundational tutorial introducing conformal prediction theory and applications by the method's creators"
        },
        {
          "title": "valeman/awesome-conformal-prediction",
          "url": "https://github.com/valeman/awesome-conformal-prediction",
          "source_type": "software_package",
          "description": "Curated collection of conformal prediction resources including videos, tutorials, books, papers, and open-source libraries"
        },
        {
          "title": "scikit-learn-contrib/MAPIE",
          "url": "https://github.com/scikit-learn-contrib/MAPIE",
          "source_type": "software_package",
          "description": "Python library for uncertainty quantification using conformal prediction across regression, classification, and time series tasks"
        },
        {
          "title": "Tutorial for classification — MAPIE 0.8.6 documentation",
          "url": "https://mapie.readthedocs.io/en/v0.8.6/examples_classification/4-tutorials/plot_main-tutorial-classification.html",
          "source_type": "tutorial",
          "description": "Practical tutorial demonstrating conformal prediction for classification tasks with guaranteed coverage"
        },
        {
          "title": "Conformal Prediction: a Unified Review of Theory and New Challenges",
          "url": "http://arxiv.org/pdf/2005.07972v2",
          "source_type": "documentation",
          "authors": [
            "Matteo Fontana",
            "Gianluca Zeni",
            "Simone Vantini"
          ],
          "publication_date": "2020-05-16",
          "description": "Comprehensive review of conformal prediction theory, recent advances, and emerging challenges in the field"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 2,
      "related_techniques": [
        "monte-carlo-dropout",
        "prediction-intervals",
        "quantile-regression",
        "deep-ensembles",
        "bootstrapping",
        "jackknife-resampling"
      ]
    },
    {
      "slug": "empirical-calibration",
      "name": "Empirical Calibration",
      "description": "Empirical calibration adjusts a model's predicted probabilities to match observed frequencies. For example, if events predicted with 80% confidence only occur 60% of the time, calibration would correct this overconfidence. Common techniques include Platt scaling and isotonic regression, which learn transformations that map the model's raw scores to well-calibrated probabilities, improving the reliability of confidence measures for downstream decisions.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/calibration-set",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting a credit default prediction model's probabilities to ensure that loan applicants with a predicted 30% default risk actually default 30% of the time, improving decision-making.",
          "goal": "Reliability"
        },
        {
          "description": "Calibrating a medical diagnosis model's confidence scores so that stakeholders can meaningfully interpret probability outputs, enabling doctors to make informed decisions about treatment urgency based on reliable confidence estimates.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring that a hiring algorithm's confidence scores are equally well-calibrated across different demographic groups, preventing systematically overconfident predictions for certain populations that could lead to biased decision-making.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires a separate held-out calibration dataset, which reduces the amount of data available for model training."
        },
        {
          "description": "Calibration performance can degrade over time if the underlying data distribution shifts, requiring periodic recalibration."
        },
        {
          "description": "May sacrifice some discriminative power in favour of calibration, potentially reducing the model's ability to distinguish between classes."
        },
        {
          "description": "Calibration methods assume that the calibration set is representative of future data, which may not hold in dynamic environments."
        }
      ],
      "resources": [
        {
          "title": "google/empirical_calibration",
          "url": "https://github.com/google/empirical_calibration",
          "source_type": "software_package"
        },
        {
          "title": "A Python Library For Empirical Calibration",
          "url": "http://arxiv.org/pdf/1906.11920v2",
          "source_type": "technical_paper",
          "authors": [
            "Xiaojing Wang",
            "Jingang Miao",
            "Yunting Sun"
          ],
          "publication_date": "2019-07-25"
        },
        {
          "title": "Assessing the effectiveness of empirical calibration under different bias scenarios",
          "url": "http://arxiv.org/pdf/2111.04233v2",
          "source_type": "technical_paper",
          "authors": [
            "Hon Hwang",
            "Juan C Quiroz",
            "Blanca Gallego"
          ],
          "publication_date": "2021-11-08"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "related_techniques": [
        "temperature-scaling"
      ]
    },
    {
      "slug": "temperature-scaling",
      "name": "Temperature Scaling",
      "description": "Temperature scaling adjusts a model's confidence by applying a single parameter (temperature) to its predictions. When a model is too confident in its wrong answers, temperature scaling can fix this by making the predictions more realistic. It works by dividing the model's outputs by the temperature value before converting them to probabilities. Higher temperatures make the model less confident, whilst lower temperatures increase confidence. The technique maintains the model's accuracy whilst ensuring that when it says it's 90% confident, it's actually right about 90% of the time.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-requirements/calibration-set",
        "data-requirements/validation-set",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "explanatory-scope/global",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting a deep learning image classifier's confidence scores to be realistic, ensuring that when it's 90% confident, it's right 90% of the time.",
          "goal": "Reliability"
        },
        {
          "description": "Making medical diagnosis model predictions more trustworthy by providing realistic confidence scores that doctors can interpret and use to make informed decisions about patient care.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring fair treatment across patient demographics by calibrating confidence scores equally across different groups, preventing systematic over-confidence in predictions for certain populations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Only addresses calibration at the overall dataset level, not subgroup-specific miscalibration issues."
        },
        {
          "description": "Does not improve the rank ordering or accuracy of predictions, only adjusts confidence levels."
        },
        {
          "description": "Assumes that calibration errors are consistent across different types of inputs and feature values."
        },
        {
          "description": "Requires a separate validation set for temperature parameter optimisation, which may not be available in small datasets."
        }
      ],
      "resources": [
        {
          "title": "gpleiss/temperature_scaling",
          "url": "https://github.com/gpleiss/temperature_scaling",
          "source_type": "software_package"
        },
        {
          "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
          "url": "http://arxiv.org/pdf/2502.20604v1",
          "source_type": "technical_paper",
          "authors": [
            "Hao Xuan",
            "Bokai Yang",
            "Xingyu Li"
          ],
          "publication_date": "2025-02-28"
        },
        {
          "title": "Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration",
          "url": "http://arxiv.org/pdf/2209.11604v2",
          "source_type": "technical_paper",
          "authors": [
            "Yung-Chen Tang",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
          ],
          "publication_date": "2024-07-24"
        },
        {
          "title": "On Calibration of Modern Neural Networks | arXiv",
          "url": "https://arxiv.org/abs/1706.04599",
          "source_type": "technical_paper",
          "authors": [
            "Chuan Guo",
            "Geoff Pleiss",
            "Yu Sun",
            "Kilian Q. Weinberger"
          ],
          "publication_date": "2017-06-14"
        },
        {
          "title": "On the Limitations of Temperature Scaling for Distributions with Overlaps",
          "url": "http://arxiv.org/pdf/2306.00740v3",
          "source_type": "technical_paper",
          "authors": [
            "Muthu Chidambaram",
            "Rong Ge"
          ],
          "publication_date": "2023-06-01"
        }
      ],
      "complexity_rating": 1,
      "computational_cost_rating": 1,
      "related_techniques": [
        "empirical-calibration"
      ]
    },
    {
      "slug": "deep-ensembles",
      "name": "Deep Ensembles",
      "description": "Deep ensembles combine predictions from multiple neural networks trained independently with different random initializations to capture epistemic uncertainty (model uncertainty). By training several models on the same data with different starting points, the ensemble reveals how much the model's predictions depend on training randomness. The disagreement between ensemble members naturally indicates prediction uncertainty - when models agree, confidence is high; when they disagree, uncertainty is revealed. This approach provides more reliable uncertainty estimates, better out-of-distribution detection, and improved calibration compared to single models.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Safety"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/reliability",
        "assurance-goal-category/reliability/uncertainty-quantification",
        "assurance-goal-category/transparency",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/prediction-interval",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Improving self-driving car safety by using multiple neural networks to detect obstacles, where disagreement between models signals uncertainty and triggers extra caution or human intervention, providing robust uncertainty quantification for critical decisions.",
          "goal": "Reliability"
        },
        {
          "description": "Communicating prediction confidence to medical professionals by showing the range of diagnoses from multiple trained models, enabling doctors to understand when the AI system is uncertain and requires additional human expertise or testing.",
          "goal": "Transparency"
        },
        {
          "description": "Detecting out-of-distribution inputs in financial fraud detection systems where ensemble disagreement signals potentially novel attack patterns that require immediate security team review and system safeguards.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive to train and deploy, requiring multiple complete neural networks which increases training time, memory usage, and inference costs proportionally to ensemble size."
        },
        {
          "description": "May still provide overconfident predictions for inputs far from the training distribution, as all ensemble members can be similarly confident about out-of-distribution examples."
        },
        {
          "description": "Requires careful hyperparameter tuning for each ensemble member to ensure diversity, as identical hyperparameters may lead to similar models that reduce uncertainty estimation quality."
        },
        {
          "description": "Storage and deployment overhead increases linearly with ensemble size, making it challenging to deploy large ensembles in resource-constrained environments or real-time applications."
        },
        {
          "description": "Ensemble predictions may be difficult to interpret individually, as the final decision emerges from averaging multiple models rather than from a single explainable pathway."
        }
      ],
      "resources": [
        {
          "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
          "url": "https://arxiv.org/abs/1612.01474",
          "source_type": "technical_paper",
          "authors": [
            "Balaji Lakshminarayanan",
            "Alexander Pritzel",
            "Charles Blundell"
          ],
          "publication_date": "2016-12-05",
          "description": "Foundational paper introducing deep ensembles for uncertainty estimation in neural networks"
        },
        {
          "title": "ENSTA-U2IS-AI/awesome-uncertainty-deeplearning",
          "url": "https://github.com/ENSTA-U2IS-AI/awesome-uncertainty-deeplearning",
          "source_type": "documentation",
          "description": "Comprehensive collection of research papers, surveys, datasets, and code for uncertainty estimation in deep learning"
        },
        {
          "title": "Deep Ensembles: A Loss Landscape Perspective",
          "url": "http://arxiv.org/pdf/1912.02757v2",
          "source_type": "technical_paper",
          "authors": [
            "Stanislav Fort",
            "Huiyi Hu",
            "Balaji Lakshminarayanan"
          ],
          "publication_date": "2019-12-05",
          "description": "Analysis of why deep ensembles work well from the perspective of loss landscape geometry and mode connectivity"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 5,
      "related_techniques": [
        "monte-carlo-dropout",
        "prediction-intervals",
        "quantile-regression",
        "conformal-prediction",
        "bootstrapping",
        "jackknife-resampling"
      ]
    },
    {
      "slug": "bootstrapping",
      "name": "Bootstrapping",
      "description": "Bootstrapping estimates uncertainty by repeatedly resampling the original dataset with replacement to create many new training sets, training a model on each sample, and analysing the variation in predictions. This approach provides confidence intervals and stability measures without making strong statistical assumptions. By showing how predictions change with different random samples of the data, it reveals how sensitive the model is to the specific training examples and provides robust uncertainty estimates.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-training-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "explanatory-scope/global",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Estimating uncertainty in financial risk models by resampling historical data to understand how predictions might vary under different historical scenarios.",
          "goal": "Reliability"
        },
        {
          "description": "Providing confidence intervals for medical diagnosis predictions to help doctors understand the reliability of AI recommendations and make more informed treatment decisions.",
          "goal": "Transparency"
        },
        {
          "description": "Assessing whether prediction uncertainty is consistent across different demographic groups in hiring algorithms, identifying if the model is systematically more uncertain for certain populations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive as it requires training multiple models on resampled datasets."
        },
        {
          "description": "Does not account for uncertainty in model structure or architecture choices."
        },
        {
          "description": "Cannot detect systematically missing data patterns or biases present in the original dataset."
        },
        {
          "description": "Assumes that the original dataset is representative of the population of interest."
        }
      ],
      "resources": [
        {
          "title": "Deterministic bootstrapping for a class of bootstrap methods",
          "url": "http://arxiv.org/pdf/1903.10816v2",
          "source_type": "technical_paper",
          "authors": [
            "Thomas Pitschel"
          ],
          "publication_date": "2019-03-26"
        },
        {
          "title": "A Gentle Introduction to the Bootstrap Method ...",
          "url": "https://www.machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/",
          "source_type": "tutorial"
        },
        {
          "title": "scipy.stats.bootstrap",
          "url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html",
          "source_type": "software_package"
        },
        {
          "title": "Bootstrapping and bagging — modAL documentation",
          "url": "https://modal-python.readthedocs.io/en/latest/content/examples/bootstrapping_and_bagging.html",
          "source_type": "tutorial"
        },
        {
          "title": "Machine Learning: What is Bootstrapping? - KDnuggets",
          "url": "https://www.kdnuggets.com/2023/03/bootstrapping.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 4,
      "related_techniques": [
        "monte-carlo-dropout",
        "prediction-intervals",
        "quantile-regression",
        "conformal-prediction",
        "deep-ensembles",
        "jackknife-resampling"
      ]
    },
    {
      "slug": "jackknife-resampling",
      "name": "Jackknife Resampling",
      "description": "Jackknife resampling (also called leave-one-out resampling) assesses model stability and uncertainty by systematically removing one data point at a time and retraining the model on the remaining data. Unlike bootstrapping which samples with replacement, jackknife creates n different models by excluding each of the n data points once. This systematic approach reveals how individual points influence results, provides robust estimates of prediction variance, and identifies unusually influential observations that may be outliers or leverage points affecting model reliability.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/reliability/uncertainty-quantification",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-training-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/prediction-interval",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating how removing individual countries from a global climate model affects predictions, identifying which regions have outsized influence and providing robust uncertainty estimates for climate projections used in policy decisions.",
          "goal": "Reliability"
        },
        {
          "description": "Providing transparent uncertainty estimates in medical risk prediction by showing how individual patient records influence model predictions, enabling clinicians to understand prediction stability and confidence intervals for treatment decisions.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring fair model evaluation in hiring algorithms by systematically testing how removing candidates from different demographic groups affects model performance, revealing whether certain populations disproportionately influence the model's behaviour.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Extremely computationally intensive for large datasets, requiring training of n separate models for n data points, making it impractical for datasets with thousands or millions of observations."
        },
        {
          "description": "May underestimate uncertainty compared to bootstrapping or other resampling methods, as it provides only n different samples rather than a broader exploration of the data distribution."
        },
        {
          "description": "Assumes that removing single data points provides meaningful insights into model stability, which may not hold when multiple correlated observations drive model behaviour."
        },
        {
          "description": "Can be sensitive to the choice of performance metric used for evaluation, as different metrics may show different patterns of sensitivity to individual data points."
        },
        {
          "description": "Provides limited insight into model behaviour on truly novel data, as each jackknife sample is only minimally different from the full training set."
        }
      ],
      "resources": [
        {
          "title": "scikit-learn model_selection.LeaveOneOut",
          "url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html",
          "source_type": "documentation",
          "description": "Scikit-learn implementation of leave-one-out cross-validation for jackknife resampling"
        },
        {
          "title": "Cross-validation: evaluating estimator performance",
          "url": "https://scikit-learn.org/stable/modules/cross_validation.html",
          "source_type": "documentation",
          "description": "Comprehensive guide to cross-validation methods including leave-one-out in scikit-learn"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 5,
      "related_techniques": [
        "monte-carlo-dropout",
        "prediction-intervals",
        "quantile-regression",
        "conformal-prediction",
        "deep-ensembles",
        "bootstrapping"
      ]
    },
    {
      "slug": "cross-validation",
      "name": "Cross-validation",
      "description": "Cross-validation evaluates model performance and robustness by systematically partitioning data into multiple subsets (folds) and training/testing repeatedly on different combinations. Common approaches include k-fold (splitting into k equal parts), stratified (preserving class distributions), and leave-one-out variants. By testing on multiple independent holdout sets, it reveals how performance varies across different data subsamples, provides robust estimates of generalisation ability, and helps detect overfitting or model instability that single train-test splits might miss.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Using 10-fold cross-validation to estimate a healthcare prediction model's true accuracy and detect overfitting, ensuring robust performance estimates that generalise beyond the specific training sample to new patient populations.",
          "goal": "Reliability"
        },
        {
          "description": "Providing transparent model evaluation in regulatory submissions by showing consistent performance across multiple validation folds, demonstrating to auditors that model performance claims are not cherry-picked from a single favourable test set.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring fair model evaluation across demographic groups by using stratified cross-validation that maintains representative proportions of protected classes in each fold, revealing whether performance is consistent across different population segments.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive for large datasets or complex models, requiring multiple training runs that scale linearly with the number of folds."
        },
        {
          "description": "Can provide overly optimistic performance estimates when data has dependencies or structure (e.g., time series, grouped observations) that violate independence assumptions."
        },
        {
          "description": "May not reflect real-world performance if the training data distribution differs significantly from future deployment conditions or population shifts."
        },
        {
          "description": "Choice of fold number (k) involves a bias-variance trade-off: fewer folds reduce computational cost but increase variance in estimates, whilst more folds increase computation but may introduce bias."
        },
        {
          "description": "Standard cross-validation doesn't account for temporal ordering in sequential data, potentially leading to data leakage where future information influences past predictions."
        }
      ],
      "resources": [
        {
          "title": "scikit-learn Cross-validation User Guide",
          "url": "https://scikit-learn.org/stable/modules/cross_validation.html",
          "source_type": "documentation",
          "description": "Comprehensive guide to cross-validation methods and implementations in scikit-learn"
        },
        {
          "title": "Cross-validation: what does it estimate and how well does it do it?",
          "url": "http://arxiv.org/pdf/2104.00673v4",
          "source_type": "technical_paper",
          "authors": [
            "Stephen Bates",
            "Trevor Hastie",
            "Robert Tibshirani"
          ],
          "publication_date": "2021-04-01",
          "description": "Theoretical analysis of what cross-validation estimates and its accuracy in practice"
        },
        {
          "title": "A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection",
          "url": "https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Ron Kohavi"
          ],
          "publication_date": "1995-01-01",
          "description": "Classic paper comparing cross-validation with bootstrap for model evaluation and selection"
        },
        {
          "title": "Cross-Validation in Machine Learning: How to Do It Right",
          "url": "https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right",
          "source_type": "tutorial",
          "description": "Practical guide covering different cross-validation strategies and common pitfalls to avoid"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 3,
      "related_techniques": [
        "permutation-tests",
        "area-under-precision-recall-curve"
      ]
    },
    {
      "slug": "area-under-precision-recall-curve",
      "name": "Area Under Precision-Recall Curve",
      "description": "Area Under Precision-Recall Curve (AUPRC) measures model performance by plotting precision (the proportion of positive predictions that are correct) against recall (the proportion of actual positives that are correctly identified) at various classification thresholds, then calculating the area under the resulting curve. Unlike accuracy or AUC-ROC, AUPRC is particularly valuable for imbalanced datasets where the minority class is of primary interest---a perfect score is 1.0, whilst random performance equals the positive class proportion. By focusing on the precision-recall trade-off, it provides a more informative assessment than overall accuracy for scenarios where false positives and false negatives have different costs, especially when positive examples are rare.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating fraud detection models where genuine transactions far outnumber fraudulent ones, using AUPRC to optimise the balance between catching fraud (high recall) and minimising false alarms (high precision) for cost-effective operations.",
          "goal": "Reliability"
        },
        {
          "description": "Providing transparent performance metrics for rare disease detection systems to medical regulators, where AUPRC clearly shows model effectiveness on the minority positive class rather than being masked by high accuracy on negative cases.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring fair evaluation of loan default prediction across demographic groups by comparing AUPRC scores, revealing whether models perform equally well at identifying high-risk borrowers regardless of protected characteristics.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "More sensitive to class distribution than ROC curves, making it difficult to compare models across datasets with different positive class proportions or to set universal performance thresholds."
        },
        {
          "description": "Can be overly optimistic on extremely imbalanced datasets where even random predictions may achieve seemingly high AUPRC scores due to the small positive class size."
        },
        {
          "description": "Provides limited insight into performance at specific operating points, requiring additional analysis to determine optimal threshold selection for deployment."
        },
        {
          "description": "Interpolation methods for calculating the area under the curve can vary between implementations, potentially leading to slightly different scores for the same model."
        },
        {
          "description": "Less interpretable than simple metrics like precision or recall at a fixed threshold, making it harder to communicate performance to non-technical stakeholders."
        }
      ],
      "resources": [
        {
          "title": "scikit-learn Precision-Recall",
          "url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html",
          "source_type": "documentation",
          "description": "Comprehensive guide to precision-recall curves and AUPRC calculation in scikit-learn with practical examples"
        },
        {
          "title": "Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence",
          "url": "https://www.semanticscholar.org/paper/c1b5b9dfc7d6e024097f63947aa5db06e1c192d8",
          "source_type": "technical_paper",
          "authors": [
            "Qi Qi",
            "Youzhi Luo",
            "Zhao Xu",
            "Shuiwang Ji",
            "Tianbao Yang"
          ],
          "description": "Technical paper on optimising AUPRC directly during model training with convergence guarantees"
        },
        {
          "title": "A Closer Look at AUROC and AUPRC under Class Imbalance",
          "url": "http://arxiv.org/pdf/2401.06091v4",
          "source_type": "technical_paper",
          "authors": [
            "Matthew B. A. McDermott",
            "Haoran Zhang",
            "Lasse Hyldig Hansen",
            "Giovanni Angelotti",
            "Jack Gallifant"
          ],
          "publication_date": "2024-01-11",
          "description": "Recent analysis of AUPRC behaviour under extreme class imbalance with practical recommendations"
        },
        {
          "title": "DominikRafacz/auprc",
          "url": "https://github.com/DominikRafacz/auprc",
          "source_type": "software_package",
          "description": "R package for calculating AUPRC with functions for plotting precision-recall curves and mlr3 integration"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "acronym": "AUPRC",
      "related_techniques": [
        "permutation-tests",
        "cross-validation"
      ]
    },
    {
      "slug": "anomaly-detection",
      "name": "Anomaly Detection",
      "description": "Anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. Applied to AI/ML systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. By establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm.",
      "assurance_goals": [
        "Safety",
        "Reliability",
        "Fairness",
        "Security"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/safety",
        "assurance-goal-category/safety/monitoring/anomaly-detection",
        "assurance-goal-category/reliability",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/system-deployment-and-use",
        "lifecycle-stage/system-deployment-and-use/monitoring",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Monitoring a content moderation AI system to detect when it starts flagging significantly more or fewer posts than usual, which could indicate model drift, adversarial attacks, or changes in user behaviour patterns that require immediate investigation to prevent harmful content from appearing.",
          "goal": "Safety"
        },
        {
          "description": "Implementing anomaly detection on a medical diagnosis AI to identify when prediction confidence scores or feature importance patterns deviate from historical norms, helping catch model degradation or data quality issues that could lead to misdiagnoses before patients are affected.",
          "goal": "Reliability"
        },
        {
          "description": "Deploying anomaly detection on a hiring algorithm to monitor for unusual patterns in how candidates from different demographic groups are scored or rejected, enabling early detection of emerging bias issues or attempts to game the system through demographic manipulation.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Setting appropriate sensitivity thresholds is challenging and requires domain expertise, as overly sensitive settings generate excessive false alarms whilst conservative settings may miss genuine anomalies."
        },
        {
          "description": "May generate false positives for legitimate edge cases or rare but valid system behaviours, potentially causing unnecessary alerts and disrupting normal operations."
        },
        {
          "description": "Limited effectiveness against novel or sophisticated attacks that deliberately mimic normal patterns or gradually shift behaviour to avoid detection thresholds."
        },
        {
          "description": "Requires substantial historical data to establish reliable baselines of normal behaviour, and may struggle with systems that have naturally high variability or seasonal patterns."
        },
        {
          "description": "Detection lag can occur between when an anomaly begins and when it exceeds detection thresholds, potentially allowing harmful behaviour to persist during the detection window."
        }
      ],
      "resources": [
        {
          "title": "Anomaly Detection Toolkit (ADTK)",
          "url": "https://adtk.readthedocs.io/en/stable/",
          "source_type": "software_package",
          "description": "Python library for unsupervised and rule-based time series anomaly detection with unified APIs, flexible algorithm combination, and support for feature engineering and ensemble methods"
        },
        {
          "title": "TimeEval: Time Series Anomaly Detection Evaluation Framework",
          "url": "https://timeeval.readthedocs.io/",
          "source_type": "software_package",
          "description": "Comprehensive evaluation tool for comparing time series anomaly detection algorithms across multiple datasets with standardized metrics and distributed execution support"
        },
        {
          "title": "DeepOD: Deep Learning for Outlier Detection",
          "url": "https://deepod.readthedocs.io/",
          "source_type": "software_package",
          "description": "Python library featuring 27 deep learning algorithms for tabular and time-series anomaly detection with unified APIs and diverse network architectures including LSTM, GRU, TCN, and Transformer"
        },
        {
          "title": "A Beginner's Guide to Anomaly Detection Techniques in Data Science",
          "url": "https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html",
          "source_type": "tutorial",
          "description": "Beginner-friendly introduction covering Isolation Forest, Local Outlier Factor, and Autoencoder techniques with explanations of point, contextual, and collective anomaly types"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "out-of-distribution-detector-for-neural-networks"
      ]
    },
    {
      "slug": "confidence-thresholding",
      "name": "Confidence Thresholding",
      "description": "Confidence thresholding creates decision boundaries based on model uncertainty scores, routing predictions into different handling workflows depending on their confidence levels. High-confidence predictions (e.g., above 95%) proceed automatically, whilst medium-confidence cases (e.g., 70-95%) may trigger additional validation or human review, and low-confidence predictions (below 70%) receive extensive oversight or default to conservative fallback actions. This technique enables organisations to maintain automated efficiency for clear-cut cases whilst ensuring appropriate human intervention for uncertain decisions, balancing operational speed with risk management across safety-critical applications.",
      "assurance_goals": [
        "Safety",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/safety",
        "assurance-goal-category/reliability",
        "assurance-goal-category/reliability/uncertainty-quantification",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Implementing tiered confidence thresholds in autonomous vehicle decision-making where high-confidence lane changes (>98%) execute automatically, medium-confidence decisions (85-98%) trigger additional sensor verification, and low-confidence situations (<85%) engage conservative defensive driving modes or request human takeover.",
          "goal": "Safety"
        },
        {
          "description": "Deploying confidence thresholding in fraud detection systems where high-confidence legitimate transactions (>90%) process immediately, medium-confidence cases (70-90%) undergo additional automated checks, and low-confidence transactions (<70%) require human analyst review, ensuring system reliability through graduated response mechanisms.",
          "goal": "Reliability"
        },
        {
          "description": "Using confidence thresholds in automated loan decisions to provide clear explanations to applicants, where high-confidence approvals include simple explanations, medium-confidence decisions provide detailed reasoning about key factors, and low-confidence cases receive comprehensive explanations with guidance on potential improvements.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Many models produce poorly calibrated confidence scores that don't accurately reflect true prediction uncertainty, leading to overconfident predictions for incorrect outputs or underconfident scores for correct predictions."
        },
        {
          "description": "Threshold selection requires careful calibration and domain expertise, as inappropriate thresholds can either overwhelm human reviewers with too many cases or miss genuinely uncertain decisions that need oversight."
        },
        {
          "description": "High-confidence predictions may still be incorrect or harmful, particularly when models encounter adversarial inputs, out-of-distribution data, or systematic biases that the confidence mechanism doesn't detect."
        },
        {
          "description": "Static thresholds may become inappropriate over time as model performance degrades, data distribution shifts occur, or operational contexts change, requiring ongoing monitoring and adjustment."
        },
        {
          "description": "Implementation complexity increases significantly when managing multiple confidence levels and routing mechanisms, potentially introducing system failures or inconsistencies in how different confidence ranges are handled."
        }
      ],
      "resources": [
        {
          "title": "A Novel Dynamic Confidence Threshold Estimation AI Algorithm for Enhanced Object Detection",
          "url": "https://www.semanticscholar.org/paper/93cda7adfa043c969639e094d6c27b1c4d507208",
          "source_type": "technical_paper",
          "authors": [
            "Mounika Thatikonda",
            "M. Pk",
            "Fathi H. Amsaad"
          ]
        },
        {
          "title": "Improving speech recognition accuracy with multi-confidence thresholding",
          "url": "https://www.semanticscholar.org/paper/bef1c8668115675f786e5a3c6d165f268e399e9d",
          "source_type": "technical_paper",
          "authors": [
            "Shuangyu Chang"
          ]
        },
        {
          "title": "Improving the Robustness and Generalization of Deep Neural Network with Confidence Threshold Reduction",
          "url": "http://arxiv.org/pdf/2206.00913v2",
          "source_type": "technical_paper",
          "authors": [
            "Xiangyuan Yang",
            "Jie Lin",
            "Hanlin Zhang",
            "Xinyu Yang",
            "Peng Zhao"
          ],
          "publication_date": "2022-06-02"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "internal-review-boards",
        "red-teaming",
        "human-in-the-loop-safeguards",
        "runtime-monitoring-and-circuit-breakers"
      ]
    },
    {
      "slug": "model-cards",
      "name": "Model Cards",
      "description": "Model cards are standardised documentation frameworks that systematically document machine learning models through structured templates. The templates cover intended use cases, performance metrics across different demographic groups and operating conditions, training data characteristics, evaluation procedures, limitations, and ethical considerations. They serve as comprehensive technical specifications that enable informed model selection, prevent inappropriate deployment, support regulatory compliance, and facilitate fair assessment by providing transparent reporting of model capabilities and constraints across diverse populations and scenarios.",
      "assurance_goals": [
        "Transparency",
        "Fairness",
        "Safety"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/transparency",
        "assurance-goal-category/transparency/documentation/model-card",
        "assurance-goal-category/fairness",
        "assurance-goal-category/safety",
        "data-requirements/access-to-training-data",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/documentation",
        "expertise-needed/ml-engineering",
        "expertise-needed/regulatory-compliance",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/documentation"
      ],
      "example_use_cases": [
        {
          "description": "Documenting a medical diagnosis AI with detailed performance metrics across different patient demographics, age groups, and clinical conditions, enabling healthcare providers to understand when the model should be trusted and when additional expert consultation is needed for patient safety.",
          "goal": "Safety"
        },
        {
          "description": "Creating comprehensive model cards for hiring algorithms that transparently report performance differences across demographic groups, helping HR departments identify potential bias issues and ensure equitable candidate evaluation processes.",
          "goal": "Fairness"
        },
        {
          "description": "Publishing detailed model documentation for a credit scoring API that clearly describes training data sources, evaluation methodologies, and performance limitations, enabling financial institutions to make informed decisions about model deployment and regulatory compliance.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Creating comprehensive model cards requires substantial time, expertise, and resources to gather performance data across diverse conditions and demographic groups, potentially delaying model deployment timelines."
        },
        {
          "description": "Information can become outdated quickly as models are retrained, updated, or deployed in new contexts, requiring ongoing maintenance and version control to remain accurate and useful."
        },
        {
          "description": "Organisations may provide incomplete or superficial documentation to avoid revealing competitive advantages or potential liabilities, undermining the transparency goals of model cards."
        },
        {
          "description": "Lack of standardised formats and enforcement mechanisms means model card quality and completeness vary significantly across different organisations and use cases."
        },
        {
          "description": "Technical complexity of documenting model behaviour across all relevant dimensions may exceed the expertise of some development teams, leading to gaps in critical information."
        }
      ],
      "resources": [
        {
          "title": "Model Cards for Model Reporting",
          "url": "http://arxiv.org/pdf/1810.03993v2",
          "source_type": "technical_paper",
          "authors": [
            "Margaret Mitchell",
            "Simone Wu",
            "Andrew Zaldivar",
            "Parker Barnes",
            "Lucy Vasserman",
            "Ben Hutchinson",
            "Elena Spitzer",
            "Inioluwa Deborah Raji",
            "Timnit Gebru"
          ],
          "publication_date": "2018-10-05",
          "description": "Foundational paper introducing model cards as a framework for transparent model reporting and responsible AI documentation"
        },
        {
          "title": "Model Card Guidebook",
          "url": "https://huggingface.co/docs/hub/en/model-card-guidebook",
          "source_type": "tutorial",
          "description": "Comprehensive guide providing updated model card templates, creator tools, and practical insights for implementing model documentation across diverse stakeholder needs"
        },
        {
          "title": "scikit-learn model cards documentation",
          "url": "https://skops.readthedocs.io/en/stable/auto_examples/plot_model_card.html",
          "source_type": "tutorial",
          "description": "Practical tutorial demonstrating how to create comprehensive model cards for scikit-learn models using the skops library with metrics, visualisations, and metadata"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "datasheets-for-datasets",
        "data-version-control",
        "automated-documentation-generation"
      ]
    },
    {
      "slug": "monotonicity-constraints",
      "name": "Monotonicity Constraints",
      "description": "Monotonicity constraints enforce consistent directional relationships between input features and model predictions, ensuring that increasing a feature value either always increases, always decreases, or has no effect on the output. These constraints integrate domain knowledge into model training, preventing counterintuitive relationships that may arise from spurious correlations in data. By maintaining logical feature relationships (e.g., experience always positively influences salary), monotonicity constraints enhance model trustworthiness, interpretability, and alignment with business logic whilst often improving generalisation to new data.",
      "assurance_goals": [
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/tree-based",
        "applicable-models/gaussian-process",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/labelled-data",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "expertise-needed/domain-knowledge",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Enforcing that a mortgage approval model always treats higher income, longer employment history, and higher credit scores as positive factors, making the decision logic transparent and intuitive for loan officers and applicants whilst preventing counterintuitive relationships that could undermine trust in the system.",
          "goal": "Transparency"
        },
        {
          "description": "Constraining a healthcare cost prediction model so that age and number of chronic conditions always increase predicted costs, ensuring the model generalises reliably to new patient populations and maintains logical behaviour even when training data contains sampling biases or unusual correlations.",
          "goal": "Reliability"
        },
        {
          "description": "Implementing monotonic constraints in an insurance premium model where driving experience always reduces premiums and accident history always increases them, creating consistent pricing logic that regulatory authorities can easily validate and customers can understand and trust.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Can reduce model accuracy when real-world relationships are inherently non-monotonic, such as the inverted-U relationship between experience and performance, where constraints force oversimplified linear relationships."
        },
        {
          "description": "Requires substantial domain expertise to identify which features should have monotonic relationships, creating dependency on subject matter experts and potential for incorrect constraint specification."
        },
        {
          "description": "Increases computational complexity during training as optimisation algorithms must respect additional constraints, potentially leading to longer training times and convergence difficulties."
        },
        {
          "description": "May mask important non-linear patterns in data that could be crucial for understanding system behaviour, particularly in exploratory analysis where discovering unexpected relationships is valuable."
        },
        {
          "description": "Limited applicability to certain model types, with implementation varying significantly across algorithms (well-supported in tree-based models, more complex in neural networks), restricting technique flexibility."
        }
      ],
      "resources": [
        {
          "title": "Monotonic Constraints — xgboost 3.1.0-dev documentation",
          "url": "https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html",
          "source_type": "documentation",
          "description": "Comprehensive tutorial on implementing monotonic constraints in XGBoost, including parameter configuration, practical examples, and visual demonstrations of constraint effects on model predictions."
        },
        {
          "title": "NONPARAMETRIC KERNEL REGRESSION SUBJECT TO MONOTONICITY CONSTRAINTS",
          "url": "https://www.semanticscholar.org/paper/28e2be532d66694d3fe3486671f5c0217f58892d",
          "source_type": "technical_paper",
          "authors": [
            "P. Hall",
            "Li-Shan Huang"
          ],
          "description": "Foundational research paper on implementing monotonicity constraints in nonparametric kernel regression methods, providing theoretical background and algorithmic approaches for enforcing monotonic relationships."
        },
        {
          "title": "scikit-learn Isotonic Regression",
          "url": "https://scikit-learn.org/stable/modules/isotonic.html",
          "source_type": "documentation",
          "description": "Documentation for scikit-learn's isotonic regression implementation, providing alternative approach to monotonic relationships through non-parametric regression that preserves monotonic order."
        },
        {
          "title": "High-dimensional additive Gaussian processes under monotonicity constraints",
          "url": "https://www.semanticscholar.org/paper/4d4f1e2de3742735dcc47d2e51cc572a4415231e",
          "source_type": "technical_paper",
          "authors": [
            "Andrés F. López-Lopera",
            "F. Bachoc",
            "O. Roustant"
          ],
          "description": "Advanced research on extending monotonicity constraints to high-dimensional Gaussian process models, addressing scalability challenges and additive model structures for complex constraint applications."
        },
        {
          "title": "cagrell/gp_constr",
          "url": "https://github.com/cagrell/gp_constr",
          "source_type": "software_package",
          "description": "Python implementation of Gaussian process regression with linear operator constraints including boundedness and monotonicity, featuring RBF and Matérn kernels with practical examples."
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "intrinsically-interpretable-models",
        "generalized-additive-models"
      ]
    },
    {
      "slug": "generalized-additive-models",
      "name": "Generalized Additive Models",
      "description": "An intrinsically interpretable modelling technique that extends linear models by allowing flexible, nonlinear relationships between individual features and the target whilst maintaining the additive structure that preserves transparency. Each feature's effect is modelled separately as a smooth function, visualised as a curve showing how the feature influences predictions across its range. GAMs achieve this through spline functions or other smoothing techniques that capture complex patterns in individual variables without interactions, making them particularly valuable for domains requiring both predictive accuracy and model interpretability.",
      "assurance_goals": [
        "Transparency",
        "Explainability",
        "Reliability"
      ],
      "tags": [
        "applicable-models/gam",
        "applicable-models/linear-model",
        "assurance-goal-category/transparency",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/feature-analysis",
        "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
        "assurance-goal-category/reliability",
        "data-requirements/labelled-data",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/visualisation",
        "expertise-needed/statistics",
        "explanatory-scope/global",
        "lifecycle-stage/project-design",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Predicting hospital readmission risk with a GAM that provides transparent, auditable risk assessments by showing how readmission probability varies nonlinearly with patient age, blood pressure, and medication adherence, enabling clinicians to understand and trust the model's reasoning for regulatory compliance.",
          "goal": "Transparency"
        },
        {
          "description": "Building a credit scoring model that explains loan decisions to applicants by visualising how income, credit history, and debt-to-income ratio individually affect approval likelihood, providing clear feature attributions that satisfy fair lending requirements and regulatory explainability mandates.",
          "goal": "Explainability"
        },
        {
          "description": "Developing an environmental monitoring system that reliably predicts air quality using GAMs to model the smooth, nonlinear relationships between weather variables, ensuring stable predictions across seasonal variations whilst maintaining interpretable relationships that environmental scientists can validate.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Cannot capture complex interactions between features unless explicitly modelled, limiting their ability to represent relationships where variables influence each other."
        },
        {
          "description": "Setup requires domain expertise to decide which features need nonlinear treatment and appropriate smoothing parameters, making model specification more challenging than linear models."
        },
        {
          "description": "Fitting process is computationally more expensive than linear models, particularly for large datasets with many features requiring smoothing."
        },
        {
          "description": "Risk of overfitting individual feature relationships if smoothing parameters are not properly regularised, potentially reducing generalisation performance."
        },
        {
          "description": "Interpretation complexity increases with the number of nonlinear features, as understanding multiple smooth curves simultaneously becomes cognitively demanding."
        }
      ],
      "resources": [
        {
          "title": "Generalized Additive Models",
          "url": "https://hastie.su.domains/Papers/gam.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Trevor Hastie",
            "Robert Tibshirani"
          ],
          "publication_date": "1986-01-01"
        },
        {
          "title": "pyGAM: Generalized Additive Models in Python",
          "url": "https://github.com/dswah/pyGAM",
          "source_type": "software_package"
        },
        {
          "title": "mgcv: Mixed GAM Computation Vehicle with Automatic Smoothness Estimation",
          "url": "https://cran.r-project.org/web/packages/mgcv/index.html",
          "source_type": "software_package"
        },
        {
          "title": "A Tour of pyGAM — pyGAM documentation",
          "url": "https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "GAMs",
      "related_techniques": [
        "monotonicity-constraints",
        "intrinsically-interpretable-models"
      ]
    },
    {
      "slug": "prompt-sensitivity-analysis",
      "name": "Prompt Sensitivity Analysis",
      "description": "Prompt Sensitivity Analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. This technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. It encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. The analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/llm",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "expertise-needed/linguistics",
        "expertise-needed/experimental-design",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "technique-type/experimental"
      ],
      "example_use_cases": [
        {
          "description": "Testing medical diagnosis LLMs with semantically equivalent but syntactically different symptom descriptions to ensure consistent diagnostic recommendations across different patient communication styles, identifying potential failure modes where slight phrasing changes could lead to dangerous misdiagnoses.",
          "goal": "Safety"
        },
        {
          "description": "Analysing how variations in candidate descriptions (gendered language, cultural references, educational institution prestige indicators) affect LLM-based CV screening recommendations to identify potential discriminatory patterns and ensure equitable treatment across diverse applicant backgrounds.",
          "goal": "Reliability"
        },
        {
          "description": "Examining how different ways of framing financial questions (formal vs informal language, technical vs layperson terminology) affect investment advice generated by LLMs to improve user understanding and model transparency whilst ensuring consistent advisory quality.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Analysis is inherently limited to the specific prompt variations tested, potentially missing important sensitivity patterns that weren't anticipated during study design, making comprehensive coverage challenging."
        },
        {
          "description": "Systematic exploration of prompt variations can be computationally expensive, particularly for large-scale sensitivity analysis across multiple dimensions of variation, requiring significant resources for thorough evaluation."
        },
        {
          "description": "Ensuring that prompt variations maintain semantic equivalence whilst introducing meaningful perturbations requires careful linguistic expertise and validation, which can be subjective and domain-dependent."
        },
        {
          "description": "Results may reveal sensitivity patterns that are difficult to interpret or act upon, particularly when multiple types of variations interact in complex ways, limiting practical applicability of findings."
        },
        {
          "description": "The meaningfulness of sensitivity measurements depends heavily on the choice of baseline prompts and variation strategies, which can introduce methodological biases and affect the generalisability of conclusions."
        }
      ],
      "resources": [
        {
          "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
          "url": "https://www.semanticscholar.org/paper/17a6116e5bbd8b87082cbb2e795885567300c483",
          "source_type": "technical_paper",
          "authors": [
            "Melanie Sclar",
            "Yejin Choi",
            "Yulia Tsvetkov",
            "Alane Suhr"
          ]
        },
        {
          "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts",
          "url": "http://arxiv.org/pdf/2505.12592v1",
          "source_type": "technical_paper",
          "authors": [
            "Sullam Jeoung",
            "Yueyan Chen",
            "Yi Zhang",
            "Shuai Wang",
            "Haibo Ding",
            "Lin Lee Cheong"
          ],
          "publication_date": "2025-05-19"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "causal-mediation-analysis-in-language-models",
        "feature-attribution-with-integrated-gradients-in-nlp"
      ]
    },
    {
      "slug": "disparate-impact-remover",
      "name": "Disparate Impact Remover",
      "description": "Disparate Impact Remover is a preprocessing technique that transforms feature values in a dataset to reduce statistical dependence between features and protected attributes (like race or gender). The method modifies non-protected features through mathematical transformations that preserve the utility of the data whilst reducing correlations that could lead to discriminatory outcomes. This approach specifically targets the '80% rule' disparate impact threshold by adjusting feature distributions to ensure more equitable treatment across demographic groups in downstream model predictions.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Transforming features in a credit scoring dataset where variables like 'years of employment' and 'education level' are correlated with race, applying mathematical transformations to reduce these correlations whilst preserving the predictive value for creditworthiness assessment.",
          "goal": "Fairness"
        },
        {
          "description": "Preprocessing a recruitment dataset where features like 'previous job titles' and 'university attended' correlate with gender, modifying these features to ensure the '80% rule' is met whilst maintaining useful information for predicting job performance.",
          "goal": "Fairness"
        },
        {
          "description": "Preprocessing financial lending data to provide transparent bias metrics showing the quantified reduction in correlation between protected attributes and creditworthiness features, enabling institutions to demonstrate compliance with the 80% rule and explain their fairness interventions to regulators.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring consistent model performance across demographic groups in healthcare risk assessment by mathematically transforming features to reduce protected attribute correlations, improving reliability of predictions for minority populations who may have been systematically under-served.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Feature transformations may reduce model accuracy by removing or distorting important predictive information during the debiasing process."
        },
        {
          "description": "Only addresses measured protected attributes and cannot eliminate bias that operates through unmeasured proxy variables."
        },
        {
          "description": "Effectiveness depends on the specific transformation method chosen and may not generalise well to different datasets or domains."
        },
        {
          "description": "May create artificial feature distributions that don't reflect real-world data patterns, potentially causing issues in model deployment."
        }
      ],
      "resources": [
        {
          "title": "holistic-ai/holisticai",
          "url": "https://github.com/holistic-ai/holisticai",
          "source_type": "software_package",
          "description": "Comprehensive open-source toolkit for AI fairness with bias measurement, mitigation techniques, and visualisation tools"
        },
        {
          "title": "Disparate Impact Remover — holisticai documentation",
          "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/preprocessing/bc_disparate_impact_remover_disparate_impact_remover.html",
          "source_type": "tutorial",
          "description": "Comprehensive tutorial covering theoretical background, methodology, and practical implementation of disparate impact removal"
        },
        {
          "title": "Trusted-AI/AIF360",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package",
          "description": "IBM Research's extensible open-source library for detecting and mitigating algorithmic bias across multiple domains"
        },
        {
          "title": "aif360.algorithms.preprocessing.DisparateImpactRemover — aif360 ...",
          "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.DisparateImpactRemover.html",
          "source_type": "documentation",
          "description": "Technical API documentation for AIF360's DisparateImpactRemover class with parameters, methods, and usage examples"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "reweighing",
        "relabelling",
        "preferential-sampling"
      ]
    },
    {
      "slug": "fairness-gan",
      "name": "Fairness GAN",
      "description": "A data generation technique that employs Generative Adversarial Networks (GANs) to create fair synthetic datasets by learning to generate data representations that preserve utility whilst obfuscating protected attributes. Unlike traditional GANs, Fairness GANs incorporate fairness constraints into the training objective, ensuring that the generated data maintains statistical parity across demographic groups. The technique can be used for data augmentation to balance underrepresented groups or to create privacy-preserving synthetic datasets that remove demographic bias from training data.",
      "assurance_goals": [
        "Fairness",
        "Privacy",
        "Reliability"
      ],
      "tags": [
        "applicable-models/gan",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/privacy",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "evidence-type/synthetic-data",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/data-collection/data-augmentation",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Generating balanced synthetic datasets for medical research by creating additional samples from underrepresented demographic groups, ensuring equal representation across ethnicity and gender whilst maintaining the statistical properties needed for robust model training.",
          "goal": "Fairness"
        },
        {
          "description": "Creating privacy-preserving synthetic datasets for financial services that remove demographic identifiers whilst preserving the underlying patterns needed for credit risk assessment, allowing secure data sharing between institutions without exposing sensitive customer information.",
          "goal": "Privacy"
        },
        {
          "description": "Augmenting recruitment datasets by generating synthetic candidate profiles that balance gender and ethnicity representation, ensuring reliable model performance across all demographic groups when real-world data exhibits significant imbalances.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "GAN training is notoriously difficult to stabilise, with potential for mode collapse or failure to converge, especially when additional fairness constraints are imposed."
        },
        {
          "description": "Ensuring fairness in generated data may come at the cost of data utility, potentially reducing the quality or realism of synthetic samples."
        },
        {
          "description": "Requires large datasets to train both generator and discriminator networks effectively, limiting applicability in data-scarce domains."
        },
        {
          "description": "Evaluation complexity is high, as it requires assessing both the quality of generated data and the preservation of fairness properties across demographic groups."
        },
        {
          "description": "May inadvertently introduce new biases if the fairness constraints are not properly specified or if the training data itself contains subtle biases."
        }
      ],
      "resources": [
        {
          "title": "Fairness GAN",
          "url": "http://arxiv.org/pdf/1805.09910v1",
          "source_type": "technical_paper",
          "authors": [
            "Prasanna Sattigeri",
            "Samuel C. Hoffman",
            "Vijil Chenthamarakshan",
            "Kush R. Varshney"
          ],
          "publication_date": "2018-05-24"
        },
        {
          "title": "Fair GANs through model rebalancing for extremely imbalanced class distributions",
          "url": "http://arxiv.org/pdf/2308.08638v2",
          "source_type": "technical_paper",
          "authors": [
            "Anubhav Jain",
            "Nasir Memon",
            "Julian Togelius"
          ],
          "publication_date": "2023-08-16"
        },
        {
          "title": "Inclusive GAN: Improving Data and Minority Coverage in Generative Models",
          "url": "http://arxiv.org/abs/2004.03355",
          "source_type": "technical_paper",
          "authors": [
            "Ning Yu",
            "Ke Li",
            "Peng Zhou",
            "Jitendra Malik",
            "Larry Davis",
            "Mario Fritz"
          ],
          "publication_date": "2020-04-07"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "sensitivity-analysis-for-fairness",
        "attribute-removal-fairness-through-unawareness",
        "bayesian-fairness-regularization"
      ]
    },
    {
      "slug": "relabelling",
      "name": "Relabelling",
      "description": "A preprocessing fairness technique that modifies class labels in training data to achieve equal positive outcome rates across protected groups. Also known as 'data massaging', this method identifies instances that contribute to discriminatory patterns and flips their labels (from positive to negative or vice versa) to balance the proportion of positive outcomes between demographic groups. The technique aims to remove historical bias from training data whilst preserving the overall class distribution, enabling standard classifiers to learn from discrimination-free datasets.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/access-to-training-data",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/dataset-analysis",
        "expertise-needed/statistics",
        "expertise-needed/domain-knowledge",
        "fairness-approach/group",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/data-collection/data-preprocessing",
        "lifecycle-stage/model-development",
        "technique-type/procedural"
      ],
      "example_use_cases": [
        {
          "description": "Preprocessing historical hiring datasets by relabelling borderline cases to ensure equal hiring rates across gender and ethnicity groups, correcting for past discriminatory practices whilst maintaining overall qualification standards for fair recruitment model training.",
          "goal": "Fairness"
        },
        {
          "description": "Creating transparent credit scoring datasets by documenting which loan applications had labels modified to address historical lending discrimination, providing clear audit trails showing how training data bias was systematically corrected before model development.",
          "goal": "Transparency"
        },
        {
          "description": "Improving reliability of medical diagnosis training data by relabelling cases where demographic bias may have influenced historical diagnoses, ensuring models learn from corrected labels that reflect true medical conditions rather than biased historical treatment patterns.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Altering training labels risks introducing new biases or artificial patterns that may not reflect genuine relationships in the data."
        },
        {
          "description": "Deciding which instances to relabel requires careful selection criteria and domain expertise to avoid inappropriate label changes."
        },
        {
          "description": "May reduce prediction accuracy if too many labels are changed, particularly when the modifications conflict with genuine patterns in the data."
        },
        {
          "description": "Requires access to ground truth or expert knowledge to determine whether original labels reflect genuine outcomes or discriminatory bias."
        },
        {
          "description": "Effectiveness depends on accurate identification of discriminatory instances, which can be challenging when bias patterns are subtle or complex."
        }
      ],
      "resources": [
        {
          "title": "Data preprocessing techniques for classification without discrimination",
          "url": "https://link.springer.com/article/10.1007/s10115-011-0463-8",
          "source_type": "technical_paper",
          "authors": [
            "Faisal Kamiran",
            "Toon Calders"
          ],
          "publication_date": "2012-06-01"
        },
        {
          "title": "Classifying without discriminating",
          "url": "https://www.researchgate.net/publication/224440330_Classifying_without_discriminating",
          "source_type": "technical_paper",
          "authors": [
            "Toon Calders",
            "Sicco Verwer"
          ],
          "publication_date": "2010-02-01"
        },
        {
          "title": "Data Pre-Processing for Discrimination Prevention",
          "url": "https://krvarshney.github.io/pubs/CalmonWVRV_jstsp2018.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Flavio Calmon",
            "Dennis Wei",
            "Bhanukiran Vinzamuri",
            "Karthikeyan Natesan Ramamurthy",
            "Kush R. Varshney"
          ],
          "publication_date": "2018-01-01"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "reweighing",
        "disparate-impact-remover",
        "preferential-sampling"
      ]
    },
    {
      "slug": "fair-adversarial-networks",
      "name": "Fair Adversarial Networks",
      "description": "An in-processing fairness technique that employs adversarial training with dual neural networks to learn fair representations. The method consists of a predictor network that learns the main task whilst an adversarial discriminator network simultaneously attempts to predict sensitive attributes from the predictor's hidden representations. Through this adversarial min-max game, the predictor is incentivised to learn features that are informative for the task but statistically independent of protected attributes, effectively removing bias at the representation level in deep learning models.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/cnn",
        "applicable-models/gan",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training a facial recognition system that maintains high accuracy for person identification whilst ensuring equal performance across different ethnic groups, using adversarial training to remove race-related features from learned representations.",
          "goal": "Fairness"
        },
        {
          "description": "Developing a resume screening neural network that provides transparent evidence of bias mitigation by demonstrating that learned features cannot predict gender, whilst maintaining predictive performance for job suitability assessment.",
          "goal": "Transparency"
        },
        {
          "description": "Creating a medical image analysis model that achieves reliable diagnostic performance across patient demographics by using adversarial debiasing to ensure age and gender information cannot be extracted from diagnostic features.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Implementation complexity is high, requiring careful design of adversarial loss functions and balancing multiple competing objectives during training."
        },
        {
          "description": "Sensitive to hyperparameter choices, particularly the trade-off weights between prediction accuracy and adversarial loss, which require extensive tuning."
        },
        {
          "description": "Adversarial training can be unstable, with potential for mode collapse or failure to converge, especially in complex deep learning architectures."
        },
        {
          "description": "Interpretability of fairness improvements can be limited, as it may be difficult to verify that sensitive attributes are truly removed from learned representations."
        },
        {
          "description": "Computational overhead is significant due to training two networks simultaneously, increasing both training time and resource requirements."
        }
      ],
      "resources": [
        {
          "title": "Fair Adversarial Networks",
          "url": "http://arxiv.org/pdf/2002.12144v1",
          "source_type": "technical_paper",
          "authors": [
            "George Cevora"
          ],
          "publication_date": "2020-02-23"
        },
        {
          "title": "Demonstrating Rosa: the fairness solution for any Data Analytic pipeline",
          "url": "http://arxiv.org/pdf/2003.00899v2",
          "source_type": "technical_paper",
          "authors": [
            "Kate Wilkinson",
            "George Cevora"
          ],
          "publication_date": "2020-02-28"
        },
        {
          "title": "Triangular Trade-off between Robustness, Accuracy, and Fairness in Deep Neural Networks: A Survey",
          "url": "https://www.semanticscholar.org/paper/13b0444d079bea1c8c57a6082200b67ab5f4616e",
          "source_type": "documentation",
          "authors": [
            "Jingyang Li",
            "Guoqiang Li"
          ],
          "publication_date": "2025-02-10"
        },
        {
          "title": "Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks",
          "url": "https://www.semanticscholar.org/paper/6995779ac582c5f2436cfb82a3c8cf5ca72bae2f",
          "source_type": "technical_paper",
          "authors": [
            "Resmi Ramachandranpillai",
            "Md Fahim Sikder",
            "David Bergström",
            "Fredrik Heintz"
          ],
          "publication_date": "2023-12-14"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "adversarial-debiasing",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "prejudice-remover-regulariser",
      "name": "Prejudice Remover Regulariser",
      "description": "An in-processing fairness technique that adds a fairness penalty to machine learning models to reduce bias against protected groups. The method works by minimising 'mutual information' - essentially reducing how much the model's predictions reveal about sensitive attributes like race or gender. By adding this penalty term to the learning objective (typically in logistic regression), the technique ensures predictions become less dependent on protected features. This addresses not only direct discrimination but also indirect bias through correlated features. Practitioners can adjust a tuning parameter to balance between maintaining accuracy and removing prejudice from the model.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/logistic-regression",
        "applicable-models/probabilistic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training credit scoring models with prejudice remover regularisation to ensure loan approval decisions are not influenced by gender or ethnicity, minimising mutual information between predictions and protected attributes whilst maintaining accurate risk assessment.",
          "goal": "Fairness"
        },
        {
          "description": "Developing transparent university admission models that provide clear evidence of bias mitigation by demonstrating reduced statistical dependence between acceptance decisions and protected characteristics, enabling regulatory compliance reporting.",
          "goal": "Transparency"
        },
        {
          "description": "Building reliable recruitment screening models that maintain consistent performance across demographic groups by regularising against indirect prejudice through correlated features like school names or postal codes that might proxy for protected attributes.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Requires careful tuning of the fairness penalty hyperparameter, where too high values severely degrade accuracy whilst too low values provide insufficient bias mitigation."
        },
        {
          "description": "Primarily applicable to probabilistic discriminative models like logistic regression, limiting its use with other model architectures such as deep neural networks or tree-based methods."
        },
        {
          "description": "Computational complexity increases with the calculation of mutual information between predictions and sensitive attributes, particularly for high-dimensional data."
        },
        {
          "description": "May not fully eliminate all forms of discrimination, particularly when complex interactions between multiple sensitive attributes create intersectional biases."
        },
        {
          "description": "Effectiveness depends on accurate identification and inclusion of all sensitive attributes, potentially missing hidden biases from unobserved protected characteristics."
        }
      ],
      "resources": [
        {
          "title": "Fairness-Aware Classifier with Prejudice Remover Regularizer",
          "url": "https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3",
          "source_type": "technical_paper",
          "authors": [
            "Toshihiro Kamishima",
            "Shotaro Akaho",
            "Hideki Asoh",
            "Jun Sakuma"
          ],
          "publication_date": "2012-09-24"
        },
        {
          "title": "Fairness-Aware Machine Learning and Data Mining",
          "url": "https://www.kamishima.net/faml/",
          "source_type": "documentation"
        },
        {
          "title": "Fairness-aware Classifier (faclass)",
          "url": "https://www.kamishima.net/faclass/",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "meta-fair-classifier",
      "name": "Meta Fair Classifier",
      "description": "An in-processing fairness technique that employs meta-learning to modify any existing classifier for optimising fairness metrics whilst maintaining predictive performance. The method learns how to adjust model parameters or decision boundaries to satisfy fairness constraints such as demographic parity or equalised odds through iterative optimisation. This approach is particularly valuable when retrofitting fairness to pre-trained models that perform well but exhibit bias, as it can incorporate fairness without requiring complete retraining from scratch.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "lifecycle-stage/model-optimization",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Retrofitting an existing hiring algorithm to achieve demographic parity across gender and ethnicity groups by using meta-learning to adjust decision boundaries, ensuring equitable candidate selection whilst maintaining the model's ability to identify qualified applicants.",
          "goal": "Fairness"
        },
        {
          "description": "Modifying a pre-trained credit scoring model to provide transparent fairness guarantees by learning optimal parameter adjustments that satisfy equalised odds constraints, enabling clear reporting on fair lending compliance to regulatory authorities.",
          "goal": "Transparency"
        },
        {
          "description": "Adapting a medical diagnosis model to ensure reliable performance across patient demographics by meta-learning fairness-aware adjustments that maintain diagnostic accuracy whilst reducing disparities in treatment recommendations across age and socioeconomic groups.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Meta-learning approach can be complex to implement, requiring expertise in both the underlying classifier and meta-optimisation techniques."
        },
        {
          "description": "Requires extensive hyperparameter tuning to balance fairness constraints with predictive performance, making optimisation challenging."
        },
        {
          "description": "May result in longer training times compared to simpler fairness techniques due to the iterative meta-learning process."
        },
        {
          "description": "Performance depends heavily on the quality and characteristics of the base classifier being modified, limiting effectiveness with poorly-performing models."
        },
        {
          "description": "Theoretical guarantees about fairness-accuracy trade-offs may not hold in practice due to finite sample effects and optimisation challenges."
        }
      ],
      "resources": [
        {
          "title": "ρ-Fair Method — holisticai documentation",
          "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/inprocessing/bc_meta_fair_classifier_rho_fair.html",
          "source_type": "documentation"
        },
        {
          "title": "aif360.algorithms.inprocessing — aif360 0.1.0 documentation",
          "url": "https://aif360.readthedocs.io/en/v0.2.3/modules/inprocessing.html",
          "source_type": "documentation"
        },
        {
          "title": "Welcome to AI Fairness 360's documentation! — aif360 0.1.0 ...",
          "url": "https://aif360.readthedocs.io/en/v0.2.3/",
          "source_type": "documentation"
        },
        {
          "title": "Algorithmic decision making methods for fair credit scoring",
          "url": "http://arxiv.org/abs/2209.07912",
          "source_type": "technical_paper",
          "authors": [
            "Moldovan, Darie"
          ],
          "publication_date": "2022-09-16"
        },
        {
          "title": "The Importance of Modeling Data Missingness in Algorithmic Fairness: A\n  Causal Perspective",
          "url": "http://arxiv.org/abs/2012.11448",
          "source_type": "technical_paper",
          "authors": [
            "Amayuelas, Alfonso",
            "Deshpande, Amit",
            "Goel, Naman",
            "Sharma, Amit"
          ],
          "publication_date": "2020-12-21"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "exponentiated-gradient-reduction",
      "name": "Exponentiated Gradient Reduction",
      "description": "An in-processing fairness technique based on Agarwal et al.'s reductions approach that transforms fair classification into a sequence of cost-sensitive classification problems. The method uses an exponentiated gradient algorithm to iteratively reweight training data, returning a randomised classifier that achieves the lowest empirical error whilst satisfying fairness constraints. This reduction-based framework provides theoretical guarantees about both accuracy and constraint violation, making it suitable for various fairness criteria including demographic parity and equalised odds.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/labelled-data",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training a hiring algorithm with demographic parity constraints to ensure equal selection rates across gender groups, using iterative reweighting to balance fairness and predictive accuracy whilst maintaining legal compliance.",
          "goal": "Fairness"
        },
        {
          "description": "Developing a loan approval model with equalised odds constraints, providing transparent documentation of the theoretical guarantees about both error rates and fairness constraint violations achieved by the reduction approach.",
          "goal": "Transparency"
        },
        {
          "description": "Creating a medical diagnosis classifier that maintains reliable performance across demographic groups by using randomised prediction averaging, ensuring consistent healthcare delivery whilst monitoring constraint satisfaction over time.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Requires convex base learners for theoretical guarantees about convergence and optimality, limiting the choice of underlying models."
        },
        {
          "description": "Produces randomised classifiers that may give different predictions for identical inputs, which can be problematic in applications requiring consistent decisions."
        },
        {
          "description": "Convergence can be slow and sensitive to hyperparameter choices, particularly the learning rate and tolerance settings."
        },
        {
          "description": "Involves iterative retraining with adjusted weights, which can be computationally expensive for large datasets or complex models."
        },
        {
          "description": "Fairness constraints may significantly reduce model accuracy, and the trade-off between fairness and performance is not always transparent to practitioners."
        }
      ],
      "resources": [
        {
          "title": "A Reductions Approach to Fair Classification",
          "url": "https://arxiv.org/abs/1803.02453",
          "source_type": "technical_paper",
          "description": "Foundational paper by Agarwal et al. introducing the exponentiated gradient reduction approach for fair classification with theoretical guarantees.",
          "authors": [
            "Alekh Agarwal",
            "Alina Beygelzimer",
            "Miroslav Dudík",
            "John Langford",
            "Hanna Wallach"
          ],
          "publication_date": "2018-03-06"
        },
        {
          "title": "Fairlearn: ExponentiatedGradient",
          "url": "https://fairlearn.org/v0.10/api_reference/generated/fairlearn.reductions.ExponentiatedGradient.html",
          "source_type": "documentation",
          "description": "Microsoft's Fairlearn implementation of the Agarwal et al. algorithm with comprehensive API documentation and examples."
        },
        {
          "title": "IBM AIF360: ExponentiatedGradientReduction",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.inprocessing.ExponentiatedGradientReduction.html",
          "source_type": "documentation",
          "description": "IBM's AIF360 implementation with scikit-learn compatible API for in-processing fairness constraints during model training."
        },
        {
          "title": "Fairlearn Reductions Guide",
          "url": "https://fairlearn.org/main/user_guide/mitigation/reductions.html",
          "source_type": "tutorial",
          "description": "Comprehensive guide to using reduction-based approaches for fairness, including practical examples of exponentiated gradient methods and fairness constraints."
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "fair-transfer-learning",
      "name": "Fair Transfer Learning",
      "description": "An in-processing fairness technique that adapts pre-trained models from one domain to another whilst explicitly preserving fairness constraints across different contexts or populations. The method addresses the challenge that fairness properties may not transfer when adapting models to new domains with different demographic compositions or data distributions. Fair transfer learning typically involves constraint-aware fine-tuning, domain adaptation techniques, or adversarial training that maintains equitable performance across groups in the target domain, ensuring that bias mitigation efforts carry over from source to target domains.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/pre-trained-model",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "lifecycle-stage/model-development/fine-tuning",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adapting a hiring algorithm trained on one country's recruitment data to another region whilst maintaining fairness across gender and ethnicity groups, ensuring equitable candidate evaluation despite different local demographic distributions and cultural contexts.",
          "goal": "Fairness"
        },
        {
          "description": "Transferring a medical diagnosis model from urban hospital data to rural clinics whilst providing transparent evidence that fairness constraints are preserved across age, gender, and socioeconomic groups despite different patient populations and healthcare infrastructure.",
          "goal": "Transparency"
        },
        {
          "description": "Adapting a fraud detection system from one financial market to another whilst ensuring reliable performance across customer demographics, maintaining consistent accuracy and fairness even when transaction patterns and customer characteristics differ between markets.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Fairness properties achieved in the source domain may not translate directly to the target domain if demographic distributions or data characteristics differ significantly."
        },
        {
          "description": "Requires careful hyperparameter tuning and constraint specification to balance fairness preservation with model performance in the new domain."
        },
        {
          "description": "Implementation complexity is high, requiring expertise in both transfer learning techniques and fairness constraint optimisation methods."
        },
        {
          "description": "May suffer from negative transfer effects where fairness constraints that worked well in the source domain actually harm performance in the target domain."
        },
        {
          "description": "Evaluation challenges arise from needing to validate fairness across multiple domains and demographic groups simultaneously."
        }
      ],
      "resources": [
        {
          "title": "Segmenting across places: The need for fair transfer learning with satellite imagery",
          "url": "http://arxiv.org/pdf/2204.04358v3",
          "source_type": "technical_paper",
          "authors": [
            "Miao Zhang",
            "Harvineet Singh",
            "Lazarus Chok",
            "Rumi Chunara"
          ],
          "publication_date": "2022-04-09"
        },
        {
          "title": "Trustworthy Transfer Learning: A Survey",
          "url": "https://www.semanticscholar.org/paper/7ee5c5b58ed0b4e585e0c30790c206bea07faacf",
          "source_type": "documentation",
          "authors": [
            "Jun Wu",
            "Jingrui He"
          ],
          "publication_date": "2024-12-18"
        },
        {
          "title": "Cross-Institutional Transfer Learning for Educational Models: Implications for Model Performance, Fairness, and Equity",
          "url": "https://arxiv.org/abs/2305.00927",
          "source_type": "technical_paper",
          "authors": [
            "Josh Gardner",
            "Renzhe Yu",
            "Quan Nguyen",
            "Christopher Brooks",
            "Rene Kizilcec"
          ],
          "publication_date": "2023-05-01"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "adaptive-sensitive-reweighting",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "multi-accuracy-boosting",
      "name": "Multi-Accuracy Boosting",
      "description": "An in-processing fairness technique that employs boosting algorithms to improve accuracy uniformly across demographic groups by iteratively correcting errors where the model performs poorly for certain subgroups. The method uses a multi-calibration approach that trains weak learners to focus on prediction errors for underperforming groups, ensuring that no group experiences systematically worse accuracy. This iterative boosting process continues until accuracy parity is achieved across all groups whilst maintaining overall model performance.",
      "assurance_goals": [
        "Fairness",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "applicable-models/ensemble",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/training",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training a medical diagnosis model that achieves equal accuracy across age, gender, and ethnicity groups by using boosting to specifically target prediction errors for underrepresented patient demographics, ensuring equitable healthcare outcomes for all populations.",
          "goal": "Fairness"
        },
        {
          "description": "Building a robust fraud detection system that maintains consistent accuracy across different customer segments by iteratively correcting errors where the model performs poorly for specific demographic or geographic groups, ensuring reliable fraud prevention across all user types.",
          "goal": "Reliability"
        },
        {
          "description": "Developing a transparent hiring algorithm that provides clear evidence of equal performance across candidate demographics by using multi-accuracy boosting to systematically address group-specific prediction errors, enabling auditable fair recruitment processes.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires identifying and defining relevant subgroups or error regions, which may be challenging when group boundaries are unclear or overlapping."
        },
        {
          "description": "Could increase model complexity significantly as the boosting process adds multiple weak learners, potentially affecting interpretability and computational efficiency."
        },
        {
          "description": "May overfit to training data if very granular corrections are made, particularly when subgroups are small or the boosting process continues for too many iterations."
        },
        {
          "description": "Performance depends on the quality of subgroup identification, and may fail to achieve fairness if important demographic intersections are not properly captured."
        },
        {
          "description": "Convergence to equal accuracy across groups is not guaranteed, especially when there are fundamental differences in data distributions between groups."
        }
      ],
      "resources": [
        {
          "title": "mcboost: Multi-Calibration Boosting for R",
          "url": "https://joss.theoj.org/papers/10.21105/joss.03453",
          "source_type": "technical_paper",
          "authors": [
            "Bernd Bischl",
            "Susanne Dandl",
            "Christoph Kern",
            "Michael P. Kim",
            "Florian Pfisterer",
            "Matthew Sun"
          ],
          "publication_date": "2021-08-24"
        },
        {
          "title": "mlr-org/mcboost",
          "url": "https://github.com/mlr-org/mcboost",
          "source_type": "software_package"
        },
        {
          "title": "Multigroup Robustness",
          "url": "http://arxiv.org/abs/2405.00614",
          "source_type": "technical_paper",
          "authors": [
            "Lunjia Hu",
            "Charlotte Peale",
            "Judy Hanwen Shen"
          ],
          "publication_date": "2024-05-01"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "adaptive-sensitive-reweighting"
      ]
    },
    {
      "slug": "equalised-odds-post-processing",
      "name": "Equalised Odds Post-Processing",
      "description": "A post-processing fairness technique based on Hardt et al.'s seminal work that adjusts classification thresholds after model training to achieve equal true positive rates and false positive rates across demographic groups. The method uses group-specific decision thresholds, potentially with randomisation, to satisfy the equalised odds constraint whilst preserving model utility. This approach enables fairness mitigation without retraining, making it applicable to existing deployed models or when training data access is restricted.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/labelled-data",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/testing",
        "lifecycle-stage/system-deployment-and-use/monitoring",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Post-processing a criminal recidivism risk assessment model to ensure equal error rates across racial groups, using group-specific thresholds to achieve equal TPR and FPR whilst maintaining predictive accuracy for judicial decision support.",
          "goal": "Fairness"
        },
        {
          "description": "Adjusting a hiring algorithm's decision thresholds to ensure equal opportunities for qualified candidates across gender groups, providing transparent evidence that the screening process treats all demographics equitably.",
          "goal": "Transparency"
        },
        {
          "description": "Calibrating a medical diagnosis model's outputs to maintain equal detection rates across age groups, ensuring reliable performance monitoring and consistent healthcare delivery regardless of patient demographics.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "May require randomisation in decision-making, leading to inconsistent outcomes for similar individuals to achieve group-level fairness constraints."
        },
        {
          "description": "Post-processing can reduce overall model accuracy or confidence scores, particularly when group-specific ROC curves do not intersect favourably."
        },
        {
          "description": "Violates calibration properties of the original model, creating a trade-off between equalised odds and predictive rate parity."
        },
        {
          "description": "Limited to combinations of error rates that lie on the intersection of group-specific ROC curves, which may represent poor trade-offs."
        },
        {
          "description": "Requires access to sensitive attributes during deployment, which may not be available or legally permissible in all contexts."
        }
      ],
      "resources": [
        {
          "title": "Equality of Opportunity in Supervised Learning",
          "url": "https://arxiv.org/abs/1610.02413",
          "source_type": "technical_paper",
          "description": "Foundational paper by Hardt et al. introducing the equalised odds post-processing algorithm and mathematical framework for fairness constraints.",
          "authors": [
            "Moritz Hardt",
            "Eric Price",
            "Nathan Srebro"
          ],
          "publication_date": "2016-10-07"
        },
        {
          "title": "Equalized odds postprocessing under imperfect group information",
          "url": "https://arxiv.org/abs/1906.03284",
          "source_type": "technical_paper",
          "description": "Extension of Hardt et al.'s method examining robustness when protected attribute information is imperfect or noisy.",
          "authors": [
            "Pranjal Awasthi",
            "Matthäus Kleindessner",
            "Jamie Morgenstern"
          ],
          "publication_date": "2019-06-07"
        },
        {
          "title": "Fairlearn: ThresholdOptimizer",
          "url": "https://fairlearn.org/v0.10/api_reference/generated/fairlearn.postprocessing.ThresholdOptimizer.html",
          "source_type": "documentation",
          "description": "Microsoft's Fairlearn implementation of the Hardt et al. algorithm with API documentation and usage examples for equalised odds constraints."
        },
        {
          "title": "IBM AIF360",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package",
          "description": "Comprehensive fairness toolkit including EqualizedOddsPostprocessing implementation based on Hardt et al.'s original algorithm."
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "threshold-optimiser",
        "reject-option-classification",
        "calibration-with-equality-of-opportunity"
      ]
    },
    {
      "slug": "threshold-optimiser",
      "name": "Threshold Optimiser",
      "description": "Threshold Optimiser adjusts decision thresholds for different demographic groups after model training to satisfy specific fairness constraints. This post-processing technique optimises group-specific thresholds by analysing the probability distribution of model outputs, allowing practitioners to achieve fairness goals like demographic parity or equalised opportunity without modifying the underlying model. The optimiser finds optimal threshold values for each group that balance fairness requirements with overall model performance, making it particularly useful when fairness considerations arise after model deployment.",
      "assurance_goals": [
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/post-processing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting hiring decision thresholds in a recruitment system to ensure equal opportunity rates across gender and ethnicity groups, where the model outputs probability scores but different demographic groups require different thresholds to achieve equitable outcomes.",
          "goal": "Fairness"
        },
        {
          "description": "Optimising credit approval thresholds for different demographic groups in loan applications to satisfy regulatory requirements for equal treatment whilst maintaining acceptable default rates across all groups.",
          "goal": "Fairness"
        },
        {
          "description": "Calibrating medical diagnosis thresholds across age and gender groups to ensure diagnostic accuracy is maintained whilst preventing systematic over-diagnosis or under-diagnosis in specific populations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires a held-out dataset with known group memberships to determine optimal thresholds for each demographic group."
        },
        {
          "description": "Threshold values may need recalibration when input data distributions shift or model performance changes over time."
        },
        {
          "description": "Using different decision thresholds per group can raise legal or ethical concerns in deployment contexts where equal treatment is mandated."
        },
        {
          "description": "Performance depends on the quality and representativeness of the calibration dataset for each demographic group."
        },
        {
          "description": "May lead to reduced overall accuracy as the optimisation trades off individual accuracy for group fairness."
        }
      ],
      "resources": [
        {
          "title": "Group-Aware Threshold Adaptation for Fair Classification",
          "url": "https://arxiv.org/abs/2111.04271",
          "source_type": "technical_paper",
          "authors": [
            "Jang, Taeuk",
            "Shi, Pengyi",
            "Wang, Xiaoqian"
          ],
          "publication_date": "2021-11-08",
          "description": "Introduces a novel post-processing method for learning adaptive classification thresholds for each demographic group by optimising confusion matrices estimated from model probability distributions."
        },
        {
          "title": "Equality of Opportunity in Supervised Learning",
          "url": "https://arxiv.org/abs/1610.02413",
          "source_type": "technical_paper",
          "authors": [
            "Hardt, Moritz",
            "Price, Eric",
            "Srebro, Nathan"
          ],
          "publication_date": "2016-10-07",
          "description": "Foundational work introducing threshold optimisation techniques to achieve equalized opportunity and demographic parity in supervised learning."
        },
        {
          "title": "AIF360: A comprehensive set of fairness metrics and algorithms",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package",
          "description": "Open-source library containing threshold optimisation implementations for various fairness constraints including equalized odds and demographic parity."
        },
        {
          "title": "Fairlearn: A toolkit for assessing and improving fairness in AI",
          "url": "https://github.com/fairlearn/fairlearn",
          "source_type": "software_package",
          "description": "Python library providing threshold optimisation methods and post-processing algorithms for achieving fairness in machine learning models."
        },
        {
          "title": "HolisticAI: Randomized Threshold Optimizer",
          "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/postprocessing/bc_ml_debiaser_rto.html",
          "source_type": "documentation",
          "description": "Documentation for the Randomized Threshold Optimizer implementation that achieves statistical parity through group-aware threshold adjustment with randomization."
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "equalised-odds-post-processing",
        "reject-option-classification",
        "calibration-with-equality-of-opportunity"
      ]
    },
    {
      "slug": "reject-option-classification",
      "name": "Reject Option Classification",
      "description": "A post-processing fairness technique that modifies predictions in regions of high uncertainty to favour disadvantaged groups and achieve fairness objectives. The method identifies a 'rejection region' where the model's confidence is low (typically near the decision boundary) and reassigns predictions within this region to benefit underrepresented groups. By leveraging model uncertainty, this approach can improve fairness metrics like demographic parity or equalised odds whilst minimising changes to confident predictions, thus preserving overall accuracy for cases where the model is certain.",
      "assurance_goals": [
        "Fairness",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-requirements/prediction-probabilities",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "fairness-approach/group",
        "lifecycle-stage/post-processing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting hiring algorithm predictions in the uncertainty region where candidate scores are close to the threshold, reassigning borderline cases to ensure equal selection rates across gender and ethnicity groups whilst maintaining decisions for clearly qualified or unqualified candidates.",
          "goal": "Fairness"
        },
        {
          "description": "Improving reliability of loan approval systems by identifying applications where the model is uncertain and adjusting these edge cases to ensure consistent approval rates across demographic groups, reducing the risk of systematic discrimination in borderline creditworthiness assessments.",
          "goal": "Reliability"
        },
        {
          "description": "Creating transparent bail decision systems that clearly document which predictions fall within the rejection region and how adjustments are made, providing courts with explainable fairness interventions that show exactly when and why decisions were modified for equity.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires models that provide reliable uncertainty estimates or probability scores, limiting applicability to deterministic classifiers without confidence outputs."
        },
        {
          "description": "Selection of the rejection region threshold is subjective and requires careful tuning to balance fairness improvements with accuracy preservation."
        },
        {
          "description": "May reject too many instances if tuned conservatively, potentially affecting a large portion of predictions and reducing the model's practical utility."
        },
        {
          "description": "Cannot address bias in confident predictions outside the rejection region, limiting effectiveness when discrimination occurs in high-certainty cases."
        },
        {
          "description": "Performance depends on the quality of uncertainty estimates, which may be poorly calibrated in some models, leading to inappropriate rejection regions."
        }
      ],
      "resources": [
        {
          "title": "Machine Learning with a Reject Option: A survey",
          "url": "https://www.semanticscholar.org/paper/24864a7f899718477c04ede9c0bea906c5dc2667",
          "source_type": "documentation",
          "authors": [
            "Kilian Hendrickx",
            "Lorenzo Perini",
            "Dries Van der Plas",
            "Wannes Meert",
            "Jesse Davis"
          ]
        },
        {
          "title": "aif360.algorithms.postprocessing.RejectOptionClassification ...",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html",
          "source_type": "documentation"
        },
        {
          "title": "Survey on Leveraging Uncertainty Estimation Towards Trustworthy Deep Neural Networks: The Case of Reject Option and Post-training Processing",
          "url": "https://www.semanticscholar.org/paper/e939c6ac58e08b539ae8a7dc54216bceb775b085",
          "source_type": "documentation",
          "authors": [
            "M. Hasan",
            "Moloud Abdar",
            "A. Khosravi",
            "U. Aickelin",
            "Pietro Lio'",
            "Ibrahim Hossain",
            "Ashikur Rahman",
            "Saeid Nahavandi"
          ]
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "equalised-odds-post-processing",
        "threshold-optimiser",
        "calibration-with-equality-of-opportunity"
      ]
    },
    {
      "slug": "calibration-with-equality-of-opportunity",
      "name": "Calibration with Equality of Opportunity",
      "description": "A post-processing fairness technique that adjusts model predictions to achieve equal true positive rates across protected groups whilst maintaining calibration within each group. The method addresses fairness by ensuring that qualified individuals from different demographic groups have equal chances of receiving positive predictions, whilst preserving the meaning of probability scores within each group. This technique attempts to balance the competing objectives of group fairness and accurate probability estimation.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/calibration-set",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/testing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting a loan approval model to ensure that qualified applicants from different ethnic backgrounds have equal approval rates, whilst maintaining that a 70% predicted repayment probability means the same thing for each ethnic group in practice.",
          "goal": "Fairness"
        },
        {
          "description": "Post-processing a university admissions algorithm to equalise acceptance rates for qualified students across gender groups, whilst ensuring the predicted success scores remain well-calibrated within each gender to support transparent decision-making.",
          "goal": "Transparency"
        },
        {
          "description": "Calibrating a medical diagnosis model to maintain equal detection rates for a disease across different age groups whilst preserving the reliability of risk scores, ensuring that a 30% risk prediction accurately reflects actual disease occurrence within each age group.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Fundamental mathematical incompatibility exists between perfect calibration and exact equality of opportunity, except in highly constrained special cases."
        },
        {
          "description": "May reduce overall model accuracy or calibration when forcing equal true positive rates across groups with genuinely different base rates."
        },
        {
          "description": "Requires access to sensitive attributes during post-processing, which may not be available or legally permissible in all contexts."
        },
        {
          "description": "The technique only addresses one aspect of fairness (true positive rates) and may allow disparities in false positive rates between groups."
        },
        {
          "description": "Post-processing approaches cannot address biases inherent in the training data or model architecture, only adjust final predictions."
        }
      ],
      "resources": [
        {
          "title": "On Fairness and Calibration",
          "url": "https://arxiv.org/abs/1709.02012",
          "source_type": "technical_paper",
          "description": "Foundational paper demonstrating the mathematical tension between calibration and equalised odds fairness constraints.",
          "authors": [
            "Geoff Pleiss",
            "Manish Raghavan",
            "Felix Wu",
            "Jon Kleinberg",
            "Kilian Q. Weinberger"
          ],
          "publication_date": "2017-09-06"
        },
        {
          "title": "equalized_odds_and_calibration",
          "url": "https://github.com/gpleiss/equalized_odds_and_calibration",
          "source_type": "software_package",
          "description": "Python implementation of post-processing methods for achieving calibration with equality of opportunity constraints."
        },
        {
          "title": "Equality of Opportunity in Supervised Learning",
          "url": "https://arxiv.org/abs/1610.02413",
          "source_type": "technical_paper",
          "description": "Original paper introducing the equality of opportunity fairness criterion and post-processing algorithms.",
          "authors": [
            "Moritz Hardt",
            "Eric Price",
            "Nathan Srebro"
          ],
          "publication_date": "2016-10-07"
        },
        {
          "title": "Fairlearn: Postprocessing Methods",
          "url": "https://fairlearn.org/v0.10/user_guide/mitigation/postprocessing.html",
          "source_type": "documentation",
          "description": "Documentation for implementing threshold optimisation and calibration methods to achieve fairness constraints."
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 2,
      "related_techniques": [
        "equalised-odds-post-processing",
        "threshold-optimiser",
        "reject-option-classification"
      ]
    },
    {
      "slug": "path-specific-counterfactual-fairness-assessment",
      "name": "Path-Specific Counterfactual Fairness Assessment",
      "description": "A causal fairness evaluation technique that assesses algorithmic discrimination by examining specific causal pathways in a model's decision-making process. Unlike general counterfactual fairness, this approach enables practitioners to identify and intervene on particular causal paths that may introduce bias whilst preserving other legitimate pathways. The method uses causal graphs to distinguish between direct discrimination (through protected attributes) and indirect discrimination (through seemingly neutral factors that correlate with protected attributes), allowing for more nuanced fairness assessments in complex causal settings.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/causal",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/causal-graph",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/causal-analysis",
        "expertise-needed/causal-inference",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "fairness-approach/causal",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "technique-type/metric"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating hiring algorithms by identifying which causal pathways from education and experience legitimately affect job performance versus those that introduce gender or racial bias, enabling targeted interventions that preserve merit-based selection whilst eliminating discriminatory pathways.",
          "goal": "Fairness"
        },
        {
          "description": "Analysing loan approval models to provide transparent evidence of which factors legitimately influence creditworthiness versus those that create indirect discrimination, enabling clear explanations to regulators about causal mechanisms underlying fair lending decisions.",
          "goal": "Transparency"
        },
        {
          "description": "Assessing medical diagnosis systems to ensure reliable performance by distinguishing between clinically relevant causal pathways (symptoms to diagnosis) and potentially biased pathways (demographics to diagnosis), maintaining diagnostic accuracy whilst preventing healthcare disparities.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Requires identifying which causal pathways are 'allowable' and which are not—a subjective decision; analyzing specific paths adds complexity to the causal model and the fairness criterion."
        }
      ],
      "resources": [
        {
          "title": "Path-Specific Counterfactual Fairness via Dividend Correction",
          "url": "https://www.semanticscholar.org/paper/197367ee337e8838fd2ef1a887101ddc84eb0612",
          "source_type": "technical_paper",
          "authors": [
            "Daisuke Hatano",
            "Satoshi Hara",
            "Hiromi Arai"
          ]
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "counterfactual-fairness-assessment"
      ]
    },
    {
      "slug": "bayesian-fairness-regularization",
      "name": "Bayesian Fairness Regularization",
      "description": "Bayesian Fairness Regularization incorporates fairness constraints into machine learning models through Bayesian methods, treating fairness as a prior distribution or regularization term. This approach includes techniques like Fair Bayesian Optimization that use constrained optimization to tune model hyperparameters whilst enforcing fairness constraints, and methods that add regularization terms to objective functions to penalize discriminatory predictions. The technique allows for probabilistic interpretation of fairness constraints and can account for uncertainty in both model parameters and fairness requirements.",
      "assurance_goals": [
        "Fairness",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Using Fair Bayesian Optimization to tune hyperparameters of credit risk models, automatically balancing predictive accuracy with fairness constraints across different demographic groups whilst accounting for uncertainty in both model performance and fairness requirements.",
          "goal": "Fairness"
        },
        {
          "description": "Implementing Bayesian neural networks with fairness-aware priors for hiring recommendation systems, where uncertainty in fairness constraints is modeled probabilistically to ensure robust fair decision-making across different candidate populations.",
          "goal": "Fairness"
        },
        {
          "description": "Applying Bayesian regularization techniques to medical diagnosis models to ensure reliable performance across patient demographics, using probabilistic constraints to maintain consistent diagnostic accuracy whilst preventing algorithmic bias in healthcare delivery.",
          "goal": "Reliability"
        },
        {
          "description": "Developing insurance premium calculation models using Bayesian fairness regularization to ensure actuarially sound pricing that meets regulatory fairness requirements, with probabilistic modeling of both risk assessment accuracy and demographic equity.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Prior selection challenges make it difficult to specify appropriate prior distributions for fairness constraints, requiring domain expertise and potentially leading to suboptimal or biased outcomes if priors are poorly chosen."
        },
        {
          "description": "Computational complexity increases significantly due to Bayesian inference requirements, including sampling methods, variational inference, or optimization over probability distributions, making the approach less scalable for large datasets."
        },
        {
          "description": "Sensitivity to hyperparameters affects both the Bayesian inference process and fairness regularization terms, requiring careful tuning of multiple parameters that control the trade-off between accuracy, fairness, and computational efficiency."
        },
        {
          "description": "Convergence and stability issues may arise in Bayesian optimization with fairness constraints, particularly when fairness objectives conflict with performance objectives or when the constraint space becomes highly complex."
        },
        {
          "description": "Limited theoretical understanding exists for the interaction between Bayesian uncertainty quantification and fairness constraints, making it challenging to provide guarantees about both predictive performance and fairness under uncertainty."
        }
      ],
      "resources": [
        {
          "title": "Bayesian fairness",
          "url": "https://arxiv.org/abs/1706.00119",
          "source_type": "technical_paper",
          "description": "Foundational paper introducing Bayesian approaches to fairness under parameter uncertainty, demonstrating how Bayesian perspectives lead to fair decision rules",
          "authors": [
            "Dimitrakakis, Christos",
            "Liu, Yang",
            "Parkes, David",
            "Radanovic, Goran"
          ],
          "publication_date": "2017-05-31"
        },
        {
          "title": "Fair Bayesian Optimization",
          "url": "https://arxiv.org/abs/2006.05109",
          "source_type": "technical_paper",
          "description": "Constrained Bayesian optimization framework for optimizing ML model performance while enforcing fairness constraints through hyperparameter tuning",
          "authors": [
            "Perrone, Valerio",
            "Donini, Michele",
            "Zafar, Muhammad Bilal",
            "Schmucker, Robin",
            "Kenthapadi, Krishnaram",
            "Archambeau, Cédric"
          ],
          "publication_date": "2020-06-09"
        },
        {
          "title": "Fair Gaussian Processes",
          "url": "https://github.com/ztanml/fgp",
          "source_type": "software_package",
          "description": "MATLAB implementation of Fair Gaussian Processes with multiple fairness criteria support including statistical parity, equality of opportunity, and equalized odds"
        },
        {
          "title": "Fairness-Aware Classifier with Prejudice Remover Regularizer",
          "url": "https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3",
          "source_type": "technical_paper",
          "description": "Seminal paper introducing regularization-based approach to fairness in probabilistic discriminative models with mathematical framework for fairness constraints",
          "authors": [
            "Kamishima, Toshihiro",
            "Akaho, Shotaro",
            "Asoh, Hideki",
            "Sakuma, Jun"
          ],
          "publication_date": "2012-09-24"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "sensitivity-analysis-for-fairness",
        "fairness-gan",
        "attribute-removal-fairness-through-unawareness"
      ]
    }
  ],
  "count": 49
}