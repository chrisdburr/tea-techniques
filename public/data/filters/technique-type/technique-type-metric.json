{
  "tag": {
    "name": "technique-type/metric",
    "slug": "technique-type-metric",
    "count": 4,
    "category": "technique-type"
  },
  "techniques": [
    {
      "slug": "coefficient-magnitudes-in-linear-models",
      "name": "Coefficient Magnitudes (in Linear Models)",
      "description": "Coefficient Magnitudes assess feature influence in linear models by examining the absolute values of their coefficients. Features with larger absolute coefficients are considered to have a stronger impact on the prediction, while the sign of the coefficient indicates the direction of that influence (positive or negative). This technique provides a straightforward and transparent way to understand the direct linear relationship between each input feature and the model's output.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/linear-model",
        "assurance-goal-category/explainability",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "expertise-needed/low",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "technique-type/metric"
      ],
      "example_use_cases": [
        {
          "description": "Interpreting which features influence housing price predictions in linear regression, such as identifying that 'number of bedrooms' has a larger positive impact than 'distance to city centre' based on coefficient magnitudes.",
          "goal": "Explainability"
        },
        {
          "description": "Explaining the factors contributing to customer lifetime value (CLV) in a linear model, showing how 'average monthly spend' has a strong positive coefficient, making the model transparent for business stakeholders.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Only valid for linear relationships; it cannot capture complex non-linear patterns or interactions between features."
        },
        {
          "description": "Highly sensitive to feature scaling; features with larger numerical ranges can appear more important even if their true impact is smaller."
        },
        {
          "description": "Can be misleading in the presence of multicollinearity, where correlated features may split importance or have unstable coefficients."
        },
        {
          "description": "Does not imply causation; a strong correlation (large coefficient) does not necessarily mean a causal relationship."
        }
      ],
      "resources": [],
      "complexity_rating": 1,
      "computational_cost_rating": 1,
      "related_techniques": [
        "permutation-importance",
        "mean-decrease-impurity",
        "sobol-indices"
      ]
    },
    {
      "slug": "equal-opportunity-difference",
      "name": "Equal Opportunity Difference",
      "description": "A fairness metric that quantifies discrimination by measuring the difference in true positive rates (recall) between protected and privileged groups. Based on Hardt et al.'s equality of opportunity framework, this metric computes the maximum difference in TPR across demographic groups, with a value of 0 indicating perfect fairness. The technique provides a mathematical measure of whether qualified individuals from different groups have equal chances of receiving positive predictions.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/labelled-data",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/fairness-metric",
        "expertise-needed/low",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-development/testing",
        "lifecycle-stage/system-deployment-and-use/monitoring",
        "technique-type/metric"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating a loan approval model to ensure that qualified applicants from different ethnic backgrounds have equal approval rates, measuring whether a 90% TPR for qualified white applicants is matched by similar rates for qualified minority applicants.",
          "goal": "Fairness"
        },
        {
          "description": "Auditing a medical diagnosis system to verify that patients with a particular condition are detected at equal rates across age groups, providing transparent evidence that diagnostic accuracy is consistent regardless of patient demographics.",
          "goal": "Transparency"
        },
        {
          "description": "Monitoring a university admissions algorithm in production to ensure that qualified students from different socioeconomic backgrounds have equal acceptance rates, validating the reliability of the fairness properties over time.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Only considers true positive rates and ignores false positive rate disparities, potentially allowing discrimination in the form of unequal false alarm rates between groups."
        },
        {
          "description": "Requires accurate ground truth labels for the positive class, which may be biased or unavailable in some domains."
        },
        {
          "description": "Improving TPR for one group might increase FPR for that group, creating trade-offs between different types of fairness."
        },
        {
          "description": "Does not account for intersectional fairness across multiple protected attributes simultaneously."
        },
        {
          "description": "May not capture fairness concerns for the negative class or individuals who should not receive positive predictions."
        }
      ],
      "resources": [
        {
          "title": "aif360.sklearn.metrics.equal_opportunity_difference — aif360 0.6.1 ...",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.metrics.equal_opportunity_difference.html",
          "source_type": "documentation"
        },
        {
          "title": "Welcome to TransparentAI's documentation! — TransparentAI 0.1.0 ...",
          "url": "https://transparentai.readthedocs.io/en/latest/",
          "source_type": "documentation"
        },
        {
          "title": "lale.lib.aif360.util module — LALE 0.9.0-dev documentation",
          "url": "https://lale.readthedocs.io/en/latest/modules/lale.lib.aif360.util.html",
          "source_type": "documentation"
        },
        {
          "title": "IBM/bias-mitigation-of-machine-learning-models-using-aif360",
          "url": "https://github.com/IBM/bias-mitigation-of-machine-learning-models-using-aif360",
          "source_type": "software_package"
        },
        {
          "title": "aif360.algorithms.postprocessing.RejectOptionClassification ...",
          "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "demographic-parity-assessment",
        "average-odds-difference"
      ]
    },
    {
      "slug": "average-odds-difference",
      "name": "Average Odds Difference",
      "description": "Average Odds Difference measures fairness by calculating the average difference in both false positive rates and true positive rates between different demographic groups. This metric captures how consistently a model performs across groups for both positive and negative predictions. A value of 0 indicates perfect fairness under the equalized odds criterion, while larger absolute values indicate greater disparities in model performance between groups.",
      "assurance_goals": [
        "Fairness",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/low",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "technique-type/metric"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating criminal risk assessment tools to ensure equal false positive rates (wrongly flagging low-risk individuals as high-risk) and true positive rates (correctly identifying high-risk individuals) across racial and ethnic groups.",
          "goal": "Fairness"
        },
        {
          "description": "Auditing hiring algorithms to verify that both the rate of correctly identifying qualified candidates and the rate of incorrectly rejecting qualified candidates remain consistent across gender and demographic groups.",
          "goal": "Fairness"
        },
        {
          "description": "Monitoring loan approval systems to ensure reliable performance by checking that both approval rates for creditworthy applicants and rejection rates for non-creditworthy applicants are consistent across protected demographic categories.",
          "goal": "Reliability"
        },
        {
          "description": "Testing medical diagnostic models to validate that diagnostic accuracy (both correctly identifying disease and correctly ruling out disease) remains consistent across patient demographics, ensuring reliable healthcare delivery.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Averaging effect can mask important disparities when false positive and true positive rate differences compensate for each other, potentially hiding significant bias in one direction."
        },
        {
          "description": "Requires access to ground truth labels and sensitive attribute information, which may not be available in all deployment scenarios or may be subject to privacy constraints."
        },
        {
          "description": "Does not account for base rate differences between groups, meaning equal error rates may not translate to equal treatment when group prevalences differ significantly."
        },
        {
          "description": "Focuses solely on prediction accuracy disparities without considering whether the underlying decision-making process or feature selection introduces systematic bias against certain groups."
        },
        {
          "description": "May encourage optimization for fairness metrics at the expense of overall model performance, potentially reducing utility for the primary prediction task."
        }
      ],
      "resources": [
        {
          "title": "Equality of Opportunity in Supervised Learning",
          "url": "https://arxiv.org/abs/1610.02413",
          "source_type": "technical_paper",
          "description": "Foundational paper introducing equalized odds and related fairness metrics including average odds difference",
          "authors": [
            "Hardt, Moritz",
            "Price, Eric",
            "Srebro, Nathan"
          ],
          "publication_date": "2016-10-07"
        },
        {
          "title": "FairBalance: How to Achieve Equalized Odds With Data Pre-processing",
          "url": "https://arxiv.org/abs/2107.08310",
          "source_type": "technical_paper",
          "description": "Research on achieving equalized odds through data preprocessing techniques with practical implementation guidance",
          "authors": [
            "Yu, Zhe",
            "Chakraborty, Joymallya",
            "Menzies, Tim"
          ],
          "publication_date": "2021-07-17"
        },
        {
          "title": "AIF360: Average Odds Difference Documentation",
          "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.metrics.average_odds_difference.html",
          "source_type": "documentation",
          "description": "IBM AIF360 toolkit implementation and documentation for computing average odds difference metrics"
        },
        {
          "title": "Fairlearn: A toolkit for assessing and improving fairness in machine learning",
          "url": "https://github.com/fairlearn/fairlearn",
          "source_type": "software_package",
          "description": "Microsoft's comprehensive fairness toolkit with implementations of various fairness metrics including average odds difference"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "demographic-parity-assessment",
        "equal-opportunity-difference"
      ]
    },
    {
      "slug": "path-specific-counterfactual-fairness-assessment",
      "name": "Path-Specific Counterfactual Fairness Assessment",
      "description": "A causal fairness evaluation technique that assesses algorithmic discrimination by examining specific causal pathways in a model's decision-making process. Unlike general counterfactual fairness, this approach enables practitioners to identify and intervene on particular causal paths that may introduce bias whilst preserving other legitimate pathways. The method uses causal graphs to distinguish between direct discrimination (through protected attributes) and indirect discrimination (through seemingly neutral factors that correlate with protected attributes), allowing for more nuanced fairness assessments in complex causal settings.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/causal",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/sensitive-attributes",
        "data-requirements/causal-graph",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/causal-analysis",
        "expertise-needed/causal-inference",
        "expertise-needed/statistics",
        "expertise-needed/ml-engineering",
        "fairness-approach/causal",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "technique-type/metric"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating hiring algorithms by identifying which causal pathways from education and experience legitimately affect job performance versus those that introduce gender or racial bias, enabling targeted interventions that preserve merit-based selection whilst eliminating discriminatory pathways.",
          "goal": "Fairness"
        },
        {
          "description": "Analysing loan approval models to provide transparent evidence of which factors legitimately influence creditworthiness versus those that create indirect discrimination, enabling clear explanations to regulators about causal mechanisms underlying fair lending decisions.",
          "goal": "Transparency"
        },
        {
          "description": "Assessing medical diagnosis systems to ensure reliable performance by distinguishing between clinically relevant causal pathways (symptoms to diagnosis) and potentially biased pathways (demographics to diagnosis), maintaining diagnostic accuracy whilst preventing healthcare disparities.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Requires identifying which causal pathways are 'allowable' and which are not—a subjective decision; analyzing specific paths adds complexity to the causal model and the fairness criterion."
        }
      ],
      "resources": [
        {
          "title": "Path-Specific Counterfactual Fairness via Dividend Correction",
          "url": "https://www.semanticscholar.org/paper/197367ee337e8838fd2ef1a887101ddc84eb0612",
          "source_type": "technical_paper",
          "authors": [
            "Daisuke Hatano",
            "Satoshi Hara",
            "Hiromi Arai"
          ]
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "counterfactual-fairness-assessment"
      ]
    }
  ],
  "count": 4
}