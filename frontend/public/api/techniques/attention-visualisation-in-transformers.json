{
  "slug": "attention-visualisation-in-transformers",
  "name": "Attention Visualisation in Transformers",
  "acronym": null,
  "description": "Attention Visualisation in Transformers analyses the multi-head self-attention mechanisms that enable transformers to process sequences by attending to different positions simultaneously. The technique visualises attention weights as heatmaps showing how strongly each token attends to every other token across different heads and layers. By examining these attention patterns, practitioners can understand how models like BERT, GPT, and T5 build contextual representations, identify which tokens influence predictions most strongly, and detect potential biases in how the model processes different types of input. This provides insights into positional encoding effects, head specialisation patterns, and the evolution of attention from local to global context across layers.",
  "complexity_rating": 3,
  "computational_cost_rating": 2,
  "assurance_goals": ["Explainability", "Fairness", "Transparency"],
  "tags": [
    "applicable-models/transformer",
    "assurance-goal-category/explainability",
    "assurance-goal-category/explainability/feature-analysis",
    "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
    "assurance-goal-category/fairness",
    "assurance-goal-category/transparency",
    "data-requirements/access-to-model-internals",
    "data-type/text",
    "data-type/image",
    "evidence-type/visualization",
    "expertise-needed/ml-engineering",
    "explanatory-scope/local",
    "explanatory-scope/global",
    "lifecycle-stage/model-development",
    "lifecycle-stage/testing",
    "technique-type/algorithmic"
  ],
  "related_techniques": [
    "integrated-gradients",
    "layer-wise-relevance-propagation",
    "saliency-maps",
    "gradient-weighted-class-activation-mapping",
    "classical-attention-analysis-in-neural-networks",
    "contrastive-explanation-method"
  ],
  "resources": [
    {
      "title": "jessevig/bertviz",
      "url": "https://github.com/jessevig/bertviz",
      "description": "Interactive tool for visualising attention patterns in transformer language models including BERT, GPT-2, and T5",
      "authors": "",
      "publication_date": null,
      "source_type": "software_package",
      "resource_type": "documentation"
    },
    {
      "title": "Attention is All You Need",
      "url": "https://arxiv.org/abs/1706.03762",
      "description": "Foundational paper introducing the transformer architecture and self-attention mechanism",
      "authors": "",
      "publication_date": null,
      "source_type": "technical_paper",
      "resource_type": "documentation"
    },
    {
      "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
      "url": "https://arxiv.org/abs/1905.09418",
      "description": "Research showing how different attention heads specialise in distinct linguistic phenomena",
      "authors": "",
      "publication_date": null,
      "source_type": "technical_paper",
      "resource_type": "documentation"
    },
    {
      "title": "What Does BERT Look At? An Analysis of BERT's Attention",
      "url": "https://arxiv.org/abs/1906.04341",
      "description": "Comprehensive analysis of attention patterns in BERT revealing syntactic and semantic specialisation",
      "authors": "",
      "publication_date": null,
      "source_type": "technical_paper",
      "resource_type": "documentation"
    },
    {
      "title": "Transformer Explainability Beyond Attention Visualization",
      "url": "https://arxiv.org/abs/2012.09838",
      "description": "Methods for attribution beyond raw attention weights including relevancy propagation and gradient-based approaches",
      "authors": "",
      "publication_date": null,
      "source_type": "technical_paper",
      "resource_type": "documentation"
    }
  ],
  "example_use_cases": [
    {
      "description": "Examining attention patterns in a medical language model processing clinical notes to verify it focuses on relevant symptoms and conditions rather than irrelevant demographic identifiers, revealing that certain attention heads specialise in medical terminology whilst others track syntactic relationships between diagnoses and treatments.",
      "goal": "Explainability"
    },
    {
      "description": "Auditing a sentiment analysis model for customer reviews by visualising how attention weights differ when processing reviews from different demographic groups, discovering that the model pays disproportionate attention to certain cultural expressions or colloquialisms that could lead to biased sentiment predictions.",
      "goal": "Fairness"
    },
    {
      "description": "Creating visual explanations for regulatory compliance in a financial document classification system, showing which specific words and phrases in loan applications or contracts triggered particular risk assessments, enabling auditors to verify that decisions are based on legitimate financial factors rather than discriminatory language patterns.",
      "goal": "Transparency"
    }
  ],
  "limitations": [
    {
      "description": "High attention weights do not necessarily indicate causal importance for predictions, as models may attend strongly to tokens that serve structural rather than semantic purposes."
    },
    {
      "description": "The sheer number of attention heads and layers in modern transformers creates visualisation overload, making it difficult to identify meaningful patterns without systematic analysis tools."
    },
    {
      "description": "Attention patterns can be misleading when models use residual connections and layer normalisation, as the final representation incorporates information beyond what attention weights suggest."
    },
    {
      "description": "Different transformer architectures (encoder-only, decoder-only, encoder-decoder) exhibit fundamentally different attention patterns, limiting the generalisability of insights across model types."
    },
    {
      "description": "The technique cannot explain the reasoning process within feed-forward layers or how attention patterns translate into specific predictions, providing only a partial view of model behaviour."
    }
  ]
}
