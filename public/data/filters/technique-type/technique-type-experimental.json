{
  "tag": {
    "name": "technique-type/experimental",
    "slug": "technique-type-experimental",
    "count": 1,
    "category": "technique-type"
  },
  "techniques": [
    {
      "slug": "prompt-sensitivity-analysis",
      "name": "Prompt Sensitivity Analysis",
      "description": "Prompt Sensitivity Analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. This technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. It encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. The analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/llm",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "expertise-needed/statistics",
        "expertise-needed/linguistics",
        "expertise-needed/experimental-design",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "technique-type/experimental"
      ],
      "example_use_cases": [
        {
          "description": "Testing medical diagnosis LLMs with semantically equivalent but syntactically different symptom descriptions to ensure consistent diagnostic recommendations across different patient communication styles, identifying potential failure modes where slight phrasing changes could lead to dangerous misdiagnoses.",
          "goal": "Safety"
        },
        {
          "description": "Analysing how variations in candidate descriptions (gendered language, cultural references, educational institution prestige indicators) affect LLM-based CV screening recommendations to identify potential discriminatory patterns and ensure equitable treatment across diverse applicant backgrounds.",
          "goal": "Reliability"
        },
        {
          "description": "Examining how different ways of framing financial questions (formal vs informal language, technical vs layperson terminology) affect investment advice generated by LLMs to improve user understanding and model transparency whilst ensuring consistent advisory quality.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Analysis is inherently limited to the specific prompt variations tested, potentially missing important sensitivity patterns that weren't anticipated during study design, making comprehensive coverage challenging."
        },
        {
          "description": "Systematic exploration of prompt variations can be computationally expensive, particularly for large-scale sensitivity analysis across multiple dimensions of variation, requiring significant resources for thorough evaluation."
        },
        {
          "description": "Ensuring that prompt variations maintain semantic equivalence whilst introducing meaningful perturbations requires careful linguistic expertise and validation, which can be subjective and domain-dependent."
        },
        {
          "description": "Results may reveal sensitivity patterns that are difficult to interpret or act upon, particularly when multiple types of variations interact in complex ways, limiting practical applicability of findings."
        },
        {
          "description": "The meaningfulness of sensitivity measurements depends heavily on the choice of baseline prompts and variation strategies, which can introduce methodological biases and affect the generalisability of conclusions."
        }
      ],
      "resources": [
        {
          "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
          "url": "https://www.semanticscholar.org/paper/17a6116e5bbd8b87082cbb2e795885567300c483",
          "source_type": "technical_paper",
          "authors": [
            "Melanie Sclar",
            "Yejin Choi",
            "Yulia Tsvetkov",
            "Alane Suhr"
          ]
        },
        {
          "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts",
          "url": "http://arxiv.org/pdf/2505.12592v1",
          "source_type": "technical_paper",
          "authors": [
            "Sullam Jeoung",
            "Yueyan Chen",
            "Yi Zhang",
            "Shuai Wang",
            "Haibo Ding",
            "Lin Lee Cheong"
          ],
          "publication_date": "2025-05-19"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 3,
      "related_techniques": [
        "causal-mediation-analysis-in-language-models",
        "feature-attribution-with-integrated-gradients-in-nlp"
      ]
    }
  ],
  "count": 1
}