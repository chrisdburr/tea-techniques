{
  "tag": {
    "name": "data-type/image",
    "slug": "data-type-image",
    "count": 4,
    "category": "data-type"
  },
  "techniques": [
    {
      "slug": "saliency-maps",
      "name": "Saliency Maps",
      "description": "Saliency maps are visual explanations for image classification models that highlight which pixels in an image most strongly influence the model's prediction. Computed by calculating gradients of the model's output with respect to input pixels, saliency maps produce heatmaps where brighter regions indicate pixels that, when changed, would most significantly affect the prediction. This technique helps users understand which parts of an image the model is 'looking at' when making decisions.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing X-ray images in a pneumonia detection model to verify that the algorithm focuses on lung regions showing inflammatory patterns rather than irrelevant areas like medical equipment or patient positioning markers.",
          "goal": "Explainability"
        },
        {
          "description": "Examining skin lesion classification models to ensure the algorithm identifies diagnostic features (irregular borders, colour variation) rather than artifacts like rulers, hair, or skin markings that shouldn't influence medical decisions.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a dermatology AI system to verify it focuses on medical symptoms rather than skin colour when diagnosing conditions, ensuring equitable treatment across racial groups by revealing inappropriate attention to demographic features.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Saliency maps are often noisy and can change dramatically with small input perturbations, making them unstable."
        },
        {
          "description": "Highlighted regions may not correspond to semantically meaningful or human-understandable features."
        },
        {
          "description": "Only indicates local gradient information, not causal importance or actual decision-making logic."
        },
        {
          "description": "May highlight irrelevant pixels that happen to have high gradients due to model artifacts rather than meaningful patterns."
        }
      ],
      "resources": [
        {
          "title": "utkuozbulak/pytorch-cnn-visualizations",
          "url": "https://github.com/utkuozbulak/pytorch-cnn-visualizations",
          "source_type": "software_package"
        },
        {
          "title": "Concepts of Saliency and Explainability in AI",
          "url": "https://xaitk-saliency.readthedocs.io/en/latest/xaitk_explanation.html",
          "source_type": "documentation"
        },
        {
          "title": "Occlusion Saliency Example",
          "url": "https://xaitk-saliency.readthedocs.io/en/latest/examples/OcclusionSaliency.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "gradient-weighted-class-activation-mapping",
        "occlusion-sensitivity"
      ]
    },
    {
      "slug": "gradient-weighted-class-activation-mapping",
      "name": "Gradient-weighted Class Activation Mapping",
      "description": "Grad-CAM creates visual heatmaps showing which regions of an image a convolutional neural network focuses on when making a specific classification. Unlike pixel-level techniques, Grad-CAM produces coarser region-based explanations by using gradients from the predicted class to weight the CNN's final feature maps, then projecting these weighted activations back to create an overlay on the original image. This provides intuitive visual explanations of where the model is 'looking' for evidence of different classes.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Validating that a melanoma detection model focuses on the actual skin lesion rather than surrounding healthy skin, medical equipment, or artifacts when making cancer/benign classifications.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging an autonomous vehicle's traffic sign recognition system by visualising whether the model correctly focuses on the sign itself rather than background objects, shadows, or irrelevant visual elements.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a medical imaging system for racial bias by examining whether diagnostic predictions inappropriately focus on skin tone regions rather than actual pathological features, ensuring equitable healthcare AI deployment.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires access to the CNN's internal feature maps and gradients, limiting use to white-box scenarios."
        },
        {
          "description": "Resolution is constrained by the final convolutional layer's feature map size, producing coarser localisation than pixel-level methods."
        },
        {
          "description": "Only applicable to CNN architectures with clearly defined convolutional layers, not suitable for other neural network types."
        },
        {
          "description": "May highlight regions that correlate with the class but aren't causally important for the model's decision-making process."
        }
      ],
      "resources": [
        {
          "title": "jacobgil/pytorch-grad-cam",
          "url": "https://github.com/jacobgil/pytorch-grad-cam",
          "source_type": "software_package"
        },
        {
          "title": "Grad-CAM: Visualize class activation maps with Keras, TensorFlow ...",
          "url": "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/",
          "source_type": "tutorial"
        },
        {
          "title": "kazuto1011/grad-cam-pytorch",
          "url": "https://github.com/kazuto1011/grad-cam-pytorch",
          "source_type": "software_package"
        },
        {
          "title": "A Tutorial on Explainable Image Classification for Dementia Stages Using Convolutional Neural Network and Gradient-weighted Class Activation Mapping",
          "url": "https://www.semanticscholar.org/paper/8b1139cb06cfe5ba69e2fd05e1450b43df031a02",
          "source_type": "documentation",
          "authors": [
            "Kevin Kam Fung Yuen"
          ]
        },
        {
          "title": "A Guide to Grad-CAM in Deep Learning - Analytics Vidhya",
          "url": "https://www.analyticsvidhya.com/blog/2023/12/grad-cam-in-deep-learning/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "Grad-CAM",
      "related_techniques": [
        "saliency-maps",
        "occlusion-sensitivity"
      ]
    },
    {
      "slug": "occlusion-sensitivity",
      "name": "Occlusion Sensitivity",
      "description": "Occlusion sensitivity tests which parts of the input are important by occluding (masking or removing) them and seeing how the model's prediction changes. For example, portions of an image can be covered up in a sliding window fashion; if the model's confidence drops significantly when a certain region is occluded, that region was important for the prediction.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/explainability",
        "data-requirements/no-special-requirements",
        "data-type/image",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Testing which regions of a chest X-ray are critical for pneumonia detection by systematically covering different areas with grey patches and measuring how much the model's confidence drops for each occluded region.",
          "goal": "Explainability"
        },
        {
          "description": "Evaluating whether a facial recognition system relies on specific facial features by masking eyes, nose, mouth, or other regions to identify which areas cause the biggest drop in recognition accuracy.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive as it requires running inference multiple times for each region tested, scaling poorly with input size."
        },
        {
          "description": "Choice of occlusion size and shape can significantly bias results - too small may miss important features, too large may occlude multiple relevant regions simultaneously."
        },
        {
          "description": "Cannot capture interactions between multiple regions that jointly contribute to the prediction but are individually less important."
        },
        {
          "description": "Results may be misleading if the model adapts to occlusion patterns or if occluded regions are filled with unrealistic pixel values."
        }
      ],
      "resources": [
        {
          "title": "kazuto1011/grad-cam-pytorch",
          "url": "https://github.com/kazuto1011/grad-cam-pytorch",
          "source_type": "software_package"
        },
        {
          "title": "Occlusion Sensitivity Analysis with Augmentation Subspace Perturbation in Deep Feature Space",
          "url": "http://arxiv.org/pdf/2311.15022v1",
          "source_type": "technical_paper",
          "authors": [
            "Pedro Valois",
            "Koichiro Niinuma",
            "Kazuhiro Fukui"
          ],
          "publication_date": "2023-11-25"
        },
        {
          "title": "Occlusion Sensitivity — tf-explain documentation",
          "url": "https://tf-explain.readthedocs.io/en/latest/methods.html#occlusion-sensitivity",
          "source_type": "documentation"
        },
        {
          "title": "Adaptive occlusion sensitivity analysis for visually explaining video recognition networks",
          "url": "http://arxiv.org/pdf/2207.12859v2",
          "source_type": "technical_paper",
          "authors": [
            "Tomoki Uchiyama",
            "Naoya Sogi",
            "Satoshi Iizuka",
            "Koichiro Niinuma",
            "Kazuhiro Fukui"
          ],
          "publication_date": "2022-07-26"
        },
        {
          "title": "sicara/tf-explain",
          "url": "https://github.com/sicara/tf-explain",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 4,
      "related_techniques": [
        "saliency-maps",
        "gradient-weighted-class-activation-mapping"
      ]
    },
    {
      "slug": "attention-visualisation-in-transformers",
      "name": "Attention Visualisation in Transformers",
      "description": "Attention Visualisation in Transformers analyses the multi-head self-attention mechanisms that enable transformers to process sequences by attending to different positions simultaneously. The technique visualises attention weights as heatmaps showing how strongly each token attends to every other token across different heads and layers. By examining these attention patterns, practitioners can understand how models like BERT, GPT, and T5 build contextual representations, identify which tokens influence predictions most strongly, and detect potential biases in how the model processes different types of input. This provides insights into positional encoding effects, head specialisation patterns, and the evolution of attention from local to global context across layers.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/feature-analysis",
        "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Examining attention patterns in a medical language model processing clinical notes to verify it focuses on relevant symptoms and conditions rather than irrelevant demographic identifiers, revealing that certain attention heads specialise in medical terminology whilst others track syntactic relationships between diagnoses and treatments.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a sentiment analysis model for customer reviews by visualising how attention weights differ when processing reviews from different demographic groups, discovering that the model pays disproportionate attention to certain cultural expressions or colloquialisms that could lead to biased sentiment predictions.",
          "goal": "Fairness"
        },
        {
          "description": "Creating visual explanations for regulatory compliance in a financial document classification system, showing which specific words and phrases in loan applications or contracts triggered particular risk assessments, enabling auditors to verify that decisions are based on legitimate financial factors rather than discriminatory language patterns.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "High attention weights do not necessarily indicate causal importance for predictions, as models may attend strongly to tokens that serve structural rather than semantic purposes."
        },
        {
          "description": "The sheer number of attention heads and layers in modern transformers creates visualisation overload, making it difficult to identify meaningful patterns without systematic analysis tools."
        },
        {
          "description": "Attention patterns can be misleading when models use residual connections and layer normalisation, as the final representation incorporates information beyond what attention weights suggest."
        },
        {
          "description": "Different transformer architectures (encoder-only, decoder-only, encoder-decoder) exhibit fundamentally different attention patterns, limiting the generalisability of insights across model types."
        },
        {
          "description": "The technique cannot explain the reasoning process within feed-forward layers or how attention patterns translate into specific predictions, providing only a partial view of model behaviour."
        }
      ],
      "resources": [
        {
          "title": "jessevig/bertviz",
          "url": "https://github.com/jessevig/bertviz",
          "source_type": "software_package",
          "description": "Interactive tool for visualising attention patterns in transformer language models including BERT, GPT-2, and T5"
        },
        {
          "title": "Attention is All You Need",
          "url": "https://arxiv.org/abs/1706.03762",
          "source_type": "technical_paper",
          "description": "Foundational paper introducing the transformer architecture and self-attention mechanism"
        },
        {
          "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
          "url": "https://arxiv.org/abs/1905.09418",
          "source_type": "technical_paper",
          "description": "Research showing how different attention heads specialise in distinct linguistic phenomena"
        },
        {
          "title": "What Does BERT Look At? An Analysis of BERT's Attention",
          "url": "https://arxiv.org/abs/1906.04341",
          "source_type": "technical_paper",
          "description": "Comprehensive analysis of attention patterns in BERT revealing syntactic and semantic specialisation"
        },
        {
          "title": "Transformer Explainability Beyond Attention Visualization",
          "url": "https://arxiv.org/abs/2012.09838",
          "source_type": "technical_paper",
          "description": "Methods for attribution beyond raw attention weights including relevancy propagation and gradient-based approaches"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "integrated-gradients",
        "layer-wise-relevance-propagation",
        "saliency-maps",
        "gradient-weighted-class-activation-mapping",
        "classical-attention-analysis-in-neural-networks",
        "contrastive-explanation-method"
      ]
    }
  ],
  "count": 4
}