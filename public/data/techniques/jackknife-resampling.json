{
  "slug": "jackknife-resampling",
  "name": "Jackknife Resampling",
  "description": "Jackknife resampling (also called leave-one-out resampling) assesses model stability and uncertainty by systematically removing one data point at a time and retraining the model on the remaining data. Unlike bootstrapping which samples with replacement, jackknife creates n different models by excluding each of the n data points once. This systematic approach reveals how individual points influence results, provides robust estimates of prediction variance, and identifies unusually influential observations that may be outliers or leverage points affecting model reliability.",
  "assurance_goals": [
    "Reliability",
    "Transparency",
    "Fairness"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/paradigm/supervised",
    "applicable-models/requirements/black-box",
    "applicable-models/requirements/training-data",
    "assurance-goal-category/fairness",
    "assurance-goal-category/reliability",
    "assurance-goal-category/reliability/uncertainty-quantification",
    "assurance-goal-category/transparency",
    "data-requirements/access-to-training-data",
    "data-type/any",
    "evidence-type/prediction-interval",
    "evidence-type/quantitative-metric",
    "expertise-needed/ml-engineering",
    "expertise-needed/statistics",
    "lifecycle-stage/model-development",
    "lifecycle-stage/system-deployment-and-use",
    "technique-type/algorithmic"
  ],
  "example_use_cases": [
    {
      "description": "Evaluating how removing individual countries from a global climate model affects predictions, identifying which regions have outsized influence and providing robust uncertainty estimates for climate projections used in policy decisions.",
      "goal": "Reliability"
    },
    {
      "description": "Providing transparent uncertainty estimates in medical risk prediction by showing how individual patient records influence model predictions, enabling clinicians to understand prediction stability and confidence intervals for treatment decisions.",
      "goal": "Transparency"
    },
    {
      "description": "Ensuring fair model evaluation in hiring algorithms by systematically testing how removing candidates from different demographic groups affects model performance, revealing whether certain populations disproportionately influence the model's behaviour.",
      "goal": "Fairness"
    }
  ],
  "limitations": [
    {
      "description": "Extremely computationally intensive for large datasets, requiring training of n separate models for n data points, making it impractical for datasets with thousands or millions of observations."
    },
    {
      "description": "May underestimate uncertainty compared to bootstrapping or other resampling methods, as it provides only n different samples rather than a broader exploration of the data distribution."
    },
    {
      "description": "Assumes that removing single data points provides meaningful insights into model stability, which may not hold when multiple correlated observations drive model behaviour."
    },
    {
      "description": "Can be sensitive to the choice of performance metric used for evaluation, as different metrics may show different patterns of sensitivity to individual data points."
    },
    {
      "description": "Provides limited insight into model behaviour on truly novel data, as each jackknife sample is only minimally different from the full training set."
    }
  ],
  "resources": [
    {
      "title": "scikit-learn model_selection.LeaveOneOut",
      "url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html",
      "source_type": "documentation",
      "description": "Scikit-learn implementation of leave-one-out cross-validation for jackknife resampling"
    },
    {
      "title": "Cross-validation: evaluating estimator performance",
      "url": "https://scikit-learn.org/stable/modules/cross_validation.html",
      "source_type": "documentation",
      "description": "Comprehensive guide to cross-validation methods including leave-one-out in scikit-learn"
    }
  ],
  "complexity_rating": 3,
  "computational_cost_rating": 5,
  "related_techniques": [
    "monte-carlo-dropout",
    "prediction-intervals",
    "quantile-regression",
    "conformal-prediction",
    "deep-ensembles",
    "bootstrapping"
  ]
}