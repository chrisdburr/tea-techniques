[
  {
    "slug": "homomorphic-encryption",
    "name": "Homomorphic Encryption",
    "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
    "assurance_goals": [
      "Privacy",
      "Safety",
      "Transparency",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/linear-models",
      "applicable-models/architecture/neural-networks/feedforward",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/parametric",
      "applicable-models/requirements/differentiable",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/privacy",
      "assurance-goal-category/privacy/formal-guarantee",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/privacy-guarantee",
      "evidence-type/quantitative-metric",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "homomorphic encryption homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. this enables secure outsourced computation where sensitive data remains encrypted throughout processing. by allowing ml operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information. privacy safety transparency security applicable-models/architecture/linear-models applicable-models/architecture/neural-networks/feedforward applicable-models/paradigm/discriminative applicable-models/paradigm/parametric applicable-models/requirements/differentiable applicable-models/requirements/white-box assurance-goal-category/privacy assurance-goal-category/privacy/formal-guarantee assurance-goal-category/safety assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/privacy-guarantee evidence-type/quantitative-metric expertise-needed/cryptography expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "red-teaming",
    "name": "Red Teaming",
    "description": "Red teaming involves systematic adversarial testing of AI/ML systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. Drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. Unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/procedural"
    ],
    "searchText": "red teaming red teaming involves systematic adversarial testing of ai/ml systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment. safety reliability fairness security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/qualitative-report expertise-needed/ml-engineering expertise-needed/security lifecycle-stage/system-deployment-and-use technique-type/procedural"
  },
  {
    "slug": "anomaly-detection",
    "name": "Anomaly Detection",
    "description": "Anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. Applied to AI/ML systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. By establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/safety/monitoring/anomaly-detection",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "searchText": "anomaly detection anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. applied to ai/ml systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. by establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm. safety reliability fairness security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/safety/monitoring/anomaly-detection data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/system-deployment-and-use lifecycle-stage/system-deployment-and-use/monitoring technique-type/algorithmic"
  }
]