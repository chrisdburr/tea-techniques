{
  "tag": {
    "name": "expertise-needed/cryptography",
    "slug": "expertise-needed-cryptography",
    "count": 3,
    "category": "expertise-needed"
  },
  "techniques": [
    {
      "slug": "synthetic-data-generation",
      "name": "Synthetic Data Generation",
      "description": "Synthetic data generation creates artificial datasets that aim to preserve the statistical properties, distributions, and relationships of real data whilst containing no actual records from real individuals. The technique encompasses various approaches including generative adversarial networks (GANs), variational autoencoders (VAEs), statistical sampling methods, and privacy-preserving techniques like differential privacy. Beyond privacy protection, synthetic data serves multiple purposes: augmenting limited datasets, balancing class distributions, testing model robustness, enabling data sharing across organisations, and supporting fairness assessments by generating representative samples for underrepresented groups.",
      "assurance_goals": [
        "Privacy",
        "Fairness",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/privacy",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/structured-output",
        "expertise-needed/cryptography",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/data-handling",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Creating realistic but synthetic electronic health records for developing and testing medical diagnosis algorithms without exposing real patient data, enabling secure collaboration between healthcare institutions.",
          "goal": "Privacy"
        },
        {
          "description": "Generating synthetic samples for underrepresented demographic groups in a hiring dataset to train fair recruitment models, ensuring all groups have sufficient representation for bias testing and mitigation.",
          "goal": "Fairness"
        },
        {
          "description": "Augmenting limited training data for rare medical conditions by generating synthetic patient records, improving model reliability and performance on edge cases where real data is insufficient.",
          "goal": "Reliability"
        },
        {
          "description": "Creating synthetic financial transaction data for testing fraud detection systems in development environments, avoiding exposure of real customer financial information whilst maintaining realistic attack patterns.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "May not capture all subtle patterns, correlations, and edge cases present in real data, potentially leading to reduced model performance when deployed on actual data with different characteristics."
        },
        {
          "description": "Generating high-quality synthetic data that maintains both statistical fidelity and utility requires sophisticated techniques and substantial computational resources, especially for complex, high-dimensional datasets."
        },
        {
          "description": "Privacy-preserving approaches may still risk information leakage through statistical inference attacks, membership inference, or model inversion, requiring careful privacy budget management and validation."
        },
        {
          "description": "Synthetic data may inadvertently amplify existing biases in the original data or introduce new biases through the generation process, particularly in generative models trained on biased datasets."
        },
        {
          "description": "Validation and quality assessment of synthetic data is challenging, as traditional metrics may not adequately capture whether the synthetic data preserves the relationships and patterns needed for specific downstream tasks."
        }
      ],
      "resources": [
        {
          "title": "sdv-dev/SDV",
          "url": "https://github.com/sdv-dev/SDV",
          "source_type": "software_package"
        },
        {
          "title": "An evaluation framework for synthetic data generation models",
          "url": "http://arxiv.org/pdf/2404.08866v1",
          "source_type": "technical_paper",
          "authors": [
            "Ioannis E. Livieris",
            "Nikos Alimpertis",
            "George Domalis",
            "Dimitris Tsakalidis"
          ],
          "publication_date": "2024-04-13"
        },
        {
          "title": "Synthetic Data â€” SecureML 0.2.2 documentation",
          "url": "https://secureml.readthedocs.io/en/latest/user_guide/synthetic_data.html",
          "source_type": "documentation"
        },
        {
          "title": "How to Generate Real-World Synthetic Data with CTGAN | Towards ...",
          "url": "https://towardsdatascience.com/how-to-generate-real-world-synthetic-data-with-ctgan-af41b4d60fde/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 4,
      "related_techniques": [
        "federated-learning",
        "differential-privacy",
        "homomorphic-encryption"
      ]
    },
    {
      "slug": "differential-privacy",
      "name": "Differential Privacy",
      "description": "Differential privacy provides mathematically rigorous privacy protection by adding carefully calibrated random noise to data queries, statistical computations, or machine learning outputs. The technique works by ensuring that the presence or absence of any individual's data has minimal impact on the results - specifically, any query result should be nearly indistinguishable whether or not a particular person's data is included. This is achieved through controlled noise addition that scales with the query's sensitivity and a privacy budget (epsilon) that quantifies the privacy-utility trade-off. The smaller the epsilon, the more noise is added and the stronger the privacy guarantee, but at the cost of reduced accuracy.",
      "assurance_goals": [
        "Privacy",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/privacy",
        "assurance-goal-category/privacy/formal-guarantee/differential-privacy",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/privacy-guarantee",
        "expertise-needed/cryptography",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "lifecycle-stage/data-handling",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Protecting individual privacy in census data analysis by adding calibrated noise to demographic statistics, ensuring households cannot be re-identified whilst maintaining accurate population insights for policy planning.",
          "goal": "Privacy"
        },
        {
          "description": "Publishing differentially private aggregate statistics about model performance across different demographic groups, enabling transparent bias auditing without exposing sensitive individual prediction details or group membership.",
          "goal": "Transparency"
        },
        {
          "description": "Enabling fair evaluation of lending algorithms by releasing differentially private performance metrics across protected groups, allowing regulatory compliance checking whilst protecting individual applicant privacy.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Adding noise inherently reduces the accuracy and utility of results, with stronger privacy guarantees (smaller epsilon values) leading to more significant degradation in data quality."
        },
        {
          "description": "Setting the privacy budget (epsilon) requires expertise and careful consideration of the privacy-utility trade-off, with no universal guidelines for appropriate values across different applications."
        },
        {
          "description": "Sequential queries consume the privacy budget cumulatively, potentially requiring careful query planning and potentially prohibiting future analyses once the budget is exhausted."
        },
        {
          "description": "Implementation complexity is high, requiring deep understanding of sensitivity analysis, noise mechanisms, and composition theorems to avoid inadvertent privacy violations."
        },
        {
          "description": "May not protect against all privacy attacks, particularly sophisticated adversaries with auxiliary information or when combined with other data sources that could aid re-identification."
        }
      ],
      "resources": [
        {
          "title": "Google Differential Privacy Library",
          "url": "https://github.com/google/differential-privacy",
          "source_type": "software_package",
          "description": "Open-source library providing implementations of differential privacy algorithms and utilities"
        },
        {
          "title": "The Algorithmic Foundations of Differential Privacy",
          "url": "https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Cynthia Dwork",
            "Aaron Roth"
          ],
          "description": "Foundational monograph on differential privacy theory and algorithms"
        },
        {
          "title": "Opacus: User-Friendly Differential Privacy Library in PyTorch",
          "url": "https://github.com/pytorch/opacus",
          "source_type": "software_package",
          "description": "PyTorch library for training neural networks with differential privacy"
        },
        {
          "title": "Programming Differential Privacy",
          "url": "https://programming-dp.com/",
          "source_type": "tutorial",
          "description": "Comprehensive online book and tutorial for learning differential privacy programming"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 3,
      "related_techniques": [
        "synthetic-data-generation",
        "federated-learning",
        "homomorphic-encryption"
      ]
    },
    {
      "slug": "homomorphic-encryption",
      "name": "Homomorphic Encryption",
      "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
      "assurance_goals": [
        "Privacy",
        "Safety",
        "Transparency",
        "Security"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/privacy",
        "assurance-goal-category/privacy/formal-guarantee",
        "assurance-goal-category/safety",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/privacy-guarantee",
        "expertise-needed/cryptography",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Enabling a cloud-based medical diagnosis service to process encrypted patient data and return encrypted results without the cloud provider ever accessing actual medical information, ensuring complete patient privacy during outsourced computation.",
          "goal": "Privacy"
        },
        {
          "description": "Securing financial risk assessment computations by allowing banks to jointly analyse encrypted transaction patterns for fraud detection without exposing individual customer data, reducing systemic security risks.",
          "goal": "Safety"
        },
        {
          "description": "Enabling transparent audit of algorithmic decision-making by allowing regulators to verify model computations on encrypted data, providing accountability whilst protecting the proprietary nature of both the algorithm and the underlying data.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Extremely computationally expensive, often 100-1000x slower than unencrypted computation, making it impractical for real-time applications or large-scale data processing."
        },
        {
          "description": "Limited range of operations supported efficiently, with complex operations like divisions, comparisons, and non-polynomial functions being particularly challenging or impossible to implement."
        },
        {
          "description": "Implementation requires deep cryptographic expertise to avoid security vulnerabilities, choose appropriate parameters, and optimise performance for specific use cases."
        },
        {
          "description": "Memory and storage requirements are significantly higher than traditional computation, as encrypted data typically requires much more space than plaintext equivalents."
        },
        {
          "description": "Current fully homomorphic encryption schemes have practical limitations on computation depth before noise accumulation requires expensive bootstrapping operations to refresh ciphertexts."
        }
      ],
      "resources": [
        {
          "title": "zama-ai/concrete-ml",
          "url": "https://github.com/zama-ai/concrete-ml",
          "source_type": "software_package",
          "description": "Privacy-preserving machine learning library that enables data scientists to run ML models on encrypted data using FHE without cryptography expertise"
        },
        {
          "title": "Survey on Fully Homomorphic Encryption, Theory, and Applications",
          "url": "https://core.ac.uk/download/579858842.pdf",
          "source_type": "documentation",
          "authors": [
            "Chiara Marcolla",
            "Frank H.P. Fitzek",
            "Marc Manzano",
            "Najwa Aaraj",
            "Riccardo Bassoli",
            "Victor Sucasas"
          ],
          "publication_date": "2022-10-06",
          "description": "Comprehensive survey covering FHE theory, cryptographic schemes, and practical applications across different domains"
        },
        {
          "title": "Welcome to OpenFHE's documentation! â€” OpenFHE documentation",
          "url": "https://openfhe-development.readthedocs.io/",
          "source_type": "documentation",
          "description": "Documentation for open-source C++ library supporting multiple FHE schemes including BFV, BGV, CKKS, and Boolean circuits"
        },
        {
          "title": "Evaluation of Privacy-Preserving Support Vector Machine (SVM) Learning Using Homomorphic Encryption",
          "url": "https://core.ac.uk/download/656115203.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Ali, Hisham",
            "Buchanan, William J."
          ],
          "publication_date": "2025-01-01",
          "description": "Technical paper evaluating performance overhead of SVM learning with homomorphic encryption for privacy-preserving ML"
        },
        {
          "title": "microsoft/SEAL",
          "url": "https://github.com/microsoft/SEAL",
          "source_type": "software_package",
          "description": "Easy-to-use homomorphic encryption library enabling computations on encrypted integers and real numbers"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "synthetic-data-generation",
        "federated-learning",
        "differential-privacy"
      ]
    }
  ],
  "count": 3
}