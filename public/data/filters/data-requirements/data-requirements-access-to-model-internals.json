{
  "tag": {
    "name": "data-requirements/access-to-model-internals",
    "slug": "data-requirements-access-to-model-internals",
    "count": 13,
    "category": "data-requirements"
  },
  "techniques": [
    {
      "slug": "deeplift",
      "name": "DeepLIFT",
      "description": "DeepLIFT (Deep Learning Important FeaTures) explains neural network predictions by decomposing the difference between the actual output and a reference output back to individual input features. It compares each neuron's activation to a reference activation (typically from a baseline input like all zeros or the dataset mean) and propagates these differences backwards through the network using chain rule modifications. Unlike gradient-based methods, DeepLIFT satisfies the sensitivity property (zero input gets zero attribution) and provides more stable attributions by using discrete differences rather than gradients.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-requirements/reference-dataset",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Identifying which genomic sequences contribute to a neural network's prediction of protein binding sites, helping biologists understand regulatory mechanisms by comparing to neutral DNA baselines.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging a deep learning image classifier that misclassifies medical scans by attributing the decision to specific image regions, revealing if the model focuses on irrelevant artifacts rather than pathological features.",
          "goal": "Explainability"
        },
        {
          "description": "Providing transparent explanations for automated loan approval decisions by showing which financial features (relative to typical applicant profiles) most influenced the neural network's recommendation.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires careful selection of reference baseline, as different choices can lead to substantially different attribution scores."
        },
        {
          "description": "Implementation complexity varies significantly across different neural network architectures and layer types."
        },
        {
          "description": "May produce unintuitive results when the chosen reference is not representative of the decision boundary."
        },
        {
          "description": "Limited to feedforward networks and specific layer types, not suitable for all modern architectures like transformers."
        }
      ],
      "resources": [
        {
          "title": "Learning Important Features Through Propagating Activation Differences",
          "url": "http://arxiv.org/pdf/1704.02685v2",
          "source_type": "technical_paper",
          "authors": [
            "Avanti Shrikumar",
            "Peyton Greenside",
            "Anshul Kundaje"
          ],
          "publication_date": "2017-04-10"
        },
        {
          "title": "pytorch/captum",
          "url": "https://github.com/pytorch/captum",
          "source_type": "software_package"
        },
        {
          "title": "Tutorial A3: DeepLIFT/SHAP — tangermeme v0.1.0 documentation",
          "url": "https://tangermeme.readthedocs.io/en/latest/tutorials/Tutorial_A3_Deep_LIFT_SHAP.html",
          "source_type": "tutorial"
        },
        {
          "title": "DeepLIFT Documentation - Captum",
          "url": "https://captum.ai/api/deep_lift.html",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "shapley-additive-explanations",
        "integrated-gradients",
        "layer-wise-relevance-propagation",
        "local-interpretable-model-agnostic-explanations",
        "contrastive-explanation-method",
        "anchor"
      ]
    },
    {
      "slug": "layer-wise-relevance-propagation",
      "name": "Layer-wise Relevance Propagation",
      "description": "Layer-wise Relevance Propagation (LRP) explains neural network predictions by working backwards through the network to show how much each input feature contributed to the final decision. It follows a simple conservation rule: the total contribution scores always add up to the original prediction. Starting from the output, LRP distributes 'relevance' backwards through each layer using different rules depending on the layer type. This creates a detailed breakdown showing which input features helped or hindered the prediction, making it easier to understand why the network made a particular decision.",
      "assurance_goals": [
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Identifying which pixels in chest X-rays contribute to pneumonia detection, helping radiologists verify AI diagnoses by highlighting anatomical regions the model considers relevant.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging a neural network's misclassification of handwritten digits by tracing relevance through layers to identify which input pixels caused the error and which network layers amplified it.",
          "goal": "Explainability"
        },
        {
          "description": "Providing transparent explanations for automated credit scoring decisions by showing which financial features received positive or negative relevance scores, enabling clear regulatory reporting.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires different propagation rules for each layer type, making implementation complex for new architectures."
        },
        {
          "description": "Can produce negative relevance scores which may be difficult to interpret intuitively."
        },
        {
          "description": "Rule selection (LRP-ε, LRP-γ, etc.) significantly affects results and requires domain expertise."
        },
        {
          "description": "Limited to feedforward networks and may not work well with modern architectures like transformers without substantial modifications."
        }
      ],
      "resources": [
        {
          "title": "rachtibat/LRP-eXplains-Transformers",
          "url": "https://github.com/rachtibat/LRP-eXplains-Transformers",
          "source_type": "software_package"
        },
        {
          "title": "sebastian-lapuschkin/lrp_toolbox",
          "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
          "source_type": "software_package"
        },
        {
          "title": "Getting started — zennit documentation",
          "url": "https://zennit.readthedocs.io/en/latest/getting-started.html",
          "source_type": "documentation"
        },
        {
          "title": "Layer-wise Relevance Propagation eXplains Transformers (LXT) documentation",
          "url": "https://lxt.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation",
          "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140",
          "source_type": "technical_paper",
          "authors": [
            "Sebastian Bach",
            "Alexander Binder",
            "Grégoire Montavon",
            "Frederick Klauschen",
            "Klaus-Robert Müller",
            "Wojciech Samek"
          ],
          "publication_date": "2015-07-10"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 3,
      "acronym": "LRP",
      "related_techniques": [
        "shapley-additive-explanations",
        "integrated-gradients",
        "deeplift",
        "local-interpretable-model-agnostic-explanations",
        "contrastive-explanation-method",
        "anchor"
      ]
    },
    {
      "slug": "taylor-decomposition",
      "name": "Taylor Decomposition",
      "description": "Taylor Decomposition is a mathematical technique that explains neural network predictions by computing first-order and higher-order derivatives of the network's output with respect to input features. It decomposes the prediction into relevance scores that indicate how much each input feature and feature interaction contributes to the final decision. The method uses Layer-wise Relevance Propagation (LRP) principles to trace prediction contributions backwards through the network layers, providing precise mathematical attributions for each input element.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analyzing which pixels in an image contribute most to a convolutional neural network's classification decision, showing both positive and negative relevance scores for different regions of the input image.",
          "goal": "Explainability"
        },
        {
          "description": "Understanding how different word embeddings in a sentiment analysis model contribute to the final sentiment score, revealing which terms drive positive vs negative predictions.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Mathematically complex requiring deep understanding of calculus and neural network architectures."
        },
        {
          "description": "Computationally intensive as it requires computing gradients and higher-order derivatives through the entire network."
        },
        {
          "description": "Approximations used in practice may introduce errors that affect attribution accuracy."
        },
        {
          "description": "Limited tooling availability compared to other explainability methods, with most implementations being research-focused rather than production-ready."
        }
      ],
      "resources": [
        {
          "title": "Explaining NonLinear Classification Decisions with Deep Taylor Decomposition",
          "url": "http://arxiv.org/pdf/1512.02479v1",
          "source_type": "technical_paper",
          "authors": [
            "Grégoire Montavon",
            "Sebastian Bach",
            "Alexander Binder",
            "Wojciech Samek",
            "Klaus-Robert Müller"
          ],
          "publication_date": "2015-12-08"
        },
        {
          "title": "A Rigorous Study Of The Deep Taylor Decomposition",
          "url": "http://arxiv.org/pdf/2211.08425v1",
          "source_type": "technical_paper",
          "authors": [
            "Leon Sixt",
            "Tim Landgraf"
          ],
          "publication_date": "2022-11-14"
        },
        {
          "title": "sebastian-lapuschkin/lrp_toolbox",
          "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "contextual-decomposition",
        "influence-functions"
      ]
    },
    {
      "slug": "saliency-maps",
      "name": "Saliency Maps",
      "description": "Saliency maps are visual explanations for image classification models that highlight which pixels in an image most strongly influence the model's prediction. Computed by calculating gradients of the model's output with respect to input pixels, saliency maps produce heatmaps where brighter regions indicate pixels that, when changed, would most significantly affect the prediction. This technique helps users understand which parts of an image the model is 'looking at' when making decisions.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing X-ray images in a pneumonia detection model to verify that the algorithm focuses on lung regions showing inflammatory patterns rather than irrelevant areas like medical equipment or patient positioning markers.",
          "goal": "Explainability"
        },
        {
          "description": "Examining skin lesion classification models to ensure the algorithm identifies diagnostic features (irregular borders, colour variation) rather than artifacts like rulers, hair, or skin markings that shouldn't influence medical decisions.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a dermatology AI system to verify it focuses on medical symptoms rather than skin colour when diagnosing conditions, ensuring equitable treatment across racial groups by revealing inappropriate attention to demographic features.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Saliency maps are often noisy and can change dramatically with small input perturbations, making them unstable."
        },
        {
          "description": "Highlighted regions may not correspond to semantically meaningful or human-understandable features."
        },
        {
          "description": "Only indicates local gradient information, not causal importance or actual decision-making logic."
        },
        {
          "description": "May highlight irrelevant pixels that happen to have high gradients due to model artifacts rather than meaningful patterns."
        }
      ],
      "resources": [
        {
          "title": "utkuozbulak/pytorch-cnn-visualizations",
          "url": "https://github.com/utkuozbulak/pytorch-cnn-visualizations",
          "source_type": "software_package"
        },
        {
          "title": "Concepts of Saliency and Explainability in AI",
          "url": "https://xaitk-saliency.readthedocs.io/en/latest/xaitk_explanation.html",
          "source_type": "documentation"
        },
        {
          "title": "Occlusion Saliency Example",
          "url": "https://xaitk-saliency.readthedocs.io/en/latest/examples/OcclusionSaliency.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "gradient-weighted-class-activation-mapping",
        "occlusion-sensitivity"
      ]
    },
    {
      "slug": "gradient-weighted-class-activation-mapping",
      "name": "Gradient-weighted Class Activation Mapping",
      "description": "Grad-CAM creates visual heatmaps showing which regions of an image a convolutional neural network focuses on when making a specific classification. Unlike pixel-level techniques, Grad-CAM produces coarser region-based explanations by using gradients from the predicted class to weight the CNN's final feature maps, then projecting these weighted activations back to create an overlay on the original image. This provides intuitive visual explanations of where the model is 'looking' for evidence of different classes.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Validating that a melanoma detection model focuses on the actual skin lesion rather than surrounding healthy skin, medical equipment, or artifacts when making cancer/benign classifications.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging an autonomous vehicle's traffic sign recognition system by visualising whether the model correctly focuses on the sign itself rather than background objects, shadows, or irrelevant visual elements.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a medical imaging system for racial bias by examining whether diagnostic predictions inappropriately focus on skin tone regions rather than actual pathological features, ensuring equitable healthcare AI deployment.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires access to the CNN's internal feature maps and gradients, limiting use to white-box scenarios."
        },
        {
          "description": "Resolution is constrained by the final convolutional layer's feature map size, producing coarser localisation than pixel-level methods."
        },
        {
          "description": "Only applicable to CNN architectures with clearly defined convolutional layers, not suitable for other neural network types."
        },
        {
          "description": "May highlight regions that correlate with the class but aren't causally important for the model's decision-making process."
        }
      ],
      "resources": [
        {
          "title": "jacobgil/pytorch-grad-cam",
          "url": "https://github.com/jacobgil/pytorch-grad-cam",
          "source_type": "software_package"
        },
        {
          "title": "Grad-CAM: Visualize class activation maps with Keras, TensorFlow ...",
          "url": "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/",
          "source_type": "tutorial"
        },
        {
          "title": "kazuto1011/grad-cam-pytorch",
          "url": "https://github.com/kazuto1011/grad-cam-pytorch",
          "source_type": "software_package"
        },
        {
          "title": "A Tutorial on Explainable Image Classification for Dementia Stages Using Convolutional Neural Network and Gradient-weighted Class Activation Mapping",
          "url": "https://www.semanticscholar.org/paper/8b1139cb06cfe5ba69e2fd05e1450b43df031a02",
          "source_type": "documentation",
          "authors": [
            "Kevin Kam Fung Yuen"
          ]
        },
        {
          "title": "A Guide to Grad-CAM in Deep Learning - Analytics Vidhya",
          "url": "https://www.analyticsvidhya.com/blog/2023/12/grad-cam-in-deep-learning/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "Grad-CAM",
      "related_techniques": [
        "saliency-maps",
        "occlusion-sensitivity"
      ]
    },
    {
      "slug": "classical-attention-analysis-in-neural-networks",
      "name": "Classical Attention Analysis in Neural Networks",
      "description": "Classical attention mechanisms in RNNs and CNNs create alignment matrices and temporal attention patterns that show how models focus on different input elements over time or space. This technique analyses these traditional attention patterns, particularly in encoder-decoder architectures and sequence-to-sequence models, where attention weights reveal which source elements influence each output step. Unlike transformer self-attention analysis, this focuses on understanding alignment patterns, temporal dependencies, and encoder-decoder attention dynamics in classical neural architectures.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/rnn",
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing encoder-decoder attention in a neural machine translation model to verify the alignment between source and target words, ensuring the model learns proper translation correspondences rather than positional biases.",
          "goal": "Explainability"
        },
        {
          "description": "Examining temporal attention patterns in an RNN-based image captioning model to understand how attention moves across different image regions as it generates each word of the caption description.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Attention weights are not always strongly correlated with feature importance for the final prediction."
        },
        {
          "description": "High attention does not necessarily imply causal influence - models can attend to irrelevant but correlated features."
        },
        {
          "description": "Only applicable to neural network architectures that explicitly use attention mechanisms."
        },
        {
          "description": "Interpretation can be misleading without understanding the specific attention mechanism implementation and training dynamics."
        }
      ],
      "resources": [
        {
          "title": "An Attentive Survey of Attention Models",
          "url": "https://www.semanticscholar.org/paper/a8427ce5aee6d62800c725588e89940ed4910e0d",
          "source_type": "documentation",
          "authors": [
            "S. Chaudhari",
            "Gungor Polatkan",
            "R. Ramanath",
            "Varun Mithal"
          ]
        },
        {
          "title": "Attention, please! A survey of neural attention models in deep learning",
          "url": "https://www.semanticscholar.org/paper/44930df2a3186edb58c4d6f6e5ed828c5d6a0089",
          "source_type": "documentation",
          "authors": [
            "Alana de Santana Correia",
            "E. Colombini"
          ]
        },
        {
          "title": "ecco - Explain, Analyze, and Visualize NLP Language Models",
          "url": "https://github.com/jalammar/ecco",
          "source_type": "software_package"
        },
        {
          "title": "Enhancing Sentiment Analysis of Twitter Data Using Recurrent Neural Networks with Attention Mechanism",
          "url": "https://www.semanticscholar.org/paper/c59e0158280a567114ae8ca64a932eefd127e0aa",
          "source_type": "technical_paper",
          "authors": [
            "S. Nithya",
            "X. A. Presskila",
            "B. Sakthivel",
            "R. Krishnan",
            "K. Narayanan",
            "S. Sundararajan"
          ]
        },
        {
          "title": "Can Neural Networks Develop Attention? Google Thinks they Can ...",
          "url": "https://www.kdnuggets.com/2019/11/neural-networks-develop-attention-google.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 1,
      "related_techniques": [
        "attention-visualisation-in-transformers"
      ]
    },
    {
      "slug": "temperature-scaling",
      "name": "Temperature Scaling",
      "description": "Temperature scaling adjusts a model's confidence by applying a single parameter (temperature) to its predictions. When a model is too confident in its wrong answers, temperature scaling can fix this by making the predictions more realistic. It works by dividing the model's outputs by the temperature value before converting them to probabilities. Higher temperatures make the model less confident, whilst lower temperatures increase confidence. The technique maintains the model's accuracy whilst ensuring that when it says it's 90% confident, it's actually right about 90% of the time.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-requirements/calibration-set",
        "data-requirements/validation-set",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "explanatory-scope/global",
        "expertise-needed/statistics",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Adjusting a deep learning image classifier's confidence scores to be realistic, ensuring that when it's 90% confident, it's right 90% of the time.",
          "goal": "Reliability"
        },
        {
          "description": "Making medical diagnosis model predictions more trustworthy by providing realistic confidence scores that doctors can interpret and use to make informed decisions about patient care.",
          "goal": "Transparency"
        },
        {
          "description": "Ensuring fair treatment across patient demographics by calibrating confidence scores equally across different groups, preventing systematic over-confidence in predictions for certain populations.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Only addresses calibration at the overall dataset level, not subgroup-specific miscalibration issues."
        },
        {
          "description": "Does not improve the rank ordering or accuracy of predictions, only adjusts confidence levels."
        },
        {
          "description": "Assumes that calibration errors are consistent across different types of inputs and feature values."
        },
        {
          "description": "Requires a separate validation set for temperature parameter optimisation, which may not be available in small datasets."
        }
      ],
      "resources": [
        {
          "title": "gpleiss/temperature_scaling",
          "url": "https://github.com/gpleiss/temperature_scaling",
          "source_type": "software_package"
        },
        {
          "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
          "url": "http://arxiv.org/pdf/2502.20604v1",
          "source_type": "technical_paper",
          "authors": [
            "Hao Xuan",
            "Bokai Yang",
            "Xingyu Li"
          ],
          "publication_date": "2025-02-28"
        },
        {
          "title": "Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration",
          "url": "http://arxiv.org/pdf/2209.11604v2",
          "source_type": "technical_paper",
          "authors": [
            "Yung-Chen Tang",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
          ],
          "publication_date": "2024-07-24"
        },
        {
          "title": "On Calibration of Modern Neural Networks | arXiv",
          "url": "https://arxiv.org/abs/1706.04599",
          "source_type": "technical_paper",
          "authors": [
            "Chuan Guo",
            "Geoff Pleiss",
            "Yu Sun",
            "Kilian Q. Weinberger"
          ],
          "publication_date": "2017-06-14"
        },
        {
          "title": "On the Limitations of Temperature Scaling for Distributions with Overlaps",
          "url": "http://arxiv.org/pdf/2306.00740v3",
          "source_type": "technical_paper",
          "authors": [
            "Muthu Chidambaram",
            "Rong Ge"
          ],
          "publication_date": "2023-06-01"
        }
      ],
      "complexity_rating": 1,
      "computational_cost_rating": 1,
      "related_techniques": [
        "empirical-calibration"
      ]
    },
    {
      "slug": "model-pruning",
      "name": "Model Pruning",
      "description": "Model pruning systematically removes less important weights, neurons, or entire layers from neural networks to create smaller, more efficient models whilst maintaining performance. This process involves iterative removal based on importance criteria (weight magnitudes, gradient information, activation patterns) followed by fine-tuning. Pruning can be structured (removing entire neurons/channels) or unstructured (removing individual weights), with structured pruning providing greater computational benefits and interpretability through simplified architectures.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/neural-network",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-optimization",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Compressing a medical imaging model from 100MB to 15MB for deployment on edge devices in remote clinics, enabling healthcare professionals to audit the remaining critical feature detectors and understand which anatomical patterns drive diagnoses whilst maintaining diagnostic accuracy.",
          "goal": "Explainability"
        },
        {
          "description": "Pruning a financial fraud detection model by 70% to eliminate redundant pathways that amplify noise, creating a more robust system that maintains consistent predictions across different transaction types and reduces false positives during market volatility.",
          "goal": "Reliability"
        },
        {
          "description": "Reducing an autonomous vehicle perception model to ensure predictable inference times under 50ms for safety-critical decisions, removing non-essential neurons to guarantee consistent computational behaviour whilst maintaining object detection accuracy for pedestrian safety.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Determining optimal pruning ratios requires extensive experimentation as over-pruning can cause dramatic accuracy degradation, whilst under-pruning provides minimal benefits, making the process time-consuming and resource-intensive."
        },
        {
          "description": "Structured pruning often requires specific hardware or software framework support to realise computational benefits, limiting deployment flexibility and potentially necessitating model architecture changes."
        },
        {
          "description": "Pruned models may exhibit reduced robustness to out-of-distribution inputs or adversarial attacks, as removing neurons can eliminate defensive redundancy that helped handle edge cases."
        },
        {
          "description": "The iterative pruning and fine-tuning process can be computationally expensive, sometimes requiring more resources than training the original model, particularly for large-scale networks."
        },
        {
          "description": "Pruning criteria based on weight magnitudes or gradients may not align with interpretability goals, potentially removing neurons that contribute to model transparency whilst retaining complex, opaque pathways."
        }
      ],
      "resources": [
        {
          "title": "horseee/LLM-Pruner",
          "url": "https://github.com/horseee/LLM-Pruner",
          "source_type": "software_package",
          "description": "Structural pruning tool for large language models supporting Llama, BLOOM, and other LLMs with three-stage compression process requiring only 50,000 training samples for post-training recovery."
        },
        {
          "title": "Pruning Quickstart — Neural Network Intelligence",
          "url": "https://nni.readthedocs.io/en/stable/tutorials/pruning_quick_start.html",
          "source_type": "tutorial",
          "description": "Step-by-step tutorial for implementing model pruning using Microsoft's NNI toolkit, covering basic usage, pruning algorithms, and practical examples for neural network compression."
        },
        {
          "title": "Overview of NNI Model Pruning — Neural Network Intelligence",
          "url": "https://nni.readthedocs.io/en/stable/compression/pruning.html",
          "source_type": "documentation",
          "description": "Comprehensive documentation for NNI's pruning capabilities covering structured and unstructured pruning strategies, supported algorithms, and integration with popular deep learning frameworks."
        },
        {
          "title": "coldlarry/YOLOv3-complete-pruning",
          "url": "https://github.com/coldlarry/YOLOv3-complete-pruning",
          "source_type": "software_package",
          "description": "Complete pruning implementation for YOLOv3 object detection models demonstrating computer vision model compression with minimal accuracy loss for real-time inference applications."
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "model-distillation"
      ]
    },
    {
      "slug": "neuron-activation-analysis",
      "name": "Neuron Activation Analysis",
      "description": "Neuron activation analysis examines the firing patterns of individual neurons in neural networks by probing them with diverse inputs and analysing their activation responses. This technique helps understand what concepts, features, or patterns different neurons have learned to recognise, providing insights into the model's internal representations. For large language models, this can reveal neurons specialised for linguistic concepts, semantic categories, or even potentially harmful patterns, enabling targeted interventions and deeper model understanding.",
      "assurance_goals": [
        "Explainability",
        "Safety",
        "Fairness"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/llm",
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/safety",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "evidence-type/visualisation",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "lifecycle-stage/monitoring",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing GPT-based models to identify specific neurons that activate on toxic or harmful content, enabling targeted interventions to reduce model toxicity whilst preserving general language capabilities for safer AI deployment.",
          "goal": "Safety"
        },
        {
          "description": "Examining activation patterns in multilingual language models to detect neurons that exhibit systematic biases when processing text from different linguistic communities, revealing implicit representation inequalities that could affect downstream applications.",
          "goal": "Fairness"
        },
        {
          "description": "Investigating individual neurons in medical language models to understand which clinical concepts and medical knowledge representations drive diagnostic suggestions, enabling healthcare professionals to validate the model's medical reasoning pathways.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Many neurons exhibit polysemantic behaviour, representing multiple unrelated concepts simultaneously, making it difficult to assign clear interpretable meanings to individual neural units."
        },
        {
          "description": "Important model behaviours are often distributed across many neurons rather than localised in single units, requiring analysis of neural circuits and interactions that can be exponentially complex."
        },
        {
          "description": "Computational costs scale dramatically with modern large language models containing billions of parameters, making comprehensive neuron-by-neuron analysis prohibitively expensive for complete model understanding."
        },
        {
          "description": "Neuron activation patterns are highly context-dependent, with the same neuron potentially serving different roles based on surrounding input context, complicating consistent interpretation across diverse scenarios."
        },
        {
          "description": "Interpretation of activation patterns often relies on subjective human analysis without rigorous validation methods, potentially leading to confirmation bias or misattribution of neural functions."
        }
      ],
      "resources": [
        {
          "title": "jalammar/ecco",
          "url": "https://github.com/jalammar/ecco",
          "source_type": "software_package"
        },
        {
          "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models",
          "url": "http://arxiv.org/pdf/2504.21053v1",
          "source_type": "technical_paper",
          "authors": [
            "Yi Zhou",
            "Wenpeng Xing",
            "Dezhang Kong",
            "Changting Lin",
            "Meng Han"
          ],
          "publication_date": "2025-04-29"
        },
        {
          "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron Activation Analysis",
          "url": "http://arxiv.org/pdf/2404.13567v1",
          "source_type": "technical_paper",
          "authors": [
            "Abhilekha Dalal",
            "Rushrukh Rayan",
            "Adrita Barua",
            "Eugene Y. Vasserman",
            "Md Kamruzzaman Sarker",
            "Pascal Hitzler"
          ],
          "publication_date": "2024-04-21"
        },
        {
          "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron\n  Activation Analysis",
          "url": "http://arxiv.org/abs/2404.13567",
          "source_type": "technical_paper",
          "authors": [
            "Barua, Adrita",
            "Dalal, Abhilekha",
            "Hitzler, Pascal",
            "Rayan, Rushrukh",
            "Sarker, Md Kamruzzaman",
            "Vasserman, Eugene Y."
          ],
          "publication_date": "2024-04-21"
        },
        {
          "title": "Ecco",
          "url": "https://ecco.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "Tracing the Thoughts in Language Models",
          "url": "https://www.anthropic.com/news/tracing-thoughts-language-model",
          "source_type": "documentation"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "prototype-and-criticism-models",
        "concept-activation-vectors"
      ]
    },
    {
      "slug": "causal-mediation-analysis-in-language-models",
      "name": "Causal Mediation Analysis in Language Models",
      "description": "Causal mediation analysis in language models is a mechanistic interpretability technique that systematically investigates how specific internal components (neurons, attention heads, or layers) causally contribute to model outputs. By performing controlled interventions—such as activating, deactivating, or modifying specific components—researchers can trace the causal pathways through which information flows and transforms within the model. This approach goes beyond correlation to establish causal relationships, enabling researchers to understand not just what features influence outputs, but how and why they do so through specific computational pathways.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/llm",
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "evidence-type/causal-analysis",
        "expertise-needed/causal-inference",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/post-deployment",
        "technique-type/mechanistic-interpretability"
      ],
      "example_use_cases": [
        {
          "description": "Investigating causal pathways in content moderation models to understand how specific attention mechanisms contribute to flagging potentially harmful content, enabling verification that safety decisions rely on appropriate features rather than spurious correlations and ensuring robust content filtering.",
          "goal": "Safety"
        },
        {
          "description": "Identifying specific neurons or attention heads that causally contribute to biased outputs in hiring or lending language models, enabling targeted interventions to reduce discriminatory behaviour whilst preserving model performance on legitimate tasks and ensuring fair treatment across demographics.",
          "goal": "Reliability"
        },
        {
          "description": "Tracing causal pathways in large language models performing mathematical reasoning tasks to understand how intermediate steps are computed and stored, revealing which components are responsible for different aspects of logical inference and enabling validation of reasoning processes.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Requires sophisticated understanding of model architecture to design meaningful interventions, as poorly chosen intervention points may yield misleading causal conclusions or fail to capture relevant computational pathways."
        },
        {
          "description": "Results are highly dependent on the validity of underlying causal assumptions, which can be difficult to verify in complex, high-dimensional neural network spaces where multiple causal pathways may interact."
        },
        {
          "description": "Comprehensive causal analysis requires extensive computational resources, particularly for large models, as each intervention requires separate forward passes and multiple intervention combinations for robust conclusions."
        },
        {
          "description": "Distinguishing between direct causal effects and indirect effects mediated through other components can be challenging, potentially leading to oversimplified causal narratives that miss important intermediate processes."
        },
        {
          "description": "Causal relationships identified in specific contexts or datasets may not generalise to different domains, tasks, or model versions, requiring careful validation across diverse scenarios to ensure robust findings."
        }
      ],
      "resources": [],
      "complexity_rating": 5,
      "computational_cost_rating": 4,
      "related_techniques": [
        "prompt-sensitivity-analysis",
        "feature-attribution-with-integrated-gradients-in-nlp"
      ]
    },
    {
      "slug": "concept-activation-vectors",
      "name": "Concept Activation Vectors",
      "description": "Concept Activation Vectors (CAVs), also known as Testing with Concept Activation Vectors (TCAV), identify mathematical directions in neural network representation space that correspond to human-understandable concepts such as 'stripes', 'young', or 'medical equipment'. The technique works by finding linear directions that separate activations of concept examples from non-concept examples, then measuring how much these concept directions influence the model's predictions. This provides quantitative answers to questions like 'How much does the concept of youth affect this model's hiring decisions?' enabling systematic bias detection and model understanding.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/neural-network",
        "applicable-models/transformer",
        "applicable-models/cnn",
        "assurance-goal-category/explainability",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/visualization",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-knowledge",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use/auditing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Auditing a medical imaging model to verify it focuses on diagnostic features (like 'tumour characteristics') rather than irrelevant concepts (like 'scanner type' or 'patient positioning') when classifying chest X-rays, ensuring clinical decisions rely on medically relevant information.",
          "goal": "Explainability"
        },
        {
          "description": "Testing whether a hiring algorithm's resume screening decisions are influenced by concepts related to protected characteristics such as 'gender-associated names', 'prestigious universities', or 'employment gaps', enabling systematic bias detection and compliance verification.",
          "goal": "Fairness"
        },
        {
          "description": "Providing regulatory-compliant explanations for financial lending decisions by quantifying how concepts like 'debt-to-income ratio', 'employment stability', and 'credit history length' influence loan approval models, with precise sensitivity scores for audit documentation.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires clearly defined concept examples and non-concept examples, which can be challenging to obtain for abstract or subjective concepts."
        },
        {
          "description": "Assumes that meaningful concept directions exist as linear separable directions in the model's internal representation space, which may not hold for all concepts."
        },
        {
          "description": "Results depend heavily on which network layer is examined, as different layers capture different levels of abstraction and concept representation."
        },
        {
          "description": "Computational cost grows significantly with model size and number of concepts tested, though recent advances like FastCAV address this limitation."
        },
        {
          "description": "Interpretation requires domain expertise to define meaningful concepts and understand the significance of sensitivity scores in practical contexts."
        }
      ],
      "resources": [
        {
          "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks",
          "url": "http://arxiv.org/pdf/2505.17883v1",
          "source_type": "technical_paper",
          "authors": [
            "Laines Schmalwasser",
            "Niklas Penzel",
            "Joachim Denzler",
            "Julia Niebling"
          ],
          "publication_date": "2025-05-23"
        },
        {
          "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
          "url": "http://arxiv.org/pdf/2311.15303v1",
          "source_type": "technical_paper",
          "authors": [
            "Avani Gupta",
            "Saurabh Saini",
            "P J Narayanan"
          ],
          "publication_date": "2023-11-26"
        },
        {
          "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations",
          "url": "http://arxiv.org/pdf/2503.05522v1",
          "source_type": "technical_paper",
          "authors": [
            "Eren Erogullari",
            "Sebastian Lapuschkin",
            "Wojciech Samek",
            "Frederik Pahde"
          ],
          "publication_date": "2025-03-07"
        },
        {
          "title": "Concept Gradient: Concept-based Interpretation Without Linear Assumption",
          "url": "http://arxiv.org/pdf/2208.14966v2",
          "source_type": "technical_paper",
          "authors": [
            "Andrew Bai",
            "Chih-Kuan Yeh",
            "Pradeep Ravikumar",
            "Neil Y. C. Lin",
            "Cho-Jui Hsieh"
          ],
          "publication_date": "2022-08-31"
        },
        {
          "title": "SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation",
          "url": "http://arxiv.org/pdf/2310.07698v1",
          "source_type": "technical_paper",
          "authors": [
            "Bo Pan",
            "Zhenke Liu",
            "Yifei Zhang",
            "Liang Zhao"
          ],
          "publication_date": "2023-10-11"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "acronym": "CAVs",
      "related_techniques": [
        "prototype-and-criticism-models",
        "neuron-activation-analysis"
      ]
    },
    {
      "slug": "attention-visualisation-in-transformers",
      "name": "Attention Visualisation in Transformers",
      "description": "Attention Visualisation in Transformers analyses the multi-head self-attention mechanisms that enable transformers to process sequences by attending to different positions simultaneously. The technique visualises attention weights as heatmaps showing how strongly each token attends to every other token across different heads and layers. By examining these attention patterns, practitioners can understand how models like BERT, GPT, and T5 build contextual representations, identify which tokens influence predictions most strongly, and detect potential biases in how the model processes different types of input. This provides insights into positional encoding effects, head specialisation patterns, and the evolution of attention from local to global context across layers.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/transformer",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/feature-analysis",
        "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/text",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "explanatory-scope/global",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Examining attention patterns in a medical language model processing clinical notes to verify it focuses on relevant symptoms and conditions rather than irrelevant demographic identifiers, revealing that certain attention heads specialise in medical terminology whilst others track syntactic relationships between diagnoses and treatments.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a sentiment analysis model for customer reviews by visualising how attention weights differ when processing reviews from different demographic groups, discovering that the model pays disproportionate attention to certain cultural expressions or colloquialisms that could lead to biased sentiment predictions.",
          "goal": "Fairness"
        },
        {
          "description": "Creating visual explanations for regulatory compliance in a financial document classification system, showing which specific words and phrases in loan applications or contracts triggered particular risk assessments, enabling auditors to verify that decisions are based on legitimate financial factors rather than discriminatory language patterns.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "High attention weights do not necessarily indicate causal importance for predictions, as models may attend strongly to tokens that serve structural rather than semantic purposes."
        },
        {
          "description": "The sheer number of attention heads and layers in modern transformers creates visualisation overload, making it difficult to identify meaningful patterns without systematic analysis tools."
        },
        {
          "description": "Attention patterns can be misleading when models use residual connections and layer normalisation, as the final representation incorporates information beyond what attention weights suggest."
        },
        {
          "description": "Different transformer architectures (encoder-only, decoder-only, encoder-decoder) exhibit fundamentally different attention patterns, limiting the generalisability of insights across model types."
        },
        {
          "description": "The technique cannot explain the reasoning process within feed-forward layers or how attention patterns translate into specific predictions, providing only a partial view of model behaviour."
        }
      ],
      "resources": [
        {
          "title": "jessevig/bertviz",
          "url": "https://github.com/jessevig/bertviz",
          "source_type": "software_package",
          "description": "Interactive tool for visualising attention patterns in transformer language models including BERT, GPT-2, and T5"
        },
        {
          "title": "Attention is All You Need",
          "url": "https://arxiv.org/abs/1706.03762",
          "source_type": "technical_paper",
          "description": "Foundational paper introducing the transformer architecture and self-attention mechanism"
        },
        {
          "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
          "url": "https://arxiv.org/abs/1905.09418",
          "source_type": "technical_paper",
          "description": "Research showing how different attention heads specialise in distinct linguistic phenomena"
        },
        {
          "title": "What Does BERT Look At? An Analysis of BERT's Attention",
          "url": "https://arxiv.org/abs/1906.04341",
          "source_type": "technical_paper",
          "description": "Comprehensive analysis of attention patterns in BERT revealing syntactic and semantic specialisation"
        },
        {
          "title": "Transformer Explainability Beyond Attention Visualization",
          "url": "https://arxiv.org/abs/2012.09838",
          "source_type": "technical_paper",
          "description": "Methods for attribution beyond raw attention weights including relevancy propagation and gradient-based approaches"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "integrated-gradients",
        "layer-wise-relevance-propagation",
        "saliency-maps",
        "gradient-weighted-class-activation-mapping",
        "classical-attention-analysis-in-neural-networks",
        "contrastive-explanation-method"
      ]
    },
    {
      "slug": "adaptive-sensitive-reweighting",
      "name": "Adaptive Sensitive Reweighting",
      "description": "Adaptive Sensitive Reweighting dynamically adjusts the importance of training examples during model training based on real-time performance across different demographic groups. Unlike traditional static reweighting that fixes weights at the start, this technique continuously monitors fairness metrics and automatically increases the weight of examples from underperforming groups whilst decreasing weights for overrepresented groups. The adaptive mechanism prevents models from perpetuating historical biases by ensuring balanced learning across all demographics throughout the training process.",
      "assurance_goals": [
        "Fairness",
        "Reliability"
      ],
      "tags": [
        "applicable-models/agnostic",
        "assurance-goal-category/fairness",
        "assurance-goal-category/reliability",
        "data-requirements/access-to-model-internals",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "fairness-approach/group",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Training speech recognition systems that adapt weights during training to ensure consistent accuracy across different accents, dialects, and linguistic backgrounds, preventing models from favouring dominant accent groups in the training data.",
          "goal": "Fairness"
        },
        {
          "description": "Developing hiring algorithms that dynamically adjust training example weights to maintain consistent evaluation performance across demographic groups, ensuring the model doesn't learn to favour candidates from overrepresented backgrounds.",
          "goal": "Fairness"
        },
        {
          "description": "Building medical diagnostic models that adaptively reweight patient examples during training to ensure reliable performance across different age groups, ethnicities, and socioeconomic backgrounds, preventing healthcare disparities.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Training instability can occur when adaptive weight adjustments cause oscillations between demographic groups, potentially preventing convergence if reweighting parameters are not carefully tuned."
        },
        {
          "description": "Computational overhead increases significantly due to continuous monitoring of fairness metrics across groups during training, requiring additional memory and processing time."
        },
        {
          "description": "Risk of overfitting to specific demographic subgroups if the adaptation mechanism becomes too aggressive in correcting for observed performance disparities during training."
        },
        {
          "description": "Requires careful hyperparameter tuning for adaptation rates and fairness thresholds, making the technique sensitive to configuration choices that may not generalise across different datasets."
        },
        {
          "description": "May inadvertently harm overall model performance if the reweighting process prioritises fairness at the expense of learning important patterns that benefit all groups."
        }
      ],
      "resources": [
        {
          "title": "Adaptive Sensitive Reweighting to Mitigate Bias in Fairness-aware Classification",
          "url": "https://dl.acm.org/doi/10.1145/3178876.3186133",
          "source_type": "technical_paper",
          "description": "Original paper introducing adaptive sensitive reweighting technique using CULEP model for bias mitigation in classification tasks",
          "authors": [
            "Krasanakis, Emmanouil",
            "Spyromitros-Xioufis, Eleftherios",
            "Papadopoulos, Symeon",
            "Kompatsiaris, Yiannis"
          ],
          "publication_date": "2018-04-23"
        },
        {
          "title": "AIF360: A comprehensive set of fairness metrics for datasets and machine learning models",
          "url": "https://github.com/Trusted-AI/AIF360",
          "source_type": "software_package",
          "description": "IBM's comprehensive fairness toolkit including implementations of various reweighting techniques and bias mitigation methods"
        },
        {
          "title": "Fairlearn: A toolkit for assessing and improving fairness in machine learning",
          "url": "https://github.com/fairlearn/fairlearn",
          "source_type": "software_package",
          "description": "Microsoft's open-source toolkit providing reweighting and other bias mitigation algorithms with comprehensive documentation"
        },
        {
          "title": "Causal Fairness-Guided Dataset Reweighting using Neural Networks",
          "url": "https://arxiv.org/abs/2311.10512",
          "source_type": "technical_paper",
          "description": "Recent research on causal fairness-guided dataset reweighting using neural networks to address fairness from causal perspective",
          "authors": [
            "Zhao, Xuan",
            "Broelemann, Klaus",
            "Ruggieri, Salvatore",
            "Kasneci, Gjergji"
          ],
          "publication_date": "2023-11-17"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 3,
      "related_techniques": [
        "adversarial-debiasing",
        "fair-adversarial-networks",
        "prejudice-remover-regulariser",
        "meta-fair-classifier",
        "exponentiated-gradient-reduction",
        "fair-transfer-learning",
        "multi-accuracy-boosting"
      ]
    }
  ],
  "count": 13
}