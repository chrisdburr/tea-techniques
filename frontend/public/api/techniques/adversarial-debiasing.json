{
  "slug": "adversarial-debiasing",
  "name": "Adversarial Debiasing",
  "acronym": null,
  "description": "Adversarial debiasing reduces bias by training models using a competitive adversarial setup, similar to Generative Adversarial Networks (GANs). The technique involves two neural networks: a predictor that learns to make accurate predictions on the main task, and an adversary (bias detector) that attempts to predict protected attributes (such as race, gender, or age) from the predictor's internal representations. Through adversarial training, the predictor learns to produce representations that retain predictive power for the main task whilst being uninformative about protected characteristics, thereby reducing discriminatory bias.",
  "complexity_rating": 4,
  "computational_cost_rating": 3,
  "assurance_goals": [
    {
      "id": 2,
      "name": "Fairness"
    }
  ],
  "tags": [
    {
      "name": "applicable-models/neural-network",
      "id": 11
    },
    {
      "name": "assurance-goal-category/fairness",
      "id": 20
    },
    {
      "name": "assurance-goal-category/fairness/group",
      "id": 22
    },
    {
      "name": "data-requirements/sensitive-attributes",
      "id": 43
    },
    {
      "name": "data-type/any",
      "id": 46
    },
    {
      "name": "evidence-type/quantitative-metric",
      "id": 59
    },
    {
      "name": "expertise-needed/ml-engineering",
      "id": 73
    },
    {
      "name": "fairness-approach/group",
      "id": 83
    },
    {
      "name": "lifecycle-stage/model-development",
      "id": 92
    },
    {
      "name": "technique-type/algorithmic",
      "id": 107
    }
  ],
  "related_techniques": [
    "fair-adversarial-networks",
    "prejudice-remover-regulariser",
    "meta-fair-classifier",
    "exponentiated-gradient-reduction",
    "fair-transfer-learning",
    "adaptive-sensitive-reweighting",
    "multi-accuracy-boosting"
  ],
  "related_technique_slugs": [
    "fair-adversarial-networks",
    "prejudice-remover-regulariser",
    "meta-fair-classifier",
    "exponentiated-gradient-reduction",
    "fair-transfer-learning",
    "adaptive-sensitive-reweighting",
    "multi-accuracy-boosting"
  ],
  "resources": [
    {
      "title": "AI Fairness 360 (AIF360)",
      "url": "https://github.com/Trusted-AI/AIF360",
      "description": "Comprehensive toolkit for bias detection and mitigation including adversarial debiasing implementations",
      "authors": "",
      "publication_date": null,
      "source_type": "software_package",
      "resource_type": "documentation"
    },
    {
      "title": "On the Fairness ROAD: Robust Optimization for Adversarial Debiasing",
      "url": "https://www.semanticscholar.org/paper/0c887592d781538a1b5c2168eae541b563c0ba9a",
      "authors": [
        "Vincent Grari",
        "Thibault Laugel",
        "Tatsunori B. Hashimoto",
        "S. Lamprier",
        "Marcin Detyniecki"
      ],
      "publication_date": null,
      "source_type": "technical_paper",
      "resource_type": "documentation"
    },
    {
      "title": "aif360.sklearn.inprocessing.AdversarialDebiasing â€” aif360 0.6.1 ...",
      "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.inprocessing.AdversarialDebiasing.html",
      "authors": "",
      "publication_date": null,
      "source_type": "documentation",
      "resource_type": "documentation"
    },
    {
      "title": "Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing",
      "url": "http://arxiv.org/pdf/2103.06179v1",
      "authors": [
        "Christian Reimers",
        "Paul Bodesheim",
        "Jakob Runge",
        "Joachim Denzler"
      ],
      "publication_date": "2021-03-10",
      "source_type": "technical_paper",
      "resource_type": "documentation"
    }
  ],
  "example_use_cases": [
    {
      "description": "Training a resume screening model for a technology company that evaluates candidates based on skills and experience whilst preventing the internal representations from encoding gender or ethnicity information, ensuring hiring decisions cannot be influenced by protected characteristics even indirectly through correlated features.",
      "goal": "Fairness"
    },
    {
      "description": "Developing a credit scoring model for loan approvals that accurately predicts default risk whilst ensuring the model's internal features cannot be used to infer applicants' race or age, thereby preventing discriminatory lending practices whilst maintaining predictive accuracy.",
      "goal": "Fairness"
    },
    {
      "description": "Creating a medical diagnosis model that makes accurate predictions about patient conditions whilst ensuring that the learned representations cannot reveal sensitive demographic information like gender or ethnicity, protecting patient privacy whilst maintaining clinical effectiveness.",
      "goal": "Fairness"
    }
  ],
  "limitations": [
    {
      "description": "Significantly more complex to implement than standard models, requiring expertise in adversarial training techniques and careful architecture design for both predictor and adversary networks."
    },
    {
      "description": "Requires careful hyperparameter tuning to balance the competing objectives of task performance and bias mitigation, as overly strong adversarial training can harm predictive accuracy."
    },
    {
      "description": "Effectiveness heavily depends on the quality and design of the adversary network - a weak adversary may fail to detect subtle biases, whilst an overly strong adversary may eliminate useful information."
    },
    {
      "description": "Training can be unstable and may suffer from convergence issues common to adversarial training, requiring careful learning rate scheduling and regularisation techniques."
    },
    {
      "description": "Provides no formal guarantees about bias elimination and may not prevent all forms of discrimination, particularly when protected attributes can be inferred from other available features."
    }
  ]
}
