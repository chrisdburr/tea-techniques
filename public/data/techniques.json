[
  {
    "slug": "shapley-additive-explanations",
    "name": "SHapley Additive exPlanations",
    "description": "SHAP explains model predictions by quantifying how much each input feature contributes to the outcome. It assigns an importance score to every feature, indicating whether it pushes the prediction towards or away from the average. The method systematically evaluates how predictions change as features are included or excluded, drawing on game theory concepts to ensure a fair distribution of contributions.",
    "assurance_goals": [
      "Explainability",
      "Fairness",
      "Reliability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic",
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box"
    ],
    "example_use_cases": [
      {
        "description": "Analysing a customer churn prediction model to understand why a specific high-value customer was flagged as likely to leave, revealing that recent support ticket interactions and declining purchase frequency were the main drivers.",
        "goal": "Explainability"
      },
      {
        "description": "Auditing a loan approval model by comparing SHAP values for applicants from different demographic groups, ensuring that protected characteristics like race or gender do not have an undue influence on credit decisions.",
        "goal": "Fairness"
      },
      {
        "description": "Validating a medical diagnosis model by confirming that its predictions are based on relevant clinical features (e.g., blood pressure, cholesterol levels) rather than spurious correlations (e.g., patient ID or appointment time), thereby improving model reliability.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Assumes feature independence, which can produce misleading explanations when features are highly correlated, as the model may attribute importance to features that are merely proxies for others."
      },
      {
        "description": "Computationally expensive for models with many features or large datasets, as the number of required predictions grows exponentially with the number of features."
      },
      {
        "description": "The choice of background dataset for generating explanations can significantly influence the results, requiring careful selection to ensure a representative baseline."
      },
      {
        "description": "Global explanations derived from averaging local SHAP values may obscure important heterogeneous effects where features impact subgroups of the population differently."
      }
    ],
    "resources": [
      {
        "title": "shap/shap",
        "url": "https://github.com/shap/shap",
        "source_type": "software_package"
      },
      {
        "title": "Introduction to SHapley Additive exPlanations (SHAP) \u2014 XAI Tutorials",
        "url": "https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/shap.html",
        "source_type": "tutorial"
      },
      {
        "title": "An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models",
        "url": "http://arxiv.org/pdf/2204.11351v3",
        "source_type": "technical_paper",
        "authors": [
          "Han Yuan",
          "Mingxuan Liu",
          "Lican Kang",
          "Chenkui Miao",
          "Ying Wu"
        ],
        "publication_date": "2022-04-24"
      },
      {
        "title": "SHAP: Shapley Additive Explanations | Towards Data Science",
        "url": "https://towardsdatascience.com/shap-shapley-additive-explanations-5a2a271ed9c3/",
        "source_type": "tutorial"
      },
      {
        "title": "MAIF/shapash",
        "url": "https://github.com/MAIF/shapash",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "acronym": "SHAP",
    "related_techniques": [
      "integrated-gradients",
      "deeplift",
      "layer-wise-relevance-propagation",
      "local-interpretable-model-agnostic-explanations",
      "contrastive-explanation-method",
      "anchor"
    ]
  },
  {
    "slug": "permutation-importance",
    "name": "Permutation Importance",
    "description": "Permutation Importance quantifies a feature's contribution to a model's performance by randomly shuffling its values and measuring the resulting drop in predictive accuracy. If shuffling a feature significantly degrades the model's performance, that feature is considered important. This model-agnostic technique helps identify which inputs are genuinely driving predictions, rather than just being correlated with the outcome.",
    "assurance_goals": [
      "Explainability",
      "Reliability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic",
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box"
    ],
    "example_use_cases": [
      {
        "description": "Assessing which patient characteristics (e.g., age, blood pressure, cholesterol) are most critical for a medical diagnosis model by observing the performance drop when each characteristic's values are randomly shuffled, ensuring the model relies on clinically relevant factors.",
        "goal": "Explainability"
      },
      {
        "description": "Validating the robustness of a fraud detection model by permuting features like transaction amount or location, and confirming that the model's ability to detect fraud significantly decreases only for truly important features, thereby improving confidence in its reliability.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Can be misleading when features are highly correlated, as shuffling one feature might indirectly affect others, leading to an overestimation of its importance."
      },
      {
        "description": "Computationally expensive for large datasets or complex models, as it requires re-evaluating the model many times for each feature."
      },
      {
        "description": "Does not account for interactions between features; it measures the marginal importance of a feature, assuming other features remain unchanged."
      },
      {
        "description": "The choice of metric for evaluating performance drop (e.g., accuracy, F1-score) can influence the perceived importance of features."
      }
    ],
    "resources": [
      {
        "title": "Asymptotic Unbiasedness of the Permutation Importance Measure in Random Forest Models",
        "url": "http://arxiv.org/pdf/1912.03306v1",
        "source_type": "technical_paper",
        "authors": [
          "Burim Ramosaj",
          "Markus Pauly"
        ],
        "publication_date": "2019-12-05"
      },
      {
        "title": "eli5.permutation_importance \u2014 ELI5 0.15.0 documentation",
        "url": "https://eli5.readthedocs.io/en/latest/autodocs/permutation_importance.html",
        "source_type": "documentation"
      },
      {
        "title": "Permutation Importance \u2014 PermutationImportance 1.2.1.5 ...",
        "url": "https://permutationimportance.readthedocs.io/en/latest/permutation.html",
        "source_type": "documentation"
      },
      {
        "title": "parrt/random-forest-importances",
        "url": "https://github.com/parrt/random-forest-importances",
        "source_type": "software_package"
      },
      {
        "title": "Statistically Valid Variable Importance Assessment through Conditional Permutations",
        "url": "http://arxiv.org/pdf/2309.07593v2",
        "source_type": "technical_paper",
        "authors": [
          "Ahmad Chamma",
          "Denis A. Engemann",
          "Bertrand Thirion"
        ],
        "publication_date": "2023-09-14"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 3,
    "related_techniques": [
      "mean-decrease-impurity",
      "coefficient-magnitudes-in-linear-models",
      "sobol-indices"
    ]
  },
  {
    "slug": "mean-decrease-impurity",
    "name": "Mean Decrease Impurity",
    "description": "Mean Decrease Impurity (MDI) quantifies a feature's importance in tree-based models (e.g., Random Forests, Gradient Boosting Machines) by measuring the total reduction in impurity (e.g., Gini impurity, entropy) across all splits where the feature is used. Features that lead to larger, more consistent reductions in impurity are considered more important, indicating their effectiveness in creating homogeneous child nodes and improving predictive accuracy.",
    "assurance_goals": [
      "Explainability",
      "Reliability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/model-specific",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/efficiency",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic",
      "applicable-models/architecture/tree-based",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/white-box"
    ],
    "example_use_cases": [
      {
        "description": "Determining the most influential genetic markers in a decision tree model predicting disease susceptibility, by identifying which markers consistently lead to the purest splits between healthy and diseased patient groups.",
        "goal": "Explainability"
      },
      {
        "description": "Assessing the key factors driving customer purchasing decisions in an e-commerce random forest model, revealing which product attributes or customer demographics are most effective in segmenting buyers.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "MDI is inherently biased towards features with more unique values or those that allow for more splits, potentially overestimating their true importance."
      },
      {
        "description": "It is only applicable to tree-based models and cannot be directly used with other model architectures."
      },
      {
        "description": "The importance scores can be unstable, varying significantly with small changes in the training data or model parameters."
      },
      {
        "description": "MDI does not account for feature interactions, meaning it might not accurately reflect the importance of features that are only relevant when combined with others."
      }
    ],
    "resources": [
      {
        "title": "Trees, forests, and impurity-based variable importance",
        "url": "http://arxiv.org/pdf/2001.04295v3",
        "source_type": "technical_paper",
        "authors": [
          "Erwan Scornet"
        ],
        "publication_date": "2020-01-13"
      },
      {
        "title": "A Debiased MDI Feature Importance Measure for Random Forests",
        "url": "http://arxiv.org/pdf/1906.10845v2",
        "source_type": "technical_paper",
        "authors": [
          "Xiao Li",
          "Yu Wang",
          "Sumanta Basu",
          "Karl Kumbier",
          "Bin Yu"
        ],
        "publication_date": "2019-06-26"
      },
      {
        "title": "Variable Importance in Random Forests | Towards Data Science",
        "url": "https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0/",
        "source_type": "tutorial"
      },
      {
        "title": "Interpreting Deep Forest through Feature Contribution and MDI Feature Importance",
        "url": "http://arxiv.org/pdf/2305.00805v1",
        "source_type": "technical_paper",
        "authors": [
          "Yi-Xiao He",
          "Shen-Huan Lyu",
          "Yuan Jiang"
        ],
        "publication_date": "2023-05-01"
      },
      {
        "title": "optuna.importance.MeanDecreaseImpurityImportanceEvaluator ...",
        "url": "https://optuna.readthedocs.io/en/stable/reference/generated/optuna.importance.MeanDecreaseImpurityImportanceEvaluator.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      "permutation-importance",
      "coefficient-magnitudes-in-linear-models",
      "sobol-indices"
    ]
  },
  {
    "slug": "coefficient-magnitudes-in-linear-models",
    "name": "Coefficient Magnitudes (in Linear Models)",
    "description": "Coefficient Magnitudes assess feature influence in linear models by examining the absolute values of their coefficients. Features with larger absolute coefficients are considered to have a stronger impact on the prediction, while the sign of the coefficient indicates the direction of that influence (positive or negative). This technique provides a straightforward and transparent way to understand the direct linear relationship between each input feature and the model's output.",
    "assurance_goals": [
      "Explainability",
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/model-specific",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/property/efficiency",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/metric",
      "applicable-models/architecture/linear-models",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/white-box"
    ],
    "example_use_cases": [
      {
        "description": "Interpreting which features influence housing price predictions in linear regression, such as identifying that 'number of bedrooms' has a larger positive impact than 'distance to city centre' based on coefficient magnitudes.",
        "goal": "Explainability"
      },
      {
        "description": "Explaining the factors contributing to customer lifetime value (CLV) in a linear model, showing how 'average monthly spend' has a strong positive coefficient, making the model transparent for business stakeholders.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Only valid for linear relationships; it cannot capture complex non-linear patterns or interactions between features."
      },
      {
        "description": "Highly sensitive to feature scaling; features with larger numerical ranges can appear more important even if their true impact is smaller."
      },
      {
        "description": "Can be misleading in the presence of multicollinearity, where correlated features may split importance or have unstable coefficients."
      },
      {
        "description": "Does not imply causation; a strong correlation (large coefficient) does not necessarily mean a causal relationship."
      }
    ],
    "resources": [],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [
      "permutation-importance",
      "mean-decrease-impurity",
      "sobol-indices"
    ]
  },
  {
    "slug": "integrated-gradients",
    "name": "Integrated Gradients",
    "description": "Integrated Gradients is an attribution technique that explains a model's prediction by quantifying the contribution of each input feature. It works by accumulating gradients along a straight path from a user-defined baseline input (e.g., a black image or an all-zero vector) to the actual input. This path integral ensures that the attributions satisfy fundamental axioms like completeness (attributions sum up to the difference between the prediction and the baseline prediction) and sensitivity (non-zero attributions for features that change the prediction). The output is a set of importance scores, often visualised as heatmaps, indicating which parts of the input were most influential for the model's decision.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/gradient-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/explainability/property/consistency",
      "data-requirements/labelled-data",
      "data-requirements/reference-dataset",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/differentiable"
    ],
    "example_use_cases": [
      {
        "description": "Analysing a medical image classification model to understand which specific pixels or regions in an X-ray image contribute most to a diagnosis of pneumonia, ensuring the model focuses on relevant pathological features rather than artifacts.",
        "goal": "Explainability"
      },
      {
        "description": "Explaining the sentiment prediction of a natural language processing model by highlighting which words or phrases in a review most strongly influenced its classification as positive or negative, revealing the model's interpretative focus.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires a carefully chosen and meaningful baseline input; an inappropriate baseline can lead to misleading or uninformative attributions."
      },
      {
        "description": "The model must be differentiable, which limits its direct application to models with non-differentiable components or discrete inputs without workarounds."
      },
      {
        "description": "Computationally more expensive than simple gradient-based methods, as it requires multiple gradient calculations along the integration path."
      },
      {
        "description": "While satisfying completeness, the attributions can sometimes be visually noisy or difficult for humans to interpret intuitively, especially for complex inputs."
      }
    ],
    "resources": [
      {
        "title": "ankurtaly/Integrated-Gradients",
        "url": "https://github.com/ankurtaly/Integrated-Gradients",
        "source_type": "software_package"
      },
      {
        "title": "pytorch/captum",
        "url": "https://github.com/pytorch/captum",
        "source_type": "software_package"
      },
      {
        "title": "Maximum Entropy Baseline for Integrated Gradients",
        "url": "http://arxiv.org/pdf/2204.05948v1",
        "source_type": "technical_paper",
        "authors": [
          "Hanxiao Tan"
        ],
        "publication_date": "2022-04-12"
      },
      {
        "title": "Integrated Gradients from Scratch | Towards Data Science",
        "url": "https://towardsdatascience.com/integrated-gradients-from-scratch-b46311e4ab4/",
        "source_type": "tutorial"
      },
      {
        "title": "Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution",
        "url": "http://arxiv.org/pdf/2004.10484v2",
        "source_type": "technical_paper",
        "authors": [
          "Gary S. W. Goh",
          "Sebastian Lapuschkin",
          "Leander Weber",
          "Wojciech Samek",
          "Alexander Binder"
        ],
        "publication_date": "2020-04-22"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      "shapley-additive-explanations",
      "deeplift",
      "layer-wise-relevance-propagation",
      "local-interpretable-model-agnostic-explanations",
      "contrastive-explanation-method",
      "anchor"
    ]
  },
  {
    "slug": "deeplift",
    "name": "DeepLIFT",
    "description": "DeepLIFT (Deep Learning Important FeaTures) explains neural network predictions by decomposing the difference between the actual output and a reference output back to individual input features. It compares each neuron's activation to a reference activation (typically from a baseline input like all zeros or the dataset mean) and propagates these differences backwards through the network using chain rule modifications. Unlike gradient-based methods, DeepLIFT satisfies the sensitivity property (zero input gets zero attribution) and provides more stable attributions by using discrete differences rather than gradients.",
    "assurance_goals": [
      "Explainability",
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/gradient-based",
      "assurance-goal-category/explainability/representation-analysis/decomposition",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-model-internals",
      "data-requirements/reference-dataset",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/white-box",
      "applicable-models/requirements/differentiable"
    ],
    "example_use_cases": [
      {
        "description": "Identifying which genomic sequences contribute to a neural network's prediction of protein binding sites, helping biologists understand regulatory mechanisms by comparing to neutral DNA baselines.",
        "goal": "Explainability"
      },
      {
        "description": "Debugging a deep learning image classifier that misclassifies medical scans by attributing the decision to specific image regions, revealing if the model focuses on irrelevant artifacts rather than pathological features.",
        "goal": "Explainability"
      },
      {
        "description": "Providing transparent explanations for automated loan approval decisions by showing which financial features (relative to typical applicant profiles) most influenced the neural network's recommendation.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Requires careful selection of reference baseline, as different choices can lead to substantially different attribution scores."
      },
      {
        "description": "Implementation complexity varies significantly across different neural network architectures and layer types."
      },
      {
        "description": "May produce unintuitive results when the chosen reference is not representative of the decision boundary."
      },
      {
        "description": "Limited to feedforward networks and specific layer types, not suitable for all modern architectures like transformers."
      }
    ],
    "resources": [
      {
        "title": "Learning Important Features Through Propagating Activation Differences",
        "url": "http://arxiv.org/pdf/1704.02685v2",
        "source_type": "technical_paper",
        "authors": [
          "Avanti Shrikumar",
          "Peyton Greenside",
          "Anshul Kundaje"
        ],
        "publication_date": "2017-04-10"
      },
      {
        "title": "pytorch/captum",
        "url": "https://github.com/pytorch/captum",
        "source_type": "software_package"
      },
      {
        "title": "Tutorial A3: DeepLIFT/SHAP \u2014 tangermeme v0.1.0 documentation",
        "url": "https://tangermeme.readthedocs.io/en/latest/tutorials/Tutorial_A3_Deep_LIFT_SHAP.html",
        "source_type": "tutorial"
      },
      {
        "title": "DeepLIFT Documentation - Captum",
        "url": "https://captum.ai/api/deep_lift.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      "shapley-additive-explanations",
      "integrated-gradients",
      "layer-wise-relevance-propagation",
      "local-interpretable-model-agnostic-explanations",
      "contrastive-explanation-method",
      "anchor"
    ]
  },
  {
    "slug": "layer-wise-relevance-propagation",
    "name": "Layer-wise Relevance Propagation",
    "description": "Layer-wise Relevance Propagation (LRP) explains neural network predictions by working backwards through the network to show how much each input feature contributed to the final decision. It follows a simple conservation rule: the total contribution scores always add up to the original prediction. Starting from the output, LRP distributes 'relevance' backwards through each layer using different rules depending on the layer type. This creates a detailed breakdown showing which input features helped or hindered the prediction, making it easier to understand why the network made a particular decision.",
    "assurance_goals": [
      "Explainability",
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/model-specific",
      "assurance-goal-category/explainability/representation-analysis/decomposition",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-model-internals",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/requirements/white-box",
      "applicable-models/requirements/differentiable"
    ],
    "example_use_cases": [
      {
        "description": "Identifying which pixels in chest X-rays contribute to pneumonia detection, helping radiologists verify AI diagnoses by highlighting anatomical regions the model considers relevant.",
        "goal": "Explainability"
      },
      {
        "description": "Debugging a neural network's misclassification of handwritten digits by tracing relevance through layers to identify which input pixels caused the error and which network layers amplified it.",
        "goal": "Explainability"
      },
      {
        "description": "Providing transparent explanations for automated credit scoring decisions by showing which financial features received positive or negative relevance scores, enabling clear regulatory reporting.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Requires different propagation rules for each layer type, making implementation complex for new architectures."
      },
      {
        "description": "Can produce negative relevance scores which may be difficult to interpret intuitively."
      },
      {
        "description": "Rule selection (LRP-\u03b5, LRP-\u03b3, etc.) significantly affects results and requires domain expertise."
      },
      {
        "description": "Limited to feedforward networks and may not work well with modern architectures like transformers without substantial modifications."
      }
    ],
    "resources": [
      {
        "title": "rachtibat/LRP-eXplains-Transformers",
        "url": "https://github.com/rachtibat/LRP-eXplains-Transformers",
        "source_type": "software_package"
      },
      {
        "title": "sebastian-lapuschkin/lrp_toolbox",
        "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
        "source_type": "software_package"
      },
      {
        "title": "Getting started \u2014 zennit documentation",
        "url": "https://zennit.readthedocs.io/en/latest/getting-started.html",
        "source_type": "documentation"
      },
      {
        "title": "Layer-wise Relevance Propagation eXplains Transformers (LXT) documentation",
        "url": "https://lxt.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation",
        "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140",
        "source_type": "technical_paper",
        "authors": [
          "Sebastian Bach",
          "Alexander Binder",
          "Gr\u00e9goire Montavon",
          "Frederick Klauschen",
          "Klaus-Robert M\u00fcller",
          "Wojciech Samek"
        ],
        "publication_date": "2015-07-10"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "acronym": "LRP",
    "related_techniques": [
      "shapley-additive-explanations",
      "integrated-gradients",
      "deeplift",
      "local-interpretable-model-agnostic-explanations",
      "contrastive-explanation-method",
      "anchor"
    ]
  },
  {
    "slug": "contextual-decomposition",
    "name": "Contextual Decomposition",
    "description": "Contextual Decomposition explains LSTM and RNN predictions by decomposing the final hidden state into contributions from individual inputs and their interactions. Unlike simpler attribution methods, it separates the direct contribution of specific words or phrases from the contextual effects of surrounding words. This is particularly useful for understanding how sequential models process language, as it can identify whether a word's influence comes from its individual meaning or from its interaction with nearby words in the sequence.",
    "assurance_goals": [
      "Explainability",
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/representation-analysis/decomposition",
      "assurance-goal-category/explainability/attribution-methods/model-specific",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic",
      "applicable-models/architecture/neural-networks/recurrent",
      "applicable-models/requirements/white-box",
      "applicable-models/requirements/architecture-specific"
    ],
    "example_use_cases": [
      {
        "description": "Analysing why an LSTM-based spam filter flagged an email by decomposing contributions from individual words ('free', 'urgent') versus their contextual interactions ('free trial' together).",
        "goal": "Explainability"
      },
      {
        "description": "Understanding how a medical text classifier diagnoses conditions from clinical notes by separating direct symptom mentions from contextual medical reasoning patterns.",
        "goal": "Explainability"
      },
      {
        "description": "Providing transparent explanations for automated content moderation decisions by showing which words and phrase interactions contributed to toxicity detection.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Primarily designed for LSTM and simple RNN architectures, not suitable for modern transformers or attention-based models."
      },
      {
        "description": "Not widely implemented in standard machine learning libraries, often requiring custom implementation."
      },
      {
        "description": "Computational overhead increases significantly with sequence length and model depth."
      },
      {
        "description": "May not scale well to very complex models or capture all types of feature interactions in deep networks."
      }
    ],
    "resources": [
      {
        "title": "Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs",
        "url": "http://arxiv.org/pdf/1801.05453v2",
        "source_type": "technical_paper",
        "authors": [
          "W. James Murdoch",
          "Peter J. Liu",
          "Bin Yu"
        ],
        "publication_date": "2018-01-16"
      },
      {
        "title": "FredericGodin/ContextualDecomposition-NLP",
        "url": "https://github.com/FredericGodin/ContextualDecomposition-NLP",
        "source_type": "software_package"
      },
      {
        "title": "Interpreting patient-Specific risk prediction using contextual decomposition of BiLSTMs: Application to children with asthma",
        "url": "https://core.ac.uk/download/294758884.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Alsaad R.",
          "Boughorbel S.",
          "Janahi I.",
          "Malluhi Q."
        ],
        "publication_date": "2019-01-01"
      },
      {
        "title": "Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models",
        "url": "http://arxiv.org/pdf/1911.06194v2",
        "source_type": "technical_paper",
        "authors": [
          "Xisen Jin",
          "Zhongyu Wei",
          "Junyi Du",
          "Xiangyang Xue",
          "Xiang Ren"
        ],
        "publication_date": "2019-11-08"
      },
      {
        "title": "Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition",
        "url": "http://arxiv.org/pdf/2407.00886v3",
        "source_type": "technical_paper",
        "authors": [
          "Aliyah R. Hsu",
          "Georgia Zhou",
          "Yeshwanth Cherapanamjeri",
          "Yaxuan Huang",
          "Anobel Y. Odisho",
          "Peter R. Carroll",
          "Bin Yu"
        ],
        "publication_date": "2024-07-01"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [
      "taylor-decomposition",
      "influence-functions"
    ]
  },
  {
    "slug": "taylor-decomposition",
    "name": "Taylor Decomposition",
    "description": "Taylor Decomposition is a mathematical technique that explains neural network predictions by computing first-order and higher-order derivatives of the network's output with respect to input features. It decomposes the prediction into relevance scores that indicate how much each input feature and feature interaction contributes to the final decision. The method uses Layer-wise Relevance Propagation (LRP) principles to trace prediction contributions backwards through the network layers, providing precise mathematical attributions for each input element.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/representation-analysis/decomposition",
      "assurance-goal-category/explainability/attribution-methods/gradient-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/explainability/property/fidelity",
      "data-requirements/access-to-model-internals",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/differentiable",
      "applicable-models/requirements/white-box"
    ],
    "example_use_cases": [
      {
        "description": "Analyzing which pixels in an image contribute most to a convolutional neural network's classification decision, showing both positive and negative relevance scores for different regions of the input image.",
        "goal": "Explainability"
      },
      {
        "description": "Understanding how different word embeddings in a sentiment analysis model contribute to the final sentiment score, revealing which terms drive positive vs negative predictions.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Mathematically complex requiring deep understanding of calculus and neural network architectures."
      },
      {
        "description": "Computationally intensive as it requires computing gradients and higher-order derivatives through the entire network."
      },
      {
        "description": "Approximations used in practice may introduce errors that affect attribution accuracy."
      },
      {
        "description": "Limited tooling availability compared to other explainability methods, with most implementations being research-focused rather than production-ready."
      }
    ],
    "resources": [
      {
        "title": "Explaining NonLinear Classification Decisions with Deep Taylor Decomposition",
        "url": "http://arxiv.org/pdf/1512.02479v1",
        "source_type": "technical_paper",
        "authors": [
          "Gr\u00e9goire Montavon",
          "Sebastian Bach",
          "Alexander Binder",
          "Wojciech Samek",
          "Klaus-Robert M\u00fcller"
        ],
        "publication_date": "2015-12-08"
      },
      {
        "title": "A Rigorous Study Of The Deep Taylor Decomposition",
        "url": "http://arxiv.org/pdf/2211.08425v1",
        "source_type": "technical_paper",
        "authors": [
          "Leon Sixt",
          "Tim Landgraf"
        ],
        "publication_date": "2022-11-14"
      },
      {
        "title": "sebastian-lapuschkin/lrp_toolbox",
        "url": "https://github.com/sebastian-lapuschkin/lrp_toolbox",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      "contextual-decomposition",
      "influence-functions"
    ]
  },
  {
    "slug": "sobol-indices",
    "name": "Sobol Indices",
    "description": "Sobol Indices quantify how much each input feature contributes to the total variance in a model's predictions through global sensitivity analysis. The technique calculates first-order indices (individual feature contributions) and total-order indices (including all interaction effects involving that feature). By systematically sampling the input space and decomposing output variance, Sobol Indices reveal which features drive model uncertainty and which interactions between features are most important for predictions.",
    "assurance_goals": [
      "Explainability",
      "Fairness"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/causal-analysis/interaction-effects",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/causal-pathways",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/fairness",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic",
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box"
    ],
    "example_use_cases": [
      {
        "description": "Analysing a climate prediction model to determine which atmospheric parameters (temperature, humidity, pressure) contribute most to rainfall forecast uncertainty, helping meteorologists understand which measurements need the highest precision.",
        "goal": "Explainability"
      },
      {
        "description": "Evaluating a financial risk model to identify which economic indicators (interest rates, inflation, GDP growth) drive the most variability in portfolio value predictions, enabling better risk management strategies.",
        "goal": "Explainability"
      },
      {
        "description": "Analysing a credit scoring model to quantify how much prediction variance stems from zip code (a potential proxy for race), helping identify features that may cause disparate impact across demographic groups.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive, requiring thousands of model evaluations to achieve stable variance estimates, making it impractical for very slow models."
      },
      {
        "description": "Assumes input features are independently distributed, which can lead to misleading results when features are correlated in real data."
      },
      {
        "description": "Curse of dimensionality makes the technique increasingly difficult and expensive to apply as the number of input features grows beyond 10-20."
      },
      {
        "description": "Requires defining appropriate probability distributions for input features, which may not accurately reflect real-world feature distributions."
      }
    ],
    "resources": [
      {
        "title": "Sobol Tensor Trains for Global Sensitivity Analysis",
        "url": "http://arxiv.org/pdf/1712.00233v1",
        "source_type": "technical_paper",
        "authors": [
          "Rafael Ballester-Ripoll",
          "Enrique G. Paredes",
          "Renato Pajarola"
        ],
        "publication_date": "2017-12-01"
      },
      {
        "title": "Sobol indices \u2014 UQpy v4.2.0 documentation",
        "url": "https://uqpyproject.readthedocs.io/en/latest/sensitivity/sobol.html",
        "source_type": "documentation"
      },
      {
        "title": "Sobol Indices to Measure Feature Importance | Towards Data Science",
        "url": "https://towardsdatascience.com/sobol-indices-to-measure-feature-importance-54cedc3281bc/",
        "source_type": "tutorial"
      },
      {
        "title": "Basics \u2014 SALib's documentation",
        "url": "https://salib.readthedocs.io/en/latest/user_guide/basics.html",
        "source_type": "documentation"
      },
      {
        "title": "UQpy (Uncertainty Quantification with python)",
        "url": "https://github.com/SURGroup/UQpy",
        "source_type": "software_package"
      },
      {
        "title": "SALib/SALib",
        "url": "https://github.com/SALib/SALib",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 5,
    "related_techniques": [
      "permutation-importance",
      "mean-decrease-impurity",
      "coefficient-magnitudes-in-linear-models"
    ]
  },
  {
    "slug": "local-interpretable-model-agnostic-explanations",
    "name": "Local Interpretable Model-Agnostic Explanations",
    "description": "LIME (Local Interpretable Model-agnostic Explanations) explains individual predictions by approximating the complex model's behaviour in a small neighbourhood around a specific instance. It works by creating perturbed versions of the input (e.g., removing words from text, changing pixel values in images, or varying feature values), obtaining the model's predictions for these variations, and training a simple interpretable model (typically linear regression) weighted by proximity to the original instance. The coefficients of this local surrogate model reveal which features most influenced the specific prediction.",
    "assurance_goals": [
      "Explainability",
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/surrogate-models/local-surrogates",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic",
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box"
    ],
    "example_use_cases": [
      {
        "description": "Explaining why a specific patient received a high-risk diagnosis by showing which symptoms (fever, blood pressure, age) contributed most to the prediction, helping doctors validate the AI's reasoning.",
        "goal": "Explainability"
      },
      {
        "description": "Debugging a text classifier's misclassification of a movie review by highlighting which words (e.g., sarcastic phrases) confused the model, enabling targeted model improvements.",
        "goal": "Explainability"
      },
      {
        "description": "Providing transparent explanations to customers about automated decisions in insurance claims, showing which claim features influenced approval or denial to meet regulatory requirements.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Explanations can be unstable due to random sampling, producing different results across multiple runs."
      },
      {
        "description": "The linear surrogate may poorly approximate highly non-linear model behaviour in the local region."
      },
      {
        "description": "Defining the neighbourhood size and perturbation strategy requires careful tuning for each data type."
      },
      {
        "description": "Can be computationally expensive for explaining many instances due to repeated model queries."
      }
    ],
    "resources": [
      {
        "title": "marcotcr/lime",
        "url": "https://github.com/marcotcr/lime",
        "source_type": "software_package"
      },
      {
        "title": "thomasp85/lime (R package)",
        "url": "https://github.com/thomasp85/lime",
        "source_type": "software_package"
      },
      {
        "title": "Local Interpretable Model-Agnostic Explanations (lime) \u2014 lime 0.1 ...",
        "url": "https://lime-ml.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "'Why Should I Trust You?' Explaining the Predictions of Any Classifier",
        "url": "https://arxiv.org/abs/1602.04938",
        "source_type": "technical_paper",
        "authors": [
          "Marco Tulio Ribeiro",
          "Sameer Singh",
          "Carlos Guestrin"
        ],
        "publication_date": "2016-02-16"
      },
      {
        "title": "How to convince your boss to trust your ML/DL models - Towards Data Science",
        "url": "https://towardsdatascience.com/how-to-convince-your-boss-to-trust-your-ml-dl-models-671f707246a8",
        "source_type": "tutorial"
      },
      {
        "title": "Enhanced LIME \u2014 ADS 2.6.5 documentation",
        "url": "https://accelerated-data-science.readthedocs.io/en/v2.6.5/user_guide/model_explainability/lime.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 3,
    "acronym": "LIME",
    "related_techniques": [
      "shapley-additive-explanations",
      "integrated-gradients",
      "deeplift",
      "layer-wise-relevance-propagation",
      "contrastive-explanation-method",
      "anchor"
    ]
  },
  {
    "slug": "ridge-regression-surrogates",
    "name": "Ridge Regression Surrogates",
    "description": "This technique approximates a complex model by training a ridge regression (a linear model with L2 regularization) on the original model's predictions. The ridge regression serves as a global surrogate that balances fidelity and interpretability, capturing the main linear relationships that the complex model learned while ignoring noise due to regularization.",
    "assurance_goals": [
      "Explainability",
      "Transparency"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/surrogate-models/global-surrogates",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic",
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box"
    ],
    "example_use_cases": [
      {
        "description": "Approximating a complex ensemble model used for credit scoring with a ridge regression surrogate to identify the most influential features (income, credit history, debt-to-income ratio) and their linear relationships for regulatory compliance reporting.",
        "goal": "Explainability"
      },
      {
        "description": "Creating a ridge regression surrogate of a neural network used for medical diagnosis to understand which patient symptoms and biomarkers have the strongest linear predictive relationships with disease outcomes.",
        "goal": "Explainability"
      },
      {
        "description": "Creating an interpretable approximation of a complex insurance pricing model for regulatory compliance, enabling stakeholders to understand and validate the decision-making process through transparent linear relationships.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Linear approximation may miss important non-linear relationships and interactions captured by the original complex model."
      },
      {
        "description": "Requires a representative dataset to train the surrogate model, which may not be available or may be expensive to generate."
      },
      {
        "description": "Ridge regularisation may oversimplify the model by shrinking coefficients, potentially hiding important but less dominant features."
      },
      {
        "description": "Surrogate fidelity depends on how well linear relationships approximate the original model's behaviour across the entire input space."
      }
    ],
    "resources": [
      {
        "title": "scikit-learn Ridge Regression Documentation",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html",
        "source_type": "documentation"
      },
      {
        "title": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable",
        "url": "https://christophm.github.io/interpretable-ml-book/global.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "rulefit",
      "model-distillation"
    ]
  },
  {
    "slug": "partial-dependence-plots",
    "name": "Partial Dependence Plots",
    "description": "Partial Dependence Plots show how changing one or two features affects a model's predictions on average. The technique works by varying the selected feature(s) across their full range whilst keeping all other features fixed at their original values, then averaging the predictions. This creates a clear visualisation of whether increasing or decreasing a feature tends to increase or decrease predictions, and reveals patterns like linear trends, plateaus, or threshold effects that help explain model behaviour.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/visualization-methods/feature-relationships",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing how house prices change with property size in a real estate prediction model, revealing whether the relationship is linear or if there are diminishing returns for very large properties.",
        "goal": "Explainability"
      },
      {
        "description": "Examining how customer age affects predicted loan default probability in a credit scoring model, showing whether risk increases steadily with age or has specific age ranges with higher risk.",
        "goal": "Explainability"
      },
      {
        "description": "Visualising how temperature affects crop yield predictions in agricultural models, identifying optimal temperature ranges and potential threshold effects.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Assumes features are independent when averaging, which can be misleading when features are highly correlated."
      },
      {
        "description": "Shows only average effects across all instances, potentially hiding important variations in how different subgroups respond to feature changes."
      },
      {
        "description": "Cannot reveal instance-specific effects or interactions between the plotted feature and other features."
      },
      {
        "description": "May be computationally expensive for large datasets since it requires making predictions across the full range of feature values."
      }
    ],
    "resources": [
      {
        "title": "DanielKerrigan/PDPilot",
        "url": "https://github.com/DanielKerrigan/PDPilot",
        "source_type": "software_package"
      },
      {
        "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation",
        "url": "http://arxiv.org/pdf/1309.6392v2",
        "source_type": "technical_paper",
        "authors": [
          "Alex Goldstein",
          "Adam Kapelner",
          "Justin Bleich",
          "Emil Pitkin"
        ],
        "publication_date": "2013-09-25"
      },
      {
        "title": "SauceCat/PDPbox",
        "url": "https://github.com/SauceCat/PDPbox",
        "source_type": "software_package"
      },
      {
        "title": "iPDP: On Partial Dependence Plots in Dynamic Modeling Scenarios",
        "url": "http://arxiv.org/pdf/2306.07775v1",
        "source_type": "technical_paper",
        "authors": [
          "Maximilian Muschalik",
          "Fabian Fumagalli",
          "Rohit Jagtani",
          "Barbara Hammer",
          "Eyke H\u00fcllermeier"
        ],
        "publication_date": "2023-06-13"
      },
      {
        "title": "How to Interpret Models: PDP and ICE | Towards Data Science",
        "url": "https://towardsdatascience.com/how-to-interpret-models-pdp-and-ice-eabed0062e2c/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "acronym": "PDP",
    "related_techniques": [
      "individual-conditional-expectation-plots"
    ]
  },
  {
    "slug": "individual-conditional-expectation-plots",
    "name": "Individual Conditional Expectation Plots",
    "description": "ICE plots display the predicted output for individual instances as a function of a feature, with all other features held fixed for each instance. Each line on an ICE plot represents one instance's prediction trajectory as the feature of interest changes, revealing whether different instances are affected differently by that feature.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/visualization-methods/feature-relationships",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/low",
      "explanatory-scope/global",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/visualization"
    ],
    "example_use_cases": [
      {
        "description": "Examining how house price predictions vary with property age for individual properties, revealing that whilst most houses follow a declining price trend with age, historic properties (built before 1900) show different patterns due to heritage value.",
        "goal": "Explainability"
      },
      {
        "description": "Analysing how individual patients' diabetes risk predictions change with BMI, showing that whilst most patients follow the expected increasing risk pattern, some patients with specific genetic markers show different response curves.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Plots can become cluttered and difficult to interpret when displaying many instances simultaneously."
      },
      {
        "description": "Does not provide automatic summarisation of overall effects, requiring manual visual inspection to identify patterns."
      },
      {
        "description": "Still assumes all other features remain fixed at their observed values, which may not reflect realistic scenarios."
      },
      {
        "description": "Cannot reveal interactions between the plotted feature and other features for individual instances."
      }
    ],
    "resources": [
      {
        "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation",
        "url": "http://arxiv.org/pdf/1309.6392v2",
        "source_type": "technical_paper",
        "authors": [
          "Alex Goldstein",
          "Adam Kapelner",
          "Justin Bleich",
          "Emil Pitkin"
        ],
        "publication_date": "2013-09-25"
      },
      {
        "title": "Bringing a Ruler Into the Black Box: Uncovering Feature Impact from Individual Conditional Expectation Plots",
        "url": "http://arxiv.org/pdf/2109.02724v1",
        "source_type": "technical_paper",
        "authors": [
          "Andrew Yeh",
          "Anhthy Ngo"
        ],
        "publication_date": "2021-09-06"
      },
      {
        "title": "Explainable AI(XAI) - A guide to 7 packages in Python to explain ...",
        "url": "https://towardsdatascience.com/explainable-ai-xai-a-guide-to-7-packages-in-python-to-explain-your-models-932967f0634b/",
        "source_type": "tutorial"
      },
      {
        "title": "Communicating Uncertainty in Machine Learning Explanations: A Visualization Analytics Approach for Predictive Process Monitoring",
        "url": "https://www.semanticscholar.org/paper/3d0090df2b73369b502559eb49fd6d1ae432b952",
        "source_type": "technical_paper",
        "authors": [
          "Nijat Mehdiyev",
          "Maxim Majlatow",
          "Peter Fettke"
        ]
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "acronym": "ICE",
    "related_techniques": [
      "partial-dependence-plots"
    ]
  },
  {
    "slug": "saliency-maps",
    "name": "Saliency Maps",
    "description": "Saliency maps are visual explanations for image classification models that highlight which pixels in an image most strongly influence the model's prediction. Computed by calculating gradients of the model's output with respect to input pixels, saliency maps produce heatmaps where brighter regions indicate pixels that, when changed, would most significantly affect the prediction. This technique helps users understand which parts of an image the model is 'looking at' when making decisions.",
    "assurance_goals": [
      "Explainability",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/differentiable",
      "applicable-models/requirements/gradient-access",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/gradient-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/efficiency",
      "assurance-goal-category/explainability/visualization-methods/activation-maps",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-type/image",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing X-ray images in a pneumonia detection model to verify that the algorithm focuses on lung regions showing inflammatory patterns rather than irrelevant areas like medical equipment or patient positioning markers.",
        "goal": "Explainability"
      },
      {
        "description": "Examining skin lesion classification models to ensure the algorithm identifies diagnostic features (irregular borders, colour variation) rather than artifacts like rulers, hair, or skin markings that shouldn't influence medical decisions.",
        "goal": "Explainability"
      },
      {
        "description": "Auditing a dermatology AI system to verify it focuses on medical symptoms rather than skin colour when diagnosing conditions, ensuring equitable treatment across racial groups by revealing inappropriate attention to demographic features.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Saliency maps are often noisy and can change dramatically with small input perturbations, making them unstable."
      },
      {
        "description": "Highlighted regions may not correspond to semantically meaningful or human-understandable features."
      },
      {
        "description": "Only indicates local gradient information, not causal importance or actual decision-making logic."
      },
      {
        "description": "May highlight irrelevant pixels that happen to have high gradients due to model artifacts rather than meaningful patterns."
      }
    ],
    "resources": [
      {
        "title": "utkuozbulak/pytorch-cnn-visualizations",
        "url": "https://github.com/utkuozbulak/pytorch-cnn-visualizations",
        "source_type": "software_package"
      },
      {
        "title": "Concepts of Saliency and Explainability in AI",
        "url": "https://xaitk-saliency.readthedocs.io/en/latest/xaitk_explanation.html",
        "source_type": "documentation"
      },
      {
        "title": "Occlusion Saliency Example",
        "url": "https://xaitk-saliency.readthedocs.io/en/latest/examples/OcclusionSaliency.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "gradient-weighted-class-activation-mapping",
      "occlusion-sensitivity"
    ]
  },
  {
    "slug": "gradient-weighted-class-activation-mapping",
    "name": "Gradient-weighted Class Activation Mapping",
    "description": "Grad-CAM creates visual heatmaps showing which regions of an image a convolutional neural network focuses on when making a specific classification. Unlike pixel-level techniques, Grad-CAM produces coarser region-based explanations by using gradients from the predicted class to weight the CNN's final feature maps, then projecting these weighted activations back to create an overlay on the original image. This provides intuitive visual explanations of where the model is 'looking' for evidence of different classes.",
    "assurance_goals": [
      "Explainability",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/convolutional",
      "applicable-models/requirements/architecture-specific",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/gradient-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/efficiency",
      "assurance-goal-category/explainability/property/sparsity",
      "assurance-goal-category/explainability/visualization-methods/activation-maps",
      "assurance-goal-category/fairness",
      "data-requirements/access-to-model-internals",
      "data-type/image",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Validating that a melanoma detection model focuses on the actual skin lesion rather than surrounding healthy skin, medical equipment, or artifacts when making cancer/benign classifications.",
        "goal": "Explainability"
      },
      {
        "description": "Debugging an autonomous vehicle's traffic sign recognition system by visualising whether the model correctly focuses on the sign itself rather than background objects, shadows, or irrelevant visual elements.",
        "goal": "Explainability"
      },
      {
        "description": "Auditing a medical imaging system for racial bias by examining whether diagnostic predictions inappropriately focus on skin tone regions rather than actual pathological features, ensuring equitable healthcare AI deployment.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires access to the CNN's internal feature maps and gradients, limiting use to white-box scenarios."
      },
      {
        "description": "Resolution is constrained by the final convolutional layer's feature map size, producing coarser localisation than pixel-level methods."
      },
      {
        "description": "Only applicable to CNN architectures with clearly defined convolutional layers, not suitable for other neural network types."
      },
      {
        "description": "May highlight regions that correlate with the class but aren't causally important for the model's decision-making process."
      }
    ],
    "resources": [
      {
        "title": "jacobgil/pytorch-grad-cam",
        "url": "https://github.com/jacobgil/pytorch-grad-cam",
        "source_type": "software_package"
      },
      {
        "title": "Grad-CAM: Visualize class activation maps with Keras, TensorFlow ...",
        "url": "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/",
        "source_type": "tutorial"
      },
      {
        "title": "kazuto1011/grad-cam-pytorch",
        "url": "https://github.com/kazuto1011/grad-cam-pytorch",
        "source_type": "software_package"
      },
      {
        "title": "A Tutorial on Explainable Image Classification for Dementia Stages Using Convolutional Neural Network and Gradient-weighted Class Activation Mapping",
        "url": "https://www.semanticscholar.org/paper/8b1139cb06cfe5ba69e2fd05e1450b43df031a02",
        "source_type": "documentation",
        "authors": [
          "Kevin Kam Fung Yuen"
        ]
      },
      {
        "title": "A Guide to Grad-CAM in Deep Learning - Analytics Vidhya",
        "url": "https://www.analyticsvidhya.com/blog/2023/12/grad-cam-in-deep-learning/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "acronym": "Grad-CAM",
    "related_techniques": [
      "saliency-maps",
      "occlusion-sensitivity"
    ]
  },
  {
    "slug": "occlusion-sensitivity",
    "name": "Occlusion Sensitivity",
    "description": "Occlusion sensitivity tests which parts of the input are important by occluding (masking or removing) them and seeing how the model's prediction changes. For example, portions of an image can be covered up in a sliding window fashion; if the model's confidence drops significantly when a certain region is occluded, that region was important for the prediction.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/sparsity",
      "data-requirements/no-special-requirements",
      "data-type/image",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Testing which regions of a chest X-ray are critical for pneumonia detection by systematically covering different areas with grey patches and measuring how much the model's confidence drops for each occluded region.",
        "goal": "Explainability"
      },
      {
        "description": "Evaluating whether a facial recognition system relies on specific facial features by masking eyes, nose, mouth, or other regions to identify which areas cause the biggest drop in recognition accuracy.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive as it requires running inference multiple times for each region tested, scaling poorly with input size."
      },
      {
        "description": "Choice of occlusion size and shape can significantly bias results - too small may miss important features, too large may occlude multiple relevant regions simultaneously."
      },
      {
        "description": "Cannot capture interactions between multiple regions that jointly contribute to the prediction but are individually less important."
      },
      {
        "description": "Results may be misleading if the model adapts to occlusion patterns or if occluded regions are filled with unrealistic pixel values."
      }
    ],
    "resources": [
      {
        "title": "kazuto1011/grad-cam-pytorch",
        "url": "https://github.com/kazuto1011/grad-cam-pytorch",
        "source_type": "software_package"
      },
      {
        "title": "Occlusion Sensitivity Analysis with Augmentation Subspace Perturbation in Deep Feature Space",
        "url": "http://arxiv.org/pdf/2311.15022v1",
        "source_type": "technical_paper",
        "authors": [
          "Pedro Valois",
          "Koichiro Niinuma",
          "Kazuhiro Fukui"
        ],
        "publication_date": "2023-11-25"
      },
      {
        "title": "Occlusion Sensitivity \u2014 tf-explain documentation",
        "url": "https://tf-explain.readthedocs.io/en/latest/methods.html#occlusion-sensitivity",
        "source_type": "documentation"
      },
      {
        "title": "Adaptive occlusion sensitivity analysis for visually explaining video recognition networks",
        "url": "http://arxiv.org/pdf/2207.12859v2",
        "source_type": "technical_paper",
        "authors": [
          "Tomoki Uchiyama",
          "Naoya Sogi",
          "Satoshi Iizuka",
          "Koichiro Niinuma",
          "Kazuhiro Fukui"
        ],
        "publication_date": "2022-07-26"
      },
      {
        "title": "sicara/tf-explain",
        "url": "https://github.com/sicara/tf-explain",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 4,
    "related_techniques": [
      "saliency-maps",
      "gradient-weighted-class-activation-mapping"
    ]
  },
  {
    "slug": "classical-attention-analysis-in-neural-networks",
    "name": "Classical Attention Analysis in Neural Networks",
    "description": "Classical attention mechanisms in RNNs and CNNs create alignment matrices and temporal attention patterns that show how models focus on different input elements over time or space. This technique analyses these traditional attention patterns, particularly in encoder-decoder architectures and sequence-to-sequence models, where attention weights reveal which source elements influence each output step. Unlike transformer self-attention analysis, this focuses on understanding alignment patterns, temporal dependencies, and encoder-decoder attention dynamics in classical neural architectures.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/recurrent",
      "applicable-models/requirements/architecture-specific",
      "applicable-models/requirements/model-internals",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/representation-analysis/decomposition",
      "assurance-goal-category/explainability/visualization-methods/attention-patterns",
      "data-requirements/access-to-model-internals",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing encoder-decoder attention in a neural machine translation model to verify the alignment between source and target words, ensuring the model learns proper translation correspondences rather than positional biases.",
        "goal": "Explainability"
      },
      {
        "description": "Examining temporal attention patterns in an RNN-based image captioning model to understand how attention moves across different image regions as it generates each word of the caption description.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Attention weights are not always strongly correlated with feature importance for the final prediction."
      },
      {
        "description": "High attention does not necessarily imply causal influence - models can attend to irrelevant but correlated features."
      },
      {
        "description": "Only applicable to neural network architectures that explicitly use attention mechanisms."
      },
      {
        "description": "Interpretation can be misleading without understanding the specific attention mechanism implementation and training dynamics."
      }
    ],
    "resources": [
      {
        "title": "An Attentive Survey of Attention Models",
        "url": "https://www.semanticscholar.org/paper/a8427ce5aee6d62800c725588e89940ed4910e0d",
        "source_type": "documentation",
        "authors": [
          "S. Chaudhari",
          "Gungor Polatkan",
          "R. Ramanath",
          "Varun Mithal"
        ]
      },
      {
        "title": "Attention, please! A survey of neural attention models in deep learning",
        "url": "https://www.semanticscholar.org/paper/44930df2a3186edb58c4d6f6e5ed828c5d6a0089",
        "source_type": "documentation",
        "authors": [
          "Alana de Santana Correia",
          "E. Colombini"
        ]
      },
      {
        "title": "ecco - Explain, Analyze, and Visualize NLP Language Models",
        "url": "https://github.com/jalammar/ecco",
        "source_type": "software_package"
      },
      {
        "title": "Enhancing Sentiment Analysis of Twitter Data Using Recurrent Neural Networks with Attention Mechanism",
        "url": "https://www.semanticscholar.org/paper/c59e0158280a567114ae8ca64a932eefd127e0aa",
        "source_type": "technical_paper",
        "authors": [
          "S. Nithya",
          "X. A. Presskila",
          "B. Sakthivel",
          "R. Krishnan",
          "K. Narayanan",
          "S. Sundararajan"
        ]
      },
      {
        "title": "Can Neural Networks Develop Attention? Google Thinks they Can ...",
        "url": "https://www.kdnuggets.com/2019/11/neural-networks-develop-attention-google.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      "attention-visualisation-in-transformers"
    ]
  },
  {
    "slug": "factor-analysis",
    "name": "Factor Analysis",
    "description": "Factor analysis is a statistical technique that identifies latent variables (hidden factors) underlying observed correlations in data. It works by analysing how variables relate to each other, finding a smaller number of unobserved factors that explain patterns among multiple observed variables. Unlike PCA which maximises total variance, factor analysis focuses on shared variance (communalities - the variance variables have in common) whilst separating out unique variance and measurement error. After extracting factors, rotation methods like varimax (which creates uncorrelated factors) or oblimin (allowing correlated factors) help make factors more interpretable by aligning them with distinct groups of variables.",
    "assurance_goals": [
      "Explainability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/unsupervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "evidence-type/structured-output",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing customer satisfaction surveys to identify key drivers (e.g., 'service quality', 'product value', 'convenience') from dozens of individual questions, helping businesses focus improvement efforts.",
        "goal": "Explainability"
      },
      {
        "description": "Reducing dimensionality of financial indicators to identify underlying economic factors (e.g., 'growth', 'inflation', 'credit risk') for more interpretable risk models.",
        "goal": "Explainability"
      },
      {
        "description": "Creating transparent feature groups for regulatory reporting by showing how multiple correlated features can be summarised into interpretable factors with clear business meaning.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Assumes linear relationships between variables and multivariate normality of data."
      },
      {
        "description": "Results can be abstract and require domain expertise to interpret meaningfully."
      },
      {
        "description": "Sensitive to the choice of number of factors and rotation method, which can significantly affect interpretability."
      },
      {
        "description": "Requires sufficiently large sample sizes relative to the number of variables for stable results."
      }
    ],
    "resources": [
      {
        "title": "Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey",
        "url": "http://arxiv.org/pdf/2101.00734v2",
        "source_type": "documentation",
        "authors": [
          "Benyamin Ghojogh",
          "Ali Ghodsi",
          "Fakhri Karray",
          "Mark Crowley"
        ],
        "publication_date": "2021-01-04"
      },
      {
        "title": "Factor Analysis in R Course | DataCamp",
        "url": "https://www.datacamp.com/courses/factor-analysis-in-r",
        "source_type": "tutorial"
      },
      {
        "title": "EducationalTestingService/factor_analyzer",
        "url": "https://github.com/EducationalTestingService/factor_analyzer",
        "source_type": "software_package"
      },
      {
        "title": "Confirmatory Factor Analysis Fundamentals | Towards Data Science",
        "url": "https://towardsdatascience.com/confirmatory-factor-analysis-theory-aac11af008a6/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      "principal-component-analysis",
      "t-sne",
      "umap"
    ]
  },
  {
    "slug": "principal-component-analysis",
    "name": "Principal Component Analysis",
    "description": "Principal Component Analysis transforms high-dimensional data into a lower-dimensional representation by finding the directions (principal components) that capture the maximum variance in the data. Each component is a linear combination of original features, with the first component explaining the most variance, the second component the most remaining variance orthogonal to the first, and so on. This technique reveals underlying patterns in data structure, enables visualization of complex datasets, and helps identify which combinations of features drive the most variation in the data.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/unsupervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/visualization",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing customer behavior data with dozens of variables (purchase frequency, spending patterns, demographics) to identify the 2-3 main dimensions that explain customer segmentation, revealing whether customers cluster by spending level, product preferences, or shopping frequency.",
        "goal": "Explainability"
      },
      {
        "description": "Reducing dimensionality of image data for facial recognition systems by finding the principal components that capture the most variation in face shapes and expressions, helping understand which facial features contribute most to distinguishing between individuals.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Principal components are abstract linear combinations of original features that often lack clear real-world interpretation or meaning."
      },
      {
        "description": "Only captures linear relationships between features, missing non-linear patterns and complex interactions in the data."
      },
      {
        "description": "Results are highly sensitive to feature scaling - features with larger numerical ranges can dominate the principal components."
      },
      {
        "description": "Information loss is inherent when reducing dimensions, and choosing the optimal number of components requires balancing simplicity with retained variance."
      }
    ],
    "resources": [
      {
        "title": "erdogant/pca",
        "url": "https://github.com/erdogant/pca",
        "source_type": "software_package"
      },
      {
        "title": "How to Calculate Principal Component Analysis (PCA) from Scratch ...",
        "url": "https://www.machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/",
        "source_type": "tutorial"
      },
      {
        "title": "A One-Stop Shop for Principal Component Analysis | Towards Data ...",
        "url": "https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c/",
        "source_type": "tutorial"
      },
      {
        "title": "Principal Component Analysis (PCA) with Scikit-Learn - KDnuggets",
        "url": "https://www.kdnuggets.com/2023/05/principal-component-analysis-pca-scikitlearn.html",
        "source_type": "tutorial"
      },
      {
        "title": "willtownes/glmpca-py",
        "url": "https://github.com/willtownes/glmpca-py",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "acronym": "PCA",
    "related_techniques": [
      "factor-analysis",
      "t-sne",
      "umap"
    ]
  },
  {
    "slug": "t-sne",
    "name": "t-SNE",
    "description": "t-SNE (t-Distributed Stochastic Neighbour Embedding) is a non-linear dimensionality reduction technique that creates 2D or 3D visualisations of high-dimensional data by preserving local neighbourhood relationships. The algorithm converts similarities between data points into joint probabilities in the high-dimensional space, then tries to minimise the divergence between these probabilities and those in the low-dimensional embedding. This approach excels at revealing cluster structures and local patterns, making it particularly effective for exploratory data analysis and understanding complex data relationships that linear methods like PCA might miss.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
      "assurance-goal-category/explainability/visualization-methods/feature-relationships",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/visualization"
    ],
    "example_use_cases": [
      {
        "description": "Analyzing genomic data with thousands of gene expression features to visualize how different cancer subtypes cluster together, revealing which tumors have similar molecular signatures and potentially similar treatment responses.",
        "goal": "Explainability"
      },
      {
        "description": "Exploring deep learning model embeddings to understand how a neural network represents different categories of images, showing whether the model groups similar objects (cars, animals, furniture) in meaningful clusters in its internal feature space.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Non-deterministic algorithm produces different results on each run, making it difficult to reproduce exact visualizations or compare results across studies."
      },
      {
        "description": "Prioritizes preserving local neighborhood structure at the expense of global relationships, potentially creating misleading impressions about overall data topology."
      },
      {
        "description": "Computationally expensive with O(n\u00b2) complexity, making it impractical for datasets with more than ~10,000 points without approximation methods."
      },
      {
        "description": "Sensitive to hyperparameter choices (perplexity, learning rate, iterations) that can dramatically affect clustering patterns and require domain expertise to tune appropriately."
      }
    ],
    "resources": [
      {
        "title": "pavlin-policar/openTSNE",
        "url": "https://github.com/pavlin-policar/openTSNE",
        "source_type": "software_package"
      },
      {
        "title": "openTSNE: Extensible, parallel implementations of t-SNE ...",
        "url": "https://opentsne.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "How t-SNE works \u2014 openTSNE 1.0.0 documentation",
        "url": "https://opentsne.readthedocs.io/en/stable/tsne_algorithm.html",
        "source_type": "documentation"
      },
      {
        "title": "t-SNE from Scratch (ft. NumPy) | Towards Data Science",
        "url": "https://towardsdatascience.com/t-sne-from-scratch-ft-numpy-172ee2a61df7/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [
      "factor-analysis",
      "principal-component-analysis",
      "umap"
    ]
  },
  {
    "slug": "umap",
    "name": "UMAP",
    "description": "UMAP (Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique that creates 2D or 3D visualisations of high-dimensional data by constructing a mathematical model of the data's underlying manifold structure. Unlike t-SNE, UMAP preserves both local neighbourhood relationships and global topology more effectively, using techniques from topological data analysis and Riemannian geometry. This approach often produces more interpretable cluster layouts while maintaining meaningful distances between clusters, making it particularly valuable for exploratory data analysis and understanding complex dataset structures.",
    "assurance_goals": [
      "Explainability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/representation-analysis/dimensionality-reduction",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/visualization",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/visualization"
    ],
    "example_use_cases": [
      {
        "description": "Analysing single-cell RNA sequencing data to visualise how different cell types cluster based on gene expression patterns, revealing developmental trajectories and identifying previously unknown cell subtypes in tissue samples.",
        "goal": "Explainability"
      },
      {
        "description": "Exploring customer segmentation by reducing hundreds of behavioural and demographic features to 2D space, showing how different customer groups relate to each other and identifying transition zones where customers might move between segments.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Hyperparameter choices (n_neighbors, min_dist, metric) significantly influence the embedding structure and can lead to very different interpretations of the same data."
      },
      {
        "description": "While preserving global structure better than t-SNE, distances in the reduced space still don't directly correspond to distances in the original feature space."
      },
      {
        "description": "Performance can be sensitive to the choice of distance metric, which may not be obvious for complex or mixed data types."
      },
      {
        "description": "Like other manifold learning techniques, it assumes the data lies on a lower-dimensional manifold, which may not hold for all datasets."
      }
    ],
    "resources": [
      {
        "title": "lmcinnes/umap",
        "url": "https://github.com/lmcinnes/umap",
        "source_type": "software_package"
      },
      {
        "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
        "url": "http://arxiv.org/pdf/1802.03426v3",
        "source_type": "technical_paper",
        "authors": [
          "Leland McInnes",
          "John Healy",
          "James Melville"
        ],
        "publication_date": "2018-02-09"
      },
      {
        "title": "Uniform Manifold Approximation and Projection (UMAP) and its Variants: Tutorial and Survey",
        "url": "http://arxiv.org/pdf/2109.02508v1",
        "source_type": "documentation",
        "authors": [
          "Benyamin Ghojogh",
          "Ali Ghodsi",
          "Fakhri Karray",
          "Mark Crowley"
        ],
        "publication_date": "2021-08-25"
      },
      {
        "title": "How UMAP Works \u2014 umap 0.5.8 documentation",
        "url": "https://umap-learn.readthedocs.io/en/latest/how_umap_works.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      "factor-analysis",
      "principal-component-analysis",
      "t-sne"
    ]
  },
  {
    "slug": "prototype-and-criticism-models",
    "name": "Prototype and Criticism Models",
    "description": "Prototype and Criticism Models provide data understanding by identifying two complementary sets of examples: prototypes represent the most typical instances that best summarise common patterns in the data, whilst criticisms are outliers or edge cases that are poorly represented by the prototypes. For example, in a dataset of customer transactions, prototypes might be the most representative buying patterns (frequent small purchases, occasional large purchases), whilst criticisms could be unusual behaviors (bulk buyers, one-time high-value customers). This dual approach reveals both what is normal and what is exceptional, helping understand data coverage and model blind spots.",
    "assurance_goals": [
      "Explainability",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/paradigm/unsupervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/instance-based/prototypes",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/sparsity",
      "assurance-goal-category/fairness",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing medical imaging datasets to identify prototype scans that represent typical healthy tissue patterns and criticism examples showing rare disease presentations, helping radiologists understand what the model considers 'normal' versus cases requiring special attention.",
        "goal": "Explainability"
      },
      {
        "description": "Evaluating credit scoring models by finding prototype borrowers who represent typical low-risk profiles and criticism cases showing unusual but legitimate financial patterns that the model might misclassify, ensuring fair treatment of edge cases.",
        "goal": "Explainability"
      },
      {
        "description": "Evaluating representation bias in hiring datasets by examining whether prototypes systematically exclude certain demographic groups and criticisms disproportionately represent minorities, revealing data collection inequities.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Selection of prototypes and criticisms is highly dependent on the choice of distance metric or similarity measure, which may not capture all meaningful relationships in the data."
      },
      {
        "description": "Computational complexity can become prohibitive for very large datasets, as the method often requires pairwise comparisons or optimisation over the entire dataset."
      },
      {
        "description": "The number of prototypes and criticisms to select is typically a hyperparameter that requires domain expertise to set appropriately."
      },
      {
        "description": "Results may not generalise well if the training data distribution differs significantly from the deployment data distribution."
      }
    ],
    "resources": [
      {
        "title": "Examples are not Enough, Learn to Criticize! Criticism for Interpretability",
        "url": "https://proceedings.neurips.cc/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Been Kim",
          "Rajiv Khanna",
          "Oluwasanmi O. Koyejo"
        ],
        "publication_date": "2016-12-05"
      },
      {
        "title": "SeldonIO/alibi",
        "url": "https://github.com/SeldonIO/alibi",
        "source_type": "software_package"
      },
      {
        "title": "Prototype Selection for Interpretable Classification",
        "url": "http://arxiv.org/pdf/1202.5933v1",
        "source_type": "technical_paper",
        "authors": [
          "Oscar Reyes",
          "Carlos Morell",
          "Sebastian Ventura"
        ],
        "publication_date": "2012-02-27"
      },
      {
        "title": "Alibi Explain Documentation",
        "url": "https://docs.seldon.io/projects/alibi/en/stable/",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      "neuron-activation-analysis",
      "concept-activation-vectors"
    ]
  },
  {
    "slug": "influence-functions",
    "name": "Influence Functions",
    "description": "Influence functions quantify how much each training example influenced a model's predictions by computing the change in prediction that would occur if that training example were removed and the model retrained. Using calculus and the implicit function theorem, they approximate this 'leave-one-out' effect without actually retraining the model by computing gradients and Hessian information. This mathematical approach reveals which specific training examples were most responsible for pushing the model toward or away from particular predictions, enabling practitioners to trace problematic outputs back to their root causes in the training data.",
    "assurance_goals": [
      "Explainability",
      "Fairness",
      "Privacy"
    ],
    "tags": [
      "applicable-models/architecture/linear-models",
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/differentiable",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/gradient-based",
      "assurance-goal-category/explainability/explains/causal-pathways",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/instance-based/influence-analysis",
      "assurance-goal-category/explainability/property/causality",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/fairness",
      "assurance-goal-category/privacy",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Investigating why a medical diagnosis model misclassified a patient by identifying which specific training cases most influenced the incorrect prediction, revealing potential mislabelled examples or problematic patterns in the training data.",
        "goal": "Explainability"
      },
      {
        "description": "Analysing a spam detection system that falsely flagged legitimate emails by tracing the prediction back to influential training examples, discovering that certain training emails contained misleading patterns that caused the model to overfit.",
        "goal": "Explainability"
      },
      {
        "description": "Auditing a loan approval model for discriminatory patterns by identifying which training examples most influenced rejections of minority applicants, revealing whether biased historical decisions are driving current unfair outcomes.",
        "goal": "Fairness"
      },
      {
        "description": "Assessing membership inference risks in a medical model by identifying whether certain patient records have disproportionate influence on predictions, indicating potential data leakage vulnerabilities.",
        "goal": "Privacy"
      }
    ],
    "limitations": [
      {
        "description": "Computationally intensive, requiring Hessian matrix computations that become intractable for very large models with millions of parameters."
      },
      {
        "description": "Requires access to the complete training dataset and training process, making it impossible to apply to pre-trained models without access to original training data."
      },
      {
        "description": "Accuracy degrades for highly non-convex models where the linear approximation underlying influence functions breaks down."
      },
      {
        "description": "Results can be sensitive to hyperparameter choices and may not generalise well across different model architectures or training procedures."
      }
    ],
    "resources": [
      {
        "title": "Understanding Black-box Predictions via Influence Functions",
        "url": "https://www.semanticscholar.org/paper/08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1",
        "source_type": "technical_paper",
        "authors": [
          "Pang Wei Koh",
          "Percy Liang"
        ]
      },
      {
        "title": "nimarb/pytorch_influence_functions",
        "url": "https://github.com/nimarb/pytorch_influence_functions",
        "source_type": "software_package"
      },
      {
        "title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions",
        "url": "https://www.semanticscholar.org/paper/f33f3dece9f34c1ec5417dccf9e0acf592d8e8cb",
        "source_type": "technical_paper",
        "authors": [
          "Sang Keun Choe",
          "Hwijeen Ahn",
          "Juhan Bae",
          "Kewen Zhao",
          "Minsoo Kang",
          "Youngseog Chung",
          "Adithya Pratapa",
          "W. Neiswanger",
          "Emma Strubell",
          "Teruko Mitamura",
          "Jeff G. Schneider",
          "Eduard H. Hovy",
          "Roger B. Grosse",
          "Eric P. Xing"
        ]
      },
      {
        "title": "Scaling Up Influence Functions",
        "url": "https://www.semanticscholar.org/paper/ef2a773c3c7848a6cc16b18164be5f8876a310af",
        "source_type": "technical_paper",
        "authors": [
          "Andrea Schioppa",
          "Polina Zablotskaia",
          "David Vilar",
          "Artem Sokolov"
        ]
      },
      {
        "title": "Welcome to torch-influence's API Reference! \u2014 torch-influence 0.1.0 ...",
        "url": "https://torch-influence.readthedocs.io/",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "related_techniques": [
      "contextual-decomposition",
      "taylor-decomposition"
    ]
  },
  {
    "slug": "contrastive-explanation-method",
    "name": "Contrastive Explanation Method",
    "description": "The Contrastive Explanation Method (CEM) explains model decisions by generating contrastive examples that reveal what makes a prediction distinctive. It identifies 'pertinent negatives' (minimal features that could be removed to change the prediction) and 'pertinent positives' (minimal features that must be present to maintain the prediction). This approach helps users understand not just what led to a decision, but what would need to change to achieve a different outcome, providing actionable insights for decision-making.",
    "assurance_goals": [
      "Explainability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/differentiable",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/instance-based/counterfactual",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/counterfactual-validity",
      "assurance-goal-category/explainability/property/sparsity",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining loan application rejections by showing that removing recent late payments (pertinent negative) or adding \u00a35,000 more annual income (pertinent positive) would change the decision to approval, giving applicants clear actionable guidance.",
        "goal": "Explainability"
      },
      {
        "description": "Analysing medical diagnosis models by identifying that removing a specific symptom combination would change a high-risk classification to low-risk, helping clinicians understand the critical diagnostic factors.",
        "goal": "Explainability"
      },
      {
        "description": "Providing transparent hiring decisions by showing job candidates exactly which qualifications (pertinent positives) they need to acquire or which application elements (pertinent negatives) might be hindering their success.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive as it requires solving an optimisation problem for each individual instance to find minimal perturbations."
      },
      {
        "description": "Results can be highly sensitive to hyperparameter settings, requiring careful tuning to produce meaningful explanations."
      },
      {
        "description": "May generate unrealistic or impossible contrastive examples if constraints are not properly specified, leading to impractical recommendations."
      },
      {
        "description": "Limited to scenarios where feature perturbations are meaningful and actionable, making it less suitable for immutable characteristics or highly constrained domains."
      }
    ],
    "resources": [
      {
        "title": "Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives",
        "url": "http://arxiv.org/pdf/1802.07623v2",
        "source_type": "technical_paper",
        "authors": [
          "Amit Dhurandhar",
          "Pin-Yu Chen",
          "Ronny Luss",
          "Chun-Chen Tu",
          "Paishun Ting",
          "Karthikeyan Shanmugam",
          "Payel Das"
        ],
        "publication_date": "2018-02-21"
      },
      {
        "title": "Interpretable Machine Learning",
        "url": "https://christophm.github.io/interpretable-ml-book/interpretability.html",
        "source_type": "documentation"
      },
      {
        "title": "Benchmarking and survey of explanation methods for black box models",
        "url": "https://core.ac.uk/download/599106733.pdf",
        "source_type": "documentation",
        "authors": [
          "Bodria Francesco",
          "Giannotti Fosca",
          "Guidotti R.",
          "Naretto Francesca",
          "Pedreschi Dino",
          "Rinzivillo S."
        ],
        "publication_date": "2023-01-01"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "acronym": "CEM",
    "related_techniques": [
      "shapley-additive-explanations",
      "integrated-gradients",
      "deeplift",
      "layer-wise-relevance-propagation",
      "local-interpretable-model-agnostic-explanations",
      "anchor"
    ]
  },
  {
    "slug": "anchor",
    "name": "ANCHOR",
    "description": "ANCHOR generates high-precision if-then rules that explain individual predictions by identifying the minimal set of feature conditions that guarantee a specific prediction with high confidence. It searches for 'anchor' conditions (e.g., 'age > 30 AND income < \u00a350k') that ensure the model gives the same prediction at least 95% of the time when those conditions are met. This creates human-readable rules that users can trust as sufficient conditions for understanding why a particular decision was made.",
    "assurance_goals": [
      "Explainability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/property/sparsity",
      "assurance-goal-category/explainability/surrogate-models/rule-extraction",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Explaining loan application decisions with rules like 'IF credit_score > 650 AND debt_ratio < 0.4 THEN approval = 95% likely', giving applicants clear, actionable conditions they can understand and potentially improve.",
        "goal": "Explainability"
      },
      {
        "description": "Generating diagnostic rules for medical predictions such as 'IF fever > 38.5\u00b0C AND white_blood_cells > 12,000 THEN infection = 92% likely', helping clinicians validate automated diagnoses with trusted clinical indicators.",
        "goal": "Explainability"
      },
      {
        "description": "Creating transparent hiring decisions with rules like 'IF experience >= 3_years AND degree = relevant THEN hire = 89% likely', providing clear justification for recruitment decisions that can be audited for fairness.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Limited to local explanations for individual instances, cannot provide global insights about overall model behaviour."
      },
      {
        "description": "Requires discretisation of continuous features, which can lose important nuanced information and create arbitrary thresholds."
      },
      {
        "description": "May fail to find suitable anchor rules if precision requirements are too strict or if the prediction space is highly complex."
      },
      {
        "description": "Computationally expensive as it requires extensive sampling to validate rule precision, especially for high-dimensional data."
      }
    ],
    "resources": [
      {
        "title": "Anchors: High-Precision Model-Agnostic Explanations",
        "url": "https://homes.cs.washington.edu/~marcotcr/aaai18.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Marco Tulio Ribeiro",
          "Sameer Singh",
          "Carlos Guestrin"
        ],
        "publication_date": "2018-01-01"
      },
      {
        "title": "marcotcr/anchor",
        "url": "https://github.com/marcotcr/anchor",
        "source_type": "software_package"
      },
      {
        "title": "alibi/alibi",
        "url": "https://github.com/SeldonIO/alibi",
        "source_type": "software_package"
      },
      {
        "title": "Interpretable Machine Learning - Anchors",
        "url": "https://christophm.github.io/interpretable-ml-book/anchors.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      "shapley-additive-explanations",
      "integrated-gradients",
      "deeplift",
      "layer-wise-relevance-propagation",
      "local-interpretable-model-agnostic-explanations",
      "contrastive-explanation-method"
    ]
  },
  {
    "slug": "rulefit",
    "name": "RuleFit",
    "description": "RuleFit creates interpretable surrogate models that can explain complex black-box models or serve as interpretable alternatives. It works by learning a sparse linear model that combines automatically extracted decision rules with original features. The technique first builds tree ensembles to generate candidate rules, then uses LASSO regression to select the most important rules and features. The resulting model provides global explanations through human-readable rules (e.g., 'IF age > 50 AND income < 30k THEN ...') combined with linear feature weights, making complex model behavior transparent and auditable.",
    "assurance_goals": [
      "Explainability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/property/sparsity",
      "assurance-goal-category/explainability/surrogate-models/global-surrogates",
      "assurance-goal-category/explainability/surrogate-models/rule-extraction",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Building customer churn prediction models with rules like 'IF contract_length < 12_months AND support_calls > 5 THEN churn_risk = high', allowing marketing teams to understand and act on the key drivers of customer attrition.",
        "goal": "Explainability"
      },
      {
        "description": "Creating credit scoring models that combine traditional linear factors (income, age) with interpretable rules (IF recent_missed_payments = 0 AND account_age > 2_years THEN creditworthy), providing transparent lending decisions.",
        "goal": "Explainability"
      },
      {
        "description": "Developing regulatory-compliant medical diagnosis models where treatment recommendations combine clinical measurements with clear decision rules (IF blood_pressure > 140 AND diabetes = true THEN high_risk), enabling audit trails for healthcare decisions.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Can generate large numbers of rules even with regularisation, potentially overwhelming users and reducing practical interpretability."
      },
      {
        "description": "Performance may be inferior to complex ensemble methods when rule complexity is constrained for interpretability."
      },
      {
        "description": "Rule extraction quality depends heavily on the underlying tree ensemble, which may miss important feature interactions if not properly configured."
      },
      {
        "description": "Requires careful hyperparameter tuning to balance between model complexity and interpretability, with no universal optimal setting."
      }
    ],
    "resources": [
      {
        "title": "christophM/rulefit",
        "url": "https://github.com/christophM/rulefit",
        "source_type": "software_package"
      },
      {
        "title": "Tree Ensembles with Rule Structured Horseshoe Regularization",
        "url": "http://arxiv.org/pdf/1702.05008v2",
        "source_type": "technical_paper",
        "authors": [
          "Malte Nalenz",
          "Mattias Villani"
        ],
        "publication_date": "2017-02-16"
      },
      {
        "title": "Safe RuleFit: Learning Optimal Sparse Rule Model by Meta Safe Screening",
        "url": "http://arxiv.org/pdf/1810.01683v2",
        "source_type": "technical_paper",
        "authors": [
          "Hiroki Kato",
          "Hiroyuki Hanada",
          "Ichiro Takeuchi"
        ],
        "publication_date": "2018-10-03"
      },
      {
        "title": "csinva/imodels",
        "url": "https://github.com/csinva/imodels",
        "source_type": "software_package"
      },
      {
        "title": "Getting More From Regression Models with RuleFit | Towards Data ...",
        "url": "https://towardsdatascience.com/getting-more-from-regression-models-with-rulefit-2e6be8d77432/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      "ridge-regression-surrogates",
      "model-distillation"
    ]
  },
  {
    "slug": "monte-carlo-dropout",
    "name": "Monte Carlo Dropout",
    "description": "Monte Carlo Dropout estimates prediction uncertainty by applying dropout (randomly setting neural network weights to zero) during inference rather than just training. It performs multiple forward passes through the network with different random dropout patterns and collects the resulting predictions to form a distribution. Low variance across predictions indicates epistemic certainty (the model is confident), while high variance suggests epistemic uncertainty (the model is unsure). This technique transforms any dropout-trained neural network into a Bayesian approximation for uncertainty quantification.",
    "assurance_goals": [
      "Explainability",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/probabilistic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/architecture-specific",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/prediction-confidence",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/efficiency",
      "assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Quantifying diagnostic uncertainty in medical imaging models by running 50+ Monte Carlo forward passes to detect when a chest X-ray classification is highly uncertain, prompting radiologist review for borderline cases.",
        "goal": "Reliability"
      },
      {
        "description": "Estimating prediction confidence in autonomous vehicle perception systems, where high uncertainty in object detection (e.g., variance > 0.3 across MC samples) triggers more conservative driving behaviour or human handover.",
        "goal": "Reliability"
      },
      {
        "description": "Providing uncertainty estimates in financial fraud detection models, where high epistemic uncertainty (wide prediction variance) indicates the model lacks sufficient training data for similar transaction patterns, requiring manual review.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Only captures epistemic (model) uncertainty, not aleatoric (data) uncertainty, providing an incomplete picture of total prediction uncertainty."
      },
      {
        "description": "Computationally expensive as it requires multiple forward passes (typically 50-100) for each prediction, significantly increasing inference time."
      },
      {
        "description": "Results depend critically on dropout rate matching the training configuration, and poorly calibrated dropout can lead to misleading uncertainty estimates."
      },
      {
        "description": "Approximation quality varies with network architecture and dropout placement, with some configurations providing poor uncertainty calibration despite theoretical foundations."
      }
    ],
    "resources": [
      {
        "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
        "url": "http://arxiv.org/pdf/1506.02142v6",
        "source_type": "technical_paper",
        "authors": [
          "Yarin Gal",
          "Zoubin Ghahramani"
        ],
        "publication_date": "2016-06-06"
      },
      {
        "title": "mattiasegu/uncertainty_estimation_deep_learning",
        "url": "https://github.com/mattiasegu/uncertainty_estimation_deep_learning",
        "source_type": "software_package"
      },
      {
        "title": "uzh-rpg/deep_uncertainty_estimation",
        "url": "https://github.com/uzh-rpg/deep_uncertainty_estimation",
        "source_type": "software_package"
      },
      {
        "title": "How certain are tansformers in image classification: uncertainty analysis with Monte Carlo dropout",
        "url": "https://www.semanticscholar.org/paper/d7ff734c5b62a4a140fd560373d890e43d5b36cf",
        "source_type": "technical_paper",
        "authors": [
          "Md. Farhadul Islam",
          "Sarah Zabeen",
          "Md. Azharul Islam",
          "Fardin Bin Rahman",
          "Anushua Ahmed",
          "Dewan Ziaul Karim",
          "Annajiat Alim Rasel",
          "Meem Arafat Manab"
        ]
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      "prediction-intervals",
      "quantile-regression",
      "conformal-prediction",
      "deep-ensembles",
      "bootstrapping",
      "jackknife-resampling"
    ]
  },
  {
    "slug": "out-of-distribution-detector-for-neural-networks",
    "name": "Out-of-DIstribution detector for Neural networks",
    "description": "ODIN (Out-of-Distribution Detector for Neural Networks) identifies when a neural network encounters inputs significantly different from its training distribution. It enhances detection by applying temperature scaling to soften the model's output distribution and adding small, carefully calibrated perturbations to the input that push in-distribution samples towards higher confidence predictions. By measuring the maximum softmax probability after these adjustments, ODIN can effectively distinguish between in-distribution and out-of-distribution inputs, flagging potentially unreliable predictions before they cause downstream errors.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/probabilistic-output",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/explains/prediction-confidence",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Detecting anomalous medical images in diagnostic systems, where ODIN flags X-rays or scans containing rare pathologies or imaging artefacts not present in training data, preventing misdiagnosis and prompting specialist review.",
        "goal": "Reliability"
      },
      {
        "description": "Protecting autonomous vehicle perception systems by identifying novel road scenarios (e.g., unusual weather conditions, rare obstacle types) that fall outside the training distribution, triggering fallback safety mechanisms.",
        "goal": "Safety"
      },
      {
        "description": "Monitoring production ML systems for data drift by detecting when incoming customer behaviour patterns deviate significantly from training data, helping explain why model performance may degrade over time.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires careful tuning of temperature scaling and perturbation magnitude parameters, which may need adjustment for different types of out-of-distribution data."
      },
      {
        "description": "Performance degrades when out-of-distribution samples are very similar to training data, making near-distribution detection challenging."
      },
      {
        "description": "Vulnerable to adversarial examples specifically crafted to evade detection by mimicking in-distribution characteristics."
      },
      {
        "description": "Computational overhead from input preprocessing and perturbation generation can impact real-time inference applications."
      }
    ],
    "resources": [
      {
        "title": "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks",
        "url": "http://arxiv.org/pdf/1706.02690v5",
        "source_type": "technical_paper",
        "authors": [
          "Shiyu Liang",
          "Yixuan Li",
          "R. Srikant"
        ],
        "publication_date": "2017-06-08"
      },
      {
        "title": "facebookresearch/odin",
        "url": "https://github.com/facebookresearch/odin",
        "source_type": "software_package"
      },
      {
        "title": "Generalized ODIN: Detecting Out-of-distribution Image without Learning from Out-of-distribution Data",
        "url": "http://arxiv.org/pdf/2002.11297v2",
        "source_type": "technical_paper",
        "authors": [
          "Yen-Chang Hsu",
          "Yilin Shen",
          "Hongxia Jin",
          "Zsolt Kira"
        ],
        "publication_date": "2020-02-26"
      },
      {
        "title": "Detection of out-of-distribution samples using binary neuron activation patterns",
        "url": "http://arxiv.org/abs/2212.14268",
        "source_type": "technical_paper",
        "authors": [
          "Chachu\u0142a, Krystian",
          "Olber, Bartlomiej",
          "Popowicz, Adam",
          "Radlak, Krystian",
          "Szczepankiewicz, Michal"
        ],
        "publication_date": "2023-03-24"
      },
      {
        "title": "Out-of-Distribution Detection with ODIN - A Tutorial",
        "url": "https://medium.com/@abhaypatil2000/out-of-distribution-detection-using-odin-f1a3e9e6b3b8",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "acronym": "ODIN",
    "related_techniques": [
      "anomaly-detection"
    ]
  },
  {
    "slug": "permutation-tests",
    "name": "Permutation Tests",
    "description": "Permutation tests assess the statistical significance of observed results (such as model accuracy, feature importance, or group differences) by comparing them to what would occur purely by chance. The technique randomly shuffles labels or data thousands of times, recalculating the metric of interest each time to build an empirical null distribution. If the actual observed result falls in the extreme tail of this distribution (typically beyond the 95th or 99th percentile), it provides strong evidence that the relationship is genuine rather than due to random chance, without requiring parametric assumptions about data distributions.",
    "assurance_goals": [
      "Explainability",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Validating feature importance in medical diagnosis models by permuting each feature 10,000 times to ensure that identified risk factors (e.g., blood pressure, cholesterol) have statistically significant predictive power beyond random chance.",
        "goal": "Reliability"
      },
      {
        "description": "Testing whether observed differences in loan approval rates between demographic groups are statistically significant by permuting group labels and calculating the approval rate difference distribution under the null hypothesis of no discrimination.",
        "goal": "Explainability"
      },
      {
        "description": "Verifying that a model's claimed 95% accuracy on test data is genuinely better than random guessing by permuting labels 5,000 times and confirming the actual accuracy falls beyond the 99th percentile of the null distribution.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive as it requires thousands of model evaluations or metric calculations, scaling poorly with dataset size and model complexity."
      },
      {
        "description": "Requires many permutations (typically 5,000-10,000) to achieve reliable p-values for strict significance thresholds like p < 0.01."
      },
      {
        "description": "Assumes exchangeability of observations under the null hypothesis, which may be violated in time series or hierarchical data structures."
      },
      {
        "description": "Cannot be easily parallelised for some metrics that require global model retraining, limiting scalability for complex machine learning pipelines."
      }
    ],
    "resources": [
      {
        "title": "Permutation Tests for Classification",
        "url": "https://core.ac.uk/download/4383831.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Golland, Polina",
          "Mukherjee, Sayan",
          "Panchenko, Dmitry"
        ],
        "publication_date": "2003-01-01"
      },
      {
        "title": "How to use Permutation Tests | Towards Data Science",
        "url": "https://towardsdatascience.com/how-to-use-permutation-tests-bacc79f45749/",
        "source_type": "tutorial"
      },
      {
        "title": "Permutation test in R | Towards Data Science",
        "url": "https://towardsdatascience.com/permutation-test-in-r-77d551a9f891/",
        "source_type": "tutorial"
      },
      {
        "title": "The Exchangeability Assumption for Permutation Tests of Multiple Regression Models: Implications for Statistics and Data Science Educators",
        "url": "http://arxiv.org/pdf/2406.07756v2",
        "source_type": "documentation",
        "authors": [
          "Johanna Hardin",
          "Lauren Quesada",
          "Julie Ye",
          "Nicholas J. Horton"
        ],
        "publication_date": "2024-06-11"
      },
      {
        "title": "scikit-learn permutation_importance",
        "url": "https://scikit-learn.org/stable/modules/permutation_importance.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [
      "cross-validation",
      "area-under-precision-recall-curve"
    ]
  },
  {
    "slug": "demographic-parity-assessment",
    "name": "Demographic Parity Assessment",
    "description": "Demographic Parity Assessment evaluates whether a model produces equal positive prediction rates across different demographic groups, regardless of underlying differences in qualifications or base rates. It quantifies fairness using metrics like Statistical Parity Difference (the absolute difference in positive outcome rates between groups) or Disparate Impact ratio (the ratio of positive rates). Unlike techniques that modify data or models, this is purely a measurement approach that highlights when protected groups receive favourable outcomes at different rates, helping organisations identify and document potential discrimination.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating credit approval algorithms by calculating that loan approval rates for different racial groups must be within 20% of each other (0.8 disparate impact ratio), ensuring compliance with anti-discrimination regulations.",
        "goal": "Fairness"
      },
      {
        "description": "Monitoring hiring platforms by measuring that job recommendation rates for male vs female candidates remain statistically equivalent (Statistical Parity Difference < 0.05), preventing systemic gender bias in career opportunities.",
        "goal": "Fairness"
      },
      {
        "description": "Auditing healthcare triage systems to verify that urgent care assignment rates are equal across ethnic groups, ensuring that automated medical prioritisation doesn't disadvantage minority patients.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Purely observational - identifies discrimination but doesn't provide solutions for remediation or bias mitigation."
      },
      {
        "description": "May penalise models for legitimate differences in base rates between groups, potentially forcing artificial equality where none should exist."
      },
      {
        "description": "Can conflict with individual fairness principles, where similarly qualified individuals might receive different treatment to achieve group parity."
      },
      {
        "description": "Doesn't account for quality of outcomes or consider whether equal rates are actually desirable given different group needs or preferences."
      }
    ],
    "resources": [
      {
        "title": "Fairness through awareness",
        "url": "http://arxiv.org/pdf/1104.3913v1",
        "source_type": "technical_paper",
        "authors": [
          "Cynthia Dwork",
          "Moritz Hardt",
          "Toniann Pitassi",
          "Omer Reingold",
          "Richard Zemel"
        ],
        "publication_date": "2011-04-20"
      },
      {
        "title": "AI Fairness 360 Toolkit",
        "url": "https://github.com/Trusted-AI/AIF360",
        "source_type": "software_package"
      },
      {
        "title": "Fairlearn - Demographic Parity",
        "url": "https://fairlearn.org/main/user_guide/fairness_in_machine_learning.html#demographic-parity",
        "source_type": "documentation"
      },
      {
        "title": "A Data-Centric Approach to Detecting and Mitigating Demographic Bias in Pediatric Mental Health Text: A Case Study in Anxiety Detection",
        "url": "https://www.semanticscholar.org/paper/ce96e451a2685485c05f06fb0d991e29a9c43dae",
        "source_type": "technical_paper",
        "authors": [
          "Julia Ive",
          "Paulina Bondaronek",
          "Vishal Yadav",
          "D. Santel",
          "Tracy Glauser",
          "Tina Cheng",
          "Jeffrey R. Strawn",
          "G. Agasthya",
          "Jordan Tschida",
          "Sanghyun Choo",
          "Mayanka Chandrashekar",
          "Anuj J. Kapadia",
          "J. Pestian"
        ]
      }
    ],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [
      "equal-opportunity-difference",
      "average-odds-difference"
    ]
  },
  {
    "slug": "adversarial-debiasing",
    "name": "Adversarial Debiasing",
    "description": "Adversarial debiasing reduces bias by training models using a competitive adversarial setup, similar to Generative Adversarial Networks (GANs). The technique involves two neural networks: a predictor that learns to make accurate predictions on the main task, and an adversary (bias detector) that attempts to predict protected attributes (such as race, gender, or age) from the predictor's internal representations. Through adversarial training, the predictor learns to produce representations that retain predictive power for the main task whilst being uninformative about protected characteristics, thereby reducing discriminatory bias.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/differentiable",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Training a resume screening model for a technology company that evaluates candidates based on skills and experience whilst preventing the internal representations from encoding gender or ethnicity information, ensuring hiring decisions cannot be influenced by protected characteristics even indirectly through correlated features.",
        "goal": "Fairness"
      },
      {
        "description": "Developing a credit scoring model for loan approvals that accurately predicts default risk whilst ensuring the model's internal features cannot be used to infer applicants' race or age, thereby preventing discriminatory lending practices whilst maintaining predictive accuracy.",
        "goal": "Fairness"
      },
      {
        "description": "Creating a medical diagnosis model that makes accurate predictions about patient conditions whilst ensuring that the learned representations cannot reveal sensitive demographic information like gender or ethnicity, protecting patient privacy whilst maintaining clinical effectiveness.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Significantly more complex to implement than standard models, requiring expertise in adversarial training techniques and careful architecture design for both predictor and adversary networks."
      },
      {
        "description": "Requires careful hyperparameter tuning to balance the competing objectives of task performance and bias mitigation, as overly strong adversarial training can harm predictive accuracy."
      },
      {
        "description": "Effectiveness heavily depends on the quality and design of the adversary network - a weak adversary may fail to detect subtle biases, whilst an overly strong adversary may eliminate useful information."
      },
      {
        "description": "Training can be unstable and may suffer from convergence issues common to adversarial training, requiring careful learning rate scheduling and regularisation techniques."
      },
      {
        "description": "Provides no formal guarantees about bias elimination and may not prevent all forms of discrimination, particularly when protected attributes can be inferred from other available features."
      }
    ],
    "resources": [
      {
        "title": "AI Fairness 360 (AIF360)",
        "url": "https://github.com/Trusted-AI/AIF360",
        "source_type": "software_package",
        "description": "Comprehensive toolkit for bias detection and mitigation including adversarial debiasing implementations"
      },
      {
        "title": "On the Fairness ROAD: Robust Optimization for Adversarial Debiasing",
        "url": "https://www.semanticscholar.org/paper/0c887592d781538a1b5c2168eae541b563c0ba9a",
        "source_type": "technical_paper",
        "authors": [
          "Vincent Grari",
          "Thibault Laugel",
          "Tatsunori B. Hashimoto",
          "S. Lamprier",
          "Marcin Detyniecki"
        ]
      },
      {
        "title": "aif360.sklearn.inprocessing.AdversarialDebiasing \u2014 aif360 0.6.1 ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.inprocessing.AdversarialDebiasing.html",
        "source_type": "documentation"
      },
      {
        "title": "Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing",
        "url": "http://arxiv.org/pdf/2103.06179v1",
        "source_type": "technical_paper",
        "authors": [
          "Christian Reimers",
          "Paul Bodesheim",
          "Jakob Runge",
          "Joachim Denzler"
        ],
        "publication_date": "2021-03-10"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      "fair-adversarial-networks",
      "prejudice-remover-regulariser",
      "meta-fair-classifier",
      "exponentiated-gradient-reduction",
      "fair-transfer-learning",
      "adaptive-sensitive-reweighting",
      "multi-accuracy-boosting"
    ]
  },
  {
    "slug": "counterfactual-fairness-assessment",
    "name": "Counterfactual Fairness Assessment",
    "description": "Counterfactual Fairness Assessment evaluates whether a model's predictions would remain unchanged if an individual's protected attributes (race, gender, age) were different, whilst keeping all other causally legitimate factors constant. The technique requires constructing a causal graph that maps relationships between variables, then using do-calculus or structural causal models to simulate counterfactual scenarios. For example, it asks: 'Would this loan application still be approved if the applicant were a different race, holding constant their actual qualifications and economic circumstances?' This individual-level fairness criterion helps identify when decisions depend improperly on protected characteristics.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "fairness-approach/causal",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating a hiring algorithm by testing whether qualified candidates would receive the same evaluation scores if their gender were different, whilst controlling for actual skills, experience, and education, revealing whether gender bias affects recruitment decisions.",
        "goal": "Fairness"
      },
      {
        "description": "Assessing a criminal sentencing model by examining whether defendants with identical criminal histories and case circumstances would receive the same sentence recommendations regardless of their race, identifying potential discriminatory patterns in judicial AI systems.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires explicit specification of causal relationships between variables, which involves subjective assumptions about what constitutes legitimate versus illegitimate causal pathways."
      },
      {
        "description": "May be mathematically impossible to satisfy simultaneously with other fairness criteria (like statistical parity), forcing practitioners to choose between competing fairness definitions."
      },
      {
        "description": "Implementation complexity is high, requiring sophisticated causal inference techniques and structural causal models that are difficult to construct and validate."
      },
      {
        "description": "Depends heavily on the quality and completeness of the causal graph, which may be incorrect or missing important confounding variables."
      }
    ],
    "resources": [
      {
        "title": "Counterfactual Fairness",
        "url": "https://proceedings.neurips.cc/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Matt J. Kusner",
          "Joshua Loftus",
          "Chris Russell",
          "Ricardo Silva"
        ],
        "publication_date": "2017-12-04"
      },
      {
        "title": "fairlearn/fairlearn",
        "url": "https://github.com/fairlearn/fairlearn",
        "source_type": "software_package"
      },
      {
        "title": "Counterfactual Fairness in Text Classification through Robustness",
        "url": "http://arxiv.org/pdf/1809.10610v2",
        "source_type": "technical_paper",
        "authors": [
          "Sahaj Garg",
          "Vincent Perot",
          "Nicole Limtiaco",
          "Ankur Taly",
          "Ed H. Chi",
          "Alex Beutel"
        ],
        "publication_date": "2018-09-27"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [
      "path-specific-counterfactual-fairness-assessment"
    ]
  },
  {
    "slug": "sensitivity-analysis-for-fairness",
    "name": "Sensitivity Analysis for Fairness",
    "description": "Sensitivity Analysis for Fairness systematically evaluates how model predictions change when sensitive attributes or their proxies are perturbed whilst holding other factors constant. The technique involves creating counterfactual instances by modifying potentially discriminatory features (race, gender, age) or their correlates (zip code, names, education institutions) and measuring the resulting prediction differences. This controlled perturbation approach quantifies the degree to which protected characteristics influence model decisions, helping detect both direct discrimination and indirect bias through proxy variables even when sensitive attributes are not explicitly used as model inputs.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/local",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Testing whether a lending model's decisions change significantly when only the applicant's zip code (which may correlate with race) is altered, while keeping all other factors constant.",
        "goal": "Fairness"
      },
      {
        "description": "Evaluating a recruitment algorithm by systematically changing candidate names from stereotypically male to female names (whilst keeping qualifications identical) to measure whether gender bias affects hiring recommendations, revealing discrimination through name-based proxies.",
        "goal": "Fairness"
      },
      {
        "description": "Assessing a healthcare resource allocation model by varying patient zip codes across different socioeconomic areas to determine whether geographic proxies for race and income inappropriately influence treatment recommendations.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires domain expertise to identify relevant proxy variables for sensitive attributes, which may not be obvious or comprehensive."
      },
      {
        "description": "Computationally intensive for complex models when testing many feature combinations or perturbation ranges."
      },
      {
        "description": "Choice of perturbation ranges and comparison points involves subjective decisions that can significantly affect results and conclusions."
      },
      {
        "description": "May miss subtle or interaction-based forms of discrimination that only manifest under specific combinations of features."
      }
    ],
    "resources": [
      {
        "title": "The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning",
        "url": "http://arxiv.org/pdf/2410.09600v2",
        "source_type": "technical_paper",
        "authors": [
          "Jake Fawkes",
          "Nic Fishman",
          "Mel Andrews",
          "Zachary C. Lipton"
        ],
        "publication_date": "2024-10-12"
      },
      {
        "title": "Fair SA: Sensitivity Analysis for Fairness in Face Recognition",
        "url": "http://arxiv.org/pdf/2202.03586v2",
        "source_type": "technical_paper",
        "authors": [
          "Aparna R. Joshi",
          "Xavier Suau",
          "Nivedha Sivakumar",
          "Luca Zappella",
          "Nicholas Apostoloff"
        ],
        "publication_date": "2022-02-08"
      },
      {
        "title": "fairlearn/fairlearn",
        "url": "https://github.com/fairlearn/fairlearn",
        "source_type": "software_package"
      },
      {
        "title": "Aequitas: Bias Audit Toolkit",
        "url": "https://github.com/dssg/aequitas",
        "source_type": "software_package"
      },
      {
        "title": "Fairness Through Sensitivity Analysis - Towards Data Science",
        "url": "https://towardsdatascience.com/fairness-through-sensitivity-analysis-3ea1b4d79e6c",
        "source_type": "tutorial"
      },
      {
        "title": "User Guide - Fairlearn documentation",
        "url": "https://fairlearn.org/v0.8.0/user_guide/assessment.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      "fairness-gan",
      "attribute-removal-fairness-through-unawareness",
      "bayesian-fairness-regularization"
    ]
  },
  {
    "slug": "synthetic-data-generation",
    "name": "Synthetic Data Generation",
    "description": "Synthetic data generation creates artificial datasets that aim to preserve the statistical properties, distributions, and relationships of real data whilst containing no actual records from real individuals. The technique encompasses various approaches including generative adversarial networks (GANs), variational autoencoders (VAEs), statistical sampling methods, and privacy-preserving techniques like differential privacy. Beyond privacy protection, synthetic data serves multiple purposes: augmenting limited datasets, balancing class distributions, testing model robustness, enabling data sharing across organisations, and supporting fairness assessments by generating representative samples for underrepresented groups.",
    "assurance_goals": [
      "Privacy",
      "Fairness",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/generative/gan",
      "applicable-models/architecture/neural-networks/generative/vae",
      "applicable-models/architecture/probabilistic",
      "applicable-models/paradigm/generative",
      "applicable-models/paradigm/unsupervised",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/privacy",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/data-handling",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Creating realistic but synthetic electronic health records for developing and testing medical diagnosis algorithms without exposing real patient data, enabling secure collaboration between healthcare institutions.",
        "goal": "Privacy"
      },
      {
        "description": "Generating synthetic samples for underrepresented demographic groups in a hiring dataset to train fair recruitment models, ensuring all groups have sufficient representation for bias testing and mitigation.",
        "goal": "Fairness"
      },
      {
        "description": "Augmenting limited training data for rare medical conditions by generating synthetic patient records, improving model reliability and performance on edge cases where real data is insufficient.",
        "goal": "Reliability"
      },
      {
        "description": "Creating synthetic financial transaction data for testing fraud detection systems in development environments, avoiding exposure of real customer financial information whilst maintaining realistic attack patterns.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "May not capture all subtle patterns, correlations, and edge cases present in real data, potentially leading to reduced model performance when deployed on actual data with different characteristics."
      },
      {
        "description": "Generating high-quality synthetic data that maintains both statistical fidelity and utility requires sophisticated techniques and substantial computational resources, especially for complex, high-dimensional datasets."
      },
      {
        "description": "Privacy-preserving approaches may still risk information leakage through statistical inference attacks, membership inference, or model inversion, requiring careful privacy budget management and validation."
      },
      {
        "description": "Synthetic data may inadvertently amplify existing biases in the original data or introduce new biases through the generation process, particularly in generative models trained on biased datasets."
      },
      {
        "description": "Validation and quality assessment of synthetic data is challenging, as traditional metrics may not adequately capture whether the synthetic data preserves the relationships and patterns needed for specific downstream tasks."
      }
    ],
    "resources": [
      {
        "title": "sdv-dev/SDV",
        "url": "https://github.com/sdv-dev/SDV",
        "source_type": "software_package"
      },
      {
        "title": "An evaluation framework for synthetic data generation models",
        "url": "http://arxiv.org/pdf/2404.08866v1",
        "source_type": "technical_paper",
        "authors": [
          "Ioannis E. Livieris",
          "Nikos Alimpertis",
          "George Domalis",
          "Dimitris Tsakalidis"
        ],
        "publication_date": "2024-04-13"
      },
      {
        "title": "Synthetic Data \u2014 SecureML 0.2.2 documentation",
        "url": "https://secureml.readthedocs.io/en/latest/user_guide/synthetic_data.html",
        "source_type": "documentation"
      },
      {
        "title": "How to Generate Real-World Synthetic Data with CTGAN | Towards ...",
        "url": "https://towardsdatascience.com/how-to-generate-real-world-synthetic-data-with-ctgan-af41b4d60fde/",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "related_techniques": [
      "federated-learning",
      "differential-privacy",
      "homomorphic-encryption"
    ]
  },
  {
    "slug": "federated-learning",
    "name": "Federated Learning",
    "description": "Federated learning enables collaborative model training across multiple distributed parties (devices, organisations, or data centres) without requiring centralised data sharing. Participants train models locally on their private datasets and only share model updates (gradients, weights, or aggregated statistics) with a central coordinator. This distributed approach serves multiple purposes: preserving data privacy and sovereignty, reducing communication costs, enabling learning from diverse data sources, improving model robustness through heterogeneous training, and facilitating compliance with data protection regulations whilst maintaining model performance comparable to centralised training.",
    "assurance_goals": [
      "Privacy",
      "Reliability",
      "Safety",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/linear-models",
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/privacy",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Developing a smartphone keyboard prediction model by learning from users' typing patterns without their text ever leaving their devices, enabling personalised predictions whilst maintaining complete data privacy.",
        "goal": "Privacy"
      },
      {
        "description": "Training a medical diagnosis model across multiple hospitals without sharing patient records, ensuring model robustness by learning from diverse patient populations and clinical practices across different institutions.",
        "goal": "Reliability"
      },
      {
        "description": "Creating a cybersecurity threat detection model by federating learning across financial institutions without exposing sensitive transaction data, reducing systemic risk whilst maintaining competitive confidentiality.",
        "goal": "Safety"
      },
      {
        "description": "Building a fair credit scoring model by training across multiple regions and demographics without centralising sensitive financial data, ensuring representation from diverse populations whilst respecting local data sovereignty laws.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Communication overhead can be substantial, especially with frequent model updates and large models, potentially limiting scalability and increasing training time compared to centralised approaches."
      },
      {
        "description": "Statistical heterogeneity across participants (non-IID data distributions) can lead to training instability, slower convergence, and reduced model performance compared to centralised training on pooled data."
      },
      {
        "description": "System heterogeneity in computational capabilities, network connectivity, and availability of participating devices can create bottlenecks and introduce bias towards more capable participants."
      },
      {
        "description": "Privacy vulnerabilities remain through gradient leakage attacks, model inversion, and membership inference attacks that can potentially reconstruct sensitive information from shared model updates."
      },
      {
        "description": "Coordination complexity increases with the number of participants, requiring sophisticated aggregation protocols, fault tolerance mechanisms, and secure communication infrastructure."
      }
    ],
    "resources": [
      {
        "title": "Open Federated Learning (OpenFL) Documentation",
        "url": "https://openfl.readthedocs.io/en/stable/",
        "source_type": "documentation"
      },
      {
        "title": "Federated Learning - DeepLearning.AI",
        "url": "https://www.deeplearning.ai/short-courses/intro-to-federated-learning/",
        "source_type": "tutorial"
      },
      {
        "title": "A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection",
        "url": "http://arxiv.org/pdf/1907.09693v7",
        "source_type": "documentation",
        "authors": [
          "Qinbin Li",
          "Zeyi Wen",
          "Zhaomin Wu",
          "Sixu Hu",
          "Naibo Wang",
          "Yuan Li",
          "Xu Liu",
          "Bingsheng He"
        ],
        "publication_date": "2019-07-23"
      },
      {
        "title": "Federated learning with hybrid differential privacy for secure and reliable cross-IoT platform knowledge sharing",
        "url": "https://core.ac.uk/download/603345619.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Algburi, S.",
          "Algburi, S.",
          "Anupallavi, S.",
          "Anupallavi, S.",
          "Ashokkumar, S. R.",
          "Ashokkumar, S. R.",
          "Elmedany, W.",
          "Elmedany, W.",
          "Khalaf, O. I.",
          "Khalaf, O. I.",
          "Selvaraj, D.",
          "Selvaraj, D.",
          "Sharif, M. S.",
          "Sharif, M. S."
        ],
        "publication_date": "2024-01-01"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 5,
    "related_techniques": [
      "synthetic-data-generation",
      "differential-privacy",
      "homomorphic-encryption"
    ]
  },
  {
    "slug": "differential-privacy",
    "name": "Differential Privacy",
    "description": "Differential privacy provides mathematically rigorous privacy protection by adding carefully calibrated random noise to data queries, statistical computations, or machine learning outputs. The technique works by ensuring that the presence or absence of any individual's data has minimal impact on the results - specifically, any query result should be nearly indistinguishable whether or not a particular person's data is included. This is achieved through controlled noise addition that scales with the query's sensitivity and a privacy budget (epsilon) that quantifies the privacy-utility trade-off. The smaller the epsilon, the more noise is added and the stronger the privacy guarantee, but at the cost of reduced accuracy.",
    "assurance_goals": [
      "Privacy",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/privacy",
      "assurance-goal-category/privacy/formal-guarantee/differential-privacy",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/privacy-guarantee",
      "evidence-type/quantitative-metric",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/data-handling",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Protecting individual privacy in census data analysis by adding calibrated noise to demographic statistics, ensuring households cannot be re-identified whilst maintaining accurate population insights for policy planning.",
        "goal": "Privacy"
      },
      {
        "description": "Publishing differentially private aggregate statistics about model performance across different demographic groups, enabling transparent bias auditing without exposing sensitive individual prediction details or group membership.",
        "goal": "Transparency"
      },
      {
        "description": "Enabling fair evaluation of lending algorithms by releasing differentially private performance metrics across protected groups, allowing regulatory compliance checking whilst protecting individual applicant privacy.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Adding noise inherently reduces the accuracy and utility of results, with stronger privacy guarantees (smaller epsilon values) leading to more significant degradation in data quality."
      },
      {
        "description": "Setting the privacy budget (epsilon) requires expertise and careful consideration of the privacy-utility trade-off, with no universal guidelines for appropriate values across different applications."
      },
      {
        "description": "Sequential queries consume the privacy budget cumulatively, potentially requiring careful query planning and potentially prohibiting future analyses once the budget is exhausted."
      },
      {
        "description": "Implementation complexity is high, requiring deep understanding of sensitivity analysis, noise mechanisms, and composition theorems to avoid inadvertent privacy violations."
      },
      {
        "description": "May not protect against all privacy attacks, particularly sophisticated adversaries with auxiliary information or when combined with other data sources that could aid re-identification."
      }
    ],
    "resources": [
      {
        "title": "Google Differential Privacy Library",
        "url": "https://github.com/google/differential-privacy",
        "source_type": "software_package",
        "description": "Open-source library providing implementations of differential privacy algorithms and utilities"
      },
      {
        "title": "The Algorithmic Foundations of Differential Privacy",
        "url": "https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Cynthia Dwork",
          "Aaron Roth"
        ],
        "description": "Foundational monograph on differential privacy theory and algorithms"
      },
      {
        "title": "Opacus: User-Friendly Differential Privacy Library in PyTorch",
        "url": "https://github.com/pytorch/opacus",
        "source_type": "software_package",
        "description": "PyTorch library for training neural networks with differential privacy"
      },
      {
        "title": "Programming Differential Privacy",
        "url": "https://programming-dp.com/",
        "source_type": "tutorial",
        "description": "Comprehensive online book and tutorial for learning differential privacy programming"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 3,
    "related_techniques": [
      "synthetic-data-generation",
      "federated-learning",
      "homomorphic-encryption"
    ]
  },
  {
    "slug": "homomorphic-encryption",
    "name": "Homomorphic Encryption",
    "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
    "assurance_goals": [
      "Privacy",
      "Safety",
      "Transparency",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/linear-models",
      "applicable-models/architecture/neural-networks/feedforward",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/parametric",
      "applicable-models/requirements/differentiable",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/privacy",
      "assurance-goal-category/privacy/formal-guarantee",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/privacy-guarantee",
      "evidence-type/quantitative-metric",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Enabling a cloud-based medical diagnosis service to process encrypted patient data and return encrypted results without the cloud provider ever accessing actual medical information, ensuring complete patient privacy during outsourced computation.",
        "goal": "Privacy"
      },
      {
        "description": "Securing financial risk assessment computations by allowing banks to jointly analyse encrypted transaction patterns for fraud detection without exposing individual customer data, reducing systemic security risks.",
        "goal": "Safety"
      },
      {
        "description": "Enabling transparent audit of algorithmic decision-making by allowing regulators to verify model computations on encrypted data, providing accountability whilst protecting the proprietary nature of both the algorithm and the underlying data.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Extremely computationally expensive, often 100-1000x slower than unencrypted computation, making it impractical for real-time applications or large-scale data processing."
      },
      {
        "description": "Limited range of operations supported efficiently, with complex operations like divisions, comparisons, and non-polynomial functions being particularly challenging or impossible to implement."
      },
      {
        "description": "Implementation requires deep cryptographic expertise to avoid security vulnerabilities, choose appropriate parameters, and optimise performance for specific use cases."
      },
      {
        "description": "Memory and storage requirements are significantly higher than traditional computation, as encrypted data typically requires much more space than plaintext equivalents."
      },
      {
        "description": "Current fully homomorphic encryption schemes have practical limitations on computation depth before noise accumulation requires expensive bootstrapping operations to refresh ciphertexts."
      }
    ],
    "resources": [
      {
        "title": "zama-ai/concrete-ml",
        "url": "https://github.com/zama-ai/concrete-ml",
        "source_type": "software_package",
        "description": "Privacy-preserving machine learning library that enables data scientists to run ML models on encrypted data using FHE without cryptography expertise"
      },
      {
        "title": "Survey on Fully Homomorphic Encryption, Theory, and Applications",
        "url": "https://core.ac.uk/download/579858842.pdf",
        "source_type": "documentation",
        "authors": [
          "Chiara Marcolla",
          "Frank H.P. Fitzek",
          "Marc Manzano",
          "Najwa Aaraj",
          "Riccardo Bassoli",
          "Victor Sucasas"
        ],
        "publication_date": "2022-10-06",
        "description": "Comprehensive survey covering FHE theory, cryptographic schemes, and practical applications across different domains"
      },
      {
        "title": "Welcome to OpenFHE's documentation! \u2014 OpenFHE documentation",
        "url": "https://openfhe-development.readthedocs.io/",
        "source_type": "documentation",
        "description": "Documentation for open-source C++ library supporting multiple FHE schemes including BFV, BGV, CKKS, and Boolean circuits"
      },
      {
        "title": "Evaluation of Privacy-Preserving Support Vector Machine (SVM) Learning Using Homomorphic Encryption",
        "url": "https://core.ac.uk/download/656115203.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Ali, Hisham",
          "Buchanan, William J."
        ],
        "publication_date": "2025-01-01",
        "description": "Technical paper evaluating performance overhead of SVM learning with homomorphic encryption for privacy-preserving ML"
      },
      {
        "title": "microsoft/SEAL",
        "url": "https://github.com/microsoft/SEAL",
        "source_type": "software_package",
        "description": "Easy-to-use homomorphic encryption library enabling computations on encrypted integers and real numbers"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "related_techniques": [
      "synthetic-data-generation",
      "federated-learning",
      "differential-privacy"
    ]
  },
  {
    "slug": "prediction-intervals",
    "name": "Prediction Intervals",
    "description": "Prediction intervals provide a range of plausible values around a model's prediction, expressing uncertainty as 'the true value will likely fall between X and Y with Z% confidence'. For example, instead of predicting 'house price: \u00a3300,000', a prediction interval might say 'house price: \u00a3280,000 to \u00a3320,000 with 95% confidence'. This technique works by calculating upper and lower bounds that account for both model uncertainty (how confident the model is) and inherent randomness in the data. Prediction intervals are crucial for informed decision-making, as they help users understand the reliability and precision of predictions, enabling better risk assessment and planning.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Providing realistic ranges for medical diagnosis predictions, such as 'patient survival time: 8-14 months with 90% confidence', enabling doctors to make informed treatment decisions and communicate uncertainty to patients and families.",
        "goal": "Reliability"
      },
      {
        "description": "Communicating uncertainty in automated loan approval systems by showing 'credit score prediction: 650-720 with 95% confidence' rather than a single score, helping loan officers understand prediction reliability and make transparent decisions.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring consistent prediction uncertainty across demographic groups in hiring algorithms, verifying that prediction intervals have similar widths for different protected groups to avoid unfair confidence disparities.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Relies on assumptions about the error distribution (often normality) which may not hold in practice, leading to inaccurate interval coverage when data exhibits heavy tails, skewness, or other non-standard patterns."
      },
      {
        "description": "Can be overconfident if the underlying model is poorly calibrated, producing intervals that are too narrow and fail to capture the true prediction uncertainty."
      },
      {
        "description": "Vulnerable to distribution shift between training and deployment data, where intervals calculated on historical data may not reflect uncertainty in new, unseen conditions."
      },
      {
        "description": "May require careful hyperparameter tuning and validation to achieve desired coverage rates, particularly when using advanced methods like conformal prediction or quantile regression."
      },
      {
        "description": "Computational overhead increases when generating intervals for large datasets or complex models, especially when using resampling-based methods like bootstrapping."
      }
    ],
    "resources": [
      {
        "title": "scikit-learn-contrib/MAPIE",
        "url": "https://github.com/scikit-learn-contrib/MAPIE",
        "source_type": "software_package",
        "description": "Open-source Python library for quantifying uncertainties using conformal prediction techniques, compatible with scikit-learn, TensorFlow, and PyTorch"
      },
      {
        "title": "MAPIE - Model Agnostic Prediction Interval Estimator",
        "url": "https://mapie.readthedocs.io/",
        "source_type": "documentation",
        "description": "Official documentation for MAPIE library implementing distribution-free uncertainty estimates for regression and classification tasks"
      },
      {
        "title": "valeman/awesome-conformal-prediction",
        "url": "https://github.com/valeman/awesome-conformal-prediction",
        "source_type": "software_package",
        "description": "Curated collection of conformal prediction resources including videos, tutorials, books, papers, and open-source libraries"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      "monte-carlo-dropout",
      "quantile-regression",
      "conformal-prediction",
      "deep-ensembles",
      "bootstrapping",
      "jackknife-resampling"
    ]
  },
  {
    "slug": "quantile-regression",
    "name": "Quantile Regression",
    "description": "Quantile regression estimates specific percentiles (quantiles) of the target variable rather than just predicting the average outcome. For example, instead of predicting 'average house price = \u00a3300,000', it can predict 'there's a 10% chance the price will be below \u00a3250,000, 50% chance below \u00a3300,000, and 90% chance below \u00a3380,000'. This technique reveals how input features affect different parts of the outcome distribution - perhaps property size strongly influences luxury homes (90th percentile) but barely affects budget properties (10th percentile). By capturing the full conditional distribution, quantile regression provides rich uncertainty information and enables robust prediction intervals.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/linear-models/regression",
      "applicable-models/architecture/neural-networks",
      "applicable-models/architecture/tree-based/gradient-boosting",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Predicting patient recovery times after surgery by estimating multiple quantiles (e.g., 25th, 50th, 75th percentiles), enabling doctors to communicate realistic timeframes: 'Most patients recover within 2-4 weeks, but some may take up to 8 weeks', providing robust uncertainty estimates for treatment planning.",
        "goal": "Reliability"
      },
      {
        "description": "Revealing how income inequality affects different segments of society by showing how education's impact varies across income quantiles - demonstrating that education benefits high earners much more than low earners, providing transparent insights into systemic inequalities.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring equitable loan amount predictions across demographic groups by verifying that the spread of predicted loan amounts (difference between 90th and 10th percentiles) is consistent across protected groups, preventing discriminatory practices in lending ranges.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Computationally intensive when fitting multiple quantiles simultaneously, especially for large datasets or complex models, as each quantile requires separate optimization."
      },
      {
        "description": "May produce crossing quantiles without proper constraints, where predicted 90th percentile values are lower than 50th percentile values, creating logically inconsistent and unusable prediction intervals."
      },
      {
        "description": "Sensitive to outliers and heavy-tailed distributions, particularly in extreme quantiles (e.g., 5th or 95th percentiles), which can lead to unstable and unreliable estimates."
      },
      {
        "description": "Requires careful selection of quantile levels and may need domain expertise to interpret results meaningfully, as different quantiles may reveal conflicting patterns in feature relationships."
      },
      {
        "description": "Less effective with small datasets where extreme quantiles cannot be reliably estimated due to insufficient data points in the tails of the distribution."
      }
    ],
    "resources": [
      {
        "title": "statsmodels/statsmodels",
        "url": "https://github.com/statsmodels/statsmodels",
        "source_type": "software_package",
        "description": "Python package providing comprehensive statistical modeling capabilities including quantile regression alongside descriptive statistics and statistical inference"
      },
      {
        "title": "Quantile Regression in Machine Learning: A Survey",
        "url": "https://www.semanticscholar.org/paper/01cd143c5a054b85afc9b99d473f84422ace7e05",
        "source_type": "documentation",
        "authors": [
          "Anshul Kumar",
          "Rajesh Wadhvani",
          "A. Rasool",
          "Muktesh Gupta"
        ],
        "description": "Comprehensive survey covering quantile regression applications, methods, and developments in machine learning contexts"
      },
      {
        "title": "Tutorial for conformalized quantile regression (CQR) \u2014 MAPIE 0.8.5 ...",
        "url": "https://mapie.readthedocs.io/en/v0.8.5/examples_regression/4-tutorials/plot_cqr_tutorial.html",
        "source_type": "tutorial"
      },
      {
        "title": "Quantile Regression Forest \u2014 sklearn_quantile 0.1.1 documentation",
        "url": "https://sklearn-quantile.readthedocs.io/en/latest/methods.html",
        "source_type": "documentation"
      },
      {
        "title": "Quantile machine learning models for python \u2014 sklearn_quantile ...",
        "url": "https://sklearn-quantile.readthedocs.io/",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "monte-carlo-dropout",
      "prediction-intervals",
      "conformal-prediction",
      "deep-ensembles",
      "bootstrapping",
      "jackknife-resampling"
    ]
  },
  {
    "slug": "conformal-prediction",
    "name": "Conformal Prediction",
    "description": "Conformal prediction provides mathematically guaranteed uncertainty quantification by creating prediction sets that contain the true outcome with a specified probability (e.g., exactly 95% coverage). The technique works by measuring how 'strange' or 'nonconforming' new predictions are compared to calibration data - if a prediction seems unusual, it gets wider intervals. For example, in medical diagnosis, instead of saying 'likely cancer', it might say 'possible diagnoses: {cancer, benign tumour} with 95% confidence'. This distribution-free method works with any underlying model (neural networks, random forests, etc.) and requires no assumptions about data distribution, making it a robust framework for reliable uncertainty estimates in high-stakes applications.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/transparency",
      "data-requirements/calibration-set",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Creating prediction sets for drug discovery that guarantee 95% coverage, such as 'this compound will likely have activity against {target A, target B, target C}', ensuring reliable decision-making in costly experimental validation.",
        "goal": "Reliability"
      },
      {
        "description": "Providing transparent multi-class predictions in judicial risk assessment by showing all plausible risk categories with guaranteed coverage, enabling judges to see the full range of possibilities rather than just a single point estimate.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring fair uncertainty quantification across demographic groups in college admissions by verifying that prediction set sizes (number of possible outcomes) are consistent across protected groups, preventing discriminatory overconfidence for certain populations.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Prediction sets can be unnecessarily wide when nonconformity scores vary greatly across the feature space, leading to conservative intervals that reduce practical utility."
      },
      {
        "description": "Requires a held-out calibration set separate from training data, reducing the amount of data available for model training, which can impact performance on small datasets."
      },
      {
        "description": "Guarantees only hold under the exchangeability assumption - if test data distribution differs significantly from calibration data, coverage guarantees may be violated."
      },
      {
        "description": "For multi-class problems, prediction sets may include many classes when the model is uncertain, making decisions difficult when sets contain opposing outcomes."
      },
      {
        "description": "Computational cost increases with the number of calibration samples, and efficient implementation requires careful design for large-scale or real-time applications."
      }
    ],
    "resources": [
      {
        "title": "A tutorial on conformal prediction",
        "url": "http://arxiv.org/pdf/0706.3188v1",
        "source_type": "documentation",
        "authors": [
          "Glenn Shafer",
          "Vladimir Vovk"
        ],
        "publication_date": "2007-06-21",
        "description": "Foundational tutorial introducing conformal prediction theory and applications by the method's creators"
      },
      {
        "title": "valeman/awesome-conformal-prediction",
        "url": "https://github.com/valeman/awesome-conformal-prediction",
        "source_type": "software_package",
        "description": "Curated collection of conformal prediction resources including videos, tutorials, books, papers, and open-source libraries"
      },
      {
        "title": "scikit-learn-contrib/MAPIE",
        "url": "https://github.com/scikit-learn-contrib/MAPIE",
        "source_type": "software_package",
        "description": "Python library for uncertainty quantification using conformal prediction across regression, classification, and time series tasks"
      },
      {
        "title": "Tutorial for classification \u2014 MAPIE 0.8.6 documentation",
        "url": "https://mapie.readthedocs.io/en/v0.8.6/examples_classification/4-tutorials/plot_main-tutorial-classification.html",
        "source_type": "tutorial",
        "description": "Practical tutorial demonstrating conformal prediction for classification tasks with guaranteed coverage"
      },
      {
        "title": "Conformal Prediction: a Unified Review of Theory and New Challenges",
        "url": "http://arxiv.org/pdf/2005.07972v2",
        "source_type": "documentation",
        "authors": [
          "Matteo Fontana",
          "Gianluca Zeni",
          "Simone Vantini"
        ],
        "publication_date": "2020-05-16",
        "description": "Comprehensive review of conformal prediction theory, recent advances, and emerging challenges in the field"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 2,
    "related_techniques": [
      "monte-carlo-dropout",
      "prediction-intervals",
      "quantile-regression",
      "deep-ensembles",
      "bootstrapping",
      "jackknife-resampling"
    ]
  },
  {
    "slug": "empirical-calibration",
    "name": "Empirical Calibration",
    "description": "Empirical calibration adjusts a model's predicted probabilities to match observed frequencies. For example, if events predicted with 80% confidence only occur 60% of the time, calibration would correct this overconfidence. Common techniques include Platt scaling and isotonic regression, which learn transformations that map the model's raw scores to well-calibrated probabilities, improving the reliability of confidence measures for downstream decisions.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/calibration-set",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adjusting a credit default prediction model's probabilities to ensure that loan applicants with a predicted 30% default risk actually default 30% of the time, improving decision-making.",
        "goal": "Reliability"
      },
      {
        "description": "Calibrating a medical diagnosis model's confidence scores so that stakeholders can meaningfully interpret probability outputs, enabling doctors to make informed decisions about treatment urgency based on reliable confidence estimates.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring that a hiring algorithm's confidence scores are equally well-calibrated across different demographic groups, preventing systematically overconfident predictions for certain populations that could lead to biased decision-making.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires a separate held-out calibration dataset, which reduces the amount of data available for model training."
      },
      {
        "description": "Calibration performance can degrade over time if the underlying data distribution shifts, requiring periodic recalibration."
      },
      {
        "description": "May sacrifice some discriminative power in favour of calibration, potentially reducing the model's ability to distinguish between classes."
      },
      {
        "description": "Calibration methods assume that the calibration set is representative of future data, which may not hold in dynamic environments."
      }
    ],
    "resources": [
      {
        "title": "google/empirical_calibration",
        "url": "https://github.com/google/empirical_calibration",
        "source_type": "software_package"
      },
      {
        "title": "A Python Library For Empirical Calibration",
        "url": "http://arxiv.org/pdf/1906.11920v2",
        "source_type": "technical_paper",
        "authors": [
          "Xiaojing Wang",
          "Jingang Miao",
          "Yunting Sun"
        ],
        "publication_date": "2019-07-25"
      },
      {
        "title": "Assessing the effectiveness of empirical calibration under different bias scenarios",
        "url": "http://arxiv.org/pdf/2111.04233v2",
        "source_type": "technical_paper",
        "authors": [
          "Hon Hwang",
          "Juan C Quiroz",
          "Blanca Gallego"
        ],
        "publication_date": "2021-11-08"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      "temperature-scaling"
    ]
  },
  {
    "slug": "temperature-scaling",
    "name": "Temperature Scaling",
    "description": "Temperature scaling adjusts a model's confidence by applying a single parameter (temperature) to its predictions. When a model is too confident in its wrong answers, temperature scaling can fix this by making the predictions more realistic. It works by dividing the model's outputs by the temperature value before converting them to probabilities. Higher temperatures make the model less confident, whilst lower temperatures increase confidence. The technique maintains the model's accuracy whilst ensuring that when it says it's 90% confident, it's actually right about 90% of the time.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/probabilistic-output",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-model-internals",
      "data-requirements/calibration-set",
      "data-requirements/validation-set",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adjusting a deep learning image classifier's confidence scores to be realistic, ensuring that when it's 90% confident, it's right 90% of the time.",
        "goal": "Reliability"
      },
      {
        "description": "Making medical diagnosis model predictions more trustworthy by providing realistic confidence scores that doctors can interpret and use to make informed decisions about patient care.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring fair treatment across patient demographics by calibrating confidence scores equally across different groups, preventing systematic over-confidence in predictions for certain populations.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Only addresses calibration at the overall dataset level, not subgroup-specific miscalibration issues."
      },
      {
        "description": "Does not improve the rank ordering or accuracy of predictions, only adjusts confidence levels."
      },
      {
        "description": "Assumes that calibration errors are consistent across different types of inputs and feature values."
      },
      {
        "description": "Requires a separate validation set for temperature parameter optimisation, which may not be available in small datasets."
      }
    ],
    "resources": [
      {
        "title": "gpleiss/temperature_scaling",
        "url": "https://github.com/gpleiss/temperature_scaling",
        "source_type": "software_package"
      },
      {
        "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness",
        "url": "http://arxiv.org/pdf/2502.20604v1",
        "source_type": "technical_paper",
        "authors": [
          "Hao Xuan",
          "Bokai Yang",
          "Xingyu Li"
        ],
        "publication_date": "2025-02-28"
      },
      {
        "title": "Neural Clamping: Joint Input Perturbation and Temperature Scaling for Neural Network Calibration",
        "url": "http://arxiv.org/pdf/2209.11604v2",
        "source_type": "technical_paper",
        "authors": [
          "Yung-Chen Tang",
          "Pin-Yu Chen",
          "Tsung-Yi Ho"
        ],
        "publication_date": "2024-07-24"
      },
      {
        "title": "On Calibration of Modern Neural Networks | arXiv",
        "url": "https://arxiv.org/abs/1706.04599",
        "source_type": "technical_paper",
        "authors": [
          "Chuan Guo",
          "Geoff Pleiss",
          "Yu Sun",
          "Kilian Q. Weinberger"
        ],
        "publication_date": "2017-06-14"
      },
      {
        "title": "On the Limitations of Temperature Scaling for Distributions with Overlaps",
        "url": "http://arxiv.org/pdf/2306.00740v3",
        "source_type": "technical_paper",
        "authors": [
          "Muthu Chidambaram",
          "Rong Ge"
        ],
        "publication_date": "2023-06-01"
      }
    ],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [
      "empirical-calibration"
    ]
  },
  {
    "slug": "deep-ensembles",
    "name": "Deep Ensembles",
    "description": "Deep ensembles combine predictions from multiple neural networks trained independently with different random initializations to capture epistemic uncertainty (model uncertainty). By training several models on the same data with different starting points, the ensemble reveals how much the model's predictions depend on training randomness. The disagreement between ensemble members naturally indicates prediction uncertainty - when models agree, confidence is high; when they disagree, uncertainty is revealed. This approach provides more reliable uncertainty estimates, better out-of-distribution detection, and improved calibration compared to single models.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Improving self-driving car safety by using multiple neural networks to detect obstacles, where disagreement between models signals uncertainty and triggers extra caution or human intervention, providing robust uncertainty quantification for critical decisions.",
        "goal": "Reliability"
      },
      {
        "description": "Communicating prediction confidence to medical professionals by showing the range of diagnoses from multiple trained models, enabling doctors to understand when the AI system is uncertain and requires additional human expertise or testing.",
        "goal": "Transparency"
      },
      {
        "description": "Detecting out-of-distribution inputs in financial fraud detection systems where ensemble disagreement signals potentially novel attack patterns that require immediate security team review and system safeguards.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive to train and deploy, requiring multiple complete neural networks which increases training time, memory usage, and inference costs proportionally to ensemble size."
      },
      {
        "description": "May still provide overconfident predictions for inputs far from the training distribution, as all ensemble members can be similarly confident about out-of-distribution examples."
      },
      {
        "description": "Requires careful hyperparameter tuning for each ensemble member to ensure diversity, as identical hyperparameters may lead to similar models that reduce uncertainty estimation quality."
      },
      {
        "description": "Storage and deployment overhead increases linearly with ensemble size, making it challenging to deploy large ensembles in resource-constrained environments or real-time applications."
      },
      {
        "description": "Ensemble predictions may be difficult to interpret individually, as the final decision emerges from averaging multiple models rather than from a single explainable pathway."
      }
    ],
    "resources": [
      {
        "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
        "url": "https://arxiv.org/abs/1612.01474",
        "source_type": "technical_paper",
        "authors": [
          "Balaji Lakshminarayanan",
          "Alexander Pritzel",
          "Charles Blundell"
        ],
        "publication_date": "2016-12-05",
        "description": "Foundational paper introducing deep ensembles for uncertainty estimation in neural networks"
      },
      {
        "title": "ENSTA-U2IS-AI/awesome-uncertainty-deeplearning",
        "url": "https://github.com/ENSTA-U2IS-AI/awesome-uncertainty-deeplearning",
        "source_type": "documentation",
        "description": "Comprehensive collection of research papers, surveys, datasets, and code for uncertainty estimation in deep learning"
      },
      {
        "title": "Deep Ensembles: A Loss Landscape Perspective",
        "url": "http://arxiv.org/pdf/1912.02757v2",
        "source_type": "technical_paper",
        "authors": [
          "Stanislav Fort",
          "Huiyi Hu",
          "Balaji Lakshminarayanan"
        ],
        "publication_date": "2019-12-05",
        "description": "Analysis of why deep ensembles work well from the perspective of loss landscape geometry and mode connectivity"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 5,
    "related_techniques": [
      "monte-carlo-dropout",
      "prediction-intervals",
      "quantile-regression",
      "conformal-prediction",
      "bootstrapping",
      "jackknife-resampling"
    ]
  },
  {
    "slug": "bootstrapping",
    "name": "Bootstrapping",
    "description": "Bootstrapping estimates uncertainty by repeatedly resampling the original dataset with replacement to create many new training sets, training a model on each sample, and analysing the variation in predictions. This approach provides confidence intervals and stability measures without making strong statistical assumptions. By showing how predictions change with different random samples of the data, it reveals how sensitive the model is to the specific training examples and provides robust uncertainty estimates.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Estimating uncertainty in financial risk models by resampling historical data to understand how predictions might vary under different historical scenarios.",
        "goal": "Reliability"
      },
      {
        "description": "Providing confidence intervals for medical diagnosis predictions to help doctors understand the reliability of AI recommendations and make more informed treatment decisions.",
        "goal": "Transparency"
      },
      {
        "description": "Assessing whether prediction uncertainty is consistent across different demographic groups in hiring algorithms, identifying if the model is systematically more uncertain for certain populations.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive as it requires training multiple models on resampled datasets."
      },
      {
        "description": "Does not account for uncertainty in model structure or architecture choices."
      },
      {
        "description": "Cannot detect systematically missing data patterns or biases present in the original dataset."
      },
      {
        "description": "Assumes that the original dataset is representative of the population of interest."
      }
    ],
    "resources": [
      {
        "title": "Deterministic bootstrapping for a class of bootstrap methods",
        "url": "http://arxiv.org/pdf/1903.10816v2",
        "source_type": "technical_paper",
        "authors": [
          "Thomas Pitschel"
        ],
        "publication_date": "2019-03-26"
      },
      {
        "title": "A Gentle Introduction to the Bootstrap Method ...",
        "url": "https://www.machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/",
        "source_type": "tutorial"
      },
      {
        "title": "scipy.stats.bootstrap",
        "url": "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html",
        "source_type": "software_package"
      },
      {
        "title": "Bootstrapping and bagging \u2014 modAL documentation",
        "url": "https://modal-python.readthedocs.io/en/latest/content/examples/bootstrapping_and_bagging.html",
        "source_type": "tutorial"
      },
      {
        "title": "Machine Learning: What is Bootstrapping? - KDnuggets",
        "url": "https://www.kdnuggets.com/2023/03/bootstrapping.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [
      "monte-carlo-dropout",
      "prediction-intervals",
      "quantile-regression",
      "conformal-prediction",
      "deep-ensembles",
      "jackknife-resampling"
    ]
  },
  {
    "slug": "jackknife-resampling",
    "name": "Jackknife Resampling",
    "description": "Jackknife resampling (also called leave-one-out resampling) assesses model stability and uncertainty by systematically removing one data point at a time and retraining the model on the remaining data. Unlike bootstrapping which samples with replacement, jackknife creates n different models by excluding each of the n data points once. This systematic approach reveals how individual points influence results, provides robust estimates of prediction variance, and identifies unusually influential observations that may be outliers or leverage points affecting model reliability.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating how removing individual countries from a global climate model affects predictions, identifying which regions have outsized influence and providing robust uncertainty estimates for climate projections used in policy decisions.",
        "goal": "Reliability"
      },
      {
        "description": "Providing transparent uncertainty estimates in medical risk prediction by showing how individual patient records influence model predictions, enabling clinicians to understand prediction stability and confidence intervals for treatment decisions.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring fair model evaluation in hiring algorithms by systematically testing how removing candidates from different demographic groups affects model performance, revealing whether certain populations disproportionately influence the model's behaviour.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Extremely computationally intensive for large datasets, requiring training of n separate models for n data points, making it impractical for datasets with thousands or millions of observations."
      },
      {
        "description": "May underestimate uncertainty compared to bootstrapping or other resampling methods, as it provides only n different samples rather than a broader exploration of the data distribution."
      },
      {
        "description": "Assumes that removing single data points provides meaningful insights into model stability, which may not hold when multiple correlated observations drive model behaviour."
      },
      {
        "description": "Can be sensitive to the choice of performance metric used for evaluation, as different metrics may show different patterns of sensitivity to individual data points."
      },
      {
        "description": "Provides limited insight into model behaviour on truly novel data, as each jackknife sample is only minimally different from the full training set."
      }
    ],
    "resources": [
      {
        "title": "scikit-learn model_selection.LeaveOneOut",
        "url": "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html",
        "source_type": "documentation",
        "description": "Scikit-learn implementation of leave-one-out cross-validation for jackknife resampling"
      },
      {
        "title": "Cross-validation: evaluating estimator performance",
        "url": "https://scikit-learn.org/stable/modules/cross_validation.html",
        "source_type": "documentation",
        "description": "Comprehensive guide to cross-validation methods including leave-one-out in scikit-learn"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 5,
    "related_techniques": [
      "monte-carlo-dropout",
      "prediction-intervals",
      "quantile-regression",
      "conformal-prediction",
      "deep-ensembles",
      "bootstrapping"
    ]
  },
  {
    "slug": "cross-validation",
    "name": "Cross-validation",
    "description": "Cross-validation evaluates model performance and robustness by systematically partitioning data into multiple subsets (folds) and training/testing repeatedly on different combinations. Common approaches include k-fold (splitting into k equal parts), stratified (preserving class distributions), and leave-one-out variants. By testing on multiple independent holdout sets, it reveals how performance varies across different data subsamples, provides robust estimates of generalisation ability, and helps detect overfitting or model instability that single train-test splits might miss.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Using 10-fold cross-validation to estimate a healthcare prediction model's true accuracy and detect overfitting, ensuring robust performance estimates that generalise beyond the specific training sample to new patient populations.",
        "goal": "Reliability"
      },
      {
        "description": "Providing transparent model evaluation in regulatory submissions by showing consistent performance across multiple validation folds, demonstrating to auditors that model performance claims are not cherry-picked from a single favourable test set.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring fair model evaluation across demographic groups by using stratified cross-validation that maintains representative proportions of protected classes in each fold, revealing whether performance is consistent across different population segments.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Computationally expensive for large datasets or complex models, requiring multiple training runs that scale linearly with the number of folds."
      },
      {
        "description": "Can provide overly optimistic performance estimates when data has dependencies or structure (e.g., time series, grouped observations) that violate independence assumptions."
      },
      {
        "description": "May not reflect real-world performance if the training data distribution differs significantly from future deployment conditions or population shifts."
      },
      {
        "description": "Choice of fold number (k) involves a bias-variance trade-off: fewer folds reduce computational cost but increase variance in estimates, whilst more folds increase computation but may introduce bias."
      },
      {
        "description": "Standard cross-validation doesn't account for temporal ordering in sequential data, potentially leading to data leakage where future information influences past predictions."
      }
    ],
    "resources": [
      {
        "title": "scikit-learn Cross-validation User Guide",
        "url": "https://scikit-learn.org/stable/modules/cross_validation.html",
        "source_type": "documentation",
        "description": "Comprehensive guide to cross-validation methods and implementations in scikit-learn"
      },
      {
        "title": "Cross-validation: what does it estimate and how well does it do it?",
        "url": "http://arxiv.org/pdf/2104.00673v4",
        "source_type": "technical_paper",
        "authors": [
          "Stephen Bates",
          "Trevor Hastie",
          "Robert Tibshirani"
        ],
        "publication_date": "2021-04-01",
        "description": "Theoretical analysis of what cross-validation estimates and its accuracy in practice"
      },
      {
        "title": "A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection",
        "url": "https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Ron Kohavi"
        ],
        "publication_date": "1995-01-01",
        "description": "Classic paper comparing cross-validation with bootstrap for model evaluation and selection"
      },
      {
        "title": "Cross-Validation in Machine Learning: How to Do It Right",
        "url": "https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right",
        "source_type": "tutorial",
        "description": "Practical guide covering different cross-validation strategies and common pitfalls to avoid"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 3,
    "related_techniques": [
      "permutation-tests",
      "area-under-precision-recall-curve"
    ]
  },
  {
    "slug": "area-under-precision-recall-curve",
    "name": "Area Under Precision-Recall Curve",
    "description": "Area Under Precision-Recall Curve (AUPRC) measures model performance by plotting precision (the proportion of positive predictions that are correct) against recall (the proportion of actual positives that are correctly identified) at various classification thresholds, then calculating the area under the resulting curve. Unlike accuracy or AUC-ROC, AUPRC is particularly valuable for imbalanced datasets where the minority class is of primary interest---a perfect score is 1.0, whilst random performance equals the positive class proportion. By focusing on the precision-recall trade-off, it provides a more informative assessment than overall accuracy for scenarios where false positives and false negatives have different costs, especially when positive examples are rare.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating fraud detection models where genuine transactions far outnumber fraudulent ones, using AUPRC to optimise the balance between catching fraud (high recall) and minimising false alarms (high precision) for cost-effective operations.",
        "goal": "Reliability"
      },
      {
        "description": "Providing transparent performance metrics for rare disease detection systems to medical regulators, where AUPRC clearly shows model effectiveness on the minority positive class rather than being masked by high accuracy on negative cases.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring fair evaluation of loan default prediction across demographic groups by comparing AUPRC scores, revealing whether models perform equally well at identifying high-risk borrowers regardless of protected characteristics.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "More sensitive to class distribution than ROC curves, making it difficult to compare models across datasets with different positive class proportions or to set universal performance thresholds."
      },
      {
        "description": "Can be overly optimistic on extremely imbalanced datasets where even random predictions may achieve seemingly high AUPRC scores due to the small positive class size."
      },
      {
        "description": "Provides limited insight into performance at specific operating points, requiring additional analysis to determine optimal threshold selection for deployment."
      },
      {
        "description": "Interpolation methods for calculating the area under the curve can vary between implementations, potentially leading to slightly different scores for the same model."
      },
      {
        "description": "Less interpretable than simple metrics like precision or recall at a fixed threshold, making it harder to communicate performance to non-technical stakeholders."
      }
    ],
    "resources": [
      {
        "title": "scikit-learn Precision-Recall",
        "url": "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html",
        "source_type": "documentation",
        "description": "Comprehensive guide to precision-recall curves and AUPRC calculation in scikit-learn with practical examples"
      },
      {
        "title": "Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence",
        "url": "https://www.semanticscholar.org/paper/c1b5b9dfc7d6e024097f63947aa5db06e1c192d8",
        "source_type": "technical_paper",
        "authors": [
          "Qi Qi",
          "Youzhi Luo",
          "Zhao Xu",
          "Shuiwang Ji",
          "Tianbao Yang"
        ],
        "description": "Technical paper on optimising AUPRC directly during model training with convergence guarantees"
      },
      {
        "title": "A Closer Look at AUROC and AUPRC under Class Imbalance",
        "url": "http://arxiv.org/pdf/2401.06091v4",
        "source_type": "technical_paper",
        "authors": [
          "Matthew B. A. McDermott",
          "Haoran Zhang",
          "Lasse Hyldig Hansen",
          "Giovanni Angelotti",
          "Jack Gallifant"
        ],
        "publication_date": "2024-01-11",
        "description": "Recent analysis of AUPRC behaviour under extreme class imbalance with practical recommendations"
      },
      {
        "title": "DominikRafacz/auprc",
        "url": "https://github.com/DominikRafacz/auprc",
        "source_type": "software_package",
        "description": "R package for calculating AUPRC with functions for plotting precision-recall curves and mlr3 integration"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "acronym": "AUPRC",
    "related_techniques": [
      "permutation-tests",
      "cross-validation"
    ]
  },
  {
    "slug": "safety-envelope-testing",
    "name": "Safety Envelope Testing",
    "description": "Safety envelope testing systematically evaluates AI system performance at the boundaries of its intended operational domain to identify potential failure modes before deployment. The technique involves defining the system's operational design domain (ODD), creating test scenarios that approach or exceed these boundaries, and measuring performance degradation as conditions become more challenging. By testing edge cases, environmental extremes, and boundary conditions, it reveals where the system transitions from safe to unsafe operation, enabling the establishment of clear operational limits and safety margins for deployment.",
    "assurance_goals": [
      "Safety",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/test-scenarios",
      "data-type/any",
      "evidence-type/boundary-analysis",
      "evidence-type/quantitative-metric",
      "expertise-needed/domain-expertise",
      "expertise-needed/safety-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/testing"
    ],
    "example_use_cases": [
      {
        "description": "Testing autonomous vehicle perception systems at the limits of weather conditions, lighting, and sensor coverage to establish safe operational boundaries and determine when human intervention is required.",
        "goal": "Safety"
      },
      {
        "description": "Evaluating medical AI diagnostic systems with edge cases near decision boundaries to ensure reliable performance and identify when the system should defer to human specialists.",
        "goal": "Reliability"
      },
      {
        "description": "Assessing financial trading algorithms under extreme market conditions and volatility to prevent catastrophic losses and ensure system shutdown protocols activate appropriately.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Requires comprehensive domain expertise to identify relevant boundary conditions and edge cases that could affect system safety."
      },
      {
        "description": "May be computationally expensive and time-consuming, especially for complex systems with high-dimensional operational domains."
      },
      {
        "description": "Difficult to achieve complete coverage of all possible boundary conditions, potentially missing critical edge cases."
      },
      {
        "description": "Results may not generalise to novel scenarios that fall outside the tested boundary conditions."
      },
      {
        "description": "Establishing appropriate safety thresholds and performance criteria requires careful calibration based on domain-specific risk tolerance."
      }
    ],
    "resources": [
      {
        "title": "On the brittleness of AI systems",
        "url": "https://arxiv.org/abs/2009.00802",
        "source_type": "technical_paper",
        "authors": [
          "Andrew J. Lohn"
        ],
        "publication_date": "2020-09-02",
        "description": "Analysis of AI system brittleness and the need for improved testing, especially for out-of-distribution performance"
      },
      {
        "title": "Safety Assurance of Artificial Intelligence-Based Systems: A Systematic Literature Review",
        "url": "https://ieeexplore.ieee.org/abstract/document/9984982/",
        "source_type": "technical_paper",
        "authors": [
          "Antonio V. Silva Neto",
          "Jo\u00e3o B. Camargo",
          "Jorge R. Almeida",
          "Paulo S. Cugnasca"
        ],
        "publication_date": "2022-12-14",
        "description": "Comprehensive systematic literature review on safety assurance methods for AI-based systems"
      },
      {
        "title": "AMLAS - Assurance of Machine Learning in Autonomous Systems",
        "url": "https://www.york.ac.uk/assuring-autonomy/guidance/amlas/amlas-tool/",
        "source_type": "documentation",
        "description": "Tool for systematically creating safety cases for machine learning components with guidance through safety envelope testing"
      },
      {
        "title": "System and Safety Analysis with SysAI A Statistical Learning Framework",
        "url": "https://ntrs.nasa.gov/citations/20220009665",
        "source_type": "technical_paper",
        "authors": [
          "Yuning He"
        ],
        "publication_date": "2022-07-12",
        "description": "NASA technical report on statistical learning framework for system and safety analysis in AI systems"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 4,
    "related_techniques": [
      "permutation-tests",
      "cross-validation",
      "area-under-precision-recall-curve",
      "red-teaming"
    ]
  },
  {
    "slug": "internal-review-boards",
    "name": "Internal Review Boards",
    "description": "Internal Review Boards (IRBs) provide independent, systematic evaluation of AI/ML projects throughout their lifecycle to identify ethical, safety, and societal risks before they materialise. Typically composed of multidisciplinary experts including ethicists, domain specialists, legal counsel, community representatives, and technical staff, IRBs review project proposals, assess potential harms to individuals and communities, evaluate mitigation strategies, and establish ongoing monitoring requirements. Unlike traditional research ethics committees, AI-focused IRBs address algorithmic bias, fairness concerns, privacy implications, and societal impact at scale, providing essential governance for responsible AI development and deployment.",
    "assurance_goals": [
      "Safety",
      "Fairness",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/governance-framework",
      "evidence-type/qualitative-report",
      "expertise-needed/domain-expertise",
      "expertise-needed/ethics",
      "expertise-needed/legal",
      "lifecycle-stage/model-development",
      "lifecycle-stage/project-planning",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "example_use_cases": [
      {
        "description": "Reviewing a proposed criminal risk assessment tool to evaluate potential discriminatory impacts, privacy implications, and societal consequences before development begins, ensuring vulnerable communities are protected from algorithmic harm.",
        "goal": "Safety"
      },
      {
        "description": "Evaluating a hiring algorithm for bias across demographic groups, requiring algorithmic audits and ongoing monitoring to ensure equitable treatment of all candidates and compliance with employment law.",
        "goal": "Fairness"
      },
      {
        "description": "Establishing transparent governance processes for a healthcare AI system, requiring clear documentation of decision-making criteria, model limitations, and performance metrics that can be communicated to patients and regulators.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Can significantly slow development timelines and increase project costs, potentially making organisations less competitive or delaying beneficial AI applications from reaching users."
      },
      {
        "description": "Effectiveness heavily depends on board composition, with inadequate diversity or expertise leading to blind spots in risk assessment and biased decision-making."
      },
      {
        "description": "May face internal pressure to approve revenue-generating projects or strategic initiatives, compromising independence and rigorous ethical evaluation."
      },
      {
        "description": "Limited authority or enforcement mechanisms can result in recommendations being ignored, particularly when they conflict with business objectives or technical constraints."
      },
      {
        "description": "Risk of becoming bureaucratic or box-ticking exercises rather than substantive evaluations, especially in organisations without strong ethical leadership or clear accountability structures."
      }
    ],
    "resources": [
      {
        "title": "Investigating Algorithm Review Boards for Organizational Responsible Artificial Intelligence Governance",
        "url": "https://link.springer.com/article/10.1007/s43681-024-00574-8",
        "source_type": "technical_paper",
        "authors": [
          "Emily Hadley",
          "Alan Blatecky",
          "Megan Comfort"
        ],
        "publication_date": "2024-09-16",
        "description": "Research on how organizations can establish algorithm review boards to govern and mitigate risks in AI deployment across sectors"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 1,
    "acronym": "IRBs",
    "related_techniques": [
      "red-teaming",
      "human-in-the-loop-safeguards",
      "confidence-thresholding",
      "runtime-monitoring-and-circuit-breakers"
    ]
  },
  {
    "slug": "red-teaming",
    "name": "Red Teaming",
    "description": "Red teaming involves systematic adversarial testing of AI/ML systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. Drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. Unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Testing a content moderation AI by attempting to make it generate harmful outputs through creative prompt injection, jailbreaking techniques, and edge case scenarios to identify safety vulnerabilities before deployment.",
        "goal": "Safety"
      },
      {
        "description": "Probing a medical diagnosis AI system with adversarial examples and edge cases to identify failure modes that could lead to incorrect diagnoses, ensuring the system fails gracefully rather than confidently providing wrong information.",
        "goal": "Reliability"
      },
      {
        "description": "Systematically testing a hiring algorithm with inputs designed to reveal hidden biases, using adversarial examples to check if the system can be manipulated to discriminate against protected groups or favour certain demographics unfairly.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires highly specialized expertise in both AI/ML systems and adversarial attack methods, making it expensive and difficult to scale across organizations."
      },
      {
        "description": "Limited by the creativity and knowledge of red team members - can only discover vulnerabilities that testers think to explore, potentially missing novel attack vectors."
      },
      {
        "description": "Time-intensive process that may not be feasible for rapid development cycles or resource-constrained projects, potentially delaying beneficial system deployments."
      },
      {
        "description": "May not generalize to real-world adversarial scenarios, as red team attacks may differ significantly from actual malicious use patterns or user behaviours."
      },
      {
        "description": "Risk of false confidence if red teaming is incomplete or superficial, leading organizations to believe systems are safer than they actually are."
      }
    ],
    "resources": [
      {
        "title": "Red Teaming LLM Applications - DeepLearning.AI",
        "url": "https://www.deeplearning.ai/short-courses/red-teaming-llm-applications/",
        "source_type": "tutorial",
        "description": "Course teaching how to identify and test vulnerabilities in large language model applications using red teaming techniques"
      },
      {
        "title": "Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models",
        "url": "https://www.semanticscholar.org/paper/a2fb135fc4bfa323bc92dd498ba45bcaf7259a02",
        "source_type": "technical_paper",
        "authors": [
          "Alberto Purpura",
          "Sahil Wadhwa",
          "Jesse Zymet",
          "Akshay Gupta",
          "Andy Luo",
          "Melissa Kazemi Rad",
          "Swapnil Shinde",
          "M. Sorower"
        ],
        "description": "Comprehensive overview of red teaming methodologies for building safe generative AI applications"
      },
      {
        "title": "Effective Automation to Support the Human Infrastructure in AI Red Teaming",
        "url": "https://www.semanticscholar.org/paper/c42dcb3a795f970d657ee46537553634eea2b014",
        "source_type": "technical_paper",
        "authors": [
          "Alice Qian Zhang",
          "Jina Suh",
          "Mary L. Gray",
          "Hong Shen"
        ],
        "description": "Research on automation tools and processes to enhance human-led red teaming efforts in AI systems"
      },
      {
        "title": "Trojan Activation Attack: Red-Teaming Large Language Models using Steering Vectors for Safety-Alignment",
        "url": "https://www.semanticscholar.org/paper/598df44f1d21a5d1fe3940c0bb2a6128a62c1c15",
        "source_type": "technical_paper",
        "authors": [
          "Haoran Wang",
          "Kai Shu"
        ],
        "description": "Technical paper on using steering vectors to conduct Trojan activation attacks as part of red teaming safety-aligned LLMs"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      "internal-review-boards",
      "human-in-the-loop-safeguards",
      "confidence-thresholding",
      "runtime-monitoring-and-circuit-breakers"
    ]
  },
  {
    "slug": "anomaly-detection",
    "name": "Anomaly Detection",
    "description": "Anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. Applied to AI/ML systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. By establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/safety/monitoring/anomaly-detection",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Monitoring a content moderation AI system to detect when it starts flagging significantly more or fewer posts than usual, which could indicate model drift, adversarial attacks, or changes in user behaviour patterns that require immediate investigation to prevent harmful content from appearing.",
        "goal": "Safety"
      },
      {
        "description": "Implementing anomaly detection on a medical diagnosis AI to identify when prediction confidence scores or feature importance patterns deviate from historical norms, helping catch model degradation or data quality issues that could lead to misdiagnoses before patients are affected.",
        "goal": "Reliability"
      },
      {
        "description": "Deploying anomaly detection on a hiring algorithm to monitor for unusual patterns in how candidates from different demographic groups are scored or rejected, enabling early detection of emerging bias issues or attempts to game the system through demographic manipulation.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Setting appropriate sensitivity thresholds is challenging and requires domain expertise, as overly sensitive settings generate excessive false alarms whilst conservative settings may miss genuine anomalies."
      },
      {
        "description": "May generate false positives for legitimate edge cases or rare but valid system behaviours, potentially causing unnecessary alerts and disrupting normal operations."
      },
      {
        "description": "Limited effectiveness against novel or sophisticated attacks that deliberately mimic normal patterns or gradually shift behaviour to avoid detection thresholds."
      },
      {
        "description": "Requires substantial historical data to establish reliable baselines of normal behaviour, and may struggle with systems that have naturally high variability or seasonal patterns."
      },
      {
        "description": "Detection lag can occur between when an anomaly begins and when it exceeds detection thresholds, potentially allowing harmful behaviour to persist during the detection window."
      }
    ],
    "resources": [
      {
        "title": "Anomaly Detection Toolkit (ADTK)",
        "url": "https://adtk.readthedocs.io/en/stable/",
        "source_type": "software_package",
        "description": "Python library for unsupervised and rule-based time series anomaly detection with unified APIs, flexible algorithm combination, and support for feature engineering and ensemble methods"
      },
      {
        "title": "TimeEval: Time Series Anomaly Detection Evaluation Framework",
        "url": "https://timeeval.readthedocs.io/",
        "source_type": "software_package",
        "description": "Comprehensive evaluation tool for comparing time series anomaly detection algorithms across multiple datasets with standardized metrics and distributed execution support"
      },
      {
        "title": "DeepOD: Deep Learning for Outlier Detection",
        "url": "https://deepod.readthedocs.io/",
        "source_type": "software_package",
        "description": "Python library featuring 27 deep learning algorithms for tabular and time-series anomaly detection with unified APIs and diverse network architectures including LSTM, GRU, TCN, and Transformer"
      },
      {
        "title": "A Beginner's Guide to Anomaly Detection Techniques in Data Science",
        "url": "https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html",
        "source_type": "tutorial",
        "description": "Beginner-friendly introduction covering Isolation Forest, Local Outlier Factor, and Autoencoder techniques with explanations of point, contextual, and collective anomaly types"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      "out-of-distribution-detector-for-neural-networks"
    ]
  },
  {
    "slug": "human-in-the-loop-safeguards",
    "name": "Human-in-the-Loop Safeguards",
    "description": "Human-in-the-loop safeguards establish systematic checkpoints where human experts review, validate, or override AI/ML system decisions before they take effect. This governance approach combines automated efficiency with human judgement by defining clear intervention criteria (such as uncertainty thresholds, risk levels, or sensitive contexts) that trigger mandatory human oversight. By incorporating domain expertise, ethical considerations, and contextual understanding that machines may lack, these safeguards help ensure that critical decisions maintain appropriate human accountability whilst preserving the benefits of automated processing for routine cases.",
    "assurance_goals": [
      "Safety",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "expertise-needed/domain-knowledge",
      "expertise-needed/stakeholder-engagement",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "example_use_cases": [
      {
        "description": "Implementing mandatory human physician review for any medical AI diagnostic recommendation before treatment decisions are made, especially for complex cases or when the system confidence is below established thresholds, ensuring patient safety through expert oversight.",
        "goal": "Safety"
      },
      {
        "description": "Requiring human review of automated loan approval decisions when applicants request explanations or appeal rejections, allowing human underwriters to provide clear reasoning and ensure customers understand the decision-making process behind their application outcomes.",
        "goal": "Transparency"
      },
      {
        "description": "Mandating human oversight when hiring algorithms flag candidates from underrepresented groups for rejection, enabling recruiters to verify that decisions are based on legitimate job-relevant criteria rather than potential algorithmic bias, and providing fair recourse mechanisms.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Scales poorly with high request volumes, creating bottlenecks that can delay critical decisions and potentially overwhelm human reviewers with excessive workload."
      },
      {
        "description": "Introduces significant latency into automated processes, potentially making time-sensitive applications impractical or reducing user satisfaction with slower response times."
      },
      {
        "description": "Human reviewers may experience decision fatigue, leading to decreased attention quality over time and potential inconsistency in review standards across different cases or time periods."
      },
      {
        "description": "Risk of automation bias where humans defer too readily to AI recommendations rather than providing meaningful independent review, undermining the safeguard's effectiveness."
      },
      {
        "description": "Requires significant ongoing investment in human resources, training, and expertise maintenance, making it expensive to implement and sustain across large-scale systems."
      }
    ],
    "resources": [
      {
        "title": "Human-in-the-Loop AI: A Comprehensive Guide",
        "url": "https://www.holisticai.com/blog/human-in-the-loop-ai",
        "source_type": "tutorial",
        "description": "Comprehensive guide covering HITL AI collaborative approach, including human oversight throughout AI lifecycle, bias mitigation, ethical alignment, and applications across healthcare, manufacturing, and finance"
      },
      {
        "title": "Improving the Applicability of AI for Psychiatric Applications through Human-in-the-loop Methodologies",
        "url": "https://core.ac.uk/download/544064129.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Chandler, Chelsea",
          "Elvev\u00e5g, Brita",
          "Foltz, Peter W."
        ],
        "publication_date": "2022-01-01",
        "description": "Technical paper exploring HITL methodologies for psychiatric AI applications, focusing on improving applicability and clinical effectiveness through human oversight integration"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      "internal-review-boards",
      "red-teaming",
      "confidence-thresholding",
      "runtime-monitoring-and-circuit-breakers"
    ]
  },
  {
    "slug": "confidence-thresholding",
    "name": "Confidence Thresholding",
    "description": "Confidence thresholding creates decision boundaries based on model uncertainty scores, routing predictions into different handling workflows depending on their confidence levels. High-confidence predictions (e.g., above 95%) proceed automatically, whilst medium-confidence cases (e.g., 70-95%) may trigger additional validation or human review, and low-confidence predictions (below 70%) receive extensive oversight or default to conservative fallback actions. This technique enables organisations to maintain automated efficiency for clear-cut cases whilst ensuring appropriate human intervention for uncertain decisions, balancing operational speed with risk management across safety-critical applications.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Implementing tiered confidence thresholds in autonomous vehicle decision-making where high-confidence lane changes (>98%) execute automatically, medium-confidence decisions (85-98%) trigger additional sensor verification, and low-confidence situations (<85%) engage conservative defensive driving modes or request human takeover.",
        "goal": "Safety"
      },
      {
        "description": "Deploying confidence thresholding in fraud detection systems where high-confidence legitimate transactions (>90%) process immediately, medium-confidence cases (70-90%) undergo additional automated checks, and low-confidence transactions (<70%) require human analyst review, ensuring system reliability through graduated response mechanisms.",
        "goal": "Reliability"
      },
      {
        "description": "Using confidence thresholds in automated loan decisions to provide clear explanations to applicants, where high-confidence approvals include simple explanations, medium-confidence decisions provide detailed reasoning about key factors, and low-confidence cases receive comprehensive explanations with guidance on potential improvements.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Many models produce poorly calibrated confidence scores that don't accurately reflect true prediction uncertainty, leading to overconfident predictions for incorrect outputs or underconfident scores for correct predictions."
      },
      {
        "description": "Threshold selection requires careful calibration and domain expertise, as inappropriate thresholds can either overwhelm human reviewers with too many cases or miss genuinely uncertain decisions that need oversight."
      },
      {
        "description": "High-confidence predictions may still be incorrect or harmful, particularly when models encounter adversarial inputs, out-of-distribution data, or systematic biases that the confidence mechanism doesn't detect."
      },
      {
        "description": "Static thresholds may become inappropriate over time as model performance degrades, data distribution shifts occur, or operational contexts change, requiring ongoing monitoring and adjustment."
      },
      {
        "description": "Implementation complexity increases significantly when managing multiple confidence levels and routing mechanisms, potentially introducing system failures or inconsistencies in how different confidence ranges are handled."
      }
    ],
    "resources": [
      {
        "title": "A Novel Dynamic Confidence Threshold Estimation AI Algorithm for Enhanced Object Detection",
        "url": "https://www.semanticscholar.org/paper/93cda7adfa043c969639e094d6c27b1c4d507208",
        "source_type": "technical_paper",
        "authors": [
          "Mounika Thatikonda",
          "M. Pk",
          "Fathi H. Amsaad"
        ]
      },
      {
        "title": "Improving speech recognition accuracy with multi-confidence thresholding",
        "url": "https://www.semanticscholar.org/paper/bef1c8668115675f786e5a3c6d165f268e399e9d",
        "source_type": "technical_paper",
        "authors": [
          "Shuangyu Chang"
        ]
      },
      {
        "title": "Improving the Robustness and Generalization of Deep Neural Network with Confidence Threshold Reduction",
        "url": "http://arxiv.org/pdf/2206.00913v2",
        "source_type": "technical_paper",
        "authors": [
          "Xiangyuan Yang",
          "Jie Lin",
          "Hanlin Zhang",
          "Xinyu Yang",
          "Peng Zhao"
        ],
        "publication_date": "2022-06-02"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      "internal-review-boards",
      "red-teaming",
      "human-in-the-loop-safeguards",
      "runtime-monitoring-and-circuit-breakers"
    ]
  },
  {
    "slug": "runtime-monitoring-and-circuit-breakers",
    "name": "Runtime Monitoring and Circuit Breakers",
    "description": "Runtime monitoring and circuit breakers establish continuous surveillance of AI/ML systems in production, tracking critical metrics such as prediction accuracy, response times, input characteristics, output distributions, and system resource usage. When monitored parameters exceed predefined safety thresholds or exhibit anomalous patterns, automated circuit breakers immediately trigger protective actions including request throttling, service degradation, system shutdown, or failover to backup mechanisms. This approach provides real-time defensive capabilities that prevent cascading failures, ensure consistent service reliability, and maintain transparent operation status for stakeholders monitoring system health.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/safety/monitoring/anomaly-detection",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Implementing circuit breakers in a medical AI system that automatically halt diagnosis recommendations if prediction confidence drops below 85%, error rates exceed 2%, or response times increase beyond acceptable limits, preventing potentially harmful misdiagnoses during system degradation.",
        "goal": "Safety"
      },
      {
        "description": "Deploying runtime monitoring for a recommendation engine that tracks recommendation diversity, click-through rates, and user engagement patterns, automatically switching to simpler algorithms when complex models show signs of performance degradation or unusual behaviour patterns.",
        "goal": "Reliability"
      },
      {
        "description": "Establishing transparent monitoring dashboards for a loan approval system that display real-time metrics on approval rates across demographic groups, processing times, and model confidence levels, enabling stakeholders to verify consistent and fair operation.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Threshold calibration requires extensive domain expertise and historical data analysis, as overly sensitive settings trigger excessive false alarms whilst conservative thresholds may miss genuine system failures."
      },
      {
        "description": "False positive alerts can unnecessarily disrupt service availability and user experience, potentially causing more harm than the issues they aim to prevent, especially in time-sensitive applications."
      },
      {
        "description": "Sophisticated attacks or gradual performance degradation may operate within normal metric ranges, evading detection by staying below established thresholds whilst still causing cumulative damage."
      },
      {
        "description": "Monitoring infrastructure introduces additional complexity and potential failure points, requiring robust implementation to avoid situations where the monitoring system itself becomes a source of system instability."
      },
      {
        "description": "High-frequency monitoring and circuit breaker mechanisms can add computational overhead and latency to system operations, potentially impacting performance in resource-constrained environments."
      }
    ],
    "resources": [
      {
        "title": "aiobreaker: Python Circuit Breaker for Asyncio",
        "url": "https://github.com/arlyon/aiobreaker",
        "source_type": "software_package",
        "description": "Python library implementing the Circuit Breaker design pattern for asyncio applications, preventing system-wide failures by protecting integration points with configurable failure thresholds and reset timeouts"
      },
      {
        "title": "Improving Alignment and Robustness with Circuit Breakers",
        "url": "https://arxiv.org/html/2406.04313v4",
        "source_type": "technical_paper",
        "authors": [
          "Andy Zou"
        ],
        "description": "Research paper introducing circuit breakers for AI safety that directly interrupt harmful model representations during generation, significantly reducing attack success rates while maintaining model capabilities"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "internal-review-boards",
      "red-teaming",
      "human-in-the-loop-safeguards",
      "confidence-thresholding"
    ]
  },
  {
    "slug": "model-cards",
    "name": "Model Cards",
    "description": "Model cards are standardised documentation frameworks that systematically document machine learning models through structured templates. The templates cover intended use cases, performance metrics across different demographic groups and operating conditions, training data characteristics, evaluation procedures, limitations, and ethical considerations. They serve as comprehensive technical specifications that enable informed model selection, prevent inappropriate deployment, support regulatory compliance, and facilitate fair assessment by providing transparent reporting of model capabilities and constraints across diverse populations and scenarios.",
    "assurance_goals": [
      "Transparency",
      "Fairness",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "assurance-goal-category/transparency/documentation/model-card",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/regulatory-compliance",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Documenting a medical diagnosis AI with detailed performance metrics across different patient demographics, age groups, and clinical conditions, enabling healthcare providers to understand when the model should be trusted and when additional expert consultation is needed for patient safety.",
        "goal": "Safety"
      },
      {
        "description": "Creating comprehensive model cards for hiring algorithms that transparently report performance differences across demographic groups, helping HR departments identify potential bias issues and ensure equitable candidate evaluation processes.",
        "goal": "Fairness"
      },
      {
        "description": "Publishing detailed model documentation for a credit scoring API that clearly describes training data sources, evaluation methodologies, and performance limitations, enabling financial institutions to make informed decisions about model deployment and regulatory compliance.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Creating comprehensive model cards requires substantial time, expertise, and resources to gather performance data across diverse conditions and demographic groups, potentially delaying model deployment timelines."
      },
      {
        "description": "Information can become outdated quickly as models are retrained, updated, or deployed in new contexts, requiring ongoing maintenance and version control to remain accurate and useful."
      },
      {
        "description": "Organisations may provide incomplete or superficial documentation to avoid revealing competitive advantages or potential liabilities, undermining the transparency goals of model cards."
      },
      {
        "description": "Lack of standardised formats and enforcement mechanisms means model card quality and completeness vary significantly across different organisations and use cases."
      },
      {
        "description": "Technical complexity of documenting model behaviour across all relevant dimensions may exceed the expertise of some development teams, leading to gaps in critical information."
      }
    ],
    "resources": [
      {
        "title": "Model Cards for Model Reporting",
        "url": "http://arxiv.org/pdf/1810.03993v2",
        "source_type": "technical_paper",
        "authors": [
          "Margaret Mitchell",
          "Simone Wu",
          "Andrew Zaldivar",
          "Parker Barnes",
          "Lucy Vasserman",
          "Ben Hutchinson",
          "Elena Spitzer",
          "Inioluwa Deborah Raji",
          "Timnit Gebru"
        ],
        "publication_date": "2018-10-05",
        "description": "Foundational paper introducing model cards as a framework for transparent model reporting and responsible AI documentation"
      },
      {
        "title": "Model Card Guidebook",
        "url": "https://huggingface.co/docs/hub/en/model-card-guidebook",
        "source_type": "tutorial",
        "description": "Comprehensive guide providing updated model card templates, creator tools, and practical insights for implementing model documentation across diverse stakeholder needs"
      },
      {
        "title": "scikit-learn model cards documentation",
        "url": "https://skops.readthedocs.io/en/stable/auto_examples/plot_model_card.html",
        "source_type": "tutorial",
        "description": "Practical tutorial demonstrating how to create comprehensive model cards for scikit-learn models using the skops library with metrics, visualisations, and metadata"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      "datasheets-for-datasets",
      "data-version-control",
      "automated-documentation-generation"
    ]
  },
  {
    "slug": "datasheets-for-datasets",
    "name": "Datasheets for Datasets",
    "description": "Datasheets for datasets establish comprehensive documentation standards for datasets, systematically recording creation methodology, data composition, collection procedures, preprocessing transformations, intended applications, potential biases, privacy considerations, and maintenance protocols. These structured documents enhance dataset transparency by providing essential context for appropriate usage, enabling informed decisions about dataset suitability for specific tasks, supporting bias detection and mitigation efforts, ensuring compliance with data protection regulations, and promoting responsible data stewardship throughout the entire data lifecycle from collection to disposal.",
    "assurance_goals": [
      "Transparency",
      "Fairness",
      "Privacy"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/privacy",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/domain-knowledge",
      "expertise-needed/regulatory-compliance",
      "lifecycle-stage/data-handling",
      "lifecycle-stage/data-handling/collection",
      "lifecycle-stage/data-handling/preparation",
      "technique-type/documentation"
    ],
    "example_use_cases": [
      {
        "description": "Documenting a medical imaging dataset with detailed information about patient privacy protections, anonymisation procedures, and data sharing constraints to ensure sensitive health information is handled appropriately and regulatory compliance is maintained.",
        "goal": "Privacy"
      },
      {
        "description": "Creating comprehensive datasheets for recruitment datasets that document demographic representation across different job categories, helping developers identify potential bias in training data and develop more equitable hiring algorithms.",
        "goal": "Fairness"
      },
      {
        "description": "Establishing transparent documentation for financial transaction datasets that clearly describes data collection methodology, preprocessing steps, and intended use cases, enabling researchers to make informed decisions about dataset appropriateness for their specific applications.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Creating thorough datasheets requires significant time investment and domain expertise to properly document collection methods, biases, and ethical considerations, potentially delaying dataset release or publication."
      },
      {
        "description": "Information may become outdated as datasets undergo preprocessing, cleaning, or augmentation, requiring ongoing maintenance to ensure documentation accuracy throughout the data lifecycle."
      },
      {
        "description": "Absence of standardised templates and enforcement mechanisms leads to inconsistent documentation quality and completeness across different organisations and research communities."
      },
      {
        "description": "Dataset creators may intentionally omit sensitive information about collection methods, participant consent, or potential biases to avoid legal liability or competitive disadvantage."
      },
      {
        "description": "Limited adoption and awareness means many existing datasets lack proper documentation, creating gaps in the historical record and making legacy dataset assessment difficult."
      }
    ],
    "resources": [
      {
        "title": "Datasheets for Datasets",
        "url": "https://arxiv.org/abs/1803.09010",
        "source_type": "technical_paper",
        "authors": [
          "Timnit Gebru",
          "Jamie Morgenstern",
          "Briana Vecchione",
          "Jennifer Wortman Vaughan",
          "Hanna Wallach",
          "Hal Daum\u00e9 III",
          "Kate Crawford"
        ],
        "publication_date": "2018-03-23",
        "description": "Foundational paper proposing standardised documentation for machine learning datasets to facilitate transparency, accountability, and better communication between dataset creators and consumers"
      },
      {
        "title": "Datasheets for AI and medical datasets (DAIMS): a data validation and documentation framework before machine learning analysis in medical research",
        "url": "http://arxiv.org/pdf/2501.14094v1",
        "source_type": "technical_paper",
        "authors": [
          "Ramtin Zargari Marandi",
          "Anne Svane Frahm",
          "Maja Milojevic"
        ],
        "publication_date": "2025-01-23",
        "description": "Recent framework extending datasheets specifically for medical AI datasets, providing validation and documentation standards for healthcare machine learning research"
      },
      {
        "title": "MT-Adapted Datasheets for Datasets: Template and Repository",
        "url": "http://arxiv.org/pdf/2005.13156v1",
        "source_type": "technical_paper",
        "authors": [
          "Marta R. Costa-juss\u00e0",
          "Roger Creus",
          "Oriol Domingo",
          "Albert Dom\u00ednguez",
          "Miquel Escobar",
          "Cayetana L\u00f3pez",
          "Marina Garcia",
          "Margarita Geleta"
        ],
        "publication_date": "2020-05-27"
      },
      {
        "title": "Understanding Machine Learning Practitioners' Data Documentation Perceptions, Needs, Challenges, and Desiderata",
        "url": "http://arxiv.org/pdf/2206.02923v2",
        "source_type": "technical_paper",
        "authors": [
          "Amy K. Heger",
          "Liz B. Marquis",
          "Mihaela Vorvoreanu",
          "Hanna Wallach",
          "Jennifer Wortman Vaughan"
        ],
        "publication_date": "2022-06-06"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      "model-cards",
      "mlflow-experiment-tracking",
      "data-version-control",
      "automated-documentation-generation"
    ]
  },
  {
    "slug": "mlflow-experiment-tracking",
    "name": "MLflow Experiment Tracking",
    "description": "MLflow is an open-source platform that tracks machine learning experiments by automatically logging parameters, metrics, models, and artifacts throughout the ML lifecycle. It provides a centralised repository for comparing different experimental runs, reproducing results, and managing model versions. Teams can track hyperparameters, evaluation metrics, model files, and execution environment details, creating a comprehensive audit trail that supports collaboration, reproducibility, and regulatory compliance across the entire machine learning development process.",
    "assurance_goals": [
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "example_use_cases": [
      {
        "description": "Tracking medical diagnosis model experiments across different hospitals, logging hyperparameters, performance metrics, and model artifacts to ensure reproducible research and enable regulatory audits of model development processes.",
        "goal": "Transparency"
      },
      {
        "description": "Managing fraud detection model versions in production, tracking which specific model configuration and training data version is deployed, enabling quick rollback and performance comparison when system reliability issues arise.",
        "goal": "Reliability"
      },
      {
        "description": "Documenting loan approval model experiments with complete parameter tracking and performance logging across demographic groups, supporting fair lending compliance by providing transparent records of model development and validation processes.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Requires teams to adopt disciplined logging practices and may introduce overhead to development workflows if not properly integrated into existing processes."
      },
      {
        "description": "Storage costs can grow substantially with extensive artifact logging, especially for large models or high-frequency experimentation."
      },
      {
        "description": "Tracking quality depends on developers consistently logging relevant information, with incomplete logging leading to gaps in experimental records."
      },
      {
        "description": "Complex multi-stage pipelines may require custom instrumentation to capture dependencies and data flow relationships effectively."
      },
      {
        "description": "Security and access control configurations require careful setup to protect sensitive model information and experimental data in shared environments."
      }
    ],
    "resources": [
      {
        "title": "MLflow Documentation",
        "url": "https://mlflow.org/docs/latest/index.html",
        "source_type": "documentation",
        "description": "Comprehensive official documentation covering MLflow setup, tracking APIs, model management, and deployment workflows with examples and best practices"
      },
      {
        "title": "mlflow/mlflow",
        "url": "https://github.com/mlflow/mlflow",
        "source_type": "software_package",
        "description": "Official MLflow open-source repository containing the complete platform for ML experiment tracking, model management, and deployment"
      },
      {
        "title": "An MLOps Framework for Explainable Network Intrusion Detection with MLflow",
        "url": "https://ieeexplore.ieee.org/abstract/document/10733700",
        "source_type": "technical_paper",
        "authors": [
          "Vincenzo Spadari",
          "Francesco Cerasuolo",
          "Giampaolo Bovenzi",
          "Antonio Pescap\u00e8"
        ],
        "publication_date": "2024-06-26",
        "description": "Research paper demonstrating MLflow framework application for managing machine learning pipelines in network intrusion detection, covering experiment tracking, model deployment, and monitoring across security datasets"
      },
      {
        "title": "MLflow Tutorial - Machine Learning Lifecycle Management",
        "url": "https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html",
        "source_type": "tutorial",
        "description": "Step-by-step tutorial demonstrating MLflow experiment tracking, model packaging, and deployment using real machine learning examples"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      "model-cards",
      "datasheets-for-datasets",
      "data-version-control",
      "automated-documentation-generation"
    ]
  },
  {
    "slug": "data-version-control",
    "name": "Data Version Control",
    "description": "Data Version Control (DVC) is a Git-like version control system specifically designed for machine learning data, models, and experiments. It tracks changes to large data files, maintains reproducible ML pipelines, and creates a complete audit trail of data transformations, model training, and evaluation processes. DVC works alongside Git to provide end-to-end lineage tracking from raw data through preprocessing, training, and deployment, enabling teams to reproduce any model version and understand exactly how datasets evolved throughout the ML lifecycle.",
    "assurance_goals": [
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/data-handling",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "example_use_cases": [
      {
        "description": "Tracking medical imaging dataset versions and model training pipelines to ensure reproducible research results, enabling hospitals to verify which specific data version and preprocessing steps were used for regulatory submissions.",
        "goal": "Transparency"
      },
      {
        "description": "Managing credit scoring model data pipelines with complete version control of training datasets, feature engineering steps, and model artifacts, ensuring reliable model reproduction and rollback capabilities when performance issues arise.",
        "goal": "Reliability"
      },
      {
        "description": "Maintaining pharmaceutical drug discovery data lineage across multiple research teams, tracking compound datasets, feature extraction processes, and model versions to support FDA submissions with complete experimental provenance.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Requires learning Git-like workflows and CLI commands, which may have a steep learning curve for teams unfamiliar with version control systems."
      },
      {
        "description": "Storage costs can be substantial for large datasets with frequent changes, especially when maintaining multiple versions and branches of data."
      },
      {
        "description": "Complex data pipelines with many interdependencies may require significant setup time and careful configuration to track properly."
      },
      {
        "description": "Performance can degrade with very large files or datasets due to checksumming and synchronisation overhead during operations."
      },
      {
        "description": "Team coordination becomes essential as improper branch management or merge conflicts can disrupt collaborative workflows."
      }
    ],
    "resources": [
      {
        "title": "DVC Documentation",
        "url": "https://dvc.org/doc",
        "source_type": "documentation",
        "description": "Comprehensive official documentation covering DVC installation, data versioning, pipeline creation, and collaborative workflows with tutorials and best practices"
      },
      {
        "title": "iterative/dvc",
        "url": "https://github.com/iterative/dvc",
        "source_type": "software_package",
        "description": "Official DVC open-source repository containing the complete data version control system for machine learning with Git integration"
      },
      {
        "title": "DVC Tutorial - Data Version Control for Machine Learning",
        "url": "https://dvc.org/doc/start",
        "source_type": "tutorial",
        "description": "Step-by-step getting started guide demonstrating DVC basics including data tracking, pipeline creation, and experiment management"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "acronym": "DVC",
    "related_techniques": [
      "model-cards",
      "datasheets-for-datasets",
      "mlflow-experiment-tracking",
      "automated-documentation-generation"
    ]
  },
  {
    "slug": "automated-documentation-generation",
    "name": "Automated Documentation Generation",
    "description": "Automated documentation generation creates and maintains up-to-date documentation using various methods including programmatic scripts, large language models (LLMs), and extraction tools. These approaches can capture model architectures, data schemas, feature importance, performance metrics, API specifications, and lineage information without manual writing. Methods range from traditional code parsing and template-based generation to modern AI-assisted documentation that can understand context and generate human-readable explanations.",
    "assurance_goals": [
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "assurance-goal-category/transparency/documentation",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/software-engineering",
      "lifecycle-stage/deployment",
      "lifecycle-stage/model-development",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Automatically generating comprehensive model cards for a healthcare AI system each time a new version is deployed, including updated performance metrics across demographic groups, data lineage information, and bias evaluation results for regulatory compliance documentation.",
        "goal": "Transparency"
      },
      {
        "description": "Using LLM-powered tools to automatically document complex financial risk models by analysing code, extracting business logic, and generating human-readable explanations of model behaviour for audit trails and stakeholder communication.",
        "goal": "Transparency"
      },
      {
        "description": "Implementing automated API documentation generation for a machine learning platform that extracts endpoint specifications, parameter definitions, and usage examples, ensuring documentation stays synchronised with code changes and reducing deployment errors from outdated documentation.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "AI-generated documentation may miss critical domain context and business logic that human experts would include, potentially leading to incomplete or misleading explanations of model behaviour."
      },
      {
        "description": "Template-based approaches often struggle with unstructured information and complex relationships between code components, limiting their ability to capture nuanced system interactions."
      },
      {
        "description": "Quality heavily depends on code quality and instrumentation comprehensiveness; poorly commented or documented source code will result in inadequate generated documentation."
      },
      {
        "description": "Maintenance overhead can be significant as automated systems require configuration updates when code structures change, and generated content may need human review for accuracy and completeness."
      },
      {
        "description": "LLM-based approaches may introduce hallucinations or inaccuracies, particularly when documenting complex technical details or domain-specific terminology without proper validation mechanisms."
      }
    ],
    "resources": [
      {
        "title": "daynin/fundoc",
        "url": "https://github.com/daynin/fundoc",
        "source_type": "software_package",
        "description": "Language-agnostic documentation generator written in Rust that enables keeping documentation synchronised with code across multiple file types and programming languages."
      },
      {
        "title": "Generative AI for Software Development - DeepLearning.AI",
        "url": "https://www.deeplearning.ai/courses/generative-ai-for-software-development/",
        "source_type": "tutorial",
        "description": "Comprehensive course covering AI-powered documentation techniques including LLM-assisted documentation generation, formatting for automated tools, and improving code documentation quality."
      },
      {
        "title": "Documentation Generator Analysis \u2014 Wiser Documentation",
        "url": "https://chiplicity.readthedocs.io/en/latest/On_Software/DocumentationGenerator.html",
        "source_type": "documentation",
        "description": "Detailed analysis and comparison of documentation generator tools including Sphinx, Doxygen, and other approaches for automated documentation workflows."
      },
      {
        "title": "pyTooling/sphinx-reports",
        "url": "https://github.com/pyTooling/sphinx-reports",
        "source_type": "software_package",
        "description": "Sphinx extension that automatically integrates software development reports (unit tests, coverage, documentation coverage) into documentation as appendix pages."
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "model-cards",
      "datasheets-for-datasets",
      "data-version-control"
    ]
  },
  {
    "slug": "model-distillation",
    "name": "Model Distillation",
    "description": "Model distillation transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) by training the student to mimic the teacher's behaviour. The student learns from the teacher's soft predictions and intermediate representations rather than just hard labels, capturing nuanced decision boundaries and uncertainty. This produces models that are faster, require less memory, and are often more interpretable whilst maintaining much of the original performance. Beyond compression, distillation can improve model reliability by regularising training and enable deployment in resource-constrained environments.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/model-simplification/knowledge-transfer",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/surrogate-models/global-surrogates",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/deployment",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Compressing a large medical diagnosis model into a smaller student model that can run on edge devices in resource-limited clinics, making the decision process more transparent for healthcare professionals whilst maintaining diagnostic accuracy for critical patient care.",
        "goal": "Explainability"
      },
      {
        "description": "Creating a compressed fraud detection model from a complex ensemble teacher that maintains detection performance whilst being more robust to adversarial attacks and data drift, ensuring consistent protection of financial transactions across varying conditions.",
        "goal": "Reliability"
      },
      {
        "description": "Distilling a large autonomous vehicle perception model into a smaller student model that can run with guaranteed inference times and lower computational requirements, ensuring predictable safety-critical decision-making under real-time constraints.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Student models typically achieve 90-95% of teacher performance, creating a trade-off between model efficiency and predictive accuracy that may be unacceptable for high-stakes applications requiring maximum precision."
      },
      {
        "description": "Distillation process can be computationally expensive, requiring extensive teacher model inference during training and careful hyperparameter tuning to balance knowledge transfer with student model capacity."
      },
      {
        "description": "Knowledge transfer quality depends heavily on teacher-student architecture compatibility and the chosen distillation objectives, with mismatched designs potentially leading to ineffective learning or mode collapse."
      },
      {
        "description": "Student models may inherit teacher model biases and vulnerabilities whilst potentially introducing new failure modes, requiring separate validation for fairness, robustness, and safety properties."
      },
      {
        "description": "Compressed models may lack the teacher's capability to handle edge cases or out-of-distribution inputs, potentially creating safety risks when deployed in environments different from the training distribution."
      }
    ],
    "resources": [
      {
        "title": "airaria/TextBrewer",
        "url": "https://github.com/airaria/TextBrewer",
        "source_type": "software_package",
        "description": "PyTorch-based knowledge distillation toolkit for natural language processing with support for transformer models, flexible distillation strategies, and multi-teacher approaches."
      },
      {
        "title": "Main features \u2014 TextBrewer 0.2.1.post1 documentation",
        "url": "https://textbrewer.readthedocs.io/",
        "source_type": "documentation",
        "description": "Comprehensive documentation for TextBrewer including tutorials, API reference, configuration guides, and experimental results for knowledge distillation in NLP tasks."
      },
      {
        "title": "A Generic Approach for Reproducible Model Distillation",
        "url": "http://arxiv.org/abs/2211.12631",
        "source_type": "technical_paper",
        "authors": [
          "Hooker, Giles",
          "Xu, Peiru",
          "Zhou, Yunzhe"
        ],
        "publication_date": "2023-04-27",
        "description": "Research paper presenting a framework for reproducible knowledge distillation with standardised evaluation protocols and benchmarking across different model architectures and distillation techniques."
      },
      {
        "title": "dkozlov/awesome-knowledge-distillation",
        "url": "https://github.com/dkozlov/awesome-knowledge-distillation",
        "source_type": "software_package",
        "description": "Curated collection of knowledge distillation resources including academic papers, implementation code across multiple frameworks (PyTorch, TensorFlow, Keras), and educational videos."
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 4,
    "related_techniques": [
      "ridge-regression-surrogates",
      "rulefit",
      "model-pruning"
    ]
  },
  {
    "slug": "monotonicity-constraints",
    "name": "Monotonicity Constraints",
    "description": "Monotonicity constraints enforce consistent directional relationships between input features and model predictions, ensuring that increasing a feature value either always increases, always decreases, or has no effect on the output. These constraints integrate domain knowledge into model training, preventing counterintuitive relationships that may arise from spurious correlations in data. By maintaining logical feature relationships (e.g., experience always positively influences salary), monotonicity constraints enhance model trustworthiness, interpretability, and alignment with business logic whilst often improving generalisation to new data.",
    "assurance_goals": [
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/probabilistic/gaussian-processes",
      "applicable-models/architecture/tree-based",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/domain-knowledge",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Enforcing that a mortgage approval model always treats higher income, longer employment history, and higher credit scores as positive factors, making the decision logic transparent and intuitive for loan officers and applicants whilst preventing counterintuitive relationships that could undermine trust in the system.",
        "goal": "Transparency"
      },
      {
        "description": "Constraining a healthcare cost prediction model so that age and number of chronic conditions always increase predicted costs, ensuring the model generalises reliably to new patient populations and maintains logical behaviour even when training data contains sampling biases or unusual correlations.",
        "goal": "Reliability"
      },
      {
        "description": "Implementing monotonic constraints in an insurance premium model where driving experience always reduces premiums and accident history always increases them, creating consistent pricing logic that regulatory authorities can easily validate and customers can understand and trust.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Can reduce model accuracy when real-world relationships are inherently non-monotonic, such as the inverted-U relationship between experience and performance, where constraints force oversimplified linear relationships."
      },
      {
        "description": "Requires substantial domain expertise to identify which features should have monotonic relationships, creating dependency on subject matter experts and potential for incorrect constraint specification."
      },
      {
        "description": "Increases computational complexity during training as optimisation algorithms must respect additional constraints, potentially leading to longer training times and convergence difficulties."
      },
      {
        "description": "May mask important non-linear patterns in data that could be crucial for understanding system behaviour, particularly in exploratory analysis where discovering unexpected relationships is valuable."
      },
      {
        "description": "Limited applicability to certain model types, with implementation varying significantly across algorithms (well-supported in tree-based models, more complex in neural networks), restricting technique flexibility."
      }
    ],
    "resources": [
      {
        "title": "Monotonic Constraints \u2014 xgboost 3.1.0-dev documentation",
        "url": "https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html",
        "source_type": "documentation",
        "description": "Comprehensive tutorial on implementing monotonic constraints in XGBoost, including parameter configuration, practical examples, and visual demonstrations of constraint effects on model predictions."
      },
      {
        "title": "NONPARAMETRIC KERNEL REGRESSION SUBJECT TO MONOTONICITY CONSTRAINTS",
        "url": "https://www.semanticscholar.org/paper/28e2be532d66694d3fe3486671f5c0217f58892d",
        "source_type": "technical_paper",
        "authors": [
          "P. Hall",
          "Li-Shan Huang"
        ],
        "description": "Foundational research paper on implementing monotonicity constraints in nonparametric kernel regression methods, providing theoretical background and algorithmic approaches for enforcing monotonic relationships."
      },
      {
        "title": "scikit-learn Isotonic Regression",
        "url": "https://scikit-learn.org/stable/modules/isotonic.html",
        "source_type": "documentation",
        "description": "Documentation for scikit-learn's isotonic regression implementation, providing alternative approach to monotonic relationships through non-parametric regression that preserves monotonic order."
      },
      {
        "title": "High-dimensional additive Gaussian processes under monotonicity constraints",
        "url": "https://www.semanticscholar.org/paper/4d4f1e2de3742735dcc47d2e51cc572a4415231e",
        "source_type": "technical_paper",
        "authors": [
          "Andr\u00e9s F. L\u00f3pez-Lopera",
          "F. Bachoc",
          "O. Roustant"
        ],
        "description": "Advanced research on extending monotonicity constraints to high-dimensional Gaussian process models, addressing scalability challenges and additive model structures for complex constraint applications."
      },
      {
        "title": "cagrell/gp_constr",
        "url": "https://github.com/cagrell/gp_constr",
        "source_type": "software_package",
        "description": "Python implementation of Gaussian process regression with linear operator constraints including boundedness and monotonicity, featuring RBF and Mat\u00e9rn kernels with practical examples."
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "intrinsically-interpretable-models",
      "generalized-additive-models"
    ]
  },
  {
    "slug": "intrinsically-interpretable-models",
    "name": "Intrinsically Interpretable Models",
    "description": "Intrinsically interpretable models are machine learning algorithms that are transparent by design, allowing users to understand their decision-making process without requiring additional explanation techniques. This category includes decision trees and rule lists (which use if-then logic), linear and logistic regression models (which use weighted feature combinations), and other simple algorithms where the model structure itself provides interpretability. These models prioritise transparency over complexity, making them ideal when stakeholder understanding and regulatory compliance are paramount.",
    "assurance_goals": [
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/linear-models",
      "applicable-models/architecture/tree-based",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/structured-output",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Developing a medical diagnosis support system using a decision tree with clear if-then rules based on symptoms and test results, allowing healthcare professionals to trace the reasoning path and explain diagnoses to patients whilst ensuring clinical transparency and accountability.",
        "goal": "Transparency"
      },
      {
        "description": "Creating a fraud detection model using logistic regression with carefully selected features (transaction amount, location, time patterns) where each coefficient's contribution can be understood and validated, ensuring reliable performance that financial institutions can audit and regulatory bodies can approve.",
        "goal": "Reliability"
      },
      {
        "description": "Implementing a hiring decision support tool using rule lists that explicitly state qualification criteria and scoring logic, providing transparent candidate evaluation that can be explained to applicants and reviewed for fairness whilst meeting legal requirements for employment decision documentation.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Generally achieve lower predictive accuracy than complex models (neural networks, ensembles) for difficult problems involving high-dimensional data, non-linear relationships, or complex feature interactions."
      },
      {
        "description": "Linear models cannot capture non-linear relationships or feature interactions without manual feature engineering, limiting their applicability to inherently non-linear domains like image recognition or natural language processing."
      },
      {
        "description": "Decision trees can become unstable with small changes in training data, potentially leading to completely different tree structures and predictions, affecting model reliability in dynamic environments."
      },
      {
        "description": "Deep decision trees may lose interpretability despite being inherently transparent, as human cognitive limits make it difficult to follow complex branching logic with many levels and conditions."
      },
      {
        "description": "Feature selection becomes critical for maintaining interpretability, requiring domain expertise to identify the most relevant variables whilst potentially missing important but subtle predictive signals."
      }
    ],
    "resources": [
      {
        "title": "scikit-learn Decision Trees",
        "url": "https://scikit-learn.org/stable/modules/tree.html",
        "source_type": "documentation",
        "description": "Comprehensive documentation for decision tree implementation in scikit-learn, including classification and regression trees with interpretability guidelines and visualisation tools."
      },
      {
        "title": "scikit-learn Linear Models",
        "url": "https://scikit-learn.org/stable/modules/linear_model.html",
        "source_type": "documentation",
        "description": "Complete guide to linear and logistic regression models in scikit-learn, covering implementation, feature selection, and coefficient interpretation for transparent modeling."
      },
      {
        "title": "Interpretable Machine Learning",
        "url": "https://christophm.github.io/interpretable-ml-book/",
        "source_type": "tutorial",
        "description": "Open-source book providing comprehensive coverage of interpretable machine learning models including decision trees, linear models, and rule-based systems with practical examples."
      },
      {
        "title": "R package 'rpart' for Recursive Partitioning",
        "url": "https://cran.r-project.org/web/packages/rpart/index.html",
        "source_type": "software_package",
        "description": "R implementation of recursive partitioning for classification, regression and survival trees with extensive documentation and plotting capabilities for interpretable tree models."
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      "monotonicity-constraints",
      "generalized-additive-models"
    ]
  },
  {
    "slug": "generalized-additive-models",
    "name": "Generalized Additive Models",
    "description": "An intrinsically interpretable modelling technique that extends linear models by allowing flexible, nonlinear relationships between individual features and the target whilst maintaining the additive structure that preserves transparency. Each feature's effect is modelled separately as a smooth function, visualised as a curve showing how the feature influences predictions across its range. GAMs achieve this through spline functions or other smoothing techniques that capture complex patterns in individual variables without interactions, making them particularly valuable for domains requiring both predictive accuracy and model interpretability.",
    "assurance_goals": [
      "Transparency",
      "Explainability",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/linear-models/gam",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/property/sparsity",
      "assurance-goal-category/explainability/surrogate-models/global-surrogates",
      "assurance-goal-category/explainability/visualization-methods/feature-relationships",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "evidence-type/visualisation",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Predicting hospital readmission risk with a GAM that provides transparent, auditable risk assessments by showing how readmission probability varies nonlinearly with patient age, blood pressure, and medication adherence, enabling clinicians to understand and trust the model's reasoning for regulatory compliance.",
        "goal": "Transparency"
      },
      {
        "description": "Building a credit scoring model that explains loan decisions to applicants by visualising how income, credit history, and debt-to-income ratio individually affect approval likelihood, providing clear feature attributions that satisfy fair lending requirements and regulatory explainability mandates.",
        "goal": "Explainability"
      },
      {
        "description": "Developing an environmental monitoring system that reliably predicts air quality using GAMs to model the smooth, nonlinear relationships between weather variables, ensuring stable predictions across seasonal variations whilst maintaining interpretable relationships that environmental scientists can validate.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Cannot capture complex interactions between features unless explicitly modelled, limiting their ability to represent relationships where variables influence each other."
      },
      {
        "description": "Setup requires domain expertise to decide which features need nonlinear treatment and appropriate smoothing parameters, making model specification more challenging than linear models."
      },
      {
        "description": "Fitting process is computationally more expensive than linear models, particularly for large datasets with many features requiring smoothing."
      },
      {
        "description": "Risk of overfitting individual feature relationships if smoothing parameters are not properly regularised, potentially reducing generalisation performance."
      },
      {
        "description": "Interpretation complexity increases with the number of nonlinear features, as understanding multiple smooth curves simultaneously becomes cognitively demanding."
      }
    ],
    "resources": [
      {
        "title": "Generalized Additive Models",
        "url": "https://hastie.su.domains/Papers/gam.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Trevor Hastie",
          "Robert Tibshirani"
        ],
        "publication_date": "1986-01-01"
      },
      {
        "title": "pyGAM: Generalized Additive Models in Python",
        "url": "https://github.com/dswah/pyGAM",
        "source_type": "software_package"
      },
      {
        "title": "mgcv: Mixed GAM Computation Vehicle with Automatic Smoothness Estimation",
        "url": "https://cran.r-project.org/web/packages/mgcv/index.html",
        "source_type": "software_package"
      },
      {
        "title": "A Tour of pyGAM \u2014 pyGAM documentation",
        "url": "https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html",
        "source_type": "tutorial"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "acronym": "GAMs",
    "related_techniques": [
      "monotonicity-constraints",
      "intrinsically-interpretable-models"
    ]
  },
  {
    "slug": "model-pruning",
    "name": "Model Pruning",
    "description": "Model pruning systematically removes less important weights, neurons, or entire layers from neural networks to create smaller, more efficient models whilst maintaining performance. This process involves iterative removal based on importance criteria (weight magnitudes, gradient information, activation patterns) followed by fine-tuning. Pruning can be structured (removing entire neurons/channels) or unstructured (removing individual weights), with structured pruning providing greater computational benefits and interpretability through simplified architectures.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/model-simplification/pruning",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/sparsity",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-model-internals",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-optimization",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Compressing a medical imaging model from 100MB to 15MB for deployment on edge devices in remote clinics, enabling healthcare professionals to audit the remaining critical feature detectors and understand which anatomical patterns drive diagnoses whilst maintaining diagnostic accuracy.",
        "goal": "Explainability"
      },
      {
        "description": "Pruning a financial fraud detection model by 70% to eliminate redundant pathways that amplify noise, creating a more robust system that maintains consistent predictions across different transaction types and reduces false positives during market volatility.",
        "goal": "Reliability"
      },
      {
        "description": "Reducing an autonomous vehicle perception model to ensure predictable inference times under 50ms for safety-critical decisions, removing non-essential neurons to guarantee consistent computational behaviour whilst maintaining object detection accuracy for pedestrian safety.",
        "goal": "Safety"
      }
    ],
    "limitations": [
      {
        "description": "Determining optimal pruning ratios requires extensive experimentation as over-pruning can cause dramatic accuracy degradation, whilst under-pruning provides minimal benefits, making the process time-consuming and resource-intensive."
      },
      {
        "description": "Structured pruning often requires specific hardware or software framework support to realise computational benefits, limiting deployment flexibility and potentially necessitating model architecture changes."
      },
      {
        "description": "Pruned models may exhibit reduced robustness to out-of-distribution inputs or adversarial attacks, as removing neurons can eliminate defensive redundancy that helped handle edge cases."
      },
      {
        "description": "The iterative pruning and fine-tuning process can be computationally expensive, sometimes requiring more resources than training the original model, particularly for large-scale networks."
      },
      {
        "description": "Pruning criteria based on weight magnitudes or gradients may not align with interpretability goals, potentially removing neurons that contribute to model transparency whilst retaining complex, opaque pathways."
      }
    ],
    "resources": [
      {
        "title": "horseee/LLM-Pruner",
        "url": "https://github.com/horseee/LLM-Pruner",
        "source_type": "software_package",
        "description": "Structural pruning tool for large language models supporting Llama, BLOOM, and other LLMs with three-stage compression process requiring only 50,000 training samples for post-training recovery."
      },
      {
        "title": "Pruning Quickstart \u2014 Neural Network Intelligence",
        "url": "https://nni.readthedocs.io/en/stable/tutorials/pruning_quick_start.html",
        "source_type": "tutorial",
        "description": "Step-by-step tutorial for implementing model pruning using Microsoft's NNI toolkit, covering basic usage, pruning algorithms, and practical examples for neural network compression."
      },
      {
        "title": "Overview of NNI Model Pruning \u2014 Neural Network Intelligence",
        "url": "https://nni.readthedocs.io/en/stable/compression/pruning.html",
        "source_type": "documentation",
        "description": "Comprehensive documentation for NNI's pruning capabilities covering structured and unstructured pruning strategies, supported algorithms, and integration with popular deep learning frameworks."
      },
      {
        "title": "coldlarry/YOLOv3-complete-pruning",
        "url": "https://github.com/coldlarry/YOLOv3-complete-pruning",
        "source_type": "software_package",
        "description": "Complete pruning implementation for YOLOv3 object detection models demonstrating computer vision model compression with minimal accuracy loss for real-time inference applications."
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "model-distillation"
    ]
  },
  {
    "slug": "neuron-activation-analysis",
    "name": "Neuron Activation Analysis",
    "description": "Neuron activation analysis examines the firing patterns of individual neurons in neural networks by probing them with diverse inputs and analysing their activation responses. This technique helps understand what concepts, features, or patterns different neurons have learned to recognise, providing insights into the model's internal representations. For large language models, this can reveal neurons specialised for linguistic concepts, semantic categories, or even potentially harmful patterns, enabling targeted interventions and deeper model understanding.",
    "assurance_goals": [
      "Explainability",
      "Safety",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/representation-analysis/concept-identification",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "evidence-type/visualisation",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "lifecycle-stage/monitoring",
      "lifecycle-stage/testing",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Analysing GPT-based models to identify specific neurons that activate on toxic or harmful content, enabling targeted interventions to reduce model toxicity whilst preserving general language capabilities for safer AI deployment.",
        "goal": "Safety"
      },
      {
        "description": "Examining activation patterns in multilingual language models to detect neurons that exhibit systematic biases when processing text from different linguistic communities, revealing implicit representation inequalities that could affect downstream applications.",
        "goal": "Fairness"
      },
      {
        "description": "Investigating individual neurons in medical language models to understand which clinical concepts and medical knowledge representations drive diagnostic suggestions, enabling healthcare professionals to validate the model's medical reasoning pathways.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Many neurons exhibit polysemantic behaviour, representing multiple unrelated concepts simultaneously, making it difficult to assign clear interpretable meanings to individual neural units."
      },
      {
        "description": "Important model behaviours are often distributed across many neurons rather than localised in single units, requiring analysis of neural circuits and interactions that can be exponentially complex."
      },
      {
        "description": "Computational costs scale dramatically with modern large language models containing billions of parameters, making comprehensive neuron-by-neuron analysis prohibitively expensive for complete model understanding."
      },
      {
        "description": "Neuron activation patterns are highly context-dependent, with the same neuron potentially serving different roles based on surrounding input context, complicating consistent interpretation across diverse scenarios."
      },
      {
        "description": "Interpretation of activation patterns often relies on subjective human analysis without rigorous validation methods, potentially leading to confirmation bias or misattribution of neural functions."
      }
    ],
    "resources": [
      {
        "title": "jalammar/ecco",
        "url": "https://github.com/jalammar/ecco",
        "source_type": "software_package"
      },
      {
        "title": "NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models",
        "url": "http://arxiv.org/pdf/2504.21053v1",
        "source_type": "technical_paper",
        "authors": [
          "Yi Zhou",
          "Wenpeng Xing",
          "Dezhang Kong",
          "Changting Lin",
          "Meng Han"
        ],
        "publication_date": "2025-04-29"
      },
      {
        "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron Activation Analysis",
        "url": "http://arxiv.org/pdf/2404.13567v1",
        "source_type": "technical_paper",
        "authors": [
          "Abhilekha Dalal",
          "Rushrukh Rayan",
          "Adrita Barua",
          "Eugene Y. Vasserman",
          "Md Kamruzzaman Sarker",
          "Pascal Hitzler"
        ],
        "publication_date": "2024-04-21"
      },
      {
        "title": "On the Value of Labeled Data and Symbolic Methods for Hidden Neuron\n  Activation Analysis",
        "url": "http://arxiv.org/abs/2404.13567",
        "source_type": "technical_paper",
        "authors": [
          "Barua, Adrita",
          "Dalal, Abhilekha",
          "Hitzler, Pascal",
          "Rayan, Rushrukh",
          "Sarker, Md Kamruzzaman",
          "Vasserman, Eugene Y."
        ],
        "publication_date": "2024-04-21"
      },
      {
        "title": "Ecco",
        "url": "https://ecco.readthedocs.io/",
        "source_type": "documentation"
      },
      {
        "title": "Tracing the Thoughts in Language Models",
        "url": "https://www.anthropic.com/news/tracing-thoughts-language-model",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      "prototype-and-criticism-models",
      "concept-activation-vectors"
    ]
  },
  {
    "slug": "prompt-sensitivity-analysis",
    "name": "Prompt Sensitivity Analysis",
    "description": "Prompt Sensitivity Analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. This technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. It encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. The analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/explains/prediction-confidence",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/uncertainty-analysis/sensitivity-testing",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/experimental-design",
      "expertise-needed/linguistics",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "technique-type/experimental"
    ],
    "example_use_cases": [
      {
        "description": "Testing medical diagnosis LLMs with semantically equivalent but syntactically different symptom descriptions to ensure consistent diagnostic recommendations across different patient communication styles, identifying potential failure modes where slight phrasing changes could lead to dangerous misdiagnoses.",
        "goal": "Safety"
      },
      {
        "description": "Analysing how variations in candidate descriptions (gendered language, cultural references, educational institution prestige indicators) affect LLM-based CV screening recommendations to identify potential discriminatory patterns and ensure equitable treatment across diverse applicant backgrounds.",
        "goal": "Reliability"
      },
      {
        "description": "Examining how different ways of framing financial questions (formal vs informal language, technical vs layperson terminology) affect investment advice generated by LLMs to improve user understanding and model transparency whilst ensuring consistent advisory quality.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Analysis is inherently limited to the specific prompt variations tested, potentially missing important sensitivity patterns that weren't anticipated during study design, making comprehensive coverage challenging."
      },
      {
        "description": "Systematic exploration of prompt variations can be computationally expensive, particularly for large-scale sensitivity analysis across multiple dimensions of variation, requiring significant resources for thorough evaluation."
      },
      {
        "description": "Ensuring that prompt variations maintain semantic equivalence whilst introducing meaningful perturbations requires careful linguistic expertise and validation, which can be subjective and domain-dependent."
      },
      {
        "description": "Results may reveal sensitivity patterns that are difficult to interpret or act upon, particularly when multiple types of variations interact in complex ways, limiting practical applicability of findings."
      },
      {
        "description": "The meaningfulness of sensitivity measurements depends heavily on the choice of baseline prompts and variation strategies, which can introduce methodological biases and affect the generalisability of conclusions."
      }
    ],
    "resources": [
      {
        "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
        "url": "https://www.semanticscholar.org/paper/17a6116e5bbd8b87082cbb2e795885567300c483",
        "source_type": "technical_paper",
        "authors": [
          "Melanie Sclar",
          "Yejin Choi",
          "Yulia Tsvetkov",
          "Alane Suhr"
        ]
      },
      {
        "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts",
        "url": "http://arxiv.org/pdf/2505.12592v1",
        "source_type": "technical_paper",
        "authors": [
          "Sullam Jeoung",
          "Yueyan Chen",
          "Yi Zhang",
          "Shuai Wang",
          "Haibo Ding",
          "Lin Lee Cheong"
        ],
        "publication_date": "2025-05-19"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 3,
    "related_techniques": [
      "causal-mediation-analysis-in-language-models",
      "feature-attribution-with-integrated-gradients-in-nlp"
    ]
  },
  {
    "slug": "causal-mediation-analysis-in-language-models",
    "name": "Causal Mediation Analysis in Language Models",
    "description": "Causal mediation analysis in language models is a mechanistic interpretability technique that systematically investigates how specific internal components (neurons, attention heads, or layers) causally contribute to model outputs. By performing controlled interventions\u2014such as activating, deactivating, or modifying specific components\u2014researchers can trace the causal pathways through which information flows and transforms within the model. This approach goes beyond correlation to establish causal relationships, enabling researchers to understand not just what features influence outputs, but how and why they do so through specific computational pathways.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/causal-analysis/mediation-analysis",
      "assurance-goal-category/explainability/explains/causal-pathways",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/causality",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/causal-analysis",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/post-deployment",
      "technique-type/mechanistic-interpretability"
    ],
    "example_use_cases": [
      {
        "description": "Investigating causal pathways in content moderation models to understand how specific attention mechanisms contribute to flagging potentially harmful content, enabling verification that safety decisions rely on appropriate features rather than spurious correlations and ensuring robust content filtering.",
        "goal": "Safety"
      },
      {
        "description": "Identifying specific neurons or attention heads that causally contribute to biased outputs in hiring or lending language models, enabling targeted interventions to reduce discriminatory behaviour whilst preserving model performance on legitimate tasks and ensuring fair treatment across demographics.",
        "goal": "Reliability"
      },
      {
        "description": "Tracing causal pathways in large language models performing mathematical reasoning tasks to understand how intermediate steps are computed and stored, revealing which components are responsible for different aspects of logical inference and enabling validation of reasoning processes.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Requires sophisticated understanding of model architecture to design meaningful interventions, as poorly chosen intervention points may yield misleading causal conclusions or fail to capture relevant computational pathways."
      },
      {
        "description": "Results are highly dependent on the validity of underlying causal assumptions, which can be difficult to verify in complex, high-dimensional neural network spaces where multiple causal pathways may interact."
      },
      {
        "description": "Comprehensive causal analysis requires extensive computational resources, particularly for large models, as each intervention requires separate forward passes and multiple intervention combinations for robust conclusions."
      },
      {
        "description": "Distinguishing between direct causal effects and indirect effects mediated through other components can be challenging, potentially leading to oversimplified causal narratives that miss important intermediate processes."
      },
      {
        "description": "Causal relationships identified in specific contexts or datasets may not generalise to different domains, tasks, or model versions, requiring careful validation across diverse scenarios to ensure robust findings."
      }
    ],
    "resources": [],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      "prompt-sensitivity-analysis",
      "feature-attribution-with-integrated-gradients-in-nlp"
    ]
  },
  {
    "slug": "feature-attribution-with-integrated-gradients-in-nlp",
    "name": "Feature Attribution with Integrated Gradients in NLP",
    "description": "Applies Integrated Gradients to natural language processing models to attribute prediction importance to individual input tokens, words, or subword units. This technique computes gradients along a straight-line path from a baseline input (typically all-zeros, padding tokens, or neutral text) to the actual input, integrating these gradients to obtain attribution scores. Unlike vanilla gradient methods, Integrated Gradients satisfies axioms of sensitivity and implementation invariance, making it particularly valuable for understanding transformer-based language models where token interactions are complex.",
    "assurance_goals": [
      "Explainability",
      "Fairness",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/gradient-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "evidence-type/visualisation",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/testing",
      "technique-type/gradient-based"
    ],
    "example_use_cases": [
      {
        "description": "In a clinical decision support system processing doctor's notes to predict patient risk, Integrated Gradients identifies which medical terms, symptoms, or phrases most strongly influence risk predictions, enabling clinicians to verify that the model focuses on clinically relevant information rather than spurious correlations and supporting regulatory compliance in healthcare AI.",
        "goal": "Safety"
      },
      {
        "description": "For automated loan approval systems processing free-text application descriptions, Integrated Gradients reveals which words or phrases drive acceptance decisions, supporting fairness audits by highlighting whether protected characteristics inadvertently influence decisions and enabling transparent explanations to customers about application outcomes.",
        "goal": "Fairness"
      },
      {
        "description": "In content moderation systems flagging potentially harmful posts, Integrated Gradients identifies which specific words or linguistic patterns trigger safety classifications, enabling platform teams to debug false positives and validate that models focus on genuinely problematic language rather than demographic markers.",
        "goal": "Explainability"
      }
    ],
    "limitations": [
      {
        "description": "Computational overhead scales significantly with document length as processing requires computing gradients across many integration steps (typically 20-300), making real-time applications or large-scale document processing challenging."
      },
      {
        "description": "Choice of baseline input (zero embeddings, padding tokens, neutral text, or average embeddings) substantially affects attribution results, but optimal baseline selection remains domain-specific and often requires extensive experimentation."
      },
      {
        "description": "In transformer models with attention mechanisms, importance often spreads across many tokens, making it difficult to identify clear, actionable insights, especially for complex reasoning tasks where multiple tokens contribute collectively."
      },
      {
        "description": "Modern NLP models use subword tokenisation (BPE, WordPiece), making attribution results difficult to interpret at the word level, as single words may split across multiple tokens with varying attribution scores."
      },
      {
        "description": "While Integrated Gradients identifies correlative relationships between tokens and predictions, it cannot establish causal relationships or distinguish between spurious correlations and meaningful semantic dependencies in the input text."
      }
    ],
    "resources": [
      {
        "title": "Captum: Model Interpretability for PyTorch",
        "url": "https://captum.ai/",
        "source_type": "software_package",
        "description": "Open-source PyTorch library implementing Integrated Gradients with multi-modal support including text, featuring easy integration with transformer models and comprehensive NLP tutorials (BERT SQUAD, IMDB classification, Llama2 attribution)."
      },
      {
        "title": "Axiomatic Attribution for Deep Networks",
        "url": "https://arxiv.org/abs/1703.01365",
        "source_type": "technical_paper",
        "authors": [
          "Mukund Sundararajan",
          "Ankur Taly",
          "Qiqi Yan"
        ],
        "publication_date": "2017-03-19",
        "description": "Original paper introducing Integrated Gradients method with fundamental axioms of sensitivity and implementation invariance, demonstrating applications across text models and providing theoretical foundations for attribution methods."
      },
      {
        "title": "The Building Blocks of Interpretability",
        "url": "https://distill.pub/2020/attribution-baselines/",
        "source_type": "tutorial",
        "description": "Interactive Distill article providing comprehensive guidance on baseline selection for Integrated Gradients, exploring different baseline types and their impact on feature attribution quality with transferable principles for NLP applications."
      },
      {
        "title": "transformers-interpret",
        "url": "https://github.com/cdpierse/transformers-interpret",
        "source_type": "software_package",
        "description": "Model explainability library designed for Hugging Face transformers, enabling transformer model explanation in two lines of code with HTML visualisations and support for sequence classification, multi-label classification, and computer vision models."
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      "prompt-sensitivity-analysis",
      "causal-mediation-analysis-in-language-models"
    ]
  },
  {
    "slug": "concept-activation-vectors",
    "name": "Concept Activation Vectors",
    "description": "Concept Activation Vectors (CAVs), also known as Testing with Concept Activation Vectors (TCAV), identify mathematical directions in neural network representation space that correspond to human-understandable concepts such as 'stripes', 'young', or 'medical equipment'. The technique works by finding linear directions that separate activations of concept examples from non-concept examples, then measuring how much these concept directions influence the model's predictions. This provides quantitative answers to questions like 'How much does the concept of youth affect this model's hiring decisions?' enabling systematic bias detection and model understanding.",
    "assurance_goals": [
      "Explainability",
      "Fairness",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/causal-pathways",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/causality",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/representation-analysis/concept-identification",
      "assurance-goal-category/fairness",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-model-internals",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/visualization",
      "expertise-needed/domain-knowledge",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use/auditing",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Auditing a medical imaging model to verify it focuses on diagnostic features (like 'tumour characteristics') rather than irrelevant concepts (like 'scanner type' or 'patient positioning') when classifying chest X-rays, ensuring clinical decisions rely on medically relevant information.",
        "goal": "Explainability"
      },
      {
        "description": "Testing whether a hiring algorithm's resume screening decisions are influenced by concepts related to protected characteristics such as 'gender-associated names', 'prestigious universities', or 'employment gaps', enabling systematic bias detection and compliance verification.",
        "goal": "Fairness"
      },
      {
        "description": "Providing regulatory-compliant explanations for financial lending decisions by quantifying how concepts like 'debt-to-income ratio', 'employment stability', and 'credit history length' influence loan approval models, with precise sensitivity scores for audit documentation.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Requires clearly defined concept examples and non-concept examples, which can be challenging to obtain for abstract or subjective concepts."
      },
      {
        "description": "Assumes that meaningful concept directions exist as linear separable directions in the model's internal representation space, which may not hold for all concepts."
      },
      {
        "description": "Results depend heavily on which network layer is examined, as different layers capture different levels of abstraction and concept representation."
      },
      {
        "description": "Computational cost grows significantly with model size and number of concepts tested, though recent advances like FastCAV address this limitation."
      },
      {
        "description": "Interpretation requires domain expertise to define meaningful concepts and understand the significance of sensitivity scores in practical contexts."
      }
    ],
    "resources": [
      {
        "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks",
        "url": "http://arxiv.org/pdf/2505.17883v1",
        "source_type": "technical_paper",
        "authors": [
          "Laines Schmalwasser",
          "Niklas Penzel",
          "Joachim Denzler",
          "Julia Niebling"
        ],
        "publication_date": "2025-05-23"
      },
      {
        "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement",
        "url": "http://arxiv.org/pdf/2311.15303v1",
        "source_type": "technical_paper",
        "authors": [
          "Avani Gupta",
          "Saurabh Saini",
          "P J Narayanan"
        ],
        "publication_date": "2023-11-26"
      },
      {
        "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations",
        "url": "http://arxiv.org/pdf/2503.05522v1",
        "source_type": "technical_paper",
        "authors": [
          "Eren Erogullari",
          "Sebastian Lapuschkin",
          "Wojciech Samek",
          "Frederik Pahde"
        ],
        "publication_date": "2025-03-07"
      },
      {
        "title": "Concept Gradient: Concept-based Interpretation Without Linear Assumption",
        "url": "http://arxiv.org/pdf/2208.14966v2",
        "source_type": "technical_paper",
        "authors": [
          "Andrew Bai",
          "Chih-Kuan Yeh",
          "Pradeep Ravikumar",
          "Neil Y. C. Lin",
          "Cho-Jui Hsieh"
        ],
        "publication_date": "2022-08-31"
      },
      {
        "title": "SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation",
        "url": "http://arxiv.org/pdf/2310.07698v1",
        "source_type": "technical_paper",
        "authors": [
          "Bo Pan",
          "Zhenke Liu",
          "Yifei Zhang",
          "Liang Zhao"
        ],
        "publication_date": "2023-10-11"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "acronym": "CAVs",
    "related_techniques": [
      "prototype-and-criticism-models",
      "neuron-activation-analysis"
    ]
  },
  {
    "slug": "attention-visualisation-in-transformers",
    "name": "Attention Visualisation in Transformers",
    "description": "Attention Visualisation in Transformers analyses the multi-head self-attention mechanisms that enable transformers to process sequences by attending to different positions simultaneously. The technique visualises attention weights as heatmaps showing how strongly each token attends to every other token across different heads and layers. By examining these attention patterns, practitioners can understand how models like BERT, GPT, and T5 build contextual representations, identify which tokens influence predictions most strongly, and detect potential biases in how the model processes different types of input. This provides insights into positional encoding effects, head specialisation patterns, and the evolution of attention from local to global context across layers.",
    "assurance_goals": [
      "Explainability",
      "Fairness",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer",
      "applicable-models/requirements/architecture-specific",
      "applicable-models/requirements/model-internals",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/feature-analysis",
      "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/efficiency",
      "assurance-goal-category/explainability/visualization-methods/attention-patterns",
      "assurance-goal-category/fairness",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-model-internals",
      "data-type/image",
      "data-type/text",
      "evidence-type/visualization",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "lifecycle-stage/testing",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Examining attention patterns in a medical language model processing clinical notes to verify it focuses on relevant symptoms and conditions rather than irrelevant demographic identifiers, revealing that certain attention heads specialise in medical terminology whilst others track syntactic relationships between diagnoses and treatments.",
        "goal": "Explainability"
      },
      {
        "description": "Auditing a sentiment analysis model for customer reviews by visualising how attention weights differ when processing reviews from different demographic groups, discovering that the model pays disproportionate attention to certain cultural expressions or colloquialisms that could lead to biased sentiment predictions.",
        "goal": "Fairness"
      },
      {
        "description": "Creating visual explanations for regulatory compliance in a financial document classification system, showing which specific words and phrases in loan applications or contracts triggered particular risk assessments, enabling auditors to verify that decisions are based on legitimate financial factors rather than discriminatory language patterns.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "High attention weights do not necessarily indicate causal importance for predictions, as models may attend strongly to tokens that serve structural rather than semantic purposes."
      },
      {
        "description": "The sheer number of attention heads and layers in modern transformers creates visualisation overload, making it difficult to identify meaningful patterns without systematic analysis tools."
      },
      {
        "description": "Attention patterns can be misleading when models use residual connections and layer normalisation, as the final representation incorporates information beyond what attention weights suggest."
      },
      {
        "description": "Different transformer architectures (encoder-only, decoder-only, encoder-decoder) exhibit fundamentally different attention patterns, limiting the generalisability of insights across model types."
      },
      {
        "description": "The technique cannot explain the reasoning process within feed-forward layers or how attention patterns translate into specific predictions, providing only a partial view of model behaviour."
      }
    ],
    "resources": [
      {
        "title": "jessevig/bertviz",
        "url": "https://github.com/jessevig/bertviz",
        "source_type": "software_package",
        "description": "Interactive tool for visualising attention patterns in transformer language models including BERT, GPT-2, and T5"
      },
      {
        "title": "Attention is All You Need",
        "url": "https://arxiv.org/abs/1706.03762",
        "source_type": "technical_paper",
        "description": "Foundational paper introducing the transformer architecture and self-attention mechanism"
      },
      {
        "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
        "url": "https://arxiv.org/abs/1905.09418",
        "source_type": "technical_paper",
        "description": "Research showing how different attention heads specialise in distinct linguistic phenomena"
      },
      {
        "title": "What Does BERT Look At? An Analysis of BERT's Attention",
        "url": "https://arxiv.org/abs/1906.04341",
        "source_type": "technical_paper",
        "description": "Comprehensive analysis of attention patterns in BERT revealing syntactic and semantic specialisation"
      },
      {
        "title": "Transformer Explainability Beyond Attention Visualization",
        "url": "https://arxiv.org/abs/2012.09838",
        "source_type": "technical_paper",
        "description": "Methods for attribution beyond raw attention weights including relevancy propagation and gradient-based approaches"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "integrated-gradients",
      "layer-wise-relevance-propagation",
      "saliency-maps",
      "gradient-weighted-class-activation-mapping",
      "classical-attention-analysis-in-neural-networks",
      "contrastive-explanation-method"
    ]
  },
  {
    "slug": "reweighing",
    "name": "Reweighing",
    "description": "Reweighing is a pre-processing technique that mitigates bias by assigning different weights to training examples based on their group membership and class label. The weights are calculated to ensure that privileged and unprivileged groups have equal influence on the model's training process, effectively balancing the dataset without altering the feature values themselves. This helps to train fairer models by correcting for historical imbalances in how different groups are represented in the data.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group/statistical-parity",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/data-handling/preprocessing",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "In a loan application system, if historical data shows that a higher proportion of applicants from a minority group were denied loans (negative outcome), reweighing would assign higher weights to these instances. This forces the model to pay more attention to correctly classifying the underrepresented group, aiming to correct for historical bias and improve fairness metrics like equal opportunity.",
        "goal": "Fairness"
      },
      {
        "description": "When developing a hiring model, if the training data contains fewer female applicants for senior roles, reweighing can be applied to increase the importance of these instances. This helps to prevent the model from learning a spurious correlation between gender and seniority, ensuring that female candidates are evaluated more equitably during the screening process.",
        "goal": "Fairness"
      },
      {
        "description": "In a medical diagnosis system, reweighing provides transparency by explicitly showing which demographic groups required adjustment for balanced representation. The computed weights serve as documentation of historical bias patterns in medical data, helping clinicians understand potential disparities and ensuring the model's decisions are based on medical evidence rather than demographic correlations.",
        "goal": "Transparency"
      },
      {
        "description": "For a credit scoring model deployed across different regions, reweighing improves reliability by ensuring consistent performance across demographic groups. By balancing the training data representation, the model maintains stable accuracy metrics across different population segments, reducing the risk of performance degradation when deployed in areas with different demographic compositions.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "The technique only adjusts the overall influence of demographic groups and does not address biases that may be encoded within the features themselves."
      },
      {
        "description": "Assigning very high weights to a small number of instances from an underrepresented group can increase the model's variance and make it sensitive to outliers, potentially harming generalisation."
      },
      {
        "description": "The effectiveness of reweighing depends on the assumption that the labels in the training data are accurate; it cannot correct for label bias where outcomes were themselves the result of historical discrimination."
      },
      {
        "description": "It may not be effective if the feature distributions for different groups are fundamentally different, as it cannot change the underlying data relationships."
      }
    ],
    "resources": [
      {
        "title": "Achieving Fairness at No Utility Cost via Data Reweighing with Influence",
        "url": "http://arxiv.org/pdf/2202.00787v2",
        "source_type": "technical_paper",
        "authors": [
          "Peizhao Li",
          "Hongfu Liu"
        ],
        "publication_date": "2022-02-01",
        "description": "Presents a novel reweighing approach that computes individual sample weights based on influence functions to achieve fairness without sacrificing model utility"
      },
      {
        "title": "aif360.sklearn.preprocessing.Reweighing \u2014 aif360 0.6.1 ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.preprocessing.Reweighing.html",
        "source_type": "documentation",
        "description": "Documentation for scikit-learn compatible implementation of the reweighing preprocessing technique in the AI Fairness 360 library"
      },
      {
        "title": "brandeis-machine-learning/influence-fairness",
        "url": "https://github.com/brandeis-machine-learning/influence-fairness",
        "source_type": "software_package",
        "description": "Python implementation of influence-based data reweighing for achieving cost-free fairness with experiments on tabular datasets"
      },
      {
        "title": "Boosting Fair Classifier Generalization through Adaptive Priority Reweighing",
        "url": "http://arxiv.org/pdf/2309.08375v3",
        "source_type": "technical_paper",
        "authors": [
          "Zhihao Hu",
          "Yiran Xu",
          "Mengnan Du",
          "Jindong Gu",
          "Xinmei Tian",
          "Fengxiang He"
        ],
        "publication_date": "2023-09-15",
        "description": "Advanced reweighing technique that adaptively prioritises samples near decision boundaries to improve fairness generalisation across different demographic groups"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      "disparate-impact-remover",
      "relabelling",
      "preferential-sampling"
    ]
  },
  {
    "slug": "disparate-impact-remover",
    "name": "Disparate Impact Remover",
    "description": "Disparate Impact Remover is a preprocessing technique that transforms feature values in a dataset to reduce statistical dependence between features and protected attributes (like race or gender). The method modifies non-protected features through mathematical transformations that preserve the utility of the data whilst reducing correlations that could lead to discriminatory outcomes. This approach specifically targets the '80% rule' disparate impact threshold by adjusting feature distributions to ensure more equitable treatment across demographic groups in downstream model predictions.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Transforming features in a credit scoring dataset where variables like 'years of employment' and 'education level' are correlated with race, applying mathematical transformations to reduce these correlations whilst preserving the predictive value for creditworthiness assessment.",
        "goal": "Fairness"
      },
      {
        "description": "Preprocessing a recruitment dataset where features like 'previous job titles' and 'university attended' correlate with gender, modifying these features to ensure the '80% rule' is met whilst maintaining useful information for predicting job performance.",
        "goal": "Fairness"
      },
      {
        "description": "Preprocessing financial lending data to provide transparent bias metrics showing the quantified reduction in correlation between protected attributes and creditworthiness features, enabling institutions to demonstrate compliance with the 80% rule and explain their fairness interventions to regulators.",
        "goal": "Transparency"
      },
      {
        "description": "Ensuring consistent model performance across demographic groups in healthcare risk assessment by mathematically transforming features to reduce protected attribute correlations, improving reliability of predictions for minority populations who may have been systematically under-served.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Feature transformations may reduce model accuracy by removing or distorting important predictive information during the debiasing process."
      },
      {
        "description": "Only addresses measured protected attributes and cannot eliminate bias that operates through unmeasured proxy variables."
      },
      {
        "description": "Effectiveness depends on the specific transformation method chosen and may not generalise well to different datasets or domains."
      },
      {
        "description": "May create artificial feature distributions that don't reflect real-world data patterns, potentially causing issues in model deployment."
      }
    ],
    "resources": [
      {
        "title": "holistic-ai/holisticai",
        "url": "https://github.com/holistic-ai/holisticai",
        "source_type": "software_package",
        "description": "Comprehensive open-source toolkit for AI fairness with bias measurement, mitigation techniques, and visualisation tools"
      },
      {
        "title": "Disparate Impact Remover \u2014 holisticai documentation",
        "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/preprocessing/bc_disparate_impact_remover_disparate_impact_remover.html",
        "source_type": "tutorial",
        "description": "Comprehensive tutorial covering theoretical background, methodology, and practical implementation of disparate impact removal"
      },
      {
        "title": "Trusted-AI/AIF360",
        "url": "https://github.com/Trusted-AI/AIF360",
        "source_type": "software_package",
        "description": "IBM Research's extensible open-source library for detecting and mitigating algorithmic bias across multiple domains"
      },
      {
        "title": "aif360.algorithms.preprocessing.DisparateImpactRemover \u2014 aif360 ...",
        "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.DisparateImpactRemover.html",
        "source_type": "documentation",
        "description": "Technical API documentation for AIF360's DisparateImpactRemover class with parameters, methods, and usage examples"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "reweighing",
      "relabelling",
      "preferential-sampling"
    ]
  },
  {
    "slug": "fairness-gan",
    "name": "Fairness GAN",
    "description": "A data generation technique that employs Generative Adversarial Networks (GANs) to create fair synthetic datasets by learning to generate data representations that preserve utility whilst obfuscating protected attributes. Unlike traditional GANs, Fairness GANs incorporate fairness constraints into the training objective, ensuring that the generated data maintains statistical parity across demographic groups. The technique can be used for data augmentation to balance underrepresented groups or to create privacy-preserving synthetic datasets that remove demographic bias from training data.",
    "assurance_goals": [
      "Fairness",
      "Privacy",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/generative/gan",
      "applicable-models/paradigm/generative",
      "applicable-models/paradigm/unsupervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/privacy",
      "assurance-goal-category/reliability",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "evidence-type/synthetic-data",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/data-collection/data-augmentation",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Generating balanced synthetic datasets for medical research by creating additional samples from underrepresented demographic groups, ensuring equal representation across ethnicity and gender whilst maintaining the statistical properties needed for robust model training.",
        "goal": "Fairness"
      },
      {
        "description": "Creating privacy-preserving synthetic datasets for financial services that remove demographic identifiers whilst preserving the underlying patterns needed for credit risk assessment, allowing secure data sharing between institutions without exposing sensitive customer information.",
        "goal": "Privacy"
      },
      {
        "description": "Augmenting recruitment datasets by generating synthetic candidate profiles that balance gender and ethnicity representation, ensuring reliable model performance across all demographic groups when real-world data exhibits significant imbalances.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "GAN training is notoriously difficult to stabilise, with potential for mode collapse or failure to converge, especially when additional fairness constraints are imposed."
      },
      {
        "description": "Ensuring fairness in generated data may come at the cost of data utility, potentially reducing the quality or realism of synthetic samples."
      },
      {
        "description": "Requires large datasets to train both generator and discriminator networks effectively, limiting applicability in data-scarce domains."
      },
      {
        "description": "Evaluation complexity is high, as it requires assessing both the quality of generated data and the preservation of fairness properties across demographic groups."
      },
      {
        "description": "May inadvertently introduce new biases if the fairness constraints are not properly specified or if the training data itself contains subtle biases."
      }
    ],
    "resources": [
      {
        "title": "Fairness GAN",
        "url": "http://arxiv.org/pdf/1805.09910v1",
        "source_type": "technical_paper",
        "authors": [
          "Prasanna Sattigeri",
          "Samuel C. Hoffman",
          "Vijil Chenthamarakshan",
          "Kush R. Varshney"
        ],
        "publication_date": "2018-05-24"
      },
      {
        "title": "Fair GANs through model rebalancing for extremely imbalanced class distributions",
        "url": "http://arxiv.org/pdf/2308.08638v2",
        "source_type": "technical_paper",
        "authors": [
          "Anubhav Jain",
          "Nasir Memon",
          "Julian Togelius"
        ],
        "publication_date": "2023-08-16"
      },
      {
        "title": "Inclusive GAN: Improving Data and Minority Coverage in Generative Models",
        "url": "http://arxiv.org/abs/2004.03355",
        "source_type": "technical_paper",
        "authors": [
          "Ning Yu",
          "Ke Li",
          "Peng Zhou",
          "Jitendra Malik",
          "Larry Davis",
          "Mario Fritz"
        ],
        "publication_date": "2020-04-07"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 5,
    "related_techniques": [
      "sensitivity-analysis-for-fairness",
      "attribute-removal-fairness-through-unawareness",
      "bayesian-fairness-regularization"
    ]
  },
  {
    "slug": "relabelling",
    "name": "Relabelling",
    "description": "A preprocessing fairness technique that modifies class labels in training data to achieve equal positive outcome rates across protected groups. Also known as 'data massaging', this method identifies instances that contribute to discriminatory patterns and flips their labels (from positive to negative or vice versa) to balance the proportion of positive outcomes between demographic groups. The technique aims to remove historical bias from training data whilst preserving the overall class distribution, enabling standard classifiers to learn from discrimination-free datasets.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/dataset-analysis",
      "evidence-type/quantitative-metric",
      "expertise-needed/domain-knowledge",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/data-collection/data-preprocessing",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Preprocessing historical hiring datasets by relabelling borderline cases to ensure equal hiring rates across gender and ethnicity groups, correcting for past discriminatory practices whilst maintaining overall qualification standards for fair recruitment model training.",
        "goal": "Fairness"
      },
      {
        "description": "Creating transparent credit scoring datasets by documenting which loan applications had labels modified to address historical lending discrimination, providing clear audit trails showing how training data bias was systematically corrected before model development.",
        "goal": "Transparency"
      },
      {
        "description": "Improving reliability of medical diagnosis training data by relabelling cases where demographic bias may have influenced historical diagnoses, ensuring models learn from corrected labels that reflect true medical conditions rather than biased historical treatment patterns.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Altering training labels risks introducing new biases or artificial patterns that may not reflect genuine relationships in the data."
      },
      {
        "description": "Deciding which instances to relabel requires careful selection criteria and domain expertise to avoid inappropriate label changes."
      },
      {
        "description": "May reduce prediction accuracy if too many labels are changed, particularly when the modifications conflict with genuine patterns in the data."
      },
      {
        "description": "Requires access to ground truth or expert knowledge to determine whether original labels reflect genuine outcomes or discriminatory bias."
      },
      {
        "description": "Effectiveness depends on accurate identification of discriminatory instances, which can be challenging when bias patterns are subtle or complex."
      }
    ],
    "resources": [
      {
        "title": "Data preprocessing techniques for classification without discrimination",
        "url": "https://link.springer.com/article/10.1007/s10115-011-0463-8",
        "source_type": "technical_paper",
        "authors": [
          "Faisal Kamiran",
          "Toon Calders"
        ],
        "publication_date": "2012-06-01"
      },
      {
        "title": "Classifying without discriminating",
        "url": "https://www.researchgate.net/publication/224440330_Classifying_without_discriminating",
        "source_type": "technical_paper",
        "authors": [
          "Toon Calders",
          "Sicco Verwer"
        ],
        "publication_date": "2010-02-01"
      },
      {
        "title": "Data Pre-Processing for Discrimination Prevention",
        "url": "https://krvarshney.github.io/pubs/CalmonWVRV_jstsp2018.pdf",
        "source_type": "technical_paper",
        "authors": [
          "Flavio Calmon",
          "Dennis Wei",
          "Bhanukiran Vinzamuri",
          "Karthikeyan Natesan Ramamurthy",
          "Kush R. Varshney"
        ],
        "publication_date": "2018-01-01"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "reweighing",
      "disparate-impact-remover",
      "preferential-sampling"
    ]
  },
  {
    "slug": "preferential-sampling",
    "name": "Preferential Sampling",
    "description": "A preprocessing fairness technique developed by Kamiran and Calders that addresses dataset imbalances by re-sampling training data with preference for underrepresented groups to achieve discrimination-free classification. This method modifies the training distribution by prioritising borderline objects (instances near decision boundaries) from underrepresented groups for duplication whilst potentially removing instances from overrepresented groups. Unlike relabelling approaches, preferential sampling maintains original class labels whilst creating a more balanced dataset that prevents models from learning biased patterns due to skewed group representation.",
    "assurance_goals": [
      "Fairness",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/dataset-analysis",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/data-collection/data-preprocessing",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "example_use_cases": [
      {
        "description": "Preprocessing hiring datasets by preferentially sampling candidates from underrepresented gender and ethnic groups, particularly focusing on borderline cases near decision boundaries, to ensure fair representation whilst maintaining original qualifications and labels for unbiased recruitment model training.",
        "goal": "Fairness"
      },
      {
        "description": "Balancing medical training datasets by oversampling patients from underrepresented demographic groups to ensure reliable diagnostic performance across all populations, preventing models from exhibiting reduced accuracy for minority patient groups due to insufficient training examples.",
        "goal": "Reliability"
      },
      {
        "description": "Creating transparent credit scoring datasets by documenting and adjusting the sampling process to ensure equal representation across demographic groups, providing clear evidence to regulators that training data imbalances have been addressed without altering original creditworthiness labels.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Oversampling minority groups can cause overfitting to duplicated examples, particularly when borderline instances are repeatedly sampled, potentially reducing model generalisation."
      },
      {
        "description": "Undersampling majority groups may remove important examples that contain valuable information, potentially degrading overall model performance."
      },
      {
        "description": "Does not address inherent algorithmic bias in the learning process itself, only correcting for representation imbalances in the training data."
      },
      {
        "description": "Selection of borderline objects requires careful threshold tuning and may be sensitive to the choice of distance metrics or similarity measures used."
      },
      {
        "description": "May not address intersectional fairness issues when multiple protected attributes create complex group combinations that require nuanced sampling strategies."
      }
    ],
    "resources": [
      {
        "title": "Data preprocessing techniques for classification without discrimination",
        "url": "https://link.springer.com/article/10.1007/s10115-011-0463-8",
        "source_type": "technical_paper",
        "authors": [
          "Faisal Kamiran",
          "Toon Calders"
        ],
        "publication_date": "2012-06-01"
      },
      {
        "title": "Classification with no discrimination by preferential sampling",
        "url": "https://research.tue.nl/en/publications/classification-with-no-discrimination-by-preferential-sampling",
        "source_type": "technical_paper",
        "authors": [
          "Faisal Kamiran",
          "Toon Calders"
        ],
        "publication_date": "2010-05-27"
      },
      {
        "title": "A Survey on Bias and Fairness in Machine Learning",
        "url": "https://arxiv.org/abs/1908.09635",
        "source_type": "documentation",
        "authors": [
          "Ninareh Mehrabi",
          "Fred Morstatter",
          "Nripsuta Saxena",
          "Kristina Lerman",
          "Aram Galstyan"
        ],
        "publication_date": "2019-08-25"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 2,
    "related_techniques": [
      "reweighing",
      "disparate-impact-remover",
      "relabelling"
    ]
  },
  {
    "slug": "attribute-removal-fairness-through-unawareness",
    "name": "Attribute Removal (Fairness Through Unawareness)",
    "description": "Attribute Removal (Fairness Through Unawareness) ensures fairness by completely excluding protected attributes such as race, gender, or age from the model's input features. While this approach prevents direct discrimination, it may not eliminate bias if other features are correlated with protected attributes (proxy discrimination). This technique represents the most basic fairness intervention but often needs to be combined with other approaches to address indirect bias through seemingly neutral features.",
    "assurance_goals": [
      "Fairness",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/transparency",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Removing gender, race, and age attributes from hiring algorithms to prevent direct discrimination, whilst acknowledging that indirect bias may persist through correlated features like education institution or postal code.",
        "goal": "Fairness"
      },
      {
        "description": "Excluding protected demographic attributes from credit scoring models to comply with fair lending regulations, ensuring no explicit consideration of race, gender, or ethnicity in loan approval decisions.",
        "goal": "Fairness"
      },
      {
        "description": "Building medical diagnosis models that exclude patient race and ethnicity to prevent biased treatment recommendations, whilst ensuring clinical decisions are based solely on medical indicators and symptoms.",
        "goal": "Fairness"
      },
      {
        "description": "Creating transparent regulatory reporting systems that demonstrate compliance by explicitly documenting which protected attributes have been excluded from decision-making algorithms, providing clear audit trails for regulatory review.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Proxy discrimination remains a major concern as seemingly neutral features (education, postal code, previous employment) may strongly correlate with protected attributes, perpetuating indirect bias."
      },
      {
        "description": "Intersectional bias cannot be addressed through simple attribute removal, as complex interactions between multiple demographic characteristics may create compounding discrimination effects."
      },
      {
        "description": "Legal and regulatory compliance may be insufficient, as many jurisdictions require demonstrating disparate impact absence rather than simply removing protected attributes from models."
      },
      {
        "description": "Identifying all potential proxy variables is practically impossible, especially with high-dimensional data where subtle correlations with protected attributes may exist in unexpected features."
      },
      {
        "description": "Performance degradation may occur if removed attributes contain legitimate predictive information, creating tension between fairness objectives and model accuracy requirements."
      }
    ],
    "resources": [
      {
        "title": "Fairness Through Awareness",
        "url": "https://arxiv.org/abs/1104.3913",
        "source_type": "technical_paper",
        "description": "Foundational paper introducing fairness through awareness concept and demonstrating limitations of fairness through unawareness",
        "authors": [
          "Dwork, Cynthia",
          "Hardt, Moritz",
          "Pitassi, Toniann",
          "Reingold, Omer",
          "Zemel, Richard"
        ],
        "publication_date": "2012-01-01"
      },
      {
        "title": "Fairness Constraints: Mechanisms for Fair Classification",
        "url": "https://arxiv.org/abs/1507.05259",
        "source_type": "technical_paper",
        "description": "Comprehensive analysis of fairness approaches including attribute removal limitations and proxy discrimination challenges",
        "authors": [
          "Zafar, Muhammad Bilal",
          "Valera, Isabel",
          "Rodriguez, Manuel Gomez",
          "Gummadi, Krishna P."
        ],
        "publication_date": "2015-07-19"
      },
      {
        "title": "Fairlearn: A toolkit for assessing and improving fairness in machine learning",
        "url": "https://github.com/fairlearn/fairlearn",
        "source_type": "software_package",
        "description": "Microsoft's comprehensive fairness toolkit with preprocessing methods including attribute removal and proxy detection tools"
      },
      {
        "title": "The Ethical Algorithm: The Science of Socially Aware Algorithm Design",
        "url": "https://global.oup.com/academic/product/the-ethical-algorithm-9780190948207",
        "source_type": "documentation",
        "description": "Accessible book covering fairness through unawareness concepts and practical considerations for practitioners",
        "authors": [
          "Kearns, Michael",
          "Roth, Aaron"
        ],
        "publication_date": "2019-11-01"
      }
    ],
    "complexity_rating": 1,
    "computational_cost_rating": 1,
    "related_techniques": [
      "sensitivity-analysis-for-fairness",
      "fairness-gan",
      "bayesian-fairness-regularization"
    ]
  },
  {
    "slug": "fair-adversarial-networks",
    "name": "Fair Adversarial Networks",
    "description": "An in-processing fairness technique that employs adversarial training with dual neural networks to learn fair representations. The method consists of a predictor network that learns the main task whilst an adversarial discriminator network simultaneously attempts to predict sensitive attributes from the predictor's hidden representations. Through this adversarial min-max game, the predictor is incentivised to learn features that are informative for the task but statistically independent of protected attributes, effectively removing bias at the representation level in deep learning models.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/training",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Training a facial recognition system that maintains high accuracy for person identification whilst ensuring equal performance across different ethnic groups, using adversarial training to remove race-related features from learned representations.",
        "goal": "Fairness"
      },
      {
        "description": "Developing a resume screening neural network that provides transparent evidence of bias mitigation by demonstrating that learned features cannot predict gender, whilst maintaining predictive performance for job suitability assessment.",
        "goal": "Transparency"
      },
      {
        "description": "Creating a medical image analysis model that achieves reliable diagnostic performance across patient demographics by using adversarial debiasing to ensure age and gender information cannot be extracted from diagnostic features.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Implementation complexity is high, requiring careful design of adversarial loss functions and balancing multiple competing objectives during training."
      },
      {
        "description": "Sensitive to hyperparameter choices, particularly the trade-off weights between prediction accuracy and adversarial loss, which require extensive tuning."
      },
      {
        "description": "Adversarial training can be unstable, with potential for mode collapse or failure to converge, especially in complex deep learning architectures."
      },
      {
        "description": "Interpretability of fairness improvements can be limited, as it may be difficult to verify that sensitive attributes are truly removed from learned representations."
      },
      {
        "description": "Computational overhead is significant due to training two networks simultaneously, increasing both training time and resource requirements."
      }
    ],
    "resources": [
      {
        "title": "Fair Adversarial Networks",
        "url": "http://arxiv.org/pdf/2002.12144v1",
        "source_type": "technical_paper",
        "authors": [
          "George Cevora"
        ],
        "publication_date": "2020-02-23"
      },
      {
        "title": "Demonstrating Rosa: the fairness solution for any Data Analytic pipeline",
        "url": "http://arxiv.org/pdf/2003.00899v2",
        "source_type": "technical_paper",
        "authors": [
          "Kate Wilkinson",
          "George Cevora"
        ],
        "publication_date": "2020-02-28"
      },
      {
        "title": "Triangular Trade-off between Robustness, Accuracy, and Fairness in Deep Neural Networks: A Survey",
        "url": "https://www.semanticscholar.org/paper/13b0444d079bea1c8c57a6082200b67ab5f4616e",
        "source_type": "documentation",
        "authors": [
          "Jingyang Li",
          "Guoqiang Li"
        ],
        "publication_date": "2025-02-10"
      },
      {
        "title": "Bt-GAN: Generating Fair Synthetic Healthdata via Bias-transforming Generative Adversarial Networks",
        "url": "https://www.semanticscholar.org/paper/6995779ac582c5f2436cfb82a3c8cf5ca72bae2f",
        "source_type": "technical_paper",
        "authors": [
          "Resmi Ramachandranpillai",
          "Md Fahim Sikder",
          "David Bergstr\u00f6m",
          "Fredrik Heintz"
        ],
        "publication_date": "2023-12-14"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      "adversarial-debiasing",
      "prejudice-remover-regulariser",
      "meta-fair-classifier",
      "exponentiated-gradient-reduction",
      "fair-transfer-learning",
      "adaptive-sensitive-reweighting",
      "multi-accuracy-boosting"
    ]
  },
  {
    "slug": "prejudice-remover-regulariser",
    "name": "Prejudice Remover Regulariser",
    "description": "An in-processing fairness technique that adds a fairness penalty to machine learning models to reduce bias against protected groups. The method works by minimising 'mutual information' - essentially reducing how much the model's predictions reveal about sensitive attributes like race or gender. By adding this penalty term to the learning objective (typically in logistic regression), the technique ensures predictions become less dependent on protected features. This addresses not only direct discrimination but also indirect bias through correlated features. Practitioners can adjust a tuning parameter to balance between maintaining accuracy and removing prejudice from the model.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/linear-models/logistic",
      "applicable-models/architecture/probabilistic",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/probabilistic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/training",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Training credit scoring models with prejudice remover regularisation to ensure loan approval decisions are not influenced by gender or ethnicity, minimising mutual information between predictions and protected attributes whilst maintaining accurate risk assessment.",
        "goal": "Fairness"
      },
      {
        "description": "Developing transparent university admission models that provide clear evidence of bias mitigation by demonstrating reduced statistical dependence between acceptance decisions and protected characteristics, enabling regulatory compliance reporting.",
        "goal": "Transparency"
      },
      {
        "description": "Building reliable recruitment screening models that maintain consistent performance across demographic groups by regularising against indirect prejudice through correlated features like school names or postal codes that might proxy for protected attributes.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Requires careful tuning of the fairness penalty hyperparameter, where too high values severely degrade accuracy whilst too low values provide insufficient bias mitigation."
      },
      {
        "description": "Primarily applicable to probabilistic discriminative models like logistic regression, limiting its use with other model architectures such as deep neural networks or tree-based methods."
      },
      {
        "description": "Computational complexity increases with the calculation of mutual information between predictions and sensitive attributes, particularly for high-dimensional data."
      },
      {
        "description": "May not fully eliminate all forms of discrimination, particularly when complex interactions between multiple sensitive attributes create intersectional biases."
      },
      {
        "description": "Effectiveness depends on accurate identification and inclusion of all sensitive attributes, potentially missing hidden biases from unobserved protected characteristics."
      }
    ],
    "resources": [
      {
        "title": "Fairness-Aware Classifier with Prejudice Remover Regularizer",
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3",
        "source_type": "technical_paper",
        "authors": [
          "Toshihiro Kamishima",
          "Shotaro Akaho",
          "Hideki Asoh",
          "Jun Sakuma"
        ],
        "publication_date": "2012-09-24"
      },
      {
        "title": "Fairness-Aware Machine Learning and Data Mining",
        "url": "https://www.kamishima.net/faml/",
        "source_type": "documentation"
      },
      {
        "title": "Fairness-aware Classifier (faclass)",
        "url": "https://www.kamishima.net/faclass/",
        "source_type": "software_package"
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "adversarial-debiasing",
      "fair-adversarial-networks",
      "meta-fair-classifier",
      "exponentiated-gradient-reduction",
      "fair-transfer-learning",
      "adaptive-sensitive-reweighting",
      "multi-accuracy-boosting"
    ]
  },
  {
    "slug": "meta-fair-classifier",
    "name": "Meta Fair Classifier",
    "description": "An in-processing fairness technique that employs meta-learning to modify any existing classifier for optimising fairness metrics whilst maintaining predictive performance. The method learns how to adjust model parameters or decision boundaries to satisfy fairness constraints such as demographic parity or equalised odds through iterative optimisation. This approach is particularly valuable when retrofitting fairness to pre-trained models that perform well but exhibit bias, as it can incorporate fairness without requiring complete retraining from scratch.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/gray-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/training",
      "lifecycle-stage/model-optimization",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Retrofitting an existing hiring algorithm to achieve demographic parity across gender and ethnicity groups by using meta-learning to adjust decision boundaries, ensuring equitable candidate selection whilst maintaining the model's ability to identify qualified applicants.",
        "goal": "Fairness"
      },
      {
        "description": "Modifying a pre-trained credit scoring model to provide transparent fairness guarantees by learning optimal parameter adjustments that satisfy equalised odds constraints, enabling clear reporting on fair lending compliance to regulatory authorities.",
        "goal": "Transparency"
      },
      {
        "description": "Adapting a medical diagnosis model to ensure reliable performance across patient demographics by meta-learning fairness-aware adjustments that maintain diagnostic accuracy whilst reducing disparities in treatment recommendations across age and socioeconomic groups.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Meta-learning approach can be complex to implement, requiring expertise in both the underlying classifier and meta-optimisation techniques."
      },
      {
        "description": "Requires extensive hyperparameter tuning to balance fairness constraints with predictive performance, making optimisation challenging."
      },
      {
        "description": "May result in longer training times compared to simpler fairness techniques due to the iterative meta-learning process."
      },
      {
        "description": "Performance depends heavily on the quality and characteristics of the base classifier being modified, limiting effectiveness with poorly-performing models."
      },
      {
        "description": "Theoretical guarantees about fairness-accuracy trade-offs may not hold in practice due to finite sample effects and optimisation challenges."
      }
    ],
    "resources": [
      {
        "title": "\u03c1-Fair Method \u2014 holisticai documentation",
        "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/inprocessing/bc_meta_fair_classifier_rho_fair.html",
        "source_type": "documentation"
      },
      {
        "title": "aif360.algorithms.inprocessing \u2014 aif360 0.1.0 documentation",
        "url": "https://aif360.readthedocs.io/en/v0.2.3/modules/inprocessing.html",
        "source_type": "documentation"
      },
      {
        "title": "Welcome to AI Fairness 360's documentation! \u2014 aif360 0.1.0 ...",
        "url": "https://aif360.readthedocs.io/en/v0.2.3/",
        "source_type": "documentation"
      },
      {
        "title": "Algorithmic decision making methods for fair credit scoring",
        "url": "http://arxiv.org/abs/2209.07912",
        "source_type": "technical_paper",
        "authors": [
          "Moldovan, Darie"
        ],
        "publication_date": "2022-09-16"
      },
      {
        "title": "The Importance of Modeling Data Missingness in Algorithmic Fairness: A\n  Causal Perspective",
        "url": "http://arxiv.org/abs/2012.11448",
        "source_type": "technical_paper",
        "authors": [
          "Amayuelas, Alfonso",
          "Deshpande, Amit",
          "Goel, Naman",
          "Sharma, Amit"
        ],
        "publication_date": "2020-12-21"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      "adversarial-debiasing",
      "fair-adversarial-networks",
      "prejudice-remover-regulariser",
      "exponentiated-gradient-reduction",
      "fair-transfer-learning",
      "adaptive-sensitive-reweighting",
      "multi-accuracy-boosting"
    ]
  },
  {
    "slug": "exponentiated-gradient-reduction",
    "name": "Exponentiated Gradient Reduction",
    "description": "An in-processing fairness technique based on Agarwal et al.'s reductions approach that transforms fair classification into a sequence of cost-sensitive classification problems. The method uses an exponentiated gradient algorithm to iteratively reweight training data, returning a randomised classifier that achieves the lowest empirical error whilst satisfying fairness constraints. This reduction-based framework provides theoretical guarantees about both accuracy and constraint violation, making it suitable for various fairness criteria including demographic parity and equalised odds.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/training",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Training a hiring algorithm with demographic parity constraints to ensure equal selection rates across gender groups, using iterative reweighting to balance fairness and predictive accuracy whilst maintaining legal compliance.",
        "goal": "Fairness"
      },
      {
        "description": "Developing a loan approval model with equalised odds constraints, providing transparent documentation of the theoretical guarantees about both error rates and fairness constraint violations achieved by the reduction approach.",
        "goal": "Transparency"
      },
      {
        "description": "Creating a medical diagnosis classifier that maintains reliable performance across demographic groups by using randomised prediction averaging, ensuring consistent healthcare delivery whilst monitoring constraint satisfaction over time.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Requires convex base learners for theoretical guarantees about convergence and optimality, limiting the choice of underlying models."
      },
      {
        "description": "Produces randomised classifiers that may give different predictions for identical inputs, which can be problematic in applications requiring consistent decisions."
      },
      {
        "description": "Convergence can be slow and sensitive to hyperparameter choices, particularly the learning rate and tolerance settings."
      },
      {
        "description": "Involves iterative retraining with adjusted weights, which can be computationally expensive for large datasets or complex models."
      },
      {
        "description": "Fairness constraints may significantly reduce model accuracy, and the trade-off between fairness and performance is not always transparent to practitioners."
      }
    ],
    "resources": [
      {
        "title": "A Reductions Approach to Fair Classification",
        "url": "https://arxiv.org/abs/1803.02453",
        "source_type": "technical_paper",
        "description": "Foundational paper by Agarwal et al. introducing the exponentiated gradient reduction approach for fair classification with theoretical guarantees.",
        "authors": [
          "Alekh Agarwal",
          "Alina Beygelzimer",
          "Miroslav Dud\u00edk",
          "John Langford",
          "Hanna Wallach"
        ],
        "publication_date": "2018-03-06"
      },
      {
        "title": "Fairlearn: ExponentiatedGradient",
        "url": "https://fairlearn.org/v0.10/api_reference/generated/fairlearn.reductions.ExponentiatedGradient.html",
        "source_type": "documentation",
        "description": "Microsoft's Fairlearn implementation of the Agarwal et al. algorithm with comprehensive API documentation and examples."
      },
      {
        "title": "IBM AIF360: ExponentiatedGradientReduction",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.inprocessing.ExponentiatedGradientReduction.html",
        "source_type": "documentation",
        "description": "IBM's AIF360 implementation with scikit-learn compatible API for in-processing fairness constraints during model training."
      },
      {
        "title": "Fairlearn Reductions Guide",
        "url": "https://fairlearn.org/main/user_guide/mitigation/reductions.html",
        "source_type": "tutorial",
        "description": "Comprehensive guide to using reduction-based approaches for fairness, including practical examples of exponentiated gradient methods and fairness constraints."
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      "adversarial-debiasing",
      "fair-adversarial-networks",
      "prejudice-remover-regulariser",
      "meta-fair-classifier",
      "fair-transfer-learning",
      "adaptive-sensitive-reweighting",
      "multi-accuracy-boosting"
    ]
  },
  {
    "slug": "fair-transfer-learning",
    "name": "Fair Transfer Learning",
    "description": "An in-processing fairness technique that adapts pre-trained models from one domain to another whilst explicitly preserving fairness constraints across different contexts or populations. The method addresses the challenge that fairness properties may not transfer when adapting models to new domains with different demographic compositions or data distributions. Fair transfer learning typically involves constraint-aware fine-tuning, domain adaptation techniques, or adversarial training that maintains equitable performance across groups in the target domain, ensuring that bias mitigation efforts carry over from source to target domains.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/pre-trained-model",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/fine-tuning",
      "lifecycle-stage/model-development/training",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adapting a hiring algorithm trained on one country's recruitment data to another region whilst maintaining fairness across gender and ethnicity groups, ensuring equitable candidate evaluation despite different local demographic distributions and cultural contexts.",
        "goal": "Fairness"
      },
      {
        "description": "Transferring a medical diagnosis model from urban hospital data to rural clinics whilst providing transparent evidence that fairness constraints are preserved across age, gender, and socioeconomic groups despite different patient populations and healthcare infrastructure.",
        "goal": "Transparency"
      },
      {
        "description": "Adapting a fraud detection system from one financial market to another whilst ensuring reliable performance across customer demographics, maintaining consistent accuracy and fairness even when transaction patterns and customer characteristics differ between markets.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Fairness properties achieved in the source domain may not translate directly to the target domain if demographic distributions or data characteristics differ significantly."
      },
      {
        "description": "Requires careful hyperparameter tuning and constraint specification to balance fairness preservation with model performance in the new domain."
      },
      {
        "description": "Implementation complexity is high, requiring expertise in both transfer learning techniques and fairness constraint optimisation methods."
      },
      {
        "description": "May suffer from negative transfer effects where fairness constraints that worked well in the source domain actually harm performance in the target domain."
      },
      {
        "description": "Evaluation challenges arise from needing to validate fairness across multiple domains and demographic groups simultaneously."
      }
    ],
    "resources": [
      {
        "title": "Segmenting across places: The need for fair transfer learning with satellite imagery",
        "url": "http://arxiv.org/pdf/2204.04358v3",
        "source_type": "technical_paper",
        "authors": [
          "Miao Zhang",
          "Harvineet Singh",
          "Lazarus Chok",
          "Rumi Chunara"
        ],
        "publication_date": "2022-04-09"
      },
      {
        "title": "Trustworthy Transfer Learning: A Survey",
        "url": "https://www.semanticscholar.org/paper/7ee5c5b58ed0b4e585e0c30790c206bea07faacf",
        "source_type": "documentation",
        "authors": [
          "Jun Wu",
          "Jingrui He"
        ],
        "publication_date": "2024-12-18"
      },
      {
        "title": "Cross-Institutional Transfer Learning for Educational Models: Implications for Model Performance, Fairness, and Equity",
        "url": "https://arxiv.org/abs/2305.00927",
        "source_type": "technical_paper",
        "authors": [
          "Josh Gardner",
          "Renzhe Yu",
          "Quan Nguyen",
          "Christopher Brooks",
          "Rene Kizilcec"
        ],
        "publication_date": "2023-05-01"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      "adversarial-debiasing",
      "fair-adversarial-networks",
      "prejudice-remover-regulariser",
      "meta-fair-classifier",
      "exponentiated-gradient-reduction",
      "adaptive-sensitive-reweighting",
      "multi-accuracy-boosting"
    ]
  },
  {
    "slug": "adaptive-sensitive-reweighting",
    "name": "Adaptive Sensitive Reweighting",
    "description": "Adaptive Sensitive Reweighting dynamically adjusts the importance of training examples during model training based on real-time performance across different demographic groups. Unlike traditional static reweighting that fixes weights at the start, this technique continuously monitors fairness metrics and automatically increases the weight of examples from underperforming groups whilst decreasing weights for overrepresented groups. The adaptive mechanism prevents models from perpetuating historical biases by ensuring balanced learning across all demographics throughout the training process.",
    "assurance_goals": [
      "Fairness",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gray-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "data-requirements/access-to-model-internals",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Training speech recognition systems that adapt weights during training to ensure consistent accuracy across different accents, dialects, and linguistic backgrounds, preventing models from favouring dominant accent groups in the training data.",
        "goal": "Fairness"
      },
      {
        "description": "Developing hiring algorithms that dynamically adjust training example weights to maintain consistent evaluation performance across demographic groups, ensuring the model doesn't learn to favour candidates from overrepresented backgrounds.",
        "goal": "Fairness"
      },
      {
        "description": "Building medical diagnostic models that adaptively reweight patient examples during training to ensure reliable performance across different age groups, ethnicities, and socioeconomic backgrounds, preventing healthcare disparities.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Training instability can occur when adaptive weight adjustments cause oscillations between demographic groups, potentially preventing convergence if reweighting parameters are not carefully tuned."
      },
      {
        "description": "Computational overhead increases significantly due to continuous monitoring of fairness metrics across groups during training, requiring additional memory and processing time."
      },
      {
        "description": "Risk of overfitting to specific demographic subgroups if the adaptation mechanism becomes too aggressive in correcting for observed performance disparities during training."
      },
      {
        "description": "Requires careful hyperparameter tuning for adaptation rates and fairness thresholds, making the technique sensitive to configuration choices that may not generalise across different datasets."
      },
      {
        "description": "May inadvertently harm overall model performance if the reweighting process prioritises fairness at the expense of learning important patterns that benefit all groups."
      }
    ],
    "resources": [
      {
        "title": "Adaptive Sensitive Reweighting to Mitigate Bias in Fairness-aware Classification",
        "url": "https://dl.acm.org/doi/10.1145/3178876.3186133",
        "source_type": "technical_paper",
        "description": "Original paper introducing adaptive sensitive reweighting technique using CULEP model for bias mitigation in classification tasks",
        "authors": [
          "Krasanakis, Emmanouil",
          "Spyromitros-Xioufis, Eleftherios",
          "Papadopoulos, Symeon",
          "Kompatsiaris, Yiannis"
        ],
        "publication_date": "2018-04-23"
      },
      {
        "title": "AIF360: A comprehensive set of fairness metrics for datasets and machine learning models",
        "url": "https://github.com/Trusted-AI/AIF360",
        "source_type": "software_package",
        "description": "IBM's comprehensive fairness toolkit including implementations of various reweighting techniques and bias mitigation methods"
      },
      {
        "title": "Fairlearn: A toolkit for assessing and improving fairness in machine learning",
        "url": "https://github.com/fairlearn/fairlearn",
        "source_type": "software_package",
        "description": "Microsoft's open-source toolkit providing reweighting and other bias mitigation algorithms with comprehensive documentation"
      },
      {
        "title": "Causal Fairness-Guided Dataset Reweighting using Neural Networks",
        "url": "https://arxiv.org/abs/2311.10512",
        "source_type": "technical_paper",
        "description": "Recent research on causal fairness-guided dataset reweighting using neural networks to address fairness from causal perspective",
        "authors": [
          "Zhao, Xuan",
          "Broelemann, Klaus",
          "Ruggieri, Salvatore",
          "Kasneci, Gjergji"
        ],
        "publication_date": "2023-11-17"
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 3,
    "related_techniques": [
      "adversarial-debiasing",
      "fair-adversarial-networks",
      "prejudice-remover-regulariser",
      "meta-fair-classifier",
      "exponentiated-gradient-reduction",
      "fair-transfer-learning",
      "multi-accuracy-boosting"
    ]
  },
  {
    "slug": "multi-accuracy-boosting",
    "name": "Multi-Accuracy Boosting",
    "description": "An in-processing fairness technique that employs boosting algorithms to improve accuracy uniformly across demographic groups by iteratively correcting errors where the model performs poorly for certain subgroups. The method uses a multi-calibration approach that trains weak learners to focus on prediction errors for underperforming groups, ensuring that no group experiences systematically worse accuracy. This iterative boosting process continues until accuracy parity is achieved across all groups whilst maintaining overall model performance.",
    "assurance_goals": [
      "Fairness",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/ensemble",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/training",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Training a medical diagnosis model that achieves equal accuracy across age, gender, and ethnicity groups by using boosting to specifically target prediction errors for underrepresented patient demographics, ensuring equitable healthcare outcomes for all populations.",
        "goal": "Fairness"
      },
      {
        "description": "Building a robust fraud detection system that maintains consistent accuracy across different customer segments by iteratively correcting errors where the model performs poorly for specific demographic or geographic groups, ensuring reliable fraud prevention across all user types.",
        "goal": "Reliability"
      },
      {
        "description": "Developing a transparent hiring algorithm that provides clear evidence of equal performance across candidate demographics by using multi-accuracy boosting to systematically address group-specific prediction errors, enabling auditable fair recruitment processes.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Requires identifying and defining relevant subgroups or error regions, which may be challenging when group boundaries are unclear or overlapping."
      },
      {
        "description": "Could increase model complexity significantly as the boosting process adds multiple weak learners, potentially affecting interpretability and computational efficiency."
      },
      {
        "description": "May overfit to training data if very granular corrections are made, particularly when subgroups are small or the boosting process continues for too many iterations."
      },
      {
        "description": "Performance depends on the quality of subgroup identification, and may fail to achieve fairness if important demographic intersections are not properly captured."
      },
      {
        "description": "Convergence to equal accuracy across groups is not guaranteed, especially when there are fundamental differences in data distributions between groups."
      }
    ],
    "resources": [
      {
        "title": "mcboost: Multi-Calibration Boosting for R",
        "url": "https://joss.theoj.org/papers/10.21105/joss.03453",
        "source_type": "technical_paper",
        "authors": [
          "Bernd Bischl",
          "Susanne Dandl",
          "Christoph Kern",
          "Michael P. Kim",
          "Florian Pfisterer",
          "Matthew Sun"
        ],
        "publication_date": "2021-08-24"
      },
      {
        "title": "mlr-org/mcboost",
        "url": "https://github.com/mlr-org/mcboost",
        "source_type": "software_package"
      },
      {
        "title": "Multigroup Robustness",
        "url": "http://arxiv.org/abs/2405.00614",
        "source_type": "technical_paper",
        "authors": [
          "Lunjia Hu",
          "Charlotte Peale",
          "Judy Hanwen Shen"
        ],
        "publication_date": "2024-05-01"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      "adversarial-debiasing",
      "fair-adversarial-networks",
      "prejudice-remover-regulariser",
      "meta-fair-classifier",
      "exponentiated-gradient-reduction",
      "fair-transfer-learning",
      "adaptive-sensitive-reweighting"
    ]
  },
  {
    "slug": "equalised-odds-post-processing",
    "name": "Equalised Odds Post-Processing",
    "description": "A post-processing fairness technique based on Hardt et al.'s seminal work that adjusts classification thresholds after model training to achieve equal true positive rates and false positive rates across demographic groups. The method uses group-specific decision thresholds, potentially with randomisation, to satisfy the equalised odds constraint whilst preserving model utility. This approach enables fairness mitigation without retraining, making it applicable to existing deployed models or when training data access is restricted.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/testing",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Post-processing a criminal recidivism risk assessment model to ensure equal error rates across racial groups, using group-specific thresholds to achieve equal TPR and FPR whilst maintaining predictive accuracy for judicial decision support.",
        "goal": "Fairness"
      },
      {
        "description": "Adjusting a hiring algorithm's decision thresholds to ensure equal opportunities for qualified candidates across gender groups, providing transparent evidence that the screening process treats all demographics equitably.",
        "goal": "Transparency"
      },
      {
        "description": "Calibrating a medical diagnosis model's outputs to maintain equal detection rates across age groups, ensuring reliable performance monitoring and consistent healthcare delivery regardless of patient demographics.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "May require randomisation in decision-making, leading to inconsistent outcomes for similar individuals to achieve group-level fairness constraints."
      },
      {
        "description": "Post-processing can reduce overall model accuracy or confidence scores, particularly when group-specific ROC curves do not intersect favourably."
      },
      {
        "description": "Violates calibration properties of the original model, creating a trade-off between equalised odds and predictive rate parity."
      },
      {
        "description": "Limited to combinations of error rates that lie on the intersection of group-specific ROC curves, which may represent poor trade-offs."
      },
      {
        "description": "Requires access to sensitive attributes during deployment, which may not be available or legally permissible in all contexts."
      }
    ],
    "resources": [
      {
        "title": "Equality of Opportunity in Supervised Learning",
        "url": "https://arxiv.org/abs/1610.02413",
        "source_type": "technical_paper",
        "description": "Foundational paper by Hardt et al. introducing the equalised odds post-processing algorithm and mathematical framework for fairness constraints.",
        "authors": [
          "Moritz Hardt",
          "Eric Price",
          "Nathan Srebro"
        ],
        "publication_date": "2016-10-07"
      },
      {
        "title": "Equalized odds postprocessing under imperfect group information",
        "url": "https://arxiv.org/abs/1906.03284",
        "source_type": "technical_paper",
        "description": "Extension of Hardt et al.'s method examining robustness when protected attribute information is imperfect or noisy.",
        "authors": [
          "Pranjal Awasthi",
          "Matth\u00e4us Kleindessner",
          "Jamie Morgenstern"
        ],
        "publication_date": "2019-06-07"
      },
      {
        "title": "Fairlearn: ThresholdOptimizer",
        "url": "https://fairlearn.org/v0.10/api_reference/generated/fairlearn.postprocessing.ThresholdOptimizer.html",
        "source_type": "documentation",
        "description": "Microsoft's Fairlearn implementation of the Hardt et al. algorithm with API documentation and usage examples for equalised odds constraints."
      },
      {
        "title": "IBM AIF360",
        "url": "https://github.com/Trusted-AI/AIF360",
        "source_type": "software_package",
        "description": "Comprehensive fairness toolkit including EqualizedOddsPostprocessing implementation based on Hardt et al.'s original algorithm."
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "threshold-optimiser",
      "reject-option-classification",
      "calibration-with-equality-of-opportunity"
    ]
  },
  {
    "slug": "threshold-optimiser",
    "name": "Threshold Optimiser",
    "description": "Threshold Optimiser adjusts decision thresholds for different demographic groups after model training to satisfy specific fairness constraints. This post-processing technique optimises group-specific thresholds by analysing the probability distribution of model outputs, allowing practitioners to achieve fairness goals like demographic parity or equalised opportunity without modifying the underlying model. The optimiser finds optimal threshold values for each group that balance fairness requirements with overall model performance, making it particularly useful when fairness considerations arise after model deployment.",
    "assurance_goals": [
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/post-processing",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adjusting hiring decision thresholds in a recruitment system to ensure equal opportunity rates across gender and ethnicity groups, where the model outputs probability scores but different demographic groups require different thresholds to achieve equitable outcomes.",
        "goal": "Fairness"
      },
      {
        "description": "Optimising credit approval thresholds for different demographic groups in loan applications to satisfy regulatory requirements for equal treatment whilst maintaining acceptable default rates across all groups.",
        "goal": "Fairness"
      },
      {
        "description": "Calibrating medical diagnosis thresholds across age and gender groups to ensure diagnostic accuracy is maintained whilst preventing systematic over-diagnosis or under-diagnosis in specific populations.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Requires a held-out dataset with known group memberships to determine optimal thresholds for each demographic group."
      },
      {
        "description": "Threshold values may need recalibration when input data distributions shift or model performance changes over time."
      },
      {
        "description": "Using different decision thresholds per group can raise legal or ethical concerns in deployment contexts where equal treatment is mandated."
      },
      {
        "description": "Performance depends on the quality and representativeness of the calibration dataset for each demographic group."
      },
      {
        "description": "May lead to reduced overall accuracy as the optimisation trades off individual accuracy for group fairness."
      }
    ],
    "resources": [
      {
        "title": "Group-Aware Threshold Adaptation for Fair Classification",
        "url": "https://arxiv.org/abs/2111.04271",
        "source_type": "technical_paper",
        "authors": [
          "Jang, Taeuk",
          "Shi, Pengyi",
          "Wang, Xiaoqian"
        ],
        "publication_date": "2021-11-08",
        "description": "Introduces a novel post-processing method for learning adaptive classification thresholds for each demographic group by optimising confusion matrices estimated from model probability distributions."
      },
      {
        "title": "Equality of Opportunity in Supervised Learning",
        "url": "https://arxiv.org/abs/1610.02413",
        "source_type": "technical_paper",
        "authors": [
          "Hardt, Moritz",
          "Price, Eric",
          "Srebro, Nathan"
        ],
        "publication_date": "2016-10-07",
        "description": "Foundational work introducing threshold optimisation techniques to achieve equalized opportunity and demographic parity in supervised learning."
      },
      {
        "title": "AIF360: A comprehensive set of fairness metrics and algorithms",
        "url": "https://github.com/Trusted-AI/AIF360",
        "source_type": "software_package",
        "description": "Open-source library containing threshold optimisation implementations for various fairness constraints including equalized odds and demographic parity."
      },
      {
        "title": "Fairlearn: A toolkit for assessing and improving fairness in AI",
        "url": "https://github.com/fairlearn/fairlearn",
        "source_type": "software_package",
        "description": "Python library providing threshold optimisation methods and post-processing algorithms for achieving fairness in machine learning models."
      },
      {
        "title": "HolisticAI: Randomized Threshold Optimizer",
        "url": "https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/postprocessing/bc_ml_debiaser_rto.html",
        "source_type": "documentation",
        "description": "Documentation for the Randomized Threshold Optimizer implementation that achieves statistical parity through group-aware threshold adjustment with randomization."
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      "equalised-odds-post-processing",
      "reject-option-classification",
      "calibration-with-equality-of-opportunity"
    ]
  },
  {
    "slug": "reject-option-classification",
    "name": "Reject Option Classification",
    "description": "A post-processing fairness technique that modifies predictions in regions of high uncertainty to favour disadvantaged groups and achieve fairness objectives. The method identifies a 'rejection region' where the model's confidence is low (typically near the decision boundary) and reassigns predictions within this region to benefit underrepresented groups. By leveraging model uncertainty, this approach can improve fairness metrics like demographic parity or equalised odds whilst minimising changes to confident predictions, thus preserving overall accuracy for cases where the model is certain.",
    "assurance_goals": [
      "Fairness",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/prediction-probabilities",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/post-processing",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adjusting hiring algorithm predictions in the uncertainty region where candidate scores are close to the threshold, reassigning borderline cases to ensure equal selection rates across gender and ethnicity groups whilst maintaining decisions for clearly qualified or unqualified candidates.",
        "goal": "Fairness"
      },
      {
        "description": "Improving reliability of loan approval systems by identifying applications where the model is uncertain and adjusting these edge cases to ensure consistent approval rates across demographic groups, reducing the risk of systematic discrimination in borderline creditworthiness assessments.",
        "goal": "Reliability"
      },
      {
        "description": "Creating transparent bail decision systems that clearly document which predictions fall within the rejection region and how adjustments are made, providing courts with explainable fairness interventions that show exactly when and why decisions were modified for equity.",
        "goal": "Transparency"
      }
    ],
    "limitations": [
      {
        "description": "Requires models that provide reliable uncertainty estimates or probability scores, limiting applicability to deterministic classifiers without confidence outputs."
      },
      {
        "description": "Selection of the rejection region threshold is subjective and requires careful tuning to balance fairness improvements with accuracy preservation."
      },
      {
        "description": "May reject too many instances if tuned conservatively, potentially affecting a large portion of predictions and reducing the model's practical utility."
      },
      {
        "description": "Cannot address bias in confident predictions outside the rejection region, limiting effectiveness when discrimination occurs in high-certainty cases."
      },
      {
        "description": "Performance depends on the quality of uncertainty estimates, which may be poorly calibrated in some models, leading to inappropriate rejection regions."
      }
    ],
    "resources": [
      {
        "title": "Machine Learning with a Reject Option: A survey",
        "url": "https://www.semanticscholar.org/paper/24864a7f899718477c04ede9c0bea906c5dc2667",
        "source_type": "documentation",
        "authors": [
          "Kilian Hendrickx",
          "Lorenzo Perini",
          "Dries Van der Plas",
          "Wannes Meert",
          "Jesse Davis"
        ]
      },
      {
        "title": "aif360.algorithms.postprocessing.RejectOptionClassification ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html",
        "source_type": "documentation"
      },
      {
        "title": "Survey on Leveraging Uncertainty Estimation Towards Trustworthy Deep Neural Networks: The Case of Reject Option and Post-training Processing",
        "url": "https://www.semanticscholar.org/paper/e939c6ac58e08b539ae8a7dc54216bceb775b085",
        "source_type": "documentation",
        "authors": [
          "M. Hasan",
          "Moloud Abdar",
          "A. Khosravi",
          "U. Aickelin",
          "Pietro Lio'",
          "Ibrahim Hossain",
          "Ashikur Rahman",
          "Saeid Nahavandi"
        ]
      }
    ],
    "complexity_rating": 3,
    "computational_cost_rating": 2,
    "related_techniques": [
      "equalised-odds-post-processing",
      "threshold-optimiser",
      "calibration-with-equality-of-opportunity"
    ]
  },
  {
    "slug": "calibration-with-equality-of-opportunity",
    "name": "Calibration with Equality of Opportunity",
    "description": "A post-processing fairness technique that adjusts model predictions to achieve equal true positive rates across protected groups whilst maintaining calibration within each group. The method addresses fairness by ensuring that qualified individuals from different demographic groups have equal chances of receiving positive predictions, whilst preserving the meaning of probability scores within each group. This technique attempts to balance the competing objectives of group fairness and accurate probability estimation.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/calibration-set",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/testing",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Adjusting a loan approval model to ensure that qualified applicants from different ethnic backgrounds have equal approval rates, whilst maintaining that a 70% predicted repayment probability means the same thing for each ethnic group in practice.",
        "goal": "Fairness"
      },
      {
        "description": "Post-processing a university admissions algorithm to equalise acceptance rates for qualified students across gender groups, whilst ensuring the predicted success scores remain well-calibrated within each gender to support transparent decision-making.",
        "goal": "Transparency"
      },
      {
        "description": "Calibrating a medical diagnosis model to maintain equal detection rates for a disease across different age groups whilst preserving the reliability of risk scores, ensuring that a 30% risk prediction accurately reflects actual disease occurrence within each age group.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Fundamental mathematical incompatibility exists between perfect calibration and exact equality of opportunity, except in highly constrained special cases."
      },
      {
        "description": "May reduce overall model accuracy or calibration when forcing equal true positive rates across groups with genuinely different base rates."
      },
      {
        "description": "Requires access to sensitive attributes during post-processing, which may not be available or legally permissible in all contexts."
      },
      {
        "description": "The technique only addresses one aspect of fairness (true positive rates) and may allow disparities in false positive rates between groups."
      },
      {
        "description": "Post-processing approaches cannot address biases inherent in the training data or model architecture, only adjust final predictions."
      }
    ],
    "resources": [
      {
        "title": "On Fairness and Calibration",
        "url": "https://arxiv.org/abs/1709.02012",
        "source_type": "technical_paper",
        "description": "Foundational paper demonstrating the mathematical tension between calibration and equalised odds fairness constraints.",
        "authors": [
          "Geoff Pleiss",
          "Manish Raghavan",
          "Felix Wu",
          "Jon Kleinberg",
          "Kilian Q. Weinberger"
        ],
        "publication_date": "2017-09-06"
      },
      {
        "title": "equalized_odds_and_calibration",
        "url": "https://github.com/gpleiss/equalized_odds_and_calibration",
        "source_type": "software_package",
        "description": "Python implementation of post-processing methods for achieving calibration with equality of opportunity constraints."
      },
      {
        "title": "Equality of Opportunity in Supervised Learning",
        "url": "https://arxiv.org/abs/1610.02413",
        "source_type": "technical_paper",
        "description": "Original paper introducing the equality of opportunity fairness criterion and post-processing algorithms.",
        "authors": [
          "Moritz Hardt",
          "Eric Price",
          "Nathan Srebro"
        ],
        "publication_date": "2016-10-07"
      },
      {
        "title": "Fairlearn: Postprocessing Methods",
        "url": "https://fairlearn.org/v0.10/user_guide/mitigation/postprocessing.html",
        "source_type": "documentation",
        "description": "Documentation for implementing threshold optimisation and calibration methods to achieve fairness constraints."
      }
    ],
    "complexity_rating": 4,
    "computational_cost_rating": 2,
    "related_techniques": [
      "equalised-odds-post-processing",
      "threshold-optimiser",
      "reject-option-classification"
    ]
  },
  {
    "slug": "equal-opportunity-difference",
    "name": "Equal Opportunity Difference",
    "description": "A fairness metric that quantifies discrimination by measuring the difference in true positive rates (recall) between protected and privileged groups. Based on Hardt et al.'s equality of opportunity framework, this metric computes the maximum difference in TPR across demographic groups, with a value of 0 indicating perfect fairness. The technique provides a mathematical measure of whether qualified individuals from different groups have equal chances of receiving positive predictions.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/testing",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating a loan approval model to ensure that qualified applicants from different ethnic backgrounds have equal approval rates, measuring whether a 90% TPR for qualified white applicants is matched by similar rates for qualified minority applicants.",
        "goal": "Fairness"
      },
      {
        "description": "Auditing a medical diagnosis system to verify that patients with a particular condition are detected at equal rates across age groups, providing transparent evidence that diagnostic accuracy is consistent regardless of patient demographics.",
        "goal": "Transparency"
      },
      {
        "description": "Monitoring a university admissions algorithm in production to ensure that qualified students from different socioeconomic backgrounds have equal acceptance rates, validating the reliability of the fairness properties over time.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Only considers true positive rates and ignores false positive rate disparities, potentially allowing discrimination in the form of unequal false alarm rates between groups."
      },
      {
        "description": "Requires accurate ground truth labels for the positive class, which may be biased or unavailable in some domains."
      },
      {
        "description": "Improving TPR for one group might increase FPR for that group, creating trade-offs between different types of fairness."
      },
      {
        "description": "Does not account for intersectional fairness across multiple protected attributes simultaneously."
      },
      {
        "description": "May not capture fairness concerns for the negative class or individuals who should not receive positive predictions."
      }
    ],
    "resources": [
      {
        "title": "aif360.sklearn.metrics.equal_opportunity_difference \u2014 aif360 0.6.1 ...",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.metrics.equal_opportunity_difference.html",
        "source_type": "documentation"
      },
      {
        "title": "Welcome to TransparentAI's documentation! \u2014 TransparentAI 0.1.0 ...",
        "url": "https://transparentai.readthedocs.io/en/latest/",
        "source_type": "documentation"
      },
      {
        "title": "lale.lib.aif360.util module \u2014 LALE 0.9.0-dev documentation",
        "url": "https://lale.readthedocs.io/en/latest/modules/lale.lib.aif360.util.html",
        "source_type": "documentation"
      },
      {
        "title": "IBM/bias-mitigation-of-machine-learning-models-using-aif360",
        "url": "https://github.com/IBM/bias-mitigation-of-machine-learning-models-using-aif360",
        "source_type": "software_package"
      },
      {
        "title": "aif360.algorithms.postprocessing.RejectOptionClassification ...",
        "url": "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.RejectOptionClassification.html",
        "source_type": "documentation"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      "demographic-parity-assessment",
      "average-odds-difference"
    ]
  },
  {
    "slug": "average-odds-difference",
    "name": "Average Odds Difference",
    "description": "Average Odds Difference measures fairness by calculating the average difference in both false positive rates and true positive rates between different demographic groups. This metric captures how consistently a model performs across groups for both positive and negative predictions. A value of 0 indicates perfect fairness under the equalized odds criterion, while larger absolute values indicate greater disparities in model performance between groups.",
    "assurance_goals": [
      "Fairness",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/testing",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating criminal risk assessment tools to ensure equal false positive rates (wrongly flagging low-risk individuals as high-risk) and true positive rates (correctly identifying high-risk individuals) across racial and ethnic groups.",
        "goal": "Fairness"
      },
      {
        "description": "Auditing hiring algorithms to verify that both the rate of correctly identifying qualified candidates and the rate of incorrectly rejecting qualified candidates remain consistent across gender and demographic groups.",
        "goal": "Fairness"
      },
      {
        "description": "Monitoring loan approval systems to ensure reliable performance by checking that both approval rates for creditworthy applicants and rejection rates for non-creditworthy applicants are consistent across protected demographic categories.",
        "goal": "Reliability"
      },
      {
        "description": "Testing medical diagnostic models to validate that diagnostic accuracy (both correctly identifying disease and correctly ruling out disease) remains consistent across patient demographics, ensuring reliable healthcare delivery.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Averaging effect can mask important disparities when false positive and true positive rate differences compensate for each other, potentially hiding significant bias in one direction."
      },
      {
        "description": "Requires access to ground truth labels and sensitive attribute information, which may not be available in all deployment scenarios or may be subject to privacy constraints."
      },
      {
        "description": "Does not account for base rate differences between groups, meaning equal error rates may not translate to equal treatment when group prevalences differ significantly."
      },
      {
        "description": "Focuses solely on prediction accuracy disparities without considering whether the underlying decision-making process or feature selection introduces systematic bias against certain groups."
      },
      {
        "description": "May encourage optimization for fairness metrics at the expense of overall model performance, potentially reducing utility for the primary prediction task."
      }
    ],
    "resources": [
      {
        "title": "Equality of Opportunity in Supervised Learning",
        "url": "https://arxiv.org/abs/1610.02413",
        "source_type": "technical_paper",
        "description": "Foundational paper introducing equalized odds and related fairness metrics including average odds difference",
        "authors": [
          "Hardt, Moritz",
          "Price, Eric",
          "Srebro, Nathan"
        ],
        "publication_date": "2016-10-07"
      },
      {
        "title": "FairBalance: How to Achieve Equalized Odds With Data Pre-processing",
        "url": "https://arxiv.org/abs/2107.08310",
        "source_type": "technical_paper",
        "description": "Research on achieving equalized odds through data preprocessing techniques with practical implementation guidance",
        "authors": [
          "Yu, Zhe",
          "Chakraborty, Joymallya",
          "Menzies, Tim"
        ],
        "publication_date": "2021-07-17"
      },
      {
        "title": "AIF360: Average Odds Difference Documentation",
        "url": "https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.metrics.average_odds_difference.html",
        "source_type": "documentation",
        "description": "IBM AIF360 toolkit implementation and documentation for computing average odds difference metrics"
      },
      {
        "title": "Fairlearn: A toolkit for assessing and improving fairness in machine learning",
        "url": "https://github.com/fairlearn/fairlearn",
        "source_type": "software_package",
        "description": "Microsoft's comprehensive fairness toolkit with implementations of various fairness metrics including average odds difference"
      }
    ],
    "complexity_rating": 2,
    "computational_cost_rating": 1,
    "related_techniques": [
      "demographic-parity-assessment",
      "equal-opportunity-difference"
    ]
  },
  {
    "slug": "path-specific-counterfactual-fairness-assessment",
    "name": "Path-Specific Counterfactual Fairness Assessment",
    "description": "A causal fairness evaluation technique that assesses algorithmic discrimination by examining specific causal pathways in a model's decision-making process. Unlike general counterfactual fairness, this approach enables practitioners to identify and intervene on particular causal paths that may introduce bias whilst preserving other legitimate pathways. The method uses causal graphs to distinguish between direct discrimination (through protected attributes) and indirect discrimination (through seemingly neutral factors that correlate with protected attributes), allowing for more nuanced fairness assessments in complex causal settings.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/causal",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/causal-graph",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/causal-analysis",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/causal",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "technique-type/metric"
    ],
    "example_use_cases": [
      {
        "description": "Evaluating hiring algorithms by identifying which causal pathways from education and experience legitimately affect job performance versus those that introduce gender or racial bias, enabling targeted interventions that preserve merit-based selection whilst eliminating discriminatory pathways.",
        "goal": "Fairness"
      },
      {
        "description": "Analysing loan approval models to provide transparent evidence of which factors legitimately influence creditworthiness versus those that create indirect discrimination, enabling clear explanations to regulators about causal mechanisms underlying fair lending decisions.",
        "goal": "Transparency"
      },
      {
        "description": "Assessing medical diagnosis systems to ensure reliable performance by distinguishing between clinically relevant causal pathways (symptoms to diagnosis) and potentially biased pathways (demographics to diagnosis), maintaining diagnostic accuracy whilst preventing healthcare disparities.",
        "goal": "Reliability"
      }
    ],
    "limitations": [
      {
        "description": "Requires identifying which causal pathways are 'allowable' and which are not\u2014a subjective decision; analyzing specific paths adds complexity to the causal model and the fairness criterion."
      }
    ],
    "resources": [
      {
        "title": "Path-Specific Counterfactual Fairness via Dividend Correction",
        "url": "https://www.semanticscholar.org/paper/197367ee337e8838fd2ef1a887101ddc84eb0612",
        "source_type": "technical_paper",
        "authors": [
          "Daisuke Hatano",
          "Satoshi Hara",
          "Hiromi Arai"
        ]
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      "counterfactual-fairness-assessment"
    ]
  },
  {
    "slug": "bayesian-fairness-regularization",
    "name": "Bayesian Fairness Regularization",
    "description": "Bayesian Fairness Regularization incorporates fairness constraints into machine learning models through Bayesian methods, treating fairness as a prior distribution or regularization term. This approach includes techniques like Fair Bayesian Optimization that use constrained optimization to tune model hyperparameters whilst enforcing fairness constraints, and methods that add regularization terms to objective functions to penalize discriminatory predictions. The technique allows for probabilistic interpretation of fairness constraints and can account for uncertainty in both model parameters and fairness requirements.",
    "assurance_goals": [
      "Fairness",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "example_use_cases": [
      {
        "description": "Using Fair Bayesian Optimization to tune hyperparameters of credit risk models, automatically balancing predictive accuracy with fairness constraints across different demographic groups whilst accounting for uncertainty in both model performance and fairness requirements.",
        "goal": "Fairness"
      },
      {
        "description": "Implementing Bayesian neural networks with fairness-aware priors for hiring recommendation systems, where uncertainty in fairness constraints is modeled probabilistically to ensure robust fair decision-making across different candidate populations.",
        "goal": "Fairness"
      },
      {
        "description": "Applying Bayesian regularization techniques to medical diagnosis models to ensure reliable performance across patient demographics, using probabilistic constraints to maintain consistent diagnostic accuracy whilst preventing algorithmic bias in healthcare delivery.",
        "goal": "Reliability"
      },
      {
        "description": "Developing insurance premium calculation models using Bayesian fairness regularization to ensure actuarially sound pricing that meets regulatory fairness requirements, with probabilistic modeling of both risk assessment accuracy and demographic equity.",
        "goal": "Fairness"
      }
    ],
    "limitations": [
      {
        "description": "Prior selection challenges make it difficult to specify appropriate prior distributions for fairness constraints, requiring domain expertise and potentially leading to suboptimal or biased outcomes if priors are poorly chosen."
      },
      {
        "description": "Computational complexity increases significantly due to Bayesian inference requirements, including sampling methods, variational inference, or optimization over probability distributions, making the approach less scalable for large datasets."
      },
      {
        "description": "Sensitivity to hyperparameters affects both the Bayesian inference process and fairness regularization terms, requiring careful tuning of multiple parameters that control the trade-off between accuracy, fairness, and computational efficiency."
      },
      {
        "description": "Convergence and stability issues may arise in Bayesian optimization with fairness constraints, particularly when fairness objectives conflict with performance objectives or when the constraint space becomes highly complex."
      },
      {
        "description": "Limited theoretical understanding exists for the interaction between Bayesian uncertainty quantification and fairness constraints, making it challenging to provide guarantees about both predictive performance and fairness under uncertainty."
      }
    ],
    "resources": [
      {
        "title": "Bayesian fairness",
        "url": "https://arxiv.org/abs/1706.00119",
        "source_type": "technical_paper",
        "description": "Foundational paper introducing Bayesian approaches to fairness under parameter uncertainty, demonstrating how Bayesian perspectives lead to fair decision rules",
        "authors": [
          "Dimitrakakis, Christos",
          "Liu, Yang",
          "Parkes, David",
          "Radanovic, Goran"
        ],
        "publication_date": "2017-05-31"
      },
      {
        "title": "Fair Bayesian Optimization",
        "url": "https://arxiv.org/abs/2006.05109",
        "source_type": "technical_paper",
        "description": "Constrained Bayesian optimization framework for optimizing ML model performance while enforcing fairness constraints through hyperparameter tuning",
        "authors": [
          "Perrone, Valerio",
          "Donini, Michele",
          "Zafar, Muhammad Bilal",
          "Schmucker, Robin",
          "Kenthapadi, Krishnaram",
          "Archambeau, C\u00e9dric"
        ],
        "publication_date": "2020-06-09"
      },
      {
        "title": "Fair Gaussian Processes",
        "url": "https://github.com/ztanml/fgp",
        "source_type": "software_package",
        "description": "MATLAB implementation of Fair Gaussian Processes with multiple fairness criteria support including statistical parity, equality of opportunity, and equalized odds"
      },
      {
        "title": "Fairness-Aware Classifier with Prejudice Remover Regularizer",
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-33486-3_3",
        "source_type": "technical_paper",
        "description": "Seminal paper introducing regularization-based approach to fairness in probabilistic discriminative models with mathematical framework for fairness constraints",
        "authors": [
          "Kamishima, Toshihiro",
          "Akaho, Shotaro",
          "Asoh, Hideki",
          "Sakuma, Jun"
        ],
        "publication_date": "2012-09-24"
      }
    ],
    "complexity_rating": 5,
    "computational_cost_rating": 4,
    "related_techniques": [
      "sensitivity-analysis-for-fairness",
      "fairness-gan",
      "attribute-removal-fairness-through-unawareness"
    ]
  }
]